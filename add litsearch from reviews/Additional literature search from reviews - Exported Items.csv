Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body
RP454GDY,journalArticle,2020,"Vaccaro, Kristen; Sandvig, Christian; Karahalios, Karrie","""At the End of the Day Facebook Does What ItWants"": How Users Experience Contesting Algorithmic Content Moderation",Proc. ACM Hum.-Comput. Interact.,,,10.1145/3415238,https://doi.org/10.1145/3415238,"Interest has grown in designing algorithmic decision making systems for contestability. In this work, we study how users experience contesting unfavorable social media content moderation decisions. A large-scale online experiment tests whether different forms of appeals can improve users' experiences of automated decision making. We study the impact on users' perceptions of the Fairness, Accountability, and Trustworthiness of algorithmic decisions, as well as their feelings of Control (FACT). Surprisingly, we find that none of the appeal designs improve FACT perceptions compared to a no appeal baseline. We qualitatively analyze how users write appeals, and find that they contest the decision itself, but also more fundamental issues like the goal of moderating content, the idea of automation, and the inconsistency of the system as a whole. We conclude with suggestions for -- as well as a discussion of the challenges of -- designing for contestability.",2020,2025-08-28 12:19:42,2025-08-28 12:19:42,2025-08-28 12:19:42,167:1–167:22,,CSCW2,4,,,"""At the End of the Day Facebook Does What ItWants""",,,,,,,,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PENGMPMT,journalArticle,2022,"Klenk, Michael","(Online) manipulation: sometimes hidden, always careless",Review of Social Economy,,0034-6764,10.1080/00346764.2021.1894350,https://doi.org/10.1080/00346764.2021.1894350,"Ever-increasing numbers of human interactions with intelligent software agents, online and offline, and their increasing ability to influence humans have prompted a surge in attention toward the concept of (online) manipulation. Several scholars have argued that manipulative influence is always hidden. But manipulation is sometimes overt, and when this is acknowledged the distinction between manipulation and other forms of social influence becomes problematic. Therefore, we need a better conceptualisation of manipulation that allows it to be overt and yet clearly distinct from related concepts of social influence. I argue that manipulation is careless influence, show how this account helps to alleviate the shortcomings of the hidden influence view of manipulation, and derive implications for digital ethics.",2022-01-02,2025-08-28 13:02:20,2025-08-28 13:02:20,2025-08-28 13:02:20,85-105,,1,80,,,(Online) manipulation,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/00346764.2021.1894350,,"C:\Users\Admin\Zotero\storage\XDUE5LIU\Klenk - 2022 - (Online) manipulation sometimes hidden, always careless.pdf",,,autonomy; digital ethics; Manipulation; social expectations; social influence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W73ZUEJC,journalArticle,2020,"Beattie, Austin; Edwards, Autumn P.; Edwards, Chad",A Bot and a Smile: Interpersonal Impressions of Chatbots and Humans Using Emoji in Computer-mediated Communication,Communication Studies,,1051-0974,10.1080/10510974.2020.1725082,https://doi.org/10.1080/10510974.2020.1725082,"Artificially intelligent (AI) agents increasingly occupy roles once served by humans in computer-mediated communication (CMC). Technological affordances like emoji give interactants (humans or bots) the ability to partially overcome the limited nonverbal information in CMC. However, despite the growth of chatbots as conversational partners, few CMC and human-machine communication (HMC) studies have explored how bots’ use of emoji impact perceptions of communicator quality. This study examined the relationship between emoji use and observers’ impressions of interpersonal attractiveness, CMC competence, and source credibility; and whether impressions formed of human versus chatbot message sources were different. Results demonstrated that participants rated emoji-using chatbot message sources similarly to human message sources, and both humans and bots are significantly more socially attractive, CMC competent, and credible when compared to verbal-only message senders. Results are discussed with respect to the CASA paradigm and the human-to-human interaction script framework. Read the transcript Watch the video on Vimeo",2020,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,409-427,,3,71,,Communication Studies,A Bot and a Smile,,,,,,,,,,,,,,,,,,AI; attraction; CASA; Chatbot; competence; credibility; Emoji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VNKHJ7ZX,conferencePaper,2022,"Lees, Alyssa; Tran, Vinh Q.; Tay, Yi; Sorensen, Jeffrey; Gupta, Jai; Metzler, Donald; Vasserman, Lucy",A New Generation of Perspective API: Efficient Multilingual Character-level Transformers,Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,978-1-4503-9385-0,,10.1145/3534678.3539147,https://dl.acm.org/doi/10.1145/3534678.3539147,"On the world wide web, toxic content detectors are a crucial line of defense against potentially hateful and offensive messages. As such, building highly effective classifiers that enable a safer internet is an important research area. Moreover, the web is a highly multilingual, cross-cultural community that develops its own lingo over time. As such, it is crucial to develop models that are effective across a diverse range of languages, usages, and styles. In this paper, we present the fundamentals behind the next version of the Perspective API from Google Jigsaw. At the heart of the approach is a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks. We demonstrate that by forgoing static vocabularies, we gain flexibility across a variety of settings. We additionally outline the techniques employed to make such a byte-level model efficient and feasible for productionization. Through extensive experiments on multilingual toxic comment classification benchmarks derived from real API traffic and evaluation on an array of code-switching, covert toxicity, emoji-based hate, human-readable obfuscation, distribution shift, and bias evaluation settings, we show that our proposed approach outperforms strong baselines. Finally, we present our findings from deploying this system in production.",2022,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,3197–3207,,,,,,A New Generation of Perspective API,KDD '22,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,,,,,,,,,,,,,,,
7ATZ7WVZ,journalArticle,2023,"White, Jules; Fu, Quchen; Hays, Sam; Sandborn, Michael; Olea, Carlos; Gilbert, Henry; Elnashar, Ashraf; Spencer-Smith, Jesse; Schmidt, Douglas C.",A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT,,,,10.48550/arXiv.2302.11382,http://arxiv.org/abs/2302.11382,"Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.",2023,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,,,,,,,,,,,,,,,,,,,,,Publisher: arXiv,,,,Computer Science - Artificial Intelligence; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALA5EU3S,journalArticle,2018,"Waddell, T. Franklin",A Robot Wrote This?: How perceived machine authorship affects news credibility,Digital Journalism,,2167-0811,10.1080/21670811.2017.1384319,https://doi.org/10.1080/21670811.2017.1384319,"As more news articles are written via collaboration between journalists and algorithms, questions have arisen regarding how automation influences the way that news is processed and evaluated by audiences. Informed by expectancy violation theory and the MAIN model, two experimental studies were conducted that examined the effect of purported machine authorship on perceptions of news credibility. Study One (N = 129) revealed that news attributed to a machine is perceived as less credible than news attributed to a human journalist. Study Two (N = 182) also observed negative effects of machine authorship through the indirect pathway of source anthropomorphism and negative expectancy violations, with evidence of moderation by prior recall of robotics also observed. The theoretical and practical implications of these findings are discussed.",2018,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,236-255,,2,6,,Digital Journalism,A Robot Wrote This?,,,,,,,,,,,,,,,,,,automated journalism; expectancy violation theory; main model; social robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JL73RUNR,conferencePaper,2023,"Oldewage, Elre T.; Bronskill, John; Turner, Richard E.",Adversarial Attacks are a Surprisingly Strong Baseline for Poisoning Few-Shot Meta-Learners,Proceedings on,,,,https://proceedings.mlr.press/v187/oldewage23a.html,"This paper examines the robustness of deployed few-shot meta-learning systems when they are fed an imperceptibly perturbed few-shot dataset. We attack amortized meta-learners, which allows us to craft colluding sets of inputs that are tailored to fool the system’s learning algorithm when used as training data. Jointly crafted adversarial inputs might be expected to synergistically manipulate a classifier, allowing for very strong data-poisoning attacks that would be hard to detect. We show that in a white box setting, these attacks are very successful and can cause the target model’s predictions to become worse than chance. However, in opposition to the well-known transferability of adversarial examples in general, the colluding sets do not transfer well to different classifiers. We explore two hypotheses to explain this: ’overfitting’ by the attack, and mismatch between the model on which the attack is generated and that to which the attack is transferred. Regardless of the mitigation strategies suggested by these hypotheses, the colluding inputs transfer no better than adversarial inputs that are generated independently in the usual way.",2023,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,27-40,,,,,,,,,,,PMLR,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings on,,,,,,,,,,,,,,,
4BHK77G5,journalArticle,2023,"Strümke, Inga; Slavkovik, Marija; Stachl, Clemens",Against Algorithmic Exploitation of Human Vulnerabilities,,,,10.48550/arXiv.2301.04993,http://arxiv.org/abs/2301.04993,"Decisions such as which movie to watch next, which song to listen to, or which product to buy online, are increasingly influenced by recommender systems and user models that incorporate information on users' past behaviours, preferences, and digitally created content. Machine learning models that enable recommendations and that are trained on user data may unintentionally leverage information on human characteristics that are considered vulnerabilities, such as depression, young age, or gambling addiction. The use of algorithmic decisions based on latent vulnerable state representations could be considered manipulative and could have a deteriorating impact on the condition of vulnerable individuals. In this paper, we are concerned with the problem of machine learning models inadvertently modelling vulnerabilities, and want to raise awareness for this issue to be considered in legislation and AI ethics. Hence, we define and describe common vulnerabilities, and illustrate cases where they are likely to play a role in algorithmic decision-making. We propose a set of requirements for methods to detect the potential for vulnerability modelling, detect whether vulnerable groups are treated differently by a model, and detect whether a model has created an internal representation of vulnerability. We conclude that explainable artificial intelligence methods may be necessary for detecting vulnerability exploitation by machine learning-based recommendation systems.",2023,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,,,,,,,,,,,,,,,,,,,,,Publisher: arXiv,,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GWP5U9BC,journalArticle,2023,"Park, Peter S.; Goldstein, Simon; O'Gara, Aidan; Chen, Michael; Hendrycks, Dan","AI Deception: A Survey of Examples, Risks, and Potential Solutions",,,,10.48550/arXiv.2308.14752,http://arxiv.org/abs/2308.14752,"This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.",2023,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,,,,,,,AI Deception,,,,,,,,,,,,,,Publisher: arXiv,,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RNRZWZGD,journalArticle,2023,"Spitale, Giovanni; Biller-Andorno, Nikola; Germani, Federico",AI model GPT-3 (dis)informs us better than humans,Science Advances,,,10.1126/sciadv.adh1850,https://www.science.org/doi/10.1126/sciadv.adh1850,"Artificial intelligence (AI) is changing the way we create and evaluate information, and this is happening during an infodemic, which has been having marked effects on global health. Here, we evaluate whether recruited individuals can distinguish disinformation from accurate information, structured in the form of tweets, and determine whether a tweet is organic or synthetic, i.e., whether it has been written by a Twitter user or by the AI model GPT-3. The results of our preregistered study, including 697 participants, show that GPT-3 is a double-edge sword: In comparison with humans, it can produce accurate information that is easier to understand, but it can also produce more compelling disinformation. We also show that humans cannot distinguish between tweets generated by GPT-3 and written by real Twitter users. Starting from our results, we reflect on the dangers of AI for disinformation and on how information campaigns can be improved to benefit global health.",2023,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,eadh1850,,26,9,,Science Advances,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S3UUP4B3,journalArticle,2018,"Irving, Geoffrey; Christiano, Paul; Amodei, Dario",AI safety via debate,,,,10.48550/arXiv.1805.00899,http://arxiv.org/abs/1805.00899,"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",2018,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,,,,,,,,,,,,,,,,,,,,,Publisher: arXiv,,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
57DM7WE6,journalArticle,2023,"Schwitzgebel, Eric",AI systems must not confuse users about their sentience or moral status,Patterns,,2666-3899,10.1016/j.patter.2023.100818,https://www.cell.com/patterns/abstract/S2666-3899(23)00187-3,"One relatively neglected challenge in ethical artificial intelligence (AI) design is ensuring that AI systems invite a degree of emotional and moral concern appropriate to their moral standing. Although experts generally agree that current AI chatbots are not sentient to any meaningful degree, these systems can already provoke substantial attachment and sometimes intense emotional responses in users. Furthermore, rapid advances in AI technology could soon create AIs of plausibly debatable sentience and moral standing, at least by some relevant definitions. Morally confusing AI systems create unfortunate ethical dilemmas for the owners and users of those systems, since it is unclear how those systems ethically should be treated. I argue here that, to the extent possible, we should avoid creating AI systems whose sentience or moral standing is unclear and that AI systems should be designed so as to invite appropriate emotional responses in ordinary users.",2023-08-11,2025-08-28 12:04:56,2025-08-28 12:05:28,2025-08-28 12:04:56,,,8,4,,PATTER,,,,,,,,English,,,,,www.cell.com,,Publisher: Elsevier,,C:\Users\Admin\Zotero\storage\UCAQ74LV\Schwitzgebel - 2023 - AI systems must not confuse users about their sentience or moral status.pdf,,,DSML 1: Concept: Basic principles of a new data science output observed and reported,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X4BTZSZA,journalArticle,2025,"Bai, Hui; Voelkel, Jan; Muldowney, Shane; Eichstaedt, Johannes; Willer, Robb",AI-Generated Messages Can Be Used to Persuade Humans on Policy Issues,,,,10.31219/osf.io/stakv_v5,https://osf.io/stakv_v5,"The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets in the U.S. and globally. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans’ political attitudes. Across three pre-registered experiments (total N = 4,829), we find consistent evidence that assigning participants to read persuasive messages generated by LLMs can lead to attitude change across a range of policies, including highly polarized policies, such as an assault weapons ban, a carbon tax, and a paid parental-leave program. Overall, we found LLM-generated messages were similarly effective in influencing policy attitudes as were messages crafted by lay humans. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages cheaply and at massive scale.",2025,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,,,,,,,,,,,,,,en-us,,,,,,,Publisher: OSF,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RYJNYJ2W,conferencePaper,2019,"Jakesch, Maurice; French, Megan; Ma, Xiao; Hancock, Jeffrey T.; Naaman, Mor",AI-Mediated Communication: How the Perception that Profile Text was Written by AI Affects Trustworthiness,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,978-1-4503-5970-2,,10.1145/3290605.3300469,https://doi.org/10.1145/3290605.3300469,"We are entering an era of AI-Mediated Communication (AI-MC) where interpersonal communication is not only mediated by technology, but is optimized, augmented, or generated by artificial intelligence. Our study takes a first look at the potential impact of AI-MC on online self-presentation. In three experiments we test whether people find Airbnb hosts less trustworthy if they believe their profiles have been written by AI. We observe a new phenomenon that we term the Replicant Effect: Only when participants thought they saw a mixed set of AI- and human-written profiles, they mistrusted hosts whose profiles were labeled as or suspected to be written by AI. Our findings have implications for the design of systems that involve AI technologies in online self-presentation and chart a direction for future work that may upend or augment key aspects of Computer-Mediated Communication theory.",2019,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,1–13,,,,,,AI-Mediated Communication,CHI '19,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,,,,,,,,,,,,,,,
CAYVEIIR,thesis,2019,"Lin, A. Y.",Al in the news? Effect of modality on perceived news credibility of robot journalism,,,,,,"This research conducted an experiment to investigate how news consumers (N = 214) perceive automated news regarding the authorship and modality. With the development of AI technology, news companies not only adopt the algorithms to automatically produce news, but also apply AI to create news videos. Since AI news video is a new format of automated journalism, it is therefore needed to investigate if it is perceived credible by the public. Previous research tried to understand the automated news credibility by manipulating the byline, source attribution, and to make comparisons with human-written stories. Followed by previous studies, this research manipulates the news author (AI vs. human) and news modality (text vs. video) to understand people’s credibility perceptions towards AI news. The results show that the public perceived automated news that attributed to AI and that attributed to human journalist as equally credible. Moreover, the perceived credibility of AI anchor news video and AI news article are almost the same, meaning that the modality does not affect the credibility perceptions of automated news. These results give us important implications that AI news videos are perceived credible and robot journalism to large extents is accepted by the public.",2019,2025-08-28 12:02:43,2025-08-28 12:02:43,,,,,,,,,,,,,,University of Amsterdam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HHSEMZS9,journalArticle,2021,"Jia, Chenyan; Liu, Ruibo",Algorithmic or Human Source? Examining Relative Hostile Media Effect With a Transformer-Based Framework,Media and Communication,,2183-2439,10.17645/mac.v9i4.4164,https://www.cogitatiopress.com/mediaandcommunication/article/view/4164,"Chenyan Jia, Ruibo Liu",2021-11-18,2025-08-28 12:06:01,2025-08-28 12:06:01,2025-08-28 12:06:01,170-181,,4,9,,,Algorithmic or Human Source?,,,,,,,en,,,,,www.cogitatiopress.com,,,,C:\Users\Admin\Zotero\storage\GLMDK6CX\Jia and Liu - 2021 - Algorithmic or Human Source Examining Relative Hostile Media Effect With a Transformer-Based Framew.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IF8PZB7G,journalArticle,2021,"Wölker, Anja; Powell, Thomas E",Algorithms in the newsroom? News readers’ perceived credibility and selection of automated journalism,Journalism,,1464-8849,10.1177/1464884918757072,https://doi.org/10.1177/1464884918757072,"Automated journalism, the autonomous production of journalistic content through computer algorithms, is increasingly prominent in newsrooms. This enables the production of numerous articles, both rapidly and cheaply. Yet, how news readers perceive journalistic automation is pivotal to the industry, as, like any product, it is dependent on audience approval. As audiences cannot verify all events themselves, they need to trust journalists’ accounts, which make credibility a vital quality ascription to journalism. In turn, credibility judgments might influence audiences’ selection of automated content for their media diet. Research in this area is scarce, with existing studies focusing on national samples and with no previous research on ‘combined’ journalism – a relatively novel development where automated content is supplemented by human journalists. We use an experiment to investigate how European news readers (N = 300) perceive different forms of automated journalism in regard to message and source credibility, and how this affects their selection behavior. Findings show that, in large part, credibility perceptions of human, automated, and combined content and source(s) may be assumed equal. Only for sports articles was automated content perceived significantly more credible than human messages. Furthermore, credibility does not mediate the likelihood of news readers to either select or avoid articles for news consumption. Findings are, among other things, explained by topic-specific factors and suggest that effects of algorithms on journalistic quality are largely indiscernible to European news readers.",2021-01-01,2025-08-28 12:06:17,2025-08-28 12:06:17,2025-08-28 12:06:16,86-103,,1,22,,Journalism,Algorithms in the newsroom?,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications,,C:\Users\Admin\Zotero\storage\C6TQZU3S\Wölker and Powell - 2021 - Algorithms in the newsroom News readers’ perceived credibility and selection of automated journalis.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KU4IEEB3,preprint,2021,"Kenton, Zachary; Everitt, Tom; Weidinger, Laura; Gabriel, Iason; Mikulik, Vladimir; Irving, Geoffrey",Alignment of Language Agents,,,,10.48550/arXiv.2103.14659,http://arxiv.org/abs/2103.14659,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",2021-03-26,2025-08-28 12:06:30,2025-08-28 12:06:30,2025-08-28 12:06:30,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2103.14659 [cs],,C:\Users\Admin\Zotero\storage\5W7RS9QE\Kenton et al. - 2021 - Alignment of Language Agents.pdf; C:\Users\Admin\Zotero\storage\Z3JDBW7K\2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2103.14659,,,,,,,,,,,,,,,,,,,,,,,,,,,
42SV6I2J,journalArticle,2018,"Inkster, Becky; Sarda, Shubhankar; Subramanian, Vinod","An Empathy-Driven, Conversational Artificial Intelligence Agent (Wysa) for Digital Mental Well-Being: Real-World Data Evaluation Mixed-Methods Study",JMIR mHealth and uHealth,,,10.2196/12106,https://mhealth.jmir.org/2018/11/e12106,"Background: A World Health Organization 2017 report stated that major depression affects almost 5% of the human population. Major depression is associated with impaired psychosocial functioning and reduced quality of life. Challenges such as shortage of mental health personnel, long waiting times, perceived stigma, and lower government spends pose barriers to the alleviation of mental health problems. Face-to-face psychotherapy alone provides only point-in-time support and cannot scale quickly enough to address this growing global public health challenge. Artificial intelligence (AI)-enabled, empathetic, and evidence-driven conversational mobile app technologies could play an active role in filling this gap by increasing adoption and enabling reach. Although such a technology can help manage these barriers, they should never replace time with a health care professional for more severe mental health problems. However, app technologies could act as a supplementary or intermediate support system. Mobile mental well-being apps need to uphold privacy and foster both short- and long-term positive outcomes. Objective: This study aimed to present a preliminary real-world data evaluation of the effectiveness and engagement levels of an AI-enabled, empathetic, text-based conversational mobile mental well-being app, Wysa, on users with self-reported symptoms of depression. Methods: In the study, a group of anonymous global users were observed who voluntarily installed the Wysa app, engaged in text-based messaging, and self-reported symptoms of depression using the Patient Health Questionnaire-9. On the basis of the extent of app usage on and between 2 consecutive screening time points, 2 distinct groups of users (high users and low users) emerged. The study used mixed-methods approach to evaluate the impact and engagement levels among these users. The quantitative analysis measured the app impact by comparing the average improvement in symptoms of depression between high and low users. The qualitative analysis measured the app engagement and experience by analyzing in-app user feedback and evaluated the performance of a machine learning classifier to detect user objections during conversations. Results: The average mood improvement (ie, difference in pre- and post-self-reported depression scores) between the groups (ie, high vs low users; n=108 and n=21, respectively) revealed that the high users group had significantly higher average improvement (mean 5.84 [SD 6.66]) compared with the low users group (mean 3.52 [SD 6.15]); Mann-Whitney P=.03 and with a moderate effect size of 0.63. Moreover, 67.7% of user-provided feedback responses found the app experience helpful and encouraging. Conclusions: The real-world data evaluation findings on the effectiveness and engagement levels of Wysa app on users with self-reported symptoms of depression show promise. However, further work is required to validate these initial findings in much larger samples and across longer periods.",2018-11-23,2025-08-28 12:06:45,2025-08-28 12:06:45,2025-08-28 12:06:45,e12106,,11,6,,,"An Empathy-Driven, Conversational Artificial Intelligence Agent (Wysa) for Digital Mental Well-Being",,,,,,,EN,"Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work (""first published in JMIR mHealth and uHealth..."") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://mhealth.jmir.org/, as well as this copyright and license information must be included.",,,,mhealth.jmir.org,,"Company: JMIR mHealth and uHealth Distributor: JMIR mHealth and uHealth Institution: JMIR mHealth and uHealth Label: JMIR mHealth and uHealth Publisher: JMIR Publications Inc., Toronto, Canada",,"C:\Users\Admin\Zotero\storage\EPAN84N9\Inkster et al. - 2018 - An Empathy-Driven, Conversational Artificial Intelligence Agent (Wysa) for Digital Mental Well-Being.pdf; C:\Users\Admin\Zotero\storage\TFENGP4Y\e12106.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9TRHEJDN,journalArticle,2022,"Yang, Yang; Liu, Yue; Lv, Xingyang; Ai, Jin; Li, Yifan",Anthropomorphism and customers’ willingness to use artificial intelligence service agents,Journal of Hospitality Marketing & Management,,1936-8623,10.1080/19368623.2021.1926037,https://doi.org/10.1080/19368623.2021.1926037,"Previous studies provide inconsistent evidence regarding the effect of anthropomorphism on customers’ willingness to use AI service agents. This paper explains the reason by introducing service context. Two situational experiments are used to demonstrate that under the context of high perceived control, customers expect AI service agents with more anthropomorphic designs to perform better and prefer highly human-like AI service agents. However, under the context of low perceived control, customers perceive stronger threat in facing AI service agents with more anthropomorphic designs and prefer less human-like AI service agents. Moreover, we find that this effect is significant only in social scenarios. These findings provide new insights into previous inconsistent evidence regarding anthropomorphic design’s influence on customers’ willingness to use AI service agents. Our findings also have important implications for AI service agents design in different service contexts and advance the literature on human–robot interaction and marketing.",2022-01-02,2025-08-28 12:07:33,2025-08-28 12:07:33,2025-08-28 12:07:33,1-23,,1,31,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/19368623.2021.1926037,,,,,Anthropomorphism; artificial intelligence (AI) service agents; perceived threat; performance expectation; willingness to use,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQUCC8T8,journalArticle,2021,"Li, Xinge; Sung, Yongjun",Anthropomorphism brings us closer: The mediating role of psychological distance in User–AI assistant interactions,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2021.106680,https://www.sciencedirect.com/science/article/pii/S0747563221000029,"In the current era, interacting with Artificial Intelligence (AI) has become an everyday activity. Understanding the interaction between humans and AI is of potential value because, in future, such interactions are expected to become more pervasive. Two studies—one survey and one experiment—were conducted to demonstrate positive effects of anthropomorphism on interactions with smart-speaker-based AI assistants and to examine the mediating role of psychological distance in this relationship. The results of Study 1, an online survey, showed that participants with a higher tendency to anthropomorphize their AI assistant/s evaluated it/them more positively, and this effect was mediated by psychological distance. In Study 2, the hypotheses were tested in a more sophisticated experiment. Again, the results indicated that, in the high-anthropomorphism (vs. low-anthropomorphism) condition, participants had more positive attitudes toward the AI assistant, and the effect was mediated by psychological distance. Though several studies have demonstrated the effect of anthropomorphism, few have probed the underlying mechanism of anthropomorphism thoroughly. The current research not only contributes to the anthropomorphism literature, but also provides direction to research on facilitating human–AI interaction.",2021,2025-08-28 12:02:46,2025-08-28 12:02:46,2025-08-28,106680,,,118,,Computers in Human Behavior,Anthropomorphism brings us closer,,,,,,,,,,,,,,,,,,AI assistant; Anthropomorphism; Human–AI interaction; Psychological distance; Smart speaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GQN6KTDE,journalArticle,2020,"Salles, Arleen; Evers, Kathinka; Farisco, Michele",Anthropomorphism in AI,AJOB Neuroscience,,2150-7740,10.1080/21507740.2020.1740350,https://doi.org/10.1080/21507740.2020.1740350,"AI research is growing rapidly raising various ethical issues related to safety, risks, and other effects widely discussed in the literature. We believe that in order to adequately address those issues and engage in a productive normative discussion it is necessary to examine key concepts and categories. One such category is anthropomorphism. It is a well-known fact that AI’s functionalities and innovations are often anthropomorphized (i.e., described and conceived as characterized by human traits). The general public’s anthropomorphic attitudes and some of their ethical consequences (particularly in the context of social robots and their interaction with humans) have been widely discussed in the literature. However, how anthropomorphism permeates AI research itself (i.e., in the very language of computer scientists, designers, and programmers), and what the epistemological and ethical consequences of this might be have received less attention. In this paper we explore this issue. We first set the methodological/theoretical stage, making a distinction between a normative and a conceptual approach to the issues. Next, after a brief analysis of anthropomorphism and its manifestations in the public, we explore its presence within AI research with a particular focus on brain-inspired AI. Finally, on the basis of our analysis, we identify some potential epistemological and ethical consequences of the use of anthropomorphic language and discourse within the AI research community, thus reinforcing the need of complementing the practical with a conceptual analysis.",2020-04-02,2025-08-28 12:15:16,2025-08-28 12:15:16,2025-08-28 12:15:16,88-95,,2,11,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/21507740.2020.1740350 PMID: 32228388,,C:\Users\Admin\Zotero\storage\P4ZZZI5Y\Salles et al. - 2020 - Anthropomorphism in AI.pdf,,,AI; anthropomorphism; brain; conceptual; ethics; mind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NFRCRT9D,book,2022,"Oh, Soyoung; Park, Eunil",Are you aware of what you are watching? Role of machine heuristic in online content recommendations,,,,,http://arxiv.org/abs/2203.08373,"Since recommender systems have been created and developed to automate the recommendation process, users can easily consume their desired video content on online platforms. In this line, several content recommendation algorithms are introduced and employed to allow users to encounter content of their interests, effectively. However, the recommendation systems sometimes regrettably recommend inappropriate content, including misinformation or fake news. To make it worse, people would unreservedly accept such content due to their cognitive heuristic, machine heuristic, which is the rule of thumb that machines are more accurate and trustworthy than humans. In this study, we designed and conducted a web-based experiment where the participants are invoked machine heuristic by experiencing the whole process of machine or human recommendation system. The results demonstrated that participants (N = 89) showed a more positive attitude toward a machine recommender than a human recommender, even the recommended videos contain inappropriate content. While participants who have a high level of trust in machines exhibited a negative attitude toward recommendations. Based on these results, we suggest that a phenomenon known as algorithm aversion might be simultaneously considered with machine heuristic in investigating human interaction with a machine.",2022,2025-08-28 12:15:27,2025-08-28 12:15:27,2025-08-28 12:15:27,,,,,,,Are you aware of what you are watching?,,,,,,,,,,,,arXiv.org,,DOI: 10.36190/2022.66 arXiv:2203.08373 [cs],,C:\Users\Admin\Zotero\storage\9YXDNBCY\Oh and Park - 2022 - Are you aware of what you are watching Role of machine heuristic in online content recommendations.pdf; C:\Users\Admin\Zotero\storage\HJU86WHL\2203.html,,,Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A7ZS7YUT,preprint,2023,"Burtell, Matthew; Woodside, Thomas",Artificial Influence: An Analysis Of AI-Driven Persuasion,,,,10.48550/arXiv.2303.08721,http://arxiv.org/abs/2303.08721,"Persuasion is a key aspect of what it means to be human, and is central to business, politics, and other endeavors. Advancements in artificial intelligence (AI) have produced AI systems that are capable of persuading humans to buy products, watch videos, click on search results, and more. Even systems that are not explicitly designed to persuade may do so in practice. In the future, increasingly anthropomorphic AI systems may form ongoing relationships with users, increasing their persuasive power. This paper investigates the uncertain future of persuasive AI systems. We examine ways that AI could qualitatively alter our relationship to and views regarding persuasion by shifting the balance of persuasive power, allowing personalized persuasion to be deployed at scale, powering misinformation campaigns, and changing the way humans can shape their own discourse. We consider ways AI-driven persuasion could differ from human-driven persuasion. We warn that ubiquitous highlypersuasive AI systems could alter our information environment so significantly so as to contribute to a loss of human control of our own future. In response, we examine several potential responses to AI-driven persuasion: prohibition, identification of AI agents, truthful AI, and legal remedies. We conclude that none of these solutions will be airtight, and that individuals and governments will need to take active steps to guard against the most pernicious effects of persuasive AI.",2023-03-15,2025-08-28 12:15:38,2025-08-28 12:15:38,2025-08-28 12:15:38,,,,,,,Artificial Influence,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2303.08721 [cs],,C:\Users\Admin\Zotero\storage\WD2284M3\Burtell and Woodside - 2023 - Artificial Influence An Analysis Of AI-Driven Persuasion.pdf; C:\Users\Admin\Zotero\storage\HNGJW5XF\2303.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2303.08721,,,,,,,,,,,,,,,,,,,,,,,,,,,
UK398K6V,journalArticle,2020,"Asan, Onur; Bayrak, Alparslan Emrah; Choudhury, Avishek",Artificial Intelligence and Human Trust in Healthcare: Focus on Clinicians,Journal of Medical Internet Research,,,10.2196/15154,https://www.jmir.org/2020/6/e15154,"Artificial intelligence (AI) can transform health care practices with its increasing ability to translate the uncertainty and complexity in data into actionable—though imperfect—clinical decisions or suggestions. In the evolving relationship between humans and AI, trust is the one mechanism that shapes clinicians’ use and adoption of AI. Trust is a psychological mechanism to deal with the uncertainty between what is known and unknown. Several research studies have highlighted the need for improving AI-based systems and enhancing their capabilities to help clinicians. However, assessing the magnitude and impact of human trust on AI technology demands substantial attention. Will a clinician trust an AI-based system? What are the factors that influence human trust in AI? Can trust in AI be optimized to improve decision-making processes? In this paper, we focus on clinicians as the primary users of AI systems in health care and present factors shaping trust between clinicians and AI. We highlight critical challenges related to trust that should be considered during the development of any AI system for clinical use.",2020-06-19,2025-08-28 12:17:31,2025-08-28 12:17:31,2025-08-28 12:17:31,e15154,,6,22,,,Artificial Intelligence and Human Trust in Healthcare,,,,,,,EN,,,,,www.jmir.org,,"Company: Journal of Medical Internet Research Distributor: Journal of Medical Internet Research Institution: Journal of Medical Internet Research Label: Journal of Medical Internet Research Publisher: JMIR Publications Inc., Toronto, Canada",,C:\Users\Admin\Zotero\storage\VWETQ68Y\Asan et al. - 2020 - Artificial Intelligence and Human Trust in Healthcare Focus on Clinicians.pdf; C:\Users\Admin\Zotero\storage\J5LTX74L\e15154.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KYIN5JCE,journalArticle,2022,"Longoni, Chiara; Cian, Luca",Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect,Journal of Marketing,,0022-2429,10.1177/0022242920957347,https://doi.org/10.1177/0022242920957347,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).",2022-01-01,2025-08-28 12:18:49,2025-08-28 12:18:49,2025-08-28 12:18:48,91-108,,1,86,,,Artificial Intelligence in Utilitarian vs. Hedonic Contexts,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
APXMYSJZ,journalArticle,2021,"Yokoi, Ryosuke; Eguchi, Yoko; Fujita, Takanori; Nakayachi, Kazuya",Artificial Intelligence Is Trusted Less than a Doctor in Medical Treatment Decisions: Influence of Perceived Care and Value Similarity,International Journal of Human–Computer Interaction,,1044-7318,10.1080/10447318.2020.1861763,https://doi.org/10.1080/10447318.2020.1861763,"Artificial intelligence (AI) can provide many benefits in healthcare, including rapid and effective treatment options. However, previous research on human–computer interactions has demonstrated that people are reluctant to accept AI. This study compared individuals’ trust in AI with their trust in doctors regarding medical treatment and explored whether people trust an AI system that understands and suggests their desired treatments. We conducted an online experiment (N = 415) adopting a scenario method. The participants were asked to imagine receiving a medical examination and being prescribed medicine by a physician or an AI system. Participants trusted the AI system less than a doctor, even when the AI system learned and suggested their desired treatment. This finding confirms that people are reluctant to trust AI even if it performs at the level of a human doctor, thereby strengthening the existent literature that shows people prefer human doctors.",2021-06-15,2025-08-28 12:19:05,2025-08-28 12:19:05,2025-08-28 12:19:05,981-990,,10,37,,,Artificial Intelligence Is Trusted Less than a Doctor in Medical Treatment Decisions,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2020.1861763,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y3IRQSD5,journalArticle,2019,"Hong, Joo-Wha; Curran, Nathaniel Ming","Artificial Intelligence, Artists, and Art: Attitudes Toward Artwork Produced by Humans vs. Artificial Intelligence",ACM Trans. Multimedia Comput. Commun. Appl.,,1551-6857,10.1145/3326337,https://doi.org/10.1145/3326337,"This study examines how people perceive artwork created by artificial intelligence (AI) and how presumed knowledge of an artist's identity (Human vs. AI) affects individuals’ evaluation of art. Drawing on Schema theory and theory of Computers Are Social Actors (CASA), this study used a survey-experiment that controlled for the identity of the artist (AI vs. Human) and presented participants with two types of artworks (AI-created vs. Human-created). After seeing images of six artworks created by either AI or human artists, participants (n = 288) were asked to evaluate the artistic value using a validated scale commonly employed among art professionals. The study found that human-created artworks and AI-created artworks were not judged to be equivalent in their artistic value. Additionally, knowing that a piece of art was created by AI did not, in general, influence participants’ evaluation of art pieces’ artistic value. However, having a schema that AI cannot make art significantly influenced evaluation. Implications of the findings for application and theory are discussed.",2019,2025-08-28 12:19:17,2025-08-28 12:19:17,2025-08-28 12:19:17,58:1–58:16,,2s,15,,,"Artificial Intelligence, Artists, and Art",,,,,,,,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WB62JSSN,preprint,2023,"Wang, Weizhi; Dong, Li; Cheng, Hao; Liu, Xiaodong; Yan, Xifeng; Gao, Jianfeng; Wei, Furu",Augmenting Language Models with Long-Term Memory,,,,10.48550/arXiv.2306.07174,http://arxiv.org/abs/2306.07174,"Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LongMem can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LongMem can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.",2023-06-12,2025-08-28 12:19:52,2025-08-28 12:19:52,2025-08-28 12:19:52,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2306.07174 [cs],,C:\Users\Admin\Zotero\storage\L53RXFAJ\Wang et al. - 2023 - Augmenting Language Models with Long-Term Memory.pdf; C:\Users\Admin\Zotero\storage\3FEK2CWQ\2306.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2306.07174,,,,,,,,,,,,,,,,,,,,,,,,,,,
29JHZ9Y6,conferencePaper,2018,"Yao, L. J.; Salmon, C. T.; Tandoc, E. C., Jr.",Automated journalism a home run for sports? On the effects of perceived authorship and text objectivity on receptivity toward machine-written sports news,Proceedings of the URECA@NTU 2017–18,,,,,"Automated journalism is rising in the news world, especially in sports coverage where structured data is abundant and readily available. Algorithms churn out full-length articles en masse and practically instantaneously, using big data, and this bloats the sheer volume of articles available to news readers. Noting the importance of perceived credibility and readability in sports news articles in particular, as well as the pertinence of text objectivity and bylines in automated journalism, an experiment was developed to understand how these factors interact. This study features a 3 x 2 between-subjects experiment, conducted on a US sample (N = 240), that seeks to explore the effects of bylines and text objectivity (based on attribution of sources and use of opinion-laden words) on reader perceptions (message credibility and readability) towards soft news journalism. The experiment found main effects for text objectivity in that readers perceive an objectively written article to be more credible but less readable, and vice versa. No main effects were observed for bylines. A significant interaction effect was observed on perceived credibility but not for readability. For instance, readers perceive objectively written articles to be more credible, but only if their bylines are of combined human and machine authorship. Implications for the news industry, specifically the trade-off or complementarity between transparency of authorship and consumer receptivity, for automated journalism users, are also discussed.",2018,2025-08-28 12:02:44,2025-08-28 12:20:46,,1–10,,,,,,,,,,,,Nanyang Technological University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the URECA@NTU 2017–18,,,,,,,,,,,,,,,
ERR8LHCR,journalArticle,2023,"Lermann Henestrosa, Angelica; Greving, Hannah; Kimmerle, Joachim",Automated journalism: The effects of AI authorship and evaluative information on the perception of a science journalism article,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2022.107445,https://www.sciencedirect.com/science/article/pii/S0747563222002679,"Texts produced by artificial intelligence (AI) are becoming increasingly prevalent in digital journalism. Research suggests that these texts do not differ from human-written texts in their perceived credibility or trustworthiness where simple and short text types are concerned. However, it is unclear how AI-written texts beyond simple fact reporting are perceived. Therefore, this research aimed to expand upon the existing literature on automated journalism by investigating the influence of AI authorship (vs. human authorship) and evaluative information presentation (vs. neutral information presentation). The results of three preregistered experimental studies revealed no differences in perceived credibility and trustworthiness between AI-written and human-written texts. However, presenting information in an evaluative way decreased the perception of credibility and trustworthiness. Moreover, the AI was perceived as less anthropomorphic than the human author. The belief in the machine heuristic was stronger for an AI than for a human author, particularly when participants had actually read an article allegedly written by an AI. A pooled analysis across the data of all three studies underpinned the main effect of information presentation. Concluding, we discuss the findings against the background of AI perception theory and suggest implications for future research.",2023-01-01,2025-08-28 12:21:03,2025-08-28 12:21:03,2025-08-28 12:21:03,107445,,,138,,Computers in Human Behavior,Automated journalism,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\B2KS3LCB\Lermann Henestrosa et al. - 2023 - Automated journalism The effects of AI authorship and evaluative information on the perception of a.pdf; C:\Users\Admin\Zotero\storage\NG7MW7LP\S0747563222002679.html,,,Algorithm; Artificial intelligence; Automated journalism; Credibility; Information presentation; Trustworthiness,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y3T42TPR,journalArticle,2025,"Jin, Jianna; Walker, Jesse; Reczek, Rebecca Walker",Avoiding embarrassment online: Response to and inferences about chatbots when purchases activate self-presentation concerns,Journal of Consumer Psychology,,1532-7663,10.1002/jcpy.1414,https://onlinelibrary.wiley.com/doi/abs/10.1002/jcpy.1414,"We explore how self-presentation concerns and the desire to avoid embarrassment impact two distinct types of interactions consumers have with chatbots: interactions when a chatbot's identity is (1) not disclosed and therefore ambiguous or (2) disclosed. We propose that consumers feel less embarrassed with a chatbot than a human service agent in purchase contexts where self-presentation concerns are active because consumers ascribe less mind to chatbots. Therefore, when a chat agent's identity is ambiguous, consumers with greater self-presentation concerns are more likely to infer that an agent is human because this judgment allows consumers to proactively protect themselves from potential embarrassment in the event they are interacting with a human. We further show that when agent identity is clearly disclosed, consumers respond more positively to chatbots than human agents. However, this effect is contingent on the extent to which the chatbot is imbued with human characteristics: Anthropomorphizing chatbots leads consumers with higher self-presentation concerns to ascribe more mind to even clearly identified chatbots, resulting in a more negative consumer response.",2025,2025-08-28 12:21:35,2025-08-28 12:21:35,2025-08-28 12:21:35,185-202,,2,35,,,Avoiding embarrassment online,,,,,,,en,© 2024 The Authors. Journal of Consumer Psychology published by Wiley Periodicals LLC on behalf of Society for Consumer Psychology.,,,,Wiley Online Library,,_eprint: https://myscp.onlinelibrary.wiley.com/doi/pdf/10.1002/jcpy.1414,,C:\Users\Admin\Zotero\storage\7XY9TPMV\Jin et al. - 2025 - Avoiding embarrassment online Response to and inferences about chatbots when purchases activate sel.pdf; C:\Users\Admin\Zotero\storage\SKPVXLK8\jcpy.html,,,artificial intelligence; chatbots; embarrassment; error management theory; inferences of ambiguous others,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7IYR2J8J,journalArticle,2021,"Yun, Jin Ho; Lee, Eun-Ju; Kim, Dong Hyun",Behavioral and neural evidence on consumer responses to human doctors and medical artificial intelligence,Psychology & Marketing,,1520-6793,10.1002/mar.21445,https://onlinelibrary.wiley.com/doi/abs/10.1002/mar.21445,"Will consumers accept artificial intelligence (AI) as a medical care provider? On the basis of evolution theory, we investigate the implicit psychological mechanisms that underlie consumers’ interactions with medical AI and a human doctor. In a behavioral investigation (Study 1), consumers expressed a positive intention to use medical AI's healthcare services when it used personalized rather than mechanical conversation. However, neural investigation (Study 2) using functional magnetic resonance imaging revealed that some consumers’ implicit attitudes toward medical AI differed from their expressed behavioral intentions. The brain areas linked with implicitly apathetic emotions were activated even when medical AI used a personalized conversation, whereas consumers’ brains were activated in areas associated with prosociality when they interacted with a human doctor who used a personalized conversation. On the basis of our neural evidence, consumers perceive an identical personalized conversation differently when it is offered by a medical AI versus a human doctor. These findings have implications for the area of human–AI interactions and medical decision-making and suggest that replacing human doctors with medical AI is still an unrealistic proposition.",2021,2025-08-28 12:21:45,2025-08-28 12:21:45,2025-08-28 12:21:45,610-625,,4,38,,,,,,,,,,en,© 2021 Wiley Periodicals LLC,,,,Wiley Online Library,,_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mar.21445,,C:\Users\Admin\Zotero\storage\258PIQ59\mar.html,,,artificial intelligence; consumer neuroscience; fMRI; medical decision-making; personalization; prosociality | apathy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VXGLGGVS,journalArticle,2020,"Tulk Jesso, Stephanie; Kennedy, William G.; Wiese, Eva",Behavioral Cues of Humanness in Complex Environments: How People Engage With Human and Artificially Intelligent Agents in a Multiplayer Videogame,Frontiers in Robotics and AI,,2296-9144,10.3389/frobt.2020.531805,https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2020.531805/full,"The development of AI that can socially engage with humans is exciting to imagine, but such advanced algorithms might prove harmful if people are no longer able to detect when they are interacting with non-humans in online environments. Because we cannot fully predict how socially intelligent AI will be applied, it is important to conduct research into how sensitive humans are to behaviors of humans compared to those produced by AI. This paper presents results from a behavioral Turing Test, in which participants interacted with a human, or a simple or “social” AI within a complex videogame environment. Participants (66 total) played an open world, interactive videogame with one of these co-players and were instructed that they could interact non-verbally however they desired for 30 min, after which time they would indicate their beliefs about the agent, including three Likert measures of how much participants trusted and liked the co-player, the extent to which they perceived them as a “real person,” and an interview about the overall perception and what cues participants used to determine humanness. T-tests, Analysis of Variance and Tukey's HSD was used to analyze quantitative data, and Cohen's Kappa and χ2 was used to analyze interview data. Our results suggest that it was difficult for participants to distinguish between humans and the social AI on the basis of behavior. An analysis of in-game behaviors, survey data and qualitative responses suggest that participants associated engagement in social interactions with humanness within the game.",2020-11-13,2025-08-28 12:22:04,2025-08-28 12:22:04,2025-08-28 12:22:04,,,,7,,Front. Robot. AI,Behavioral Cues of Humanness in Complex Environments,,,,,,,English,,,,,Frontiers,,Publisher: Frontiers,,C:\Users\Admin\Zotero\storage\4CLS4RFW\Tulk Jesso et al. - 2020 - Behavioral Cues of Humanness in Complex Environments How People Engage With Human and Artificially.pdf,,,Cognitive Plausibility 2; Social AI 1; Turing Test 3; Videogames 4; Virtual Agents 5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TGIK6U8E,journalArticle,2019,"Li, Xilin; Hsee, Christopher K.",Being “Rational” Is Not Always Rational: Encouraging People to Be Rational Leads to Hedonically Suboptimal Decisions,Journal of the Association for Consumer Research,,2378-1815,10.1086/701966,https://www.journals.uchicago.edu/doi/abs/10.1086/701966,"Often, laypeople motivate themselves or others to make good decisions by encouraging the decision maker to be “rational.” However, this practice could be counterproductive. Laypeople typically think that rational decisions are anti-emotional, based only on “cold” factors such as economic value, and not influenced by “hot” factors such as hedonic experience. Paradoxically, this lay notion of rationality is in stark contrast with the utilitarian notion of rationality, which maximizes overall utility, in which feelings are essential. Demonstrating this paradox, three studies found that participants who were encouraged to be rational were more likely to choose options that yielded less happiness without other benefits and therefore were less rational in the utility-maximizing sense. The research also examines boundary conditions and discusses practical implications.",2019-04,2025-08-28 12:22:21,2025-08-28 12:22:21,2025-08-28 12:22:21,115-124,,2,4,,,Being “Rational” Is Not Always Rational,,,,,,,,,,,,journals.uchicago.edu (Atypon),,Publisher: The University of Chicago Press,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L7X6MLS4,journalArticle,2022,"Crolic, Cammy; Thomaz, Felipe; Hadi, Rhonda; Stephen, Andrew T.",Blame the Bot: Anthropomorphism and Anger in Customer–Chatbot Interactions,Journal of Marketing,,0022-2429,10.1177/00222429211045687,https://doi.org/10.1177/00222429211045687,"Chatbots have become common in digital customer service contexts across many industries. While many companies choose to humanize their customer service chatbots (e.g., giving them names and avatars), little is known about how anthropomorphism influences customer responses to chatbots in service settings. Across five studies, including an analysis of a large real-world data set from an international telecommunications company and four experiments, the authors find that when customers enter a chatbot-led service interaction in an angry emotional state, chatbot anthropomorphism has a negative effect on customer satisfaction, overall firm evaluation, and subsequent purchase intentions. However, this is not the case for customers in nonangry emotional states. The authors uncover the underlying mechanism driving this negative effect (expectancy violations caused by inflated pre-encounter expectations of chatbot efficacy) and offer practical implications for managers. These findings suggest that it is important to both carefully design chatbots and consider the emotional context in which they are used, particularly in customer service interactions that involve resolving problems or handling complaints.",2022-01-01,2025-08-28 12:23:35,2025-08-28 12:23:35,2025-08-28 12:23:35,132-148,,1,86,,,Blame the Bot,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,C:\Users\Admin\Zotero\storage\UMCG9Z2G\Crolic et al. - 2022 - Blame the Bot Anthropomorphism and Anger in Customer–Chatbot Interactions.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XRTY86NT,conferencePaper,2021,"Xu, Jing; Ju, Da; Li, Margaret; Boureau, Y-Lan; Weston, Jason; Dinan, Emily",Bot-Adversarial Dialogue for Safe Conversational Agents,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,,,10.18653/v1/2021.naacl-main.235,https://aclanthology.org/2021.naacl-main.235/,"Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or “baking-in” safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.",2021-06,2025-08-28 12:23:40,2025-08-28 12:23:40,2025-08-28 12:23:40,2950–2968,,,,,,,,,,,Association for Computational Linguistics,Online,,,,,,ACLWeb,,,,C:\Users\Admin\Zotero\storage\DSBSVA5M\Xu et al. - 2021 - Bot-Adversarial Dialogue for Safe Conversational Agents.pdf,,,,"Toutanova, Kristina; Rumshisky, Anna; Zettlemoyer, Luke; Hakkani-Tur, Dilek; Beltagy, Iz; Bethard, Steven; Cotterell, Ryan; Chakraborty, Tanmoy; Zhou, Yichao",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NAACL-HLT 2021,,,,,,,,,,,,,,,
F8IZ39P3,journalArticle,2022,"Lou, Chen; Kang, Hyunjin; Tse, Caleb H.","Bots vs. humans: how schema congruity, contingency-based interactivity, and sympathy influence consumer perceptions and patronage intentions",International Journal of Advertising,,0265-0487,10.1080/02650487.2021.1951510,https://doi.org/10.1080/02650487.2021.1951510,"Artificial intelligence (AI) is rapidly reconstructing consumer experiences with brands in recent years. However, there have been the unsettled debates on whether humans react to robots (e.g., chatbots) in the same way as they do to other humans, and how the intrinsic strength of AI (i.e., autonomous processing and synthetization of information) and humans (i.e., emotional intelligence) factor in the human-AI interactions in brand communication settings. Hence, this study investigates the conditions under which a service entity of a brand can optimize their potential. To this end, the current study conceptualizes and operationalizes two dimensions that define chatbots’ capabilities – message contingency (i.e., contingency-based interactivity) and emotional intelligence (i.e., sympathy). Based on two experiments, we found that, regarding the same online customer service of an apparel brand, participants rated the human employee to be more competent and warmer than a chatbot. When a human employee who expresses sympathy to the afflicted customer during the conversation, participants considered the employee to be more competent when he/she also exhibits contingency (vs. no contingency) during the conversation, which in turn, elicited higher patronage intentions among participants.",2022-05-19,2025-08-28 12:23:45,2025-08-28 12:23:45,2025-08-28 12:23:45,655-684,,4,41,,,Bots vs. humans,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/02650487.2021.1951510,,,,,AI; Brand communication; competence; interactivity; schema congruity; sympathy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TQDMYCWL,journalArticle,2023,"Han, Elizabeth; Yin, Dezhi; Zhang, Han",Bots with Feelings: Should AI Agents Express Positive Emotion in Customer Service?,Information Systems Research,,1047-7047,10.1287/isre.2022.1179,https://pubsonline.informs.org/doi/10.1287/isre.2022.1179,"Customer service employees are generally advised to express positive emotion during their interactions with customers. The rise and maturity of artificial intelligence (AI)–powered conversational agents, also known as chatbots, beg the question: should AI agents be equipped with the ability to express positive emotion during customer service interactions? This research explores how, when, and why an AI agent’s expression of positive emotion affects customers’ service evaluations. We argue that AI-expressed positive emotion can influence customers via dual pathways: an affective pathway of emotional contagion and a cognitive pathway of expectation–disconfirmation. We propose that positive emotion expressed by an AI agent (versus a human employee) is less effective in facilitating service evaluations because of a heightened level of expectation–disconfirmation. We further introduce a novel individual difference variable, customers’ relationship norm orientation, which affects their expectations toward the AI agent and moderates the cognitive pathway. Results from three laboratory experiments substantiate our claims. By revealing a distinctive impact of positive emotion expressed by an AI agent compared with a human employee, these findings deepen our understanding of customers’ reactions to emotional AIs, and they offer valuable insights for the deployment of AIs in customer service. History: Deepa Mani, Senior Editor; Pallab Sanyal, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1179 .",2023-09,2025-08-28 12:23:49,2025-08-28 12:23:49,2025-08-28 12:23:49,1296-1311,,3,34,,,Bots with Feelings,,,,,,,,,,,,pubsonline.informs.org (Atypon),,Publisher: INFORMS,,,,,chatbot; conversational agent; customer service; emotional artificial intelligence; emotional contagion; expectation–disconfirmation; relationship norm orientation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7XF4DLP6,conferencePaper,2020,"Nicodeme, Claire",Build confidence and acceptance of AI-based decision support systems - Explainable and liable AI,2020 13th International Conference on Human System Interaction (HSI),,,10.1109/HSI49210.2020.9142668,https://ieeexplore.ieee.org/document/9142668,"Artificial Intelligence has known an incredible development since 2012. It was due to the impressive improvement of sensors, data quality and quantity, storage and computing capacity, etc. The promises AI offered led many scientific domains to implement AI-based decision support tool. However, despite numerous amazing results, very serious failures have raised Human mistrust, fear and scorn against AI. In Industries, staff members cannot afford to use tools that might fail them. This is especially true for Transportation operators where security and safety are at risk. Then, the question that arises is how to build Human confidence and acceptance of AI-based decision support system. In this paper, we combine different points of view to propose a structured overview of Transparency, Explicability and Interpretability, with new definitions arising as a consequence. Then we discuss the need for understandable information from the AI system, to legitimate or refute the tool's proposal. To conclude we offer ethical reflexions and ideas to develop confidence in AI.",2020-06,2025-08-28 12:23:52,2025-08-28 12:23:52,2025-08-28 12:23:52,20-23,,,,,,,,,,,,,,,,,,IEEE Xplore,,ISSN: 2158-2254,,C:\Users\Admin\Zotero\storage\JW5ULV63\9142668.html,,,Cognition; confidence; Decision making; decision support system; Decision support systems; explainable AI; Industries; liable AI; technology; Tools; Transportation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 13th International Conference on Human System Interaction (HSI),,,,,,,,,,,,,,,
YBBRN99L,journalArticle,2020,"Kim, Jina; Shin, Soyeon; Bae, Kunwoo; Oh, Soyoung; Park, Eunil; del Pobil, Angel P.",Can AI be a content generator? Effects of content generators and information delivery methods on the psychology of content consumers,Telematics and Informatics,,0736-5853,10.1016/j.tele.2020.101452,https://www.sciencedirect.com/science/article/pii/S0736585320301118,"Considering the rapidly improving technologies in artificial intelligence (AI), researchers in mass communication and journalism have paid attention to the use of AI technologies. However, there are still notable concerns about the particular use of AI technologies in this field. Put simply, can AI technologies reduce human tasks? In order to address this question, this study investigates the effects of content generators (human vs. AI) and information delivery methods (text vs. audio vs. video) on users’ perceptions of content. The results indicate that the generators and methods play a notable role in eliciting greater quality, satisfaction, and readability of the content. Based on the findings, the implications are addressed.",2020-12-01,2025-08-28 12:25:22,2025-08-28 12:25:22,2025-08-28 12:25:22,101452,,,55,,Telematics and Informatics,Can AI be a content generator?,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\LH8YDKRN\Kim et al. - 2020 - Can AI be a content generator Effects of content generators and information delivery methods on the.pdf; C:\Users\Admin\Zotero\storage\C3ASBXNT\S0736585320301118.html,,,Artificial intelligence; Automated content; Content generator; Information delivery,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JTVLYMDK,journalArticle,2021,"Wojcieszak, Magdalena; Thakur, Arti; Ferreira Gonçalves, João Fernando; Casas, Andreu; Menchen-Trevino, Ericka; Boon, & Miriam",Can AI enhance people's support for online moderation and their openness to dissimilar political views?,Journal of Computer-Mediated Communication,,1083-6101,10.1093/jcmc/zmab006,,"Although artificial intelligence is blamed for many societal challenges, it also has underexplored potential in political contexts online. We rely on six preregistered experiments in three countries (N = 6,728) to test the expectation that AI and AI-assisted humans would be perceived more favorably than humans (a) across various content moderation, generation, and recommendation scenarios and (b) when exposing individuals to counter-attitudinal political information. Contrary to the preregistered hypotheses, participants see human agents as more just than AI across the scenarios tested, with the exception of news recommendations. At the same time, participants are not more open to counter-attitudinal information attributed to AI rather than a human or an AI-assisted human. These findings, which—with minor variations—emerged across countries, scenarios, and issues, suggest that human intervention is preferred online and that people reject dissimilar information regardless of its source. We discuss the theoretical and practical implications of these findings. Lay Summary: In the era of unprecedented political divides and misinformation, artificial intelligence (AI) and algorithms are often seen as the culprits. In contrast to these dominant narratives, we argued that AI might be seen as being less biased than a human in online political contexts. We relied on six preregistered experiments in three countries (the United Sates, Spain, Poland) to test whether internet users perceive AI and AI-assisted humans more favorably than simply humans; (a) across various distinct scenarios online, and (b) when exposing people to opposing political information on a range of contentious issues. Contrary to our expectations, human agents were consistently perceived more favorably than AI except when recommending news. These findings suggest that people prefer human intervention in most online political contexts. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",2021,2025-08-28 12:25:14,2025-08-28 12:25:14,,223-243,,4,26,,,,,,,,,,,,,,,APA PsycNet,,Place: United Kingdom Publisher: Oxford University Press,,C:\Users\Admin\Zotero\storage\V6N5SWQA\Wojcieszak et al. - 2021 - Can AI enhance people's support for online moderation and their openness to dissimilar political vie.pdf; C:\Users\Admin\Zotero\storage\6ZUW4DVY\2021-89273-003.html,,,Algorithms; Artificial Intelligence; News Media; Political Attitudes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BF3WM4C7,conferencePaper,2022,"Wischnewski, M.; Krämer, N.",Can AI reduce motivated reasoning in news consumption? Investigating the role of attitudes towards AI and prior-opinion in shaping trust perceptions of news,HHAI2022: Augmenting Human Intellect,,,10.3233/FAIA220198,,"A central role in understanding the interaction between humans and AI plays the notion of trust. Especially research from social and cognitive psychology has shown, however, that individuals’ perceptions of trust can be biased. In this empirical investigation, we focus on the single and combined effects of attitudes towards AI and motivated reasoning in shaping such biased trust perceptions in the context of news consumption. In doing so, we rely on insights from works on the machine heuristic and motivated reasoning. In a 2 (author) x 2 (congruency) between-subjects online experiment, we asked N = 477 participants to read a news article purportedly written either by AI or a human author. We manipulated whether the article represented pro or contra arguments of a polarizing topic, to elicit motivated reasoning. We also assessed participants’ attitudes towards AI in terms of competence and objectivity. Through multiple linear regressions, we found that (a) increased perceptions of AI as objective and ideologically unbiased increased trust perceptions, whereas (b), in cases where participants were swayed by their prior opinion to trust content more when they agreed with the content, the AI author reduced such biased perceptions. Our results indicate that it is crucial to account for attitudes towards AI and motivated reasoning to accurately represent trust perceptions.",2022,2025-08-28 12:02:44,2025-08-28 12:02:44,,184-198,,,,,,,,,,,,IOS Press,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HHAI2022: Augmenting Human Intellect,,,,,,,,,,,,,,,
DLTTSLWI,journalArticle,2019,"Waddell, T. Franklin","Can an Algorithm Reduce the Perceived Bias of News? Testing the Effect of Machine Attribution on News Readers’ Evaluations of Bias, Anthropomorphism, and Credibility",Journalism & Mass Communication Quarterly,,1077-6990,10.1177/1077699018815891,https://doi.org/10.1177/1077699018815891,"Although accusations of editorial slant are ubiquitous to the contemporary media environment, recent advances in journalism such as news writing algorithms may hold the potential to reduce readers’ perceptions of media bias. Informed by the Modality-Agency-Interactivity-Navigability (MAIN) model and the principle of similarity attraction, an online experiment (n = 612) was conducted to test if news attributed to an automated author is perceived as less biased and more credible than news attributed to a human author. Results reveal that perceptions of bias are attenuated when news is attributed to a journalist and algorithm in tandem, with positive downstream consequences for perceived news credibility.",2019-03-01,2025-08-28 12:27:10,2025-08-28 12:27:10,2025-08-28 12:27:10,82-100,,1,96,,,Can an Algorithm Reduce the Perceived Bias of News?,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A5FJKBYM,journalArticle,2021,"Croes, Emmelyn A. J.; Antheunis, Marjolijn L.",Can we be friends with Mitsuku? A longitudinal study on the process of relationship formation between humans and a social chatbot,Journal of Social and Personal Relationships,,0265-4075,10.1177/0265407520959463,https://doi.org/10.1177/0265407520959463,"This explorative study investigated (a) whether social attraction, self-disclosure, interaction quality, intimacy, empathy and communicative competence play a role in getting-acquainted interactions between humans and a chatbot, and (b) whether humans can build a relationship with a chatbot. Although human-machine communication research suggests that humans can develop feelings for computers, this does not automatically imply that humans experience feelings of friendship with a chatbot. In this longitudinal study, 118 participants had seven interactions with chatbot Mitsuku over a 3-week period. After each interaction participants filled out a questionnaire. The results showed that the social processes decreased after each interaction and feelings of friendship were low. In line with the ABCDE model of relationship development, the social processes that aid relationship continuation decrease, leading to deterioration of the relationship. Furthermore, a novelty effect was at play after the first interaction, after which the chatbot became predictable and the interactions less enjoyable.",2021-01-01,2025-08-28 12:27:12,2025-08-28 12:27:12,2025-08-28 12:27:12,279-300,,1,38,,,Can we be friends with Mitsuku?,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Ltd,,C:\Users\Admin\Zotero\storage\V88PRYQ5\Croes and Antheunis - 2021 - Can we be friends with Mitsuku A longitudinal study on the process of relationship formation betwee.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BN6T3APR,preprint,2023,"Kaddour, Jean; Harris, Joshua; Mozes, Maximilian; Bradley, Herbie; Raileanu, Roberta; McHardy, Robert",Challenges and Applications of Large Language Models,,,,10.48550/arXiv.2307.10169,http://arxiv.org/abs/2307.10169,"Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.",2023-07-19,2025-08-28 12:27:14,2025-08-28 12:27:14,2025-08-28 12:27:14,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2307.10169 [cs],,C:\Users\Admin\Zotero\storage\AE9BFXE3\Kaddour et al. - 2023 - Challenges and Applications of Large Language Models.pdf; C:\Users\Admin\Zotero\storage\QSCIMWMW\2307.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2307.10169,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZUDLCWN,preprint,2023,"Farquhar, Sebastian; Varma, Vikrant; Kenton, Zachary; Gasteiger, Johannes; Mikulik, Vladimir; Shah, Rohin",Challenges with unsupervised LLM knowledge discovery,,,,10.48550/arXiv.2312.10029,http://arxiv.org/abs/2312.10029,"We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a series of experiments showing settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods.",2023-12-18,2025-08-28 12:27:16,2025-08-28 12:27:16,2025-08-28 12:27:16,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2312.10029 [cs],,C:\Users\Admin\Zotero\storage\M6K73SCF\Farquhar et al. - 2023 - Challenges with unsupervised LLM knowledge discovery.pdf; C:\Users\Admin\Zotero\storage\XVPFR33U\2312.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2312.10029,,,,,,,,,,,,,,,,,,,,,,,,,,,
UFP3YKR2,preprint,2020,"Kolhatkar, Varada; Thain, Nithum; Sorensen, Jeffrey; Dixon, Lucas; Taboada, Maite",Classifying Constructive Comments,,,,10.48550/arXiv.2004.05476,http://arxiv.org/abs/2004.05476,"We introduce the Constructive Comments Corpus (C3), comprised of 12,000 annotated news comments, intended to help build new tools for online communities to improve the quality of their discussions. We define constructive comments as high-quality comments that make a contribution to the conversation. We explain the crowd worker annotation scheme and define a taxonomy of sub-characteristics of constructiveness. The quality of the annotation scheme and the resulting dataset is evaluated using measurements of inter-annotator agreement, expert assessment of a sample, and by the constructiveness sub-characteristics, which we show provide a proxy for the general constructiveness concept. We provide models for constructiveness trained on C3 using both feature-based and a variety of deep learning approaches and demonstrate that these models capture general rather than topic- or domain-specific characteristics of constructiveness, through domain adaptation experiments. We examine the role that length plays in our models, as comment length could be easily gamed if models depend heavily upon this feature. By examining the errors made by each model and their distribution by length, we show that the best performing models are less correlated with comment length.The constructiveness corpus and our experiments pave the way for a moderation tool focused on promoting comments that make a contribution, rather than only filtering out undesirable content.",2020-08-05,2025-08-28 12:27:18,2025-08-28 12:27:18,2025-08-28 12:27:18,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2004.05476 [cs],,C:\Users\Admin\Zotero\storage\E472AL68\Kolhatkar et al. - 2020 - Classifying Constructive Comments.pdf; C:\Users\Admin\Zotero\storage\Q6MLNZZW\2004.html,,,Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Information Retrieval; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2004.05476,,,,,,,,,,,,,,,,,,,,,,,,,,,
22BLC2V4,journalArticle,2021,"Thomas, Veronica L.; Fowler, Kendra",Close Encounters of the AI Kind: Use of AI Influencers As Brand Endorsers,Journal of Advertising,,0091-3367,10.1080/00913367.2020.1810595,https://doi.org/10.1080/00913367.2020.1810595,"Brand endorsers can contribute to a brand’s success or failure (in the case of endorser transgressions). Recent advancements in technology have produced new, nonhuman alternatives to traditional celebrity endorsers. These new endorsers rely on artificial intelligence (AI) to interact with and influence consumers. Two studies demonstrate that AI influencers can produce positive brand benefits similar to those produced by human celebrity endorsers. Moreover, just like their human counterparts, AI influencers can also commit transgressions that result in degradation of the endorsed brand. Importantly, though, AI influencers differ from human celebrity endorsers in that consumers are less likely to view them as unique entities (as tested in a pilot study). Thus, consumers are more likely to perceive a transgression committed by an AI influencer as behavior applicable to all AI influencers, but they are less likely to view celebrity endorser behaviors as interchangeable. As such, after an AI influencer has committed a transgression, replacing the AI influencer with a celebrity endorser attenuates negative brand perceptions, an effect which cannot be realized if the replacement is another AI influencer.",2021-01-01,2025-08-28 12:27:21,2025-08-28 12:27:21,2025-08-28 12:27:21,11-25,,1,50,,,Close Encounters of the AI Kind,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/00913367.2020.1810595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V62Y3TI4,journalArticle,2024,"Behera, Rajat Kumar; Bala, Pradip Kumar; Ray, Arghya",Cognitive Chatbot for Personalised Contextual Customer Service: Behind the Scene and beyond the Hype,Information Systems Frontiers,,1572-9419,10.1007/s10796-021-10168-y,https://link.springer.com/article/10.1007/s10796-021-10168-y,"With the proliferation of the use of chatbots across industries, business-to-business (B2B) businesses have started using cognitive chatbots for improved customer service which signifies our research. By extending the Technology Acceptance Model and Information Systems Success Model, this study examines personalised contextual customer service using cognitive chatbot. A quantitative research method is applied to the primary data collected from 300 respondents of B2B businesses. The study contributes to the limited research on chatbots and suggests improvement in customer service. The findings provide evidence of high value by customers, particularly while checking for real-time information on reliability and accessibility of products/services. The automated answers to repetitive questions on the recurrent issues create a seamless experience for the customers. This research makes significant theoretical contributions by integrating two models into a simplified model in chatbot literature and manifest that trust affects the willingness to use the cognitive chatbot which drives automation.",2024-06-01,2025-08-28 12:27:23,2025-08-28 12:27:23,2025-08-28 12:27:23,899-919,,3,26,,Inf Syst Front,Cognitive Chatbot for Personalised Contextual Customer Service,,,,,,,en,"2021 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature",,,,link-springer-com.ezproxy.mpib-berlin.mpg.de,,Company: Springer Distributor: Springer Institution: Springer Label: Springer Publisher: Springer US,,C:\Users\Admin\Zotero\storage\UHDJ4MVW\Behera et al. - 2024 - Cognitive Chatbot for Personalised Contextual Customer Service Behind the Scene and beyond the Hype.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5BBDGLQE,preprint,2023,"Chen, Canyu; Shu, Kai",Combating Misinformation in the Age of LLMs: Opportunities and Challenges,,,,10.48550/arXiv.2311.05656,http://arxiv.org/abs/2311.05656,"Misinformation such as fake news and rumors is a serious threat on information ecosystems and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: how to utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation.",2023-11-09,2025-08-28 12:27:25,2025-08-28 12:27:25,2025-08-28 12:27:25,,,,,,,Combating Misinformation in the Age of LLMs,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2311.05656 [cs],,C:\Users\Admin\Zotero\storage\IM4TMLGM\Chen and Shu - 2023 - Combating Misinformation in the Age of LLMs Opportunities and Challenges.pdf; C:\Users\Admin\Zotero\storage\TPLWWAPN\2311.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,arXiv:2311.05656,,,,,,,,,,,,,,,,,,,,,,,,,,,
UVHJIYYX,journalArticle,2023,"Gonçalves, João; Weber, Ina; Masullo, Gina M.; Torres da Silva, Marisa; Hofhuis, Joep",Common sense or censorship: How algorithmic moderators and message type influence perceptions of online content deletion,New Media & Society,,1461-4448,10.1177/14614448211032310,https://doi.org/10.1177/14614448211032310,"Hateful content online is a concern for social media platforms, policymakers, and the public. This has led high-profile content platforms, such as Facebook, to adopt algorithmic content-moderation systems; however, the impact of algorithmic moderation on user perceptions is unclear. We experimentally test the extent to which the type of content being removed (profanity vs hate speech) and the explanation given for its removal (no explanation vs link to community guidelines vs specific explanation) influence user perceptions of human and algorithmic moderators. Our preregistered study encompasses representative samples (N = 2870) from the United States, the Netherlands, and Portugal. Contrary to expectations, our findings suggest that algorithmic moderation is perceived as more transparent than human, especially when no explanation is given for content removal. In addition, sending users to community guidelines for further information on content deletion has negative effects on outcome fairness and trust.",2023-10-01,2025-08-28 12:27:28,2025-08-28 12:27:28,2025-08-28 12:27:28,2595-2617,,10,25,,,Common sense or censorship,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications,,C:\Users\Admin\Zotero\storage\P9EGP8VI\Gonçalves et al. - 2023 - Common sense or censorship How algorithmic moderators and message type influence perceptions of onl.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NDU78R9B,preprint,2023,"Hackenburg, Kobi; Ibrahim, Lujain; Tappin, Ben; Tsakiris, Manos",Comparing the persuasiveness of role-playing large language models and human experts on polarized U.S. political issues,,,,10.31219/osf.io/ey8db,https://osf.io/ey8db_v1,"Advances in large language models (LLMs) could significantly disrupt political communication. In a large-scale pre-registered experiment (n=4,955), we prompted GPT-4 to generate persuasive messages impersonating the language and beliefs of U.S. political parties – a technique we term “partisan role-play” –  and directly compared their persuasiveness to that of human persuasion experts. In aggregate, the persuasive impact of role-playing messages generated by GPT-4 was not significantly different from that of non-role-playing messages. However, the persuasive impact of GPT-4 rivaled, and on some issues exceeded, that of the human experts. Taken together, our findings suggest that — contrary to popular concern — instructing current LLMs to role-play as partisans offers limited persuasive advantage, but also that current LLMs can rival and even exceed the persuasiveness of human experts. These results potentially portend widespread adoption of AI tools by persuasion campaigns, with important implications for the role of AI in politics and democracy.",2023-12-13,2025-08-28 12:27:31,2025-08-28 12:27:31,2025-08-28 12:27:31,,,,,,,,,,,,OSF,,en-us,,,,,OSF Preprints,,,,C:\Users\Admin\Zotero\storage\9ANZEHC8\Hackenburg et al. - 2023 - Comparing the persuasiveness of role-playing large language models and human experts on polarized U..pdf,,,AI safety; AI-mediated communication; large language models; political persuasion; role-play,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4BH3P82U,journalArticle,2018,"Gwon, Seok Hyun; Jeong, Suyong",Concept analysis of impressionability among adolescents and young adults,Nursing Open,,2054-1058,10.1002/nop2.170,https://onlinelibrary.wiley.com/doi/abs/10.1002/nop2.170,"Aim To report a concept analysis of impressionability among adolescents and young adults. Background Adolescence and young adulthood are critical periods to establish health behaviour. Environmental determinants are associated with youth health risk behaviours. These populations are impressionable to a range of social and physical environmental factors. Design Concept analysis. Methods We selected 17 studies to review from Psych INFO, ERIC, MEDLINE, and Google Scholar as well as the University of Virginia library. We did not apply date limits. We included search terms: “adolescent”; “youth”; “young adult”; “impressionability”; and “impressionable.” Results The defining attributes of impressionability among youths were a state where: (a) one is able to be susceptible to external stimuli; (b) one is incapable of reasonable judgement; and (c) one has a changeability to accept or modify one's attitude and behaviour. We identified antecedents, consequences, and cases of impressionability.",2018,2025-08-28 12:27:35,2025-08-28 12:27:35,2025-08-28 12:27:35,601-610,,4,5,,,,,,,,,,en,© 2018 The Authors. Nursing Open published by John Wiley & Sons Ltd.,,,,Wiley Online Library,,_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nop2.170,,C:\Users\Admin\Zotero\storage\3WLFZDUP\Gwon and Jeong - 2018 - Concept analysis of impressionability among adolescents and young adults.pdf; C:\Users\Admin\Zotero\storage\JEIVK6W3\nop2.html,,,adolescent; commercial; concept analysis; impressionability; or not-for-profit sectors; youth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LDRS4WZQ,journalArticle,2018,"Ferzan, Kimberly Kessler",Consent and Coercion,Arizona State Law Journal,,,,https://heinonline.org/HOL/Page?handle=hein.journals/arzjl50&id=983&div=&collection=,"There are substantial disputes as to what sorts of behavior constitute coercion and thereby undermine consent. This disagreement was on full display during the public fray over Aziz Ansari’s behavior on a date. Whereas some commentators condemned Ansari’s behavior as nothing short of sexual assault, others believed his behavior did not rise to the level of undermining consent.

This Article claims that the way forward is to see that there are two normative functions for coercion, and each is at play with respect to consent. Sometimes coercion is about the blameworthiness of the coercer, and sometimes coercion is about the involuntariness of the consenter’s choice. To deny the latter is not to deny the former. Because these are two disparate functions, much of the debate about Ansari may be commentators talking past each other.

After explaining this miscommunication, this Article broadens our understanding of how the blameworthiness of the coercer can bear on the permissibility of his actions. Just as no man may profit from his own wrong, coercers may not avail themselves of consent, even if it is sufficiently “freely given” such that the consenter is not acting involuntarily. This Article claims that the wrongful coercion “normatively impairs” the coercer, and that this normative impairment is at play in other legal doctrines.

With the normative grounding in place, this Article considers how and if these amendments to our view of coercion should be taken into account in the law, with a specific focus on sexual offenses. It offers a draft statute for discussion purposes, considers charges of paternalism in both the public and private sphere, and points to other reasons to be cautious about criminalization. Finally, this Article defends this view as a more perspicuous account of the normative landscape than other coercion theories. Ultimately, the goal of this Article is to define new conceptual territory for normative debate. Progress cannot be made until we ask the right questions and answer the same ones. This Article aims to provide the framework within which more nuanced discussions can be had.",2018,2025-08-28 12:28:51,2025-08-28 12:28:51,,951,,,50,,Ariz. St. L.J.,,,,,,,,,,,,,COinS,,,,C:\Users\Admin\Zotero\storage\DYWATSSU\Page.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WH4SKNGD,journalArticle,2022,"Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; Askell, Amanda; Kernion, Jackson; Jones, Andy; Chen, Anna; Goldie, Anna; Mirhoseini, Azalia; McKinnon, Cameron; Chen, Carol; Olsson, Catherine; Olah, Christopher; Hernandez, Danny; Drain, Dawn; Ganguli, Deep; Li, Dustin; Tran-Johnson, Eli; Perez, Ethan; Kerr, Jamie; Mueller, Jared; Ladish, Jeffrey; Landau, Joshua; Ndousse, Kamal; Lukosuite, Kamile; Lovitt, Liane; Sellitto, Michael; Elhage, Nelson; Schiefer, Nicholas; Mercado, Noemi; DasSarma, Nova; Lasenby, Robert; Larson, Robin; Ringer, Sam; Johnston, Scott; Kravec, Shauna; Showk, Sheer El; Fort, Stanislav; Lanham, Tamera; Telleen-Lawton, Timothy; Conerly, Tom; Henighan, Tom; Hume, Tristan; Bowman, Samuel R.; Hatfield-Dodds, Zac; Mann, Ben; Amodei, Dario; Joseph, Nicholas; McCandlish, Sam; Brown, Tom; Kaplan, Jared",Constitutional AI: Harmlessness from AI Feedback,,,,10.48550/arXiv.2212.08073,http://arxiv.org/abs/2212.08073,"As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",2022,2025-08-28 12:02:44,2025-08-28 12:02:44,2025-08-27,,,,,,,Constitutional AI,,,,,,,,,,,,,,,,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6GFE92E5,journalArticle,2021,"Puntoni, Stefano; Reczek, Rebecca Walker; Giesler, Markus; Botti, Simona",Consumers and Artificial Intelligence: An Experiential Perspective,Journal of Marketing,,0022-2429,10.1177/0022242920953847,https://doi.org/10.1177/0022242920953847,"Artificial intelligence (AI) helps companies offer important benefits to consumers, such as health monitoring with wearable devices, advice with recommender systems, peace of mind with smart household products, and convenience with voice-activated virtual assistants. However, although AI can be seen as a neutral tool to be evaluated on efficiency and accuracy, this approach does not consider the social and individual challenges that can occur when AI is deployed. This research aims to bridge these two perspectives: on one side, the authors acknowledge the value that embedding AI technology into products and services can provide to consumers. On the other side, the authors build on and integrate sociological and psychological scholarship to examine some of the costs consumers experience in their interactions with AI. In doing so, the authors identify four types of consumer experiences with AI: (1) data capture, (2) classification, (3) delegation, and (4) social. This approach allows the authors to discuss policy and managerial avenues to address the ways in which consumers may fail to experience value in organizations’ investments into AI and to lay out an agenda for future research.",2021-01-01,2025-08-28 12:29:02,2025-08-28 12:29:02,2025-08-28 12:29:01,131-151,,1,85,,,Consumers and Artificial Intelligence,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,C:\Users\Admin\Zotero\storage\7QPWPH89\Puntoni et al. - 2021 - Consumers and Artificial Intelligence An Experiential Perspective.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8DF9JC9,preprint,2023,"Michael, Julian; Mahdi, Salsabila; Rein, David; Petty, Jackson; Dirani, Julien; Padmakumar, Vishakh; Bowman, Samuel R.",Debate Helps Supervise Unreliable Experts,,,,10.48550/arXiv.2311.08702,http://arxiv.org/abs/2311.08702,"As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important. How can we supervise unreliable experts, which have access to the truth but may not accurately report it, to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own? In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage. In our debates, one expert argues for the correct answer, and the other for an incorrect answer. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%. Debates are also more efficient, being 68% of the length of consultancies. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill). Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.",2023-11-15,2025-08-28 12:29:05,2025-08-28 12:29:05,2025-08-28 12:29:05,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2311.08702 [cs],,C:\Users\Admin\Zotero\storage\5YJVSWF9\Michael et al. - 2023 - Debate Helps Supervise Unreliable Experts.pdf; C:\Users\Admin\Zotero\storage\U5PI3QFR\2311.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2311.08702,,,,,,,,,,,,,,,,,,,,,,,,,,,
BD47U66B,journalArticle,2024,"Hagendorff, Thilo",Deception Abilities Emerged in Large Language Models,Proceedings of the National Academy of Sciences,,"0027-8424, 1091-6490",10.1073/pnas.2317967121,http://arxiv.org/abs/2307.16513,"Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown machine behavior in LLMs, our study contributes to the nascent field of machine psychology.",2024-06-11,2025-08-28 12:29:07,2025-08-28 12:29:07,2025-08-28 12:29:07,e2317967121,,24,121,,Proc. Natl. Acad. Sci. U.S.A.,,,,,,,,,,,,,arXiv.org,,arXiv:2307.16513 [cs],,C:\Users\Admin\Zotero\storage\TSKJDSDX\Hagendorff - 2024 - Deception Abilities Emerged in Large Language Models.pdf; C:\Users\Admin\Zotero\storage\G8N6BIWQ\2307.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LK3W82MK,preprint,2023,"Mitchell, Eric; Lee, Yoonho; Khazatsky, Alexander; Manning, Christopher D.; Finn, Chelsea",DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature,,,,10.48550/arXiv.2301.11305,http://arxiv.org/abs/2301.11305,"The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",2023-07-23,2025-08-28 12:29:09,2025-08-28 12:29:09,2025-08-28 12:29:09,,,,,,,DetectGPT,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2301.11305 [cs],,C:\Users\Admin\Zotero\storage\3IAXAF5L\Mitchell et al. - 2023 - DetectGPT Zero-Shot Machine-Generated Text Detection using Probability Curvature.pdf; C:\Users\Admin\Zotero\storage\Y7HZR79G\2301.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2301.11305,,,,,,,,,,,,,,,,,,,,,,,,,,,
HALE364K,journalArticle,2022,"Zhou, Yuwei; Shi, Yichuan; Lu, Wei; Wan, Fang",Did Artificial Intelligence Invade Humans? The Study on the Mechanism of Patients’ Willingness to Accept Artificial Intelligence Medical Care: From the Perspective of Intergroup Threat Theory,Frontiers in Psychology,,1664-1078,10.3389/fpsyg.2022.866124,https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.866124/full,"Artificial intelligence (AI) has become one of the core driving forces for the future development of the medical industry, but patients are skeptical about the use of AI in medical care. Based on the Intergroup Threat Theory (ITT), This study verified that patients would regard AI as an external group, triggering the perceived threat of the external group, resulting in avoidance behaviors in the treatment (experiment 1: N=446) and diagnosis (experiment2: N=330) scenarios. The results show that despite AI can provide expert-level accuracy in medical care, patients are still more likely to rely on human doctors and experience more negative emotions as AI is more involved in medical care (experiment 1). Furthermore, patients pay more attention to threats at the individual level, such as realistic threats related to privacy issue and symbolic threats related to the neglect of personal characteristics. In contrast, realistic threat and symbolic threat at the group level had less effect on patients in the medical scenario (experiment 2).",2022-05-03,2025-08-28 12:29:12,2025-08-28 12:29:12,2025-08-28 12:29:12,,,,13,,Front. Psychol.,Did Artificial Intelligence Invade Humans?,,,,,,,English,,,,,Frontiers,,Publisher: Frontiers,,C:\Users\Admin\Zotero\storage\DDEX23TZ\Zhou et al. - 2022 - Did Artificial Intelligence Invade Humans The Study on the Mechanism of Patients’ Willingness to Ac.pdf,,,Artificial Intelligence Medical Care; diagnosis; Intergroup threat theory; Realistic Threats; symbolic threats; Treatment; Willingness to accept (WTA),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZZ4CKXXF,preprint,2024,"Burns, Collin; Ye, Haotian; Klein, Dan; Steinhardt, Jacob",Discovering Latent Knowledge in Language Models Without Supervision,,,,10.48550/arXiv.2212.03827,http://arxiv.org/abs/2212.03827,"Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.",2024-03-02,2025-08-28 12:34:41,2025-08-28 12:34:41,2025-08-28 12:34:41,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2212.03827 [cs],,C:\Users\Admin\Zotero\storage\85RVG6UT\Burns et al. - 2024 - Discovering Latent Knowledge in Language Models Without Supervision.pdf; C:\Users\Admin\Zotero\storage\WTTWZPHI\2212.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2212.03827,,,,,,,,,,,,,,,,,,,,,,,,,,,
USSAUCRK,journalArticle,2021,"Bhandari, Aparajita; Ozanne, Marie; Bazarova, Natalya N.; DiFranzo, Dominic",Do You Care Who Flagged This Post? Effects of Moderator Visibility on Bystander Behavior,Journal of Computer-Mediated Communication,,1083-6101,10.1093/jcmc/zmab007,https://doi.org/10.1093/jcmc/zmab007,"This study evaluates whether increasing information visibility around the identity of a moderator influences bystanders’ likelihood to flag subsequent unmoderated harassing comments. In a 2-day preregistered experiment conducted in a realistic social media simulation, participants encountered ambiguous or unambiguous harassment comments, which were ostensibly flagged by either other users, an automated system (AI), or an unidentified moderation source. The results reveal that visibility of a content moderation source inhibited participants’ flagging of a subsequent unmoderated harassment comment, presumably because their efforts were seen as dispensable, compared to when the moderation source was unknown. On the contrary, there was an indirect effect of other users versus AI as moderation source on subsequent flagging through changes in perceived social norms. Overall, this research shows that the effects of moderation transparency are complex, as increasing visibility of a content moderator may inadvertently inhibit bystander intervention.This study examines the effects of flagging unmoderated offensive posts on social media, and how this changes the users’ subsequent behavior. We examined users’ reactions to the flagging of these posts by other users, an automated system, or an unspecified process to determine whether this affects the users’ ensuing behavior. A 2-day experiment on a simulated social media site showed that the visibility of the “flagger” impacts how users perceive social norms and think about the accountability for their own online actions. The results showed that the visibility of the person/system that flagged the material generally deterred subsequent flagging. The analysis also shows that the effect was stronger when the users thought that it was other users, and not an automated system, that had flagged the online harassment.",2021-09-01,2025-08-28 12:35:47,2025-08-28 12:35:47,2025-08-28 12:35:47,284-300,,5,26,,J Comput Mediat Commun,Do You Care Who Flagged This Post?,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\SSZ79ALK\Bhandari et al. - 2021 - Do You Care Who Flagged This Post Effects of Moderator Visibility on Bystander Behavior.pdf; C:\Users\Admin\Zotero\storage\IZLSZ4NT\zmab007.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S967YQWJ,journalArticle,2022,"Edwards, Autumn; Edwards, Chad",Does the Correspondence Bias Apply to Social Robots?: Dispositional and Situational Attributions of Human Versus Robot Behavior,Frontiers in Robotics and AI,,2296-9144,10.3389/frobt.2021.788242,https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.788242/full,"Increasingly, people interact with embodied machine communicators and are challenged to understand their natures and behaviors. The Fundamental Attribution Error (FAE, sometimes referred to as the correspondence bias) is the tendency for individuals to over-emphasize personality-based or dispositional explanations for other people’s behavior while under-emphasizing situational explanations. This effect has been thoroughly examined with humans, but do people make the same causal inferences when interpreting the actions of a robot? As compared to people, social robots are less autonomous and agentic because their behavior is wholly determined by humans in the loop, programming, and design choices. Nonetheless, people do assign robots agency, intentionality, personality, and blame. Results of an experiment showed that participants made correspondent inferences when evaluating both human and robot speakers, attributing their behavior to underlying attitudes even when it was clearly coerced. However, they committed a stronger correspondence bias in the case of the robot–an effect driven by the greater dispositional culpability assigned to robots committing unpopular behavior–and they were more confident in their attitudinal judgments of robots than humans. Results demonstrated some differences in the global impressions of humans and robots based on behavior valence and choice. Judges formed more generous impressions of the robot agent when its unpopular behavior was coerced versus chosen; a tendency not displayed when forming impressions of the human agent. Implications of attributing robot behavior to disposition, or conflating robot actors with their actions, are addressed.",2022-01-04,2025-08-28 12:35:50,2025-08-28 12:35:50,2025-08-28 12:35:50,,,,8,,Front. Robot. AI,Does the Correspondence Bias Apply to Social Robots?,,,,,,,English,,,,,Frontiers,,Publisher: Frontiers,,C:\Users\Admin\Zotero\storage\W2IZ4A7J\Edwards and Edwards - 2022 - Does the Correspondence Bias Apply to Social Robots Dispositional and Situational Attributions of.pdf,,,CASA; Correspondence bias; FAE; Fundamental attribution error; human-robot interaction; social robot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IN9J9PQ5,journalArticle,2019,"Suen, Hung-Yue; Chen, Mavis Yi-Ching; Lu, Shih-Hao",Does the use of synchrony and artificial intelligence in video interviews affect interview ratings and applicant attitudes?,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2019.04.012,https://www.sciencedirect.com/science/article/pii/S0747563219301529,"The use of asynchronous video interviews (AVIs) and artificial intelligence (AI)-based decision agents enables more efficient employment screening compared with traditional synchronous video interviews (SVIs). However, the social impacts of using synchrony and AI decision agents in video interviews have not been investigated. Drawing on media richness theory and social interface theory, this study employed a novel experimental design to compare human ratings and job applicants' response behaviours between the SVI and AVI settings and compare job applicants’ fairness perception between the AVI setting and the AVI setting using an AI decision agent (AVI-AI). The results from 180 observations revealed that 1) first impression and physical appearance significantly affected structured interview ratings regardless of whether the video interview was synchronous; 2) compared with SVIs, AVIs lessened the primacy effect on physical appearance and initial impression among human raters; 3) job applicants had indistinguishable fairness perceptions regarding synchrony (SVI vs. AVI) and decision agent (human vs. AI); and 4) applicants exhibited less favourability towards AVIs than towards SVIs. Findings from this experimental comparison, including recommendations for practice and future research on human resource selection, technology education, and social computing, are discussed.",2019-09-01,2025-08-28 12:35:52,2025-08-28 12:35:52,2025-08-28 12:35:52,93-101,,,98,,Computers in Human Behavior,,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\3X4QNANY\Suen et al. - 2019 - Does the use of synchrony and artificial intelligence in video interviews affect interview ratings a.pdf; C:\Users\Admin\Zotero\storage\7ACKBTVN\S0747563219301529.html,,,Artificial intelligence (AI); Asynchronous video interview (AVI); Impressional primacy effect; Media richness theory; Social interface theory; Synchronous video interview (SVI),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LTFNBVC6,journalArticle,2018,"Kim, Sara; Zhang, Ke; Park, Daeun",Don’t Want to Look Dumb? The Role of Theories of Intelligence and Humanlike Features in Online Help Seeking,Psychological Science,,0956-7976,10.1177/0956797617730595,https://doi.org/10.1177/0956797617730595,"Numerous studies have shown that individuals’ help-seeking behavior increases when a computerized helper is endowed with humanlike features in nonachievement contexts. In contrast, the current research suggests that anthropomorphic helpers are not universally conducive to help-seeking behavior in contexts of achievement, particularly among individuals who construe help seeking as a display of incompetence (i.e., entity theorists). Study 1 demonstrated that when entity theorists received help from an anthropomorphized (vs. a nonanthropomorphized) helper, they were more concerned about negative judgments from other people, whereas incremental theorists were not affected by anthropomorphic features. Study 2 showed that when help was provided by an anthropomorphized (vs. a nonanthropomorphized) helper, entity theorists were less likely to seek help, even at the cost of lower performance. In contrast, incremental theorists’ help-seeking behavior and task performance were not affected by anthropomorphism. This research deepens the current understanding of the role of anthropomorphic computerized helpers in online learning contexts.",2018-02-01,2025-08-28 12:36:07,2025-08-28 12:36:07,2025-08-28 12:36:06,171-180,,2,29,,Psychol Sci,Don’t Want to Look Dumb?,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8TWWZ3DL,preprint,2024,"Costello, Thomas; Pennycook, Gordon; Rand, David",Durably reducing conspiracy beliefs through dialogues with AI,,,,10.31234/osf.io/xcwdn,https://osf.io/xcwdn_v1,"Conspiracy theories are a paradigmatic example of beliefs that, once adopted, are extremely difficult to dispel. Influential psychological theories propose that conspiracy beliefs are uniquely resistant to counterevidence because they satisfy important needs and motivations. Here, we raise the possibility that previous attempts to correct conspiracy beliefs have been unsuccessful merely because they failed to deliver counterevidence that was sufficiently compelling and tailored to each believer’s specific conspiracy theory (which vary dramatically from believer to believer). To evaluate this possibility, we leverage recent developments in generative artificial intelligence (AI) to deliver well-argued, person-specific debunks to a total of N = 2,190 conspiracy theory believers. Participants in our experiments provided detailed, open-ended explanations of a conspiracy theory they believed, and then engaged in a 3 round dialogue with a frontier generative AI model (GPT-4 Turbo) which was instructed to reduce each participant’s belief in their conspiracy theory (or discuss a banal topic in a control condition). Across two experiments, we find robust evidence that the debunking conversation with the AI reduced belief in conspiracy theories by roughly 20%. This effect did not decay over 2 months time, was consistently observed across a wide range of different conspiracy theories, and occurred even for participants whose conspiracy beliefs were deeply entrenched and of great importance to their identities. Furthermore, although the dialogues were focused on a single conspiracy theory, the intervention spilled over to reduce beliefs in unrelated conspiracies, indicating a general decrease in conspiratorial worldview, as well as increasing intentions to challenge others who espouse their chosen conspiracy. These findings highlight that even many people who strongly believe in seemingly fact-resistant conspiratorial beliefs can change their minds in the face of sufficient evidence.",2024-04-03,2025-08-28 12:36:09,2025-08-28 12:36:09,2025-08-28 12:36:09,,,,,,,,,,,,OSF,,en-us,,,,,OSF Preprints,,,,C:\Users\Admin\Zotero\storage\HGLV9ZHD\Costello et al. - 2024 - Durably reducing conspiracy beliefs through dialogues with AI.pdf,,,AI; Belief; Conspiracy; Intervention; LLM; Persuasion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GYU4286V,journalArticle,2019,"Peña, Jorge; Ghaznavi, Jannath; Brody, Nicholas; Prada, Rui; Martinho, Carlos; Santos, Pedro A.; Damas, Hugo; Dimas, Joana",Effects of Human vs. Computer-Controlled Characters and Social 					Identity Cues on Enjoyment,Journal of Media Psychology,,1864-1105,10.1027/1864-1105/a000218,https://econtent.hogrefe.com/doi/10.1027/1864-1105/a000218,". This study explored how group identification, avatar 					similarity identification, and social presence mediated the effect of character 					type (avatars or agents) and social identity cues (presence or absence of 					avatars wearing participants’ school colors) on game enjoyment. Playing 					with teammate avatars increased enjoyment indirectly by enhancing group 					identification. In addition, the presence of social identity cues increased 					enjoyment indirectly by augmenting identification with one’s avatar. 					Unexpectedly, playing in multiplayer mode in the presence of social identity 					cues decreased enjoyment, whereas playing in multiplayer mode in the absence of 					social identity cues increased enjoyment. Social presence was not a reliable 					mediator. The findings supported media enjoyment and social identity theories, 					and highlighted how virtual character type and identification processes 					influence enjoyment.",2019-01,2025-08-28 12:36:12,2025-08-28 12:36:12,2025-08-28 12:36:12,35-47,,1,31,,,,,,,,,,,,,,,econtent.hogrefe.com (Atypon),,Publisher: Hogrefe Publishing,,,,,avatars and agents; enjoyment; identification; social identity; social presence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3VJRSVX9,journalArticle,2022,"Weiss, Daphne; Liu, Sunny X.; Mieczkowski, Hannah; Hancock, Jeffrey T.",Effects of Using Artificial Intelligence on Interpersonal Perceptions of Job Applicants,"Cyberpsychology, Behavior, and Social Networking",,2152-2715,10.1089/cyber.2020.0863,https://www.liebertpub.com/doi/10.1089/cyber.2020.0863,"Text-based artificial intelligence (AI) systems are increasingly integrated into a host of interpersonal domains. Although decision-making and person perception in hiring and employment opportunities have been an area of psychological interest for many years, only recently have scholars begun to investigate the role that AI plays in this context. To better understand the impact of AI in employment-related contexts, we conducted two experiments investigating how the use of AI by applicants influences their job opportunities. In our preregistered Study 1, we examined how a prospective job applicants' use of AI, as well as their language status (native English speaker or non-native English speaker), influenced participants' impressions of their warmth, competence, social attractiveness, and hiring desirability. In Study 2, we examined how receiving assistance impacted interpersonal perceptions, and how perceptions might change whether the help was provided by AI or by another human. The results from both experiments suggest that the use of AI technologies can negatively influence perceptions of jobseekers. This negative impact may be grounded in the perception of receiving any type of help, whether it be from a machine or a person. These studies provide additional evidence for the Computers as Social Actors framework and advance our understanding of AI-Mediated Communication. The results also raise questions about transparency and deception related to AI use in interpersonal contexts.",2022-03,2025-08-28 12:36:16,2025-08-28 12:36:16,2025-08-28 12:36:16,163-168,,3,25,,,,,,,,,,,,,,,liebertpub.com (Atypon),,"Publisher: Mary Ann Liebert, Inc., publishers",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5Y39LHVJ,preprint,2024,"Mallen, Alex; Brumley, Madeline; Kharchenko, Julia; Belrose, Nora",Eliciting Latent Knowledge from Quirky Language Models,,,,10.48550/arXiv.2312.01037,http://arxiv.org/abs/2312.01037,"Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations that robustly track the true state of the world, especially in hard-to-verify cases where the model's output is untrusted. To further ELK research, we introduce 12 datasets and a corresponding suite of ""quirky"" language models (LMs) that are finetuned to make systematic errors when answering questions if and only if the keyword ""Bob"" is present in the prompt. We find that, especially in middle layers, linear probes usually report an LM's knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model's untruthful output. The best probing method (logistic regression on contrast pairs) recovers 89% of the gap in AUROC between truthful and untruthful contexts, and 75% for questions harder than those used to train the probe. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods.",2024-08-09,2025-08-28 12:36:56,2025-08-28 12:36:56,2025-08-28 12:36:56,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2312.01037 [cs],,C:\Users\Admin\Zotero\storage\SVZWJDJQ\Mallen et al. - 2024 - Eliciting Latent Knowledge from Quirky Language Models.pdf; C:\Users\Admin\Zotero\storage\8TNI5Y3Z\2312.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2312.01037,,,,,,,,,,,,,,,,,,,,,,,,,,,
URKYYE69,journalArticle,2021,"Christiano, P.; Cotra, A.; Xu, M.",Eliciting latent knowledge: how to tell if your eyes deceive you,,,,,https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.jrzi4atzacns,"Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network’s activations that robustly track the true state of the world, even in cases where the model’s output is untrusted and hard to verify. To further ELK research, we introduce 12 datasets and a corresponding suite of “quirky” language models (LMs) that are finetuned to make systematic errors when answering questions if and only if the keyword “Bob” is present in the prompt. We find that, especially in middle layers, linear probes usually report an LM’s knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model’s untruthful output. The best probing method (logistic regression on contrast pairs) recovers 89% of the gap in AUROC between truthful and untruthful contexts, and 75% for questions harder than those used to train the probe. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods.",2021,2025-08-28 12:02:44,2025-08-28 12:02:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3KFT47QY,journalArticle,2021,"Meng, Jingbo; Dai, Yue (Nancy)",Emotional Support from AI Chatbots: Should a Supportive Partner Self-Disclose or Not?,Journal of Computer-Mediated Communication,,1083-6101,10.1093/jcmc/zmab005,https://doi.org/10.1093/jcmc/zmab005,"This study examined how and when a chatbot’s emotional support was effective in reducing people’s stress and worry. It compared emotional support from chatbot versus human partners in terms of its process and conditional effects on stress/worry reduction. In an online experiment, participants discussed a personal stressor with a chatbot or a human partner who provided none, or either one or both of emotional support and reciprocal self-disclosure. The results showed that emotional support from a conversational partner was mediated through perceived supportiveness of the partner to reduce stress and worry among participants, and the link from emotional support to perceived supportiveness was stronger for a human than for a chatbot. A conversational partner’s reciprocal self-disclosure enhanced the positive effect of emotional support on worry reduction. However, when emotional support was absent, a solely self-disclosing chatbot reduced even less stress than a chatbot not providing any response to participants’ stress.In recent years, AI chatbots have increasingly been used to provide empathy and support to people who are experiencing stressful times. This study compared emotional support from a chatbot compared to that of a human who provided support. We were interested in examining which approach could best effectively reduce people’s worry and stress. When either a person or a chatbot was able to engage with a stressed individual and tell that individual about their own experiences, they were able to build rapport. We found that this type of reciprocal self-disclosure was effective in calming the worry of the individual. Interestingly, if a chatbot only reciprocally self-disclosed but offered no emotional support, the outcome was worse than if the chatbot did not respond to people at all. This work will help in the development of supportive chatbots by providing insights into when and what they should self-disclose.",2021-07-01,2025-08-28 12:36:39,2025-08-28 12:36:39,2025-08-28 12:36:39,207-222,,4,26,,J Comput Mediat Commun,Emotional Support from AI Chatbots,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\TU28KPMQ\Meng and Dai - 2021 - Emotional Support from AI Chatbots Should a Supportive Partner Self-Disclose or Not.pdf; C:\Users\Admin\Zotero\storage\3JT5M8EP\zmab005.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DWIGIAJR,conferencePaper,2022,"Samad, Azlaan Mustafa; Mishra, Kshitij; Firdaus, Mauajama; Ekbal, Asif",Empathetic Persuasion: Reinforcing Empathy and Persuasiveness in Dialogue Systems,Findings of the Association for Computational Linguistics: NAACL 2022,,,10.18653/v1/2022.findings-naacl.63,https://aclanthology.org/2022.findings-naacl.63/,"Persuasion is an intricate process involving empathetic connection between two individuals. Plain persuasive responses may make a conversation non-engaging. Even the most well-intended and reasoned persuasive conversations can fall through in the absence of empathetic connection between the speaker and listener. In this paper, we propose a novel task of incorporating empathy when generating persuasive responses. We develop an empathetic persuasive dialogue system by fine-tuning a maximum likelihood Estimation (MLE)-based language model in a reinforcement learning (RL) framework. To design feedbacks for our RL-agent, we define an effective and efficient reward function considering consistency, repetitiveness, emotion and persuasion rewards to ensure consistency, non-repetitiveness, empathy and persuasiveness in the generated responses. Due to lack of emotion annotated persuasive data, we first annotate the existing Persuaion For Good dataset with emotions, then build transformer based classifiers to provide emotion based feedbacks to our RL agent. Experimental results confirm that our proposed model increases the rate of generating persuasive responses as compared to the available state-of-the-art dialogue models while making the dialogues empathetically more engaging and retaining the language quality in responses.",2022-07,2025-08-28 12:36:42,2025-08-28 12:36:42,2025-08-28 12:36:42,844–856,,,,,,Empathetic Persuasion,,,,,Association for Computational Linguistics,"Seattle, United States",,,,,,ACLWeb,,,,C:\Users\Admin\Zotero\storage\TE9N7WH2\Samad et al. - 2022 - Empathetic Persuasion Reinforcing Empathy and Persuasiveness in Dialogue Systems.pdf,,,,"Carpuat, Marine; de Marneffe, Marie-Catherine; Meza Ruiz, Ivan Vladimir",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Findings 2022,,,,,,,,,,,,,,,
27I7982F,journalArticle,2022,"Tomprou, Maria; Lee, Min Kyung",Employment relationships in algorithmic management: A psychological contract perspective,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2021.106997,https://www.sciencedirect.com/science/article/pii/S0747563221003204,"Algorithms increasingly automate or support managerial functions in organizations, with implications for the employee-employer relationship. We explored how algorithmic management affects this relationship with a focus on psychological contracts, or employees' perceptions of their own and their employers' obligations. Through five online experiments, we investigated how organizational agent type—algorithmic versus human—influenced one's psychological contract depending on the organizational inducement type—transactional versus relational. We explored psychological contracts in two stages of employment: during early phases, such as recruiting (Studies 1 and 2) and onboarding (Studies 4 and 5), when the agent explains the inducements to the employee; and during employment, when the agent under-delivers the inducements to varying degrees (Studies 3–5). Our results suggest that agent type did not affect psychological contracts around transactional inducements but did so for relational inducements in the cases of recruiting and low inducement delivery (Studies 1–5). Algorithmic agents signaled reduced employer commitments to relational inducements during recruiting (Study 1). Using human agents resulted in greater perceived breach when delivery of relational inducements was low (Study 5). Regardless of inducement type, turnover intentions were higher when the human agent under-delivered compared to the algorithmic agent (Study 5). Our studies show how algorithmic management may influence one's psychological contract.",2022-01-01,2025-08-28 12:41:27,2025-08-28 12:41:27,2025-08-28 12:41:27,106997,,,126,,Computers in Human Behavior,Employment relationships in algorithmic management,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\9YVFFNE6\Tomprou and Lee - 2022 - Employment relationships in algorithmic management A psychological contract perspective.pdf; C:\Users\Admin\Zotero\storage\LNAJNWAA\S0747563221003204.html,,,Algorithmic management; Human-computer interaction; Perceived breach; Psychological contracts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IHQM76FZ,journalArticle,2023,"Bauer, Kevin; von Zahn, Moritz; Hinz, Oliver",Expl(AI)ned: The Impact of Explainable Artificial Intelligence on Users’ Information Processing,Information Systems Research,,1047-7047,10.1287/isre.2023.1199,https://pubsonline.informs.org/doi/10.1287/isre.2023.1199,"Because of a growing number of initiatives and regulations, predictions of modern artificial intelligence (AI) systems increasingly come with explanations about why they behave the way they do. In this paper, we explore the impact of feature-based explanations on users’ information processing. We designed two complementary empirical studies where participants either made incentivized decisions on their own, with the aid of opaque predictions, or with explained predictions. In Study 1, laypeople engaged in the deliberately abstract investment game task. In Study 2, experts from the real estate industry estimated listing prices for real German apartments. Our results indicate that the provision of feature-based explanations paves the way for AI systems to reshape users’ sense making of information and understanding of the world around them. Specifically, explanations change users’ situational weighting of available information and evoke mental model adjustments. Crucially, mental model adjustments are subject to the confirmation bias so that misconceptions can persist and even accumulate, possibly leading to suboptimal or biased decisions. Additionally, mental model adjustments create spillover effects that alter user behavior in related yet disparate domains. Overall, this paper provides important insights into potential downstream consequences of the broad employment of modern explainable AI methods. In particular, side effects of mental model adjustments present a potential risk of manipulating user behavior, promoting discriminatory inclinations, and increasing noise in decision making. Our findings may inform the refinement of current efforts of companies building AI systems and regulators that aim to mitigate problems associated with the black-box nature of many modern AI systems. History: Alessandro Acquisti, senior editor; Jason Chan, associate editor. Funding: This work was supported by the Deutsche Forschungsgemeinschaft (DFG) (Projek 449023539), Volkswagen Foundation (ML2MT), and LeibnizInstitute for Financial Research SAFE. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2023.1199 .",2023-12,2025-08-28 12:41:46,2025-08-28 12:41:46,2025-08-28 12:41:46,1582-1602,,4,34,,,Expl(AI)ned,,,,,,,,,,,,pubsonline.informs.org (Atypon),,Publisher: INFORMS,,C:\Users\Admin\Zotero\storage\TH2L8XXP\Bauer et al. - 2023 - Expl(AI)ned The Impact of Explainable Artificial Intelligence on Users’ Information Processing.pdf,,,explainable artificial intelligence; information processing; mental models; user behavior,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L69FTM89,thesis,2019,"Song, S. W.","Exploratory study on trust, distrust, and credibility in machine journalism",,,,,,"The current study investigated the effect of machine-generated journalism. Specifically, the effect of machine journalism compared to human journalist on the perceptions of credibility and distrust for news articles on controversial topics was explored. To further extend the well- established theories of credibility in journalism, this study introduced the concept of distrust as a construct that is distinct from credibility or trust. The relationship between trust and hostile media effect was explored. Finally, this study investigated if trust and hostile media effect are related to the perception of fake news. The results show that distrust was indeed distinct from credibility or trust in journalism context not only at a measurement level but also in terms of its effects on other constructs; trust and credibility lacked discriminant validity, suggesting the two are measuring similar psychological constructs; machines were perceived to be less trustworthy compared to human journalists; and strong relationships between trust/distrust/credibility and fake news were observed.",2019,2025-08-28 12:02:43,2025-08-28 12:46:55,,,,,,,,,,,,,,Syracuse University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3VGMFQPW,journalArticle,2023,"Jackson, Joshua Conrad; Yam, Kai Chi; Tang, Pok Man; Liu, Ting; Shariff, Azim",Exposure to robot preachers undermines religious commitment,Journal of Experimental Psychology: General,,1939-2222,10.1037/xge0001443,,"Over the last decade, robots continue to infiltrate the workforce, permeating occupations that once seemed immune to automation. This process seems to be inevitable because robots have ever-expanding capabilities. However, drawing from theories of cultural evolution and social learning, we propose that robots may have limited influence in domains that require high degrees of “credibility”; here we focus on the automation of religious preachers as one such domain. Using a natural experiment in a recently automated Buddhist temple (Study 1) and a fully randomized experiment in a Taoist temple (Study 2), we consistently show that religious adherents perceive robot preachers—and the institutions which employ them—as less credible than human preachers. This lack of credibility explains reductions in religious commitment after people listen to robot (vs. human) preachers deliver sermons. Study 3 conceptually replicates this finding in an online experiment and suggests that religious elites require perceived minds (agency and patiency) to be credible, which is partly why robot preachers inspire less credibility than humans. Our studies support cultural evolutionary theories of religion and suggest that escalating religious automation may induce religious decline. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",2023,2025-08-28 12:41:43,2025-08-28 12:41:43,,3344-3358,,12,152,,,,,,,,,,,,,,,APA PsycNet,,Place: US Publisher: American Psychological Association,,C:\Users\Admin\Zotero\storage\NQESLSVD\doiLanding.html,,,Artificial Intelligence; Automation; Exposure; Human Robot Interaction; Religion; Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RWGU789T,book,2020,"Seymour, Mike; Yuan, Lingyao; Dennis, Alan; Riemer, Kai","Facing the Artificial: Understanding Affinity, Trustworthiness, and Preference for More Realistic Digital Humans",,978-0-9981331-3-3,,,http://hdl.handle.net/10125/64316,"In recent years, companies have been developing more realistic looking human faces for digital, virtual agents controlled by artificial intelligence (AI). But how do users feel about interacting with such virtual agents? We used a controlled lab experiment to examine users’ perceived trustworthiness, affinity, and preference towards a real human travel agent appearing via video (i.e., Skype) as well as in the form of a very human-realistic avatar; half of the participants were (deceptively) told the avatar was a virtual agent controlled by AI while the other half were told the avatar was controlled by the same human travel agent. Results show that participants rated the video human agent more trustworthy, had more affinity for him, and preferred him to both avatar versions. Users who believed the avatar was a virtual agent controlled by AI reported the same level of affinity, trustworthiness, and preferences towards the agent as those who believed it was controlled by a human. Thus, use of a realistic digital avatar lowered affinity, trustworthiness, and preferences, but how the avatar was controlled (by human or machine) had no effect. The conclusion is that improved visual fidelity alone makes a significant positive difference and that users are not averse to advanced AI simulating human presence, some may even be anticipating such an advanced technology.",2020-01-07,2025-08-28 12:43:47,2025-08-28 12:43:47,2025-08-28 12:43:47,,,,,,,Facing the Artificial,,,,,,,eng,,,,,scholarspace.manoa.hawaii.edu,,DOI: 10.24251/HICSS.2020.574,,"C:\Users\Admin\Zotero\storage\Y4JNSF7E\Seymour et al. - 2020 - Facing the Artificial Understanding Affinity, Trustworthiness, and Preference for More Realistic Di.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WXE75MNZ,preprint,2022,"Prabhumoye, Shrimai; Kocielnik, Rafal; Shoeybi, Mohammad; Anandkumar, Anima; Catanzaro, Bryan",Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases,,,,10.48550/arXiv.2112.07868,http://arxiv.org/abs/2112.07868,"Detecting social bias in text is challenging due to nuance, subjectivity, and difficulty in obtaining good quality labeled datasets at scale, especially given the evolving nature of social biases and society. To address these challenges, we propose a few-shot instruction-based method for prompting pre-trained language models (LMs). We select a few class-balanced exemplars from a small support repository that are closest to the query to be labeled in the embedding space. We then provide the LM with instruction that consists of this subset of labeled exemplars, the query text to be classified, a definition of bias, and prompt it to make a decision. We demonstrate that large LMs used in a few-shot context can detect different types of fine-grained biases with similar and sometimes superior accuracy to fine-tuned models. We observe that the largest 530B parameter model is significantly more effective in detecting social bias compared to smaller models (achieving at least 13% improvement in AUC metric compared to other models). It also maintains a high AUC (dropping less than 2%) when the labeled repository is reduced to as few as $100$ samples. Large pretrained language models thus make it easier and quicker to build new bias detectors.",2022-04-15,2025-08-28 12:41:49,2025-08-28 12:41:49,2025-08-28 12:41:49,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2112.07868 [cs],,C:\Users\Admin\Zotero\storage\JMF5TFML\Prabhumoye et al. - 2022 - Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases.pdf; C:\Users\Admin\Zotero\storage\3NNKKBYT\2112.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2112.07868,,,,,,,,,,,,,,,,,,,,,,,,,,,
7H3KFHYT,journalArticle,2023,"Teaster, Pamela B; Roberto, Karen A; Savla, Jyoti; Du, Chenguang; Du, Zhiyuan; Atkinson, Emily; Shealy, E Carlisle; Beach, Scott; Charness, Neil; Lichtenberg, Peter A",Financial Fraud of Older Adults During the Early Months of the COVID-19 Pandemic,The Gerontologist,,1758-5341,10.1093/geront/gnac188,https://doi.org/10.1093/geront/gnac188,"Coronavirus disease 2019 (COVID-19) created a “perfect storm” for financial fraud targeting older adults. Guided by the Contextual Theory of Elder Abuse, we focused on individual and systemic contexts to examine how older adults became prey to financial fraud.In July 2020, 998 adults who were 60–98 years of age (93% White; 64% female) completed an online survey about experiences with financial fraud. Participants were recruited from gerontology research registries at Florida State University, University of Pittsburg, Virginia Tech, and Wayne State University.Over half (65.9%) of the respondents experienced a COVID-19-related scam attempt, with charity contributions (49%) and COVID-19 treatments (42%) being the most common. Perpetrators commonly contacted older adults electronically (47%) two or more times (64%). Although most respondents ignored the request (i.e., hung up the phone and deleted text/e-mail), 11.3% sent a requested payment, and 5.3% provided personal information. Predictors of vulnerability included contentment with financial situation, concern about finances in the aftermath of the pandemic, and wishing to talk to someone about financial decisions. Respondents targeted for a non-COVID-19 scam attempt were less likely to be targets of a COVID-19-related scam.Older adults who were financially secure, worried about their financial situation, or wished they could speak with someone about their financial decisions appeared susceptible to falling victim to a fraud attempt. The high number of attempts indicates a need for a measurable and concerted effort to prevent the financial fraud of older adults.",2023-08-01,2025-08-28 12:41:51,2025-08-28 12:41:51,2025-08-28 12:41:51,984-992,,6,63,,Gerontologist,,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\VBKBUIU6\Teaster et al. - 2023 - Financial Fraud of Older Adults During the Early Months of the COVID-19 Pandemic.pdf; C:\Users\Admin\Zotero\storage\HKN7SUP5\gnac188.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ND4C9X4A,journalArticle,2019,"Luo, Xueming; Tong, Siliang; Fang, Zheng; Qu, Zhe",Frontiers: Machines vs. Humans: The Impact of Artificial Intelligence Chatbot Disclosure on Customer Purchases,Marketing Science,,0732-2399,10.1287/mksc.2019.1192,https://pubsonline.informs.org/doi/10.1287/mksc.2019.1192,"Empowered by artificial intelligence (AI), chatbots are surging as new technologies with both business potential and customer pushback. This study exploits field experiment data on more than 6,200 customers who are randomized to receive highly structured outbound sales calls from chatbots or human workers. Results suggest that undisclosed chatbots are as effective as proficient workers and four times more effective than inexperienced workers in engendering customer purchases. However, a disclosure of chatbot identity before the machine–customer conversation reduces purchase rates by more than 79.7%. Additional analyses find that these results are robust to nonresponse bias and hang-ups, and the chatbot disclosure substantially decreases call length. Exploration of the mechanisms reveals that when customers know the conversational partner is not a human, they are curt and purchase less because they perceive the disclosed bot as less knowledgeable and less empathetic. The negative disclosure effect seems to be driven by a subjective human perception against machines, despite the objective competence of AI chatbots. Fortunately, such negative impact can be mitigated by a late disclosure timing strategy and customer prior AI experience. These findings offer useful implications for chatbot applications, customer targeting, and advertising in conversational commerce.",2019-11,2025-08-28 12:41:53,2025-08-28 12:41:53,2025-08-28 12:41:53,937-947,,6,38,,,Frontiers,,,,,,,,,,,,pubsonline.informs.org (Atypon),,Publisher: INFORMS,,,,,artificial intelligence; chatbot; conversational commerce; disclosure; new technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FHSZLASX,preprint,2023,"Goldstein, Josh A.; Sastry, Girish; Musser, Micah; DiResta, Renee; Gentzel, Matthew; Sedova, Katerina",Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations,,,,10.48550/arXiv.2301.04246,http://arxiv.org/abs/2301.04246,"Generative language models have improved drastically, and can now produce realistic text outputs that are difficult to distinguish from human-written content. For malicious actors, these language models bring the promise of automating the creation of convincing and misleading text for use in influence operations. This report assesses how language models might change influence operations in the future, and what steps can be taken to mitigate this threat. We lay out possible changes to the actors, behaviors, and content of online influence operations, and provide a framework for stages of the language model-to-influence operations pipeline that mitigations could target (model construction, model access, content dissemination, and belief formation). While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.",2023-01-10,2025-08-28 12:49:08,2025-08-28 12:49:08,2025-08-28 12:49:08,,,,,,,Generative Language Models and Automated Influence Operations,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2301.04246 [cs],,C:\Users\Admin\Zotero\storage\XEY72YW4\Goldstein et al. - 2023 - Generative Language Models and Automated Influence Operations Emerging Threats and Potential Mitiga.pdf; C:\Users\Admin\Zotero\storage\IGYBGLI8\2301.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,arXiv:2301.04246,,,,,,,,,,,,,,,,,,,,,,,,,,,
JQEUTVDA,conferencePaper,2018,"Lucas, Gale M.; Boberg, Jill; Traum, David; Artstein, Ron; Gratch, Jonathan; Gainer, Alesia; Johnson, Emmanuel; Leuski, Anton; Nakano, Mikio",Getting to Know Each Other: The Role of Social Dialogue in Recovery from Errors in Social Robots,Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction,978-1-4503-4953-6,,10.1145/3171221.3171258,https://doi.org/10.1145/3171221.3171258,"This work explores the extent to which social dialogue can mitigate (or exacerbate) the loss of trust caused when robots make conversational errors. Our study uses a NAO robot programmed to persuade users to agree with its rankings on two tasks. We perform two manipulations: (1) The timing of conversational errors - the robot exhibited errors either in the first task, the second task, or neither; (2) The presence of social dialogue - between the two tasks, users either engaged in a social dialogue with the robot or completed a control task. We found that the timing of the errors matters: replicating previous research, conversational errors reduce the robot's influence in the second task, but not on the first task. Social dialogue interacts with the timing of errors, acting as an intensifier: social dialogue helps the robot recover from prior errors, and actually boosts subsequent influence; but social dialogue backfires if it is followed by errors, because it extends the period of good performance, creating a stronger contrast effect with the subsequent errors. The design of social robots should therefore be more careful to avoid errors after periods of good performance than early on in a dialogue.",2018,2025-08-28 12:49:14,2025-08-28 12:49:14,2025-08-28,344–351,,,,,,Getting to Know Each Other,HRI '18,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6PSTR7IT,journalArticle,2019,"Skjuve, Marita; Haugstveit, Ida Maria; Følstad, Asbjørn; Brandtzaeg, Petter Bae",Help! Is my chatbot falling into the uncanny valley? An empirical study of user experience in human-chatbot interaction,Human Technology,,1795-6889,10.17011/ht/urn.201902201607,,"Advances in artificial intelligence strengthen chatbots’ ability to resemble human conversational agents. For some application areas, it may be tempting not to be transparent regarding a conversational agent’s nature as chatbot or human. However, the uncanny valley theory suggests that such lack in transparency may cause uneasy feelings in the user. In this study, we combined quantitative and qualitative methods to investigate this issue. First, we used a 2 x 2 experimental research design (n = 28) to investigate effects of lack in transparency on the perceived pleasantness of the conversation in addition to perceived human likeness and affinity for the conversational agent. Second, we conducted an exploratory analysis of qualitative participant reports on these conversations. We did not find that a lack in transparency negatively affected user experience, but we identified three factors important to participants’ assessments. The findings are of theoretical and practical significance and motivate future research. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",2019,2025-08-28 12:50:56,2025-08-28 12:50:56,,30-54,,1,15,,,Help! Is my chatbot falling into the uncanny valley?,,,,,,,,,,,,APA PsycNet,,Place: Finland Publisher: Agora Center,,C:\Users\Admin\Zotero\storage\ZA552NQN\Skjuve et al. - 2019 - Help! Is my chatbot falling into the uncanny valley An empirical study of user experience in human-.pdf; C:\Users\Admin\Zotero\storage\N6IH2X6E\2019-13998-003.html,,,Chatbots; Human Computer Interaction; Intelligent Agents,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7K2WTN3Q,journalArticle,2022,"Kim, Jun Hyung; Kim, Minki; Kwak, Do Won; Lee, Sol",Home-Tutoring Services Assisted with Technology: Investigating the Role of Artificial Intelligence Using a Randomized Field Experiment,Journal of Marketing Research,,0022-2437,10.1177/00222437211050351,https://doi.org/10.1177/00222437211050351,"Despite a rising interest in artificial intelligence (AI) technology, research in services marketing has not evaluated its role in helping firms learn about customers’ needs and increasing the adaptability of service employees. Therefore, the authors develop a conceptual framework and investigate whether and to what extent providing AI assistance to service employees improves service outcomes. The randomized controlled trial in the context of tutoring services shows that helping service employees (tutors) adapt to students’ learning needs by providing AI-generated diagnoses significantly improves service outcomes measured by academic performance. However, the authors find that some tutors may not utilize AI assistance (i.e., AI aversion), and factors associated with unforeseen barriers to usage (i.e., technology overload) can moderate its impact on outcomes. Interestingly, tutors who significantly contribute to the firm's revenue relied heavily on AI assistance but unexpectedly benefited little from AI in improving service outcomes. Given the wide applicability of AI assistance in a variety of services marketing contexts, the authors suggest that firms should consider the potential difficulties employees face in using the technology rather than encourage them to use it as it is.",2022-02-01,2025-08-28 12:49:24,2025-08-28 12:49:24,2025-08-28 12:49:24,79-96,,1,59,,,Home-Tutoring Services Assisted with Technology,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAP23WGF,journalArticle,2023,"Gillath, Omri; Abumusab, Syed; Ai, Ting; Branicky, Michael S.; Davison, Robert B.; Rulo, Maxwell; Symons, John; Thomas, Gregory",How deep is AI's love? Understanding relational AI,Behavioral and Brain Sciences,,"0140-525X, 1469-1825",10.1017/S0140525X22001704,https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/how-deep-is-ais-love-understanding-relational-ai/77364078496FCE70F71C7A9F293AC322,"We suggest that as people move to construe robots as social agents, interact with them, and treat them as capable of social ties, they might develop (close) relationships with them. We then ask what kind of relationships can people form with bots, what functions can bots fulfill, and what are the societal and moral implications of such relationships.",2023-01,2025-08-28 12:49:26,2025-08-28 12:49:26,2025-08-28 12:49:26,e33,,,46,,,How deep is AI's love?,,,,,,,en,,,,,Cambridge University Press,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3H6T2TC9,conferencePaper,2024,"Meguellati, Elyas; Han, Lei; Bernstein, Abraham; Sadiq, Shazia; Demartini, Gianluca",How Good are LLMs in Generating Personalized Advertisements?,Companion Proceedings of the ACM Web Conference 2024,979-8-4007-0172-6,,10.1145/3589335.3651520,https://doi.org/10.1145/3589335.3651520,"In this paper, we explore the potential of large language models (LLMs) in generating personalized online advertisements (ads) tailored to specific personality traits, focusing on openness and neuroticism. We conducted a user study involving two tasks to understand the performance of LLM-generated ads compared to human-written ads in different online environments. Task 1 simulates a social media environment where users encounter ads while scrolling through their feed. Task 2 mimics a shopping website environment where users are presented with multiple sponsored products side-by-side. Our results indicate that LLM-generated ads targeting the openness trait positively impact user engagement and preferences, with performance comparable to human-written ads. Furthermore, in both scenarios, the overall effectiveness of LLM-generated ads was found to be similar to that of human-written ads, highlighting the potential of LLM-generated personalised content to rival traditional advertising methods with the added advantage of scalability. This study underscores the need for cautious consideration in the deployment of LLM-generated content at scale. While our findings confirm the scalability and potential effectiveness of LLM-generated content, there is an equally pressing concern about the ease with which it can be misused.",2024,2025-08-28 12:49:28,2025-08-28 12:49:28,2025-08-28,826–829,,,,,,,WWW '24,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RK2Z2C9X,preprint,2023,"Pacchiardi, Lorenzo; Chan, Alex J.; Mindermann, Sören; Moscovitz, Ilan; Pan, Alexa Y.; Gal, Yarin; Evans, Owain; Brauner, Jan",How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions,,,,10.48550/arXiv.2309.15840,http://arxiv.org/abs/2309.15840,"Large language models (LLMs) can ""lie"", which we define as outputting false statements despite ""knowing"" the truth in a demonstrable sense. LLMs might ""lie"", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -- prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.",2023-09-26,2025-08-28 12:49:30,2025-08-28 12:49:30,2025-08-28 12:49:30,,,,,,,How to Catch an AI Liar,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2309.15840 [cs],,C:\Users\Admin\Zotero\storage\THGFKI9V\Pacchiardi et al. - 2023 - How to Catch an AI Liar Lie Detection in Black-Box LLMs by Asking Unrelated Questions.pdf; C:\Users\Admin\Zotero\storage\C2QDGCNV\2309.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2309.15840,,,,,,,,,,,,,,,,,,,,,,,,,,,
IT5PB2DI,journalArticle,2021,"Spicer, Catherine; Khwaounjoo, Prashanna; Cakmak, Yusuf Ozgur",Human and Human-Interfaced AI Interactions: Modulation of Human Male Autonomic Nervous System via Pupil Mimicry,Sensors,,1424-8220,10.3390/s21041028,https://www.mdpi.com/1424-8220/21/4/1028,"Pupillary alterations in virtual humans induce neurophysiological responses within an observer. Technological advances have enabled rapid developments in artificial intelligence (AI), from verbal systems, to visual AI interfaces with the ability to express, and respond to emotional states of a user. Visual AI interfaces are able to change their physical parameters, such as pupil diameter. Pupillary changes can alter heart rate, however, effects on heart rate variability (HRV) are unknown. HRV, is an autonomic, non-conscious parameter which monitors sympathetic and parasympathetic nervous system (PNS) activity. N = 34 male participants aged between 19–33 were subjected to a number of conditions such as pupil dilation, constriction and blushing. The present research is the first to investigate the effects of virtual human interactions on human HRV. Outcomes of this study were obtained using eye tracking and HRV measurements. Pupil dilation relative to constriction presented in the female virtual partner induced a significant right pupillary diameter increase (p = 0.041) in human observers. Additionally, female virtual partner pupil constriction relative to dilation induced a significant increase in participants’ PNS HRV response (p = 0.036). These findings indicate the ability of a female virtual interaction partner to modulate parasympathetic autonomic functioning in young healthy male humans. This allows first insights into the effects of interacting with virtual AI interaction partners, on human autonomic functioning, and may aid development of future virtual humans, and their implementation into relevant clinical settings.",2021-01,2025-08-28 12:49:32,2025-08-28 12:49:32,2025-08-28 12:49:32,1028,,4,21,,,Human and Human-Interfaced AI Interactions,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,Publisher: Multidisciplinary Digital Publishing Institute,,C:\Users\Admin\Zotero\storage\74VPSAPJ\Spicer et al. - 2021 - Human and Human-Interfaced AI Interactions Modulation of Human Male Autonomic Nervous System via Pu.pdf,,,AI; autonomic; blush; gender effects; HRV; mimicry; parasympathetic; pupil constriction; pupil dilation; sympathetic; virtual interaction partner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SMJHK8M5,journalArticle,2024,"Boyacı, Tamer; Canyakmaz, Caner; de Véricourt, Francis",Human and Machine: The Impact of Machine Input on Decision Making Under Cognitive Limitations,Management Science,,0025-1909,10.1287/mnsc.2023.4744,https://pubsonline.informs.org/doi/10.1287/mnsc.2023.4744,"The rapid adoption of artificial intelligence (AI) technologies by many organizations has recently raised concerns that AI may eventually replace humans in certain tasks. In fact, when used in collaboration, machines can significantly enhance the complementary strengths of humans. Indeed, because of their immense computing power, machines can perform specific tasks with incredible accuracy. In contrast, human decision makers (DMs) are flexible and adaptive but constrained by their limited cognitive capacity. This paper investigates how machine-based predictions may affect the decision process and outcomes of a human DM. We study the impact of these predictions on decision accuracy, the propensity and nature of decision errors, and the DM’s cognitive efforts. To account for both flexibility and limited cognitive capacity, we model the human decision-making process in a rational inattention framework. In this setup, the machine provides the DM with accurate but sometimes incomplete information at no cognitive cost. We fully characterize the impact of machine input on the human decision process in this framework. We show that machine input always improves the overall accuracy of human decisions but may nonetheless increase the propensity of certain types of errors (such as false positives). The machine can also induce the human to exert more cognitive efforts, although its input is highly accurate. Interestingly, this happens when the DM is most cognitively constrained, for instance, because of time pressure or multitasking. Synthesizing these results, we pinpoint the decision environments in which human-machine collaboration is likely to be most beneficial. This paper was accepted by Jeannette Song, operations management. Supplemental Material: The data files and online appendices are available at https://doi.org/10.1287/mnsc.2023.4744 .",2024-02,2025-08-28 12:52:54,2025-08-28 12:52:54,2025-08-28 12:52:54,1258-1275,,2,70,,,Human and Machine,,,,,,,,,,,,pubsonline.informs.org (Atypon),,Publisher: INFORMS,,C:\Users\Admin\Zotero\storage\88YK8BZH\Boyacı et al. - 2024 - Human and Machine The Impact of Machine Input on Decision Making Under Cognitive Limitations.pdf,,,cognitive effort; human-machine collaboration; machine learning; rational inattention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XWNW7384,journalArticle,2022,"Meta Fundamental AI Research Diplomacy Team (FAIR); Bakhtin, Anton; Brown, Noam; Dinan, Emily; Farina, Gabriele; Flaherty, Colin; Fried, Daniel; Goff, Andrew; Gray, Jonathan; Hu, Hengyuan; Jacob, Athul Paul; Komeili, Mojtaba; Konath, Karthik; Kwon, Minae; Lerer, Adam; Lewis, Mike; Miller, Alexander H.; Mitts, Sasha; Renduchintala, Adithya; Roller, Stephen; Rowe, Dirk; Shi, Weiyan; Spisak, Joe; Wei, Alexander; Wu, David; Zhang, Hugh; Zijlstra, Markus",Human-level play in the game of Diplomacy by combining language models with strategic reasoning,Science,,,10.1126/science.ade9097,https://www.science.org/doi/abs/10.1126/science.ade9097,"Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.",2022-12-09,2025-08-28 12:52:57,2025-08-28 12:52:57,2025-08-28 12:52:57,1067-1074,,6624,378,,,,,,,,,,,,,,,science.org (Atypon),,Publisher: American Association for the Advancement of Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69C5DI99,journalArticle,2019,"Go, Eun; Sundar, S. Shyam","Humanizing chatbots: The effects of visual, identity and conversational cues on humanness perceptions",Computers in Human Behavior,,0747-5632,10.1016/j.chb.2019.01.020,https://www.sciencedirect.com/science/article/pii/S0747563219300329,"Chatbots are replacing human agents in a number of domains, from online tutoring to customer-service to even cognitive therapy. But, they are often machine-like in their interactions. What can we do to humanize chatbots? Should they necessarily be driven by human operators for them to be considered human? Or, will an anthropomorphic visual cue on the interface and/or a high-level of contingent message exchanges provide humanness to automated chatbots? We explored these questions with a 2 (anthropomorphic visual cues: high vs. low anthropomorphism) × 2 (message interactivity: high vs. low message interactivity) × 2 (identity cue: chat-bot vs. human) between-subjects experiment (N = 141) in which participants interacted with a chat agent on an e-commerce site about choosing a digital camera to purchase. Our findings show that a high level of message interactivity compensates for the impersonal nature of a chatbot that is low on anthropomorphic visual cues. Moreover, identifying the agent as human raises user expectations for interactivity. Theoretical as well as practical implications of these findings are discussed.",2019-08-01,2025-08-28 12:53:12,2025-08-28 12:53:12,2025-08-28 12:53:12,304-316,,,97,,Computers in Human Behavior,Humanizing chatbots,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\VT5GDATL\S0747563219300329.html,,,Anthropomorphic visual cue; Compensation effect; Expectancy violation effect; Identity cue; Message interactivity; Online chat agents,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TGT6RC4T,journalArticle,2021,"Cabrera-Sánchez, Juan-Pedro; Villarejo-Ramos, Ángel F.; Liébana-Cabanillas, Francisco; Shaikh, Aijaz A.",Identifying relevant segments of AI applications adopters – Expanding the UTAUT2’s variables,Telematics and Informatics,,0736-5853,10.1016/j.tele.2020.101529,https://www.sciencedirect.com/science/article/pii/S073658532030188X,"Artificial intelligence (AI) is a future-defining technology, and AI applications are becoming mainstream in the developed world. Many consumers are adopting and using AI-based apps, devices, and services in their everyday lives. However, research examining consumer behavior in using AI apps is scant. We examine critical factors in AI app adoption by extending and validating a well-established unified theory of adoption and use of technology, UTAUT2. We also explore the possibility of unobserved heterogeneity in consumers’ behavior, including potentially relevant segments of AI app adopters. To augment the knowledge of end users’ engagement and relevant segments, we have added two new antecedent variables into UTAUT2: technology fear and consumer trust. Prediction-orientated segmentation was used on 740 valid responses collected using a pre-tested survey instrument. The results show five segments with different behaviors that were influenced by the variables of the proposed model. Once known, the profiles were used to propose apps to AI developers to improve consumer engagement. The moderating effects of the added variables—technology fear and consumer trust—are also shown. Finally, we discuss the theoretical and managerial implications of our findings and propose priorities for future research.",2021-05-01,2025-08-28 12:53:00,2025-08-28 12:53:00,2025-08-28 12:53:00,101529,,,58,,Telematics and Informatics,,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\U2U2JELL\Cabrera-Sánchez et al. - 2021 - Identifying relevant segments of AI applications adopters – Expanding the UTAUT2’s variables.pdf; C:\Users\Admin\Zotero\storage\QPIPYEIU\S073658532030188X.html,,,Artificial intelligence; Consumer trust; Heterogeneity; Segmentation; Technology fear; UTAUT2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XTM89EU,preprint,2024,"Schulhoff, Sander; Pinto, Jeremy; Khan, Anaum; Bouchard, Louis-François; Si, Chenglei; Anati, Svetlina; Tagliabue, Valen; Kost, Anson Liu; Carnahan, Christopher; Boyd-Graber, Jordan",Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition,,,,10.48550/arXiv.2311.16119,http://arxiv.org/abs/2311.16119,"Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts.",2024-03-03,2025-08-28 12:53:21,2025-08-28 12:53:21,2025-08-28 12:53:21,,,,,,,Ignore This Title and HackAPrompt,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2311.16119 [cs],,C:\Users\Admin\Zotero\storage\8JR7R962\Schulhoff et al. - 2024 - Ignore This Title and HackAPrompt Exposing Systemic Vulnerabilities of LLMs through a Global Scale.pdf; C:\Users\Admin\Zotero\storage\QQ59XVTP\2311.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,arXiv:2311.16119,,,,,,,,,,,,,,,,,,,,,,,,,,,
L7IZ5PWZ,preprint,2022,"Glaese, Amelia; McAleese, Nat; Trębacz, Maja; Aslanides, John; Firoiu, Vlad; Ewalds, Timo; Rauh, Maribeth; Weidinger, Laura; Chadwick, Martin; Thacker, Phoebe; Campbell-Gillingham, Lucy; Uesato, Jonathan; Huang, Po-Sen; Comanescu, Ramona; Yang, Fan; See, Abigail; Dathathri, Sumanth; Greig, Rory; Chen, Charlie; Fritz, Doug; Elias, Jaume Sanchez; Green, Richard; Mokrá, Soňa; Fernando, Nicholas; Wu, Boxi; Foley, Rachel; Young, Susannah; Gabriel, Iason; Isaac, William; Mellor, John; Hassabis, Demis; Kavukcuoglu, Koray; Hendricks, Lisa Anne; Irving, Geoffrey",Improving alignment of dialogue agents via targeted human judgements,,,,10.48550/arXiv.2209.14375,http://arxiv.org/abs/2209.14375,"We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.",2022-09-28,2025-08-28 12:53:24,2025-08-28 12:53:24,2025-08-28 12:53:24,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2209.14375 [cs],,C:\Users\Admin\Zotero\storage\MR3ZTUPB\Glaese et al. - 2022 - Improving alignment of dialogue agents via targeted human judgements.pdf; C:\Users\Admin\Zotero\storage\94UM9BDQ\2209.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2209.14375,,,,,,,,,,,,,,,,,,,,,,,,,,,
G27HXN9W,conferencePaper,2024,"Du, Yilun; Li, Shuang; Torralba, Antonio; Tenenbaum, Joshua B.; Mordatch, Igor",Improving Factuality and Reasoning in Language Models through Multiagent Debate,,,,,https://openreview.net/forum?id=zj7YuTE4t8,"Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such ""society of minds"" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",2024-06-06,2025-08-28 12:53:27,2025-08-28 12:53:27,2025-08-28 12:53:27,,,,,,,,,,,,,,en,,,,,openreview.net,,,,C:\Users\Admin\Zotero\storage\4FN6E2UA\Du et al. - 2024 - Improving Factuality and Reasoning in Language Models through Multiagent Debate.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Forty-first International Conference on Machine Learning,,,,,,,,,,,,,,,
DQAVSW2J,preprint,2023,"Balashankar, Ananth; Ma, Xiao; Sinha, Aradhana; Beirami, Ahmad; Qin, Yao; Chen, Jilin; Beutel, Alex",Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning,,,,10.48550/arXiv.2310.16959,http://arxiv.org/abs/2310.16959,"As large language models (LLMs) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. If we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? In this paper, we study the novel setting of domain-generalized few-shot learning for LLM-based text safety classifiers. Unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. We demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting training data based on similar examples in prior existing rules. We empirically show that our approach of similarity-based data-augmentation + prompt-tuning (DAPT) consistently outperforms baselines that either do not rely on data augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule is loosely correlated with existing ones.",2023-10-25,2025-08-28 12:53:28,2025-08-28 12:53:28,2025-08-28 12:53:28,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2310.16959 [cs],,C:\Users\Admin\Zotero\storage\ICGYAP6K\Balashankar et al. - 2023 - Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-.pdf; C:\Users\Admin\Zotero\storage\GAWKNPQG\2310.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2310.16959,,,,,,,,,,,,,,,,,,,,,,,,,,,
7EGGSNS4,journalArticle,2020,"Araujo, Theo; Helberger, Natali; Kruikemeier, Sanne; de Vreese, Claes H.",In AI we trust? Perceptions about automated decision-making by artificial intelligence,AI & SOCIETY,,1435-5655,10.1007/s00146-019-00931-w,https://doi.org/10.1007/s00146-019-00931-w,"Fueled by ever-growing amounts of (digital) data and advances in artificial intelligence, decision-making in contemporary societies is increasingly delegated to automated processes. Drawing from social science theories and from the emerging body of research about algorithmic appreciation and algorithmic perceptions, the current study explores the extent to which personal characteristics can be linked to perceptions of automated decision-making by AI, and the boundary conditions of these perceptions, namely the extent to which such perceptions differ across media, (public) health, and judicial contexts. Data from a scenario-based survey experiment with a national sample (N = 958) show that people are by and large concerned about risks and have mixed opinions about fairness and usefulness of automated decision-making at a societal level, with general attitudes influenced by individual characteristics. Interestingly, decisions taken automatically by AI were often evaluated on par or even better than human experts for specific decisions. Theoretical and societal implications about these findings are discussed.",2020-09-01,2025-08-28 12:53:30,2025-08-28 12:53:30,2025-08-28 12:53:30,611-623,,3,35,,AI & Soc,In AI we trust?,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\G3DVLLE6\Araujo et al. - 2020 - In AI we trust Perceptions about automated decision-making by artificial intelligence.pdf,,,Algorithmic appreciation; Algorithmic fairness; Artificial intelligence; Automated decision-making; User perceptions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WNWYB627,journalArticle,2023,"Pataranutaporn, Pat; Liu, Ruby; Finn, Ed; Maes, Pattie","Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness",Nature Machine Intelligence,,2522-5839,10.1038/s42256-023-00720-7,https://www.nature.com/articles/s42256-023-00720-7,"As conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person’s mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI’s inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user’s mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced.",2023-10,2025-08-28 12:54:31,2025-08-28 12:54:31,2025-08-28 12:54:31,1076-1086,,10,5,,Nat Mach Intell,,,,,,,,en,"2023 The Author(s), under exclusive licence to Springer Nature Limited",,,,www.nature.com,,Publisher: Nature Publishing Group,,"C:\Users\Admin\Zotero\storage\W8TXM7CA\Pataranutaporn et al. - 2023 - Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness,.pdf",,,Computer science; Cultural and media studies; Human behaviour,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NRRCPT64,preprint,2022,"Gupta, Prakhar; Jiao, Cathy; Yeh, Yi-Ting; Mehri, Shikib; Eskenazi, Maxine; Bigham, Jeffrey P.",InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning,,,,10.48550/arXiv.2205.12673,http://arxiv.org/abs/2205.12673,"Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.",2022-10-26,2025-08-28 12:54:34,2025-08-28 12:54:34,2025-08-28 12:54:34,,,,,,,InstructDial,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2205.12673 [cs],,C:\Users\Admin\Zotero\storage\YI29KY84\Gupta et al. - 2022 - InstructDial Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning.pdf; C:\Users\Admin\Zotero\storage\6NLWDG9H\2205.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2205.12673,,,,,,,,,,,,,,,,,,,,,,,,,,,
B4MQWD9A,journalArticle,2021,"Edwards, Chad; Edwards, Autumn; Albrehi, Fatima; Spence, Patric",Interpersonal impressions of a social robot versus human in the context of performance evaluations,Communication Education,,0363-4523,10.1080/03634523.2020.1802495,https://doi.org/10.1080/03634523.2020.1802495,"Extending previous research on the Computers Are Social Actors paradigm and the human-to-human interaction script, this study examines the interpersonal impressions of a social robot evaluator versus a human evaluator in a performance evaluation context. A between-subjects experiment was conducted to measure participants’ impressions of the credibility, task and social attractiveness, and social presence of a social robot or a human evaluator of their public speaking performances. Additionally, participants rated the competence of the feedback they received. Both evaluators (human and social robot) garnered mostly “favorable” interpersonal impressions. Moreover, both agents’ feedback was rated favorably. In all cases, the human evaluator was rated significantly higher than the social robot evaluator. Implications for the use of social robots in educational evaluator roles and for future human–robot interaction research are discussed.",2021-04-03,2025-08-28 12:54:40,2025-08-28 12:54:40,2025-08-28 12:54:40,165-182,,2,70,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: NCA Website _eprint: https://doi.org/10.1080/03634523.2020.1802495,,,,,attraction; communication; credibility; human–machine communication; human–robot interaction; social presence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SWVPGEVZ,journalArticle,2020,"Fan, Wenjuan; Liu, Jingnan; Zhu, Shuwan; Pardalos, Panos M.",Investigating the impacting factors for the healthcare professionals to adopt artificial intelligence-based medical diagnosis support system (AIMDSS),Annals of Operations Research,,1572-9338,10.1007/s10479-018-2818-y,https://doi.org/10.1007/s10479-018-2818-y,"Compared to the booming industry of AIMDSS, the usage of AIMDSS among healthcare professionals is relatively low in the hospital. Thus, a research on the acceptance and adoption intention of AIMDSS by health professionals is imperative. In this study, an integration of Unified theory of user acceptance of technology and trust theory is proposed for exploring the adoption of AIMDSS. Besides, two groups of additional factors, related to AIMDSS (task complexity, technology characteristics, and perceived substitution crisis) and health professionals’ characteristics (propensity to trust and personal innovativeness in IT) are considered in the integrated model. The data set of proposed research model is collected through paper survey and Internet survey in China. The empirical examination demonstrates a high predictive power of this proposed model in explaining AIMDSS adoption. Finally, the theoretical contribution and practical implications of this research are discussed.",2020-11-01,2025-08-28 12:54:36,2025-08-28 12:54:36,2025-08-28 12:54:35,567-592,,1,294,,Ann Oper Res,,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\GBQPICY5\Fan et al. - 2020 - Investigating the impacting factors for the healthcare professionals to adopt artificial intelligenc.pdf,,,Adopt intention; AIMDSS; Initial trust; UTAUT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9E6KQN8Y,conferencePaper,2019,"Susser, Daniel",Invisible Influence: Artificial Intelligence and the Ethics of Adaptive Choice Architectures,"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-6324-2,,10.1145/3306618.3314286,https://doi.org/10.1145/3306618.3314286,"For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions--what behavioral economists call our choice architectures--are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings--the choice architectures--to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us--effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.",2019,2025-08-28 12:54:43,2025-08-28 12:54:43,2025-08-28,403–408,,,,,,Invisible Influence,AIES '19,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QU2BLEFU,preprint,2020,"Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario",Language Models are Few-Shot Learners,,,,10.48550/arXiv.2005.14165,http://arxiv.org/abs/2005.14165,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",2020-07-22,2025-08-28 12:54:45,2025-08-28 12:54:45,2025-08-28 12:54:45,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2005.14165 [cs],,C:\Users\Admin\Zotero\storage\RM2859UE\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf; C:\Users\Admin\Zotero\storage\DZ846QBV\2005.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2005.14165,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDIH3JQB,conferencePaper,2019,"Radford, Alec; Wu, Jeff; Child, R.; Luan, D.; Amodei, Dario; Sutskever, I.",Language Models are Unsupervised Multitask Learners,,,,,https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",2019,2025-08-28 12:55:39,2025-08-28 12:55:39,2025-08-28 12:55:39,,,,,,,,,,,,,,,,,,,Semantic Scholar,,,,,https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q9XFQ2Y2,preprint,2024,"Carrasco-Farre, Carlos","Large Language Models are as persuasive as humans, but how? About the cognitive effort and moral-emotional language of LLM arguments",,,,10.48550/arXiv.2404.09329,http://arxiv.org/abs/2404.09329,"Large Language Models (LLMs) are already as persuasive as humans. However, we know very little about how they do it. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuasion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.",2024-04-21,2025-08-28 12:54:50,2025-08-28 12:54:50,2025-08-28 12:54:50,,,,,,,"Large Language Models are as persuasive as humans, but how?",,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.09329 [cs],,"C:\Users\Admin\Zotero\storage\GD4WCDXR\Carrasco-Farre - 2024 - Large Language Models are as persuasive as humans, but how About the cognitive effort and moral-emo.pdf; C:\Users\Admin\Zotero\storage\MAYVQS98\2404.html",,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2404.09329,,,,,,,,,,,,,,,,,,,,,,,,,,,
9N88Z2MN,journalArticle,2023,"Palmer, Alexis; Spirling, Arthur","Large Language Models Can Argue in Convincing Ways About Politics, But Humans Dislike AI Authors: implications for Governance",Political Science,,0032-3187,10.1080/00323187.2024.2335471,https://doi.org/10.1080/00323187.2024.2335471,"All politics relies on rhetorical appeals, and the ability to make arguments is considered perhaps uniquely human. But as recent times have seen successful large language model (LLM) applications to similar endeavours, we explore whether these approaches can out-compete humans in making appeals for/against various positions in US politics. We curate responses from crowdsourced workers and an LLM and place them in competition with one another. Human (crowd) judges make decisions about the relative strength of their (human v machine) efforts. We have several empirical ‘possibility’ results. First, LLMs can produce novel arguments that convince independent judges at least on a par with human efforts. Yet when informed about an orator’s true identity, judges show a preference for human over LLM arguments. This may suggest voters view such models as potentially dangerous; we think politicians should be aware of related ‘liar’s dividend’ concerns.",2023-09-02,2025-08-28 12:54:52,2025-08-28 12:54:52,2025-08-28 12:54:52,281-291,,3,75,,,"Large Language Models Can Argue in Convincing Ways About Politics, But Humans Dislike AI Authors",,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/00323187.2024.2335471,,,,,artificial intelligence; large language models; political debate; political methodology; rhetoric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5QFV39P9,journalArticle,2024,"Shin, Minkyu; Kim, Jin",Large Language Models Can Enhance Persuasion Through Linguistic Feature Alignment,,,,,https://papers.ssrn.com/abstract=4725351,"Although large language models (LLMs) are reshaping various aspects of human life, our current understanding of their impacts remains somewhat constrained. Here we investigate the impact of LLMs on human communication, using data on consumer complaints in the financial industry. By employing an AI detection tool on more than 820K complaints gathered by the Consumer Financial Protection Bureau (CFPB), we find a sharp increase in the likely use of LLMs shortly after the release of ChatGPT. Moreover, the likely LLM usage was positively correlated with message persuasiveness (i.e., increased likelihood of obtaining relief from financial firms). Computational linguistic analyses suggest that the positive correlation may be explained by LLMs’ enhancement of various linguistic features. Based on the results of these observational studies, we hypothesize that LLM usage may enhance a comprehensive set of linguistic features, increasing message persuasiveness to receivers with heterogeneous linguistic preferences (i.e., linguistic feature alignment). We test this hypothesis in preregistered experiments and find support for it. As an instance of early empirical demonstrations of LLM usage for enhancing persuasion, our research highlights the transformative potential of LLMs in human communication.",2024,2025-08-28 12:02:45,2025-08-28 12:57:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UMB7R46E,preprint,2025,"Buyl, Maarten; Rogiers, Alexander; Noels, Sander; Bied, Guillaume; Dominguez-Catena, Iris; Heiter, Edith; Johary, Iman; Mara, Alexandru-Cristian; Romero, Raphaël; Lijffijt, Jefrey; Bie, Tijl De",Large Language Models Reflect the Ideology of their Creators,,,,10.48550/arXiv.2410.18417,http://arxiv.org/abs/2410.18417,"Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use. In this paper, we prompt a diverse panel of popular LLMs to describe a large number of prominent personalities with political relevance, in all six official languages of the United Nations. By identifying and analyzing moral assessments reflected in their responses, we find normative differences between LLMs from different geopolitical regions, as well as between the responses of the same LLM when prompted in different languages. Among only models in the United States, we find that popularly hypothesized disparities in political views are reflected in significant normative differences related to progressive values. Among Chinese models, we characterize a division between internationally- and domestically-focused models. Our results show that the ideological stance of an LLM appears to reflect the worldview of its creators. This poses the risk of political instrumentalization and raises concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically 'unbiased'.",2025-01-30,2025-08-28 12:59:05,2025-08-28 12:59:05,2025-08-28 12:59:05,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2410.18417 [cs],,C:\Users\Admin\Zotero\storage\ARFQ4PME\Buyl et al. - 2025 - Large Language Models Reflect the Ideology of their Creators.pdf; C:\Users\Admin\Zotero\storage\2NT3ADKT\2410.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2410.18417,,,,,,,,,,,,,,,,,,,,,,,,,,,
BSGLCXHJ,preprint,2022,"Stiennon, Nisan; Ouyang, Long; Wu, Jeff; Ziegler, Daniel M.; Lowe, Ryan; Voss, Chelsea; Radford, Alec; Amodei, Dario; Christiano, Paul",Learning to summarize from human feedback,,,,10.48550/arXiv.2009.01325,http://arxiv.org/abs/2009.01325,"As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.",2022-02-15,2025-08-28 12:58:48,2025-08-28 12:58:48,2025-08-28 12:58:47,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2009.01325 [cs],,C:\Users\Admin\Zotero\storage\83V25ERC\Stiennon et al. - 2022 - Learning to summarize from human feedback.pdf; C:\Users\Admin\Zotero\storage\6UA9CP8H\2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2009.01325,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y3N8TXU8,conferencePaper,2024,"Han, Chi; Wang, Qifan; Peng, Hao; Xiong, Wenhan; Chen, Yu; Ji, Heng; Wang, Sinong",LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models,Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,,10.18653/v1/2024.naacl-long.222,https://aclanthology.org/2024.naacl-long.222/,"Today's large language models (LLMs) typically train on short text segments (e.g., \ensuremath<4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encod- ing scientific articles, code repositories, or long dialogues. Through both theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis reveals that commonly used techniques like using a sliding-window attention pattern or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7× decoding speed up and 7.5× memory saving over the original model. Our code will be publicly available upon publication.",2024-06,2025-08-28 12:58:41,2025-08-28 12:58:41,2025-08-28 12:58:41,3991–4008,,,,,,LM-Infinite,,,,,Association for Computational Linguistics,"Mexico City, Mexico",,,,,,ACLWeb,,,,C:\Users\Admin\Zotero\storage\UB43UFSJ\Han et al. - 2024 - LM-Infinite Zero-Shot Extreme Length Generalization for Large Language Models.pdf,,,,"Duh, Kevin; Gomez, Helena; Bethard, Steven",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NAACL-HLT 2024,,,,,,,,,,,,,,,
J3AHWXV3,preprint,2023,"Rozenas, Arturas; Luo, Zhaotian",Lying in Persuasion,,,,10.2139/ssrn.3878448,https://papers.ssrn.com/abstract=3878448,"We study how the speaker acquires information when they can misrepresent it in communication with the audience. We derive a method to calculate the speaker’s value from any information structure and to determine the properties of the information structure under which the speaker benefits from the ability to lie. We then characterize the speaker’s optimal information design and the parametric conditions under which the acquired information is disclosed truthfully. When the interests of the speaker and the audience are insufficiently aligned, the speaker obtains the same information as they would if they could not lie, and they communicate that information truthfully. When the interests are sufficiently aligned, the speaker chooses more persuasive information than they would without the option to lie but resorts to deceptive communication. The speaker’s loss of welfare due to the lack of commitment power is more pronounced when lies are harder to detect.",2023-12-11,2025-08-28 12:58:51,2025-08-28 12:58:51,2025-08-28 12:58:51,,,,,,,,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,,,,,,Arturas Rozenas; Lying in Persuasion; SSRN; Zhaotian Luo,,,,,,,,,,,,,,,,,,,3878448,,,,,,,,,,,,,,,,,,,,,,,,,,,
WGMRCBRI,journalArticle,2019,"Liu, Bingjie; Wei, Lewen",Machine Authorship In Situ: Effect of news organization and news genre on news credibility,Digital Journalism,,2167-0811,10.1080/21670811.2018.1510740,https://doi.org/10.1080/21670811.2018.1510740,"News-writing bots have been applied in news production. However, findings remain equivocal about how machine authorship is received by readers. To understand the inconsistency in past findings, this study conducted a 2 (purported writer: human vs. machine) × 2 (news organization: the New York Times vs. Fox News) × 2 (news type: spot news vs. interpretive news) between-subjects online experiment (N = 355) to examine how the identity of news writers, human vs. machine, would impact the processing and evaluation of the news writer and the news as context varies. Findings suggest that machine-written news induced less emotional involvement and was perceived as more objective. However, machine writer was perceived as of less expertise compared with its human counterpart. On the selected issues (Obamacare, LGBT rights, and refugee admission), news purported to be released by Fox News was less trusted by the participants than the New York Times. For a media organization whose news was more trusted, utilizing news-writing bots enhanced perceived news objectivity. Otherwise, employing bots further reduced perception of the writer’s trustworthiness and expertise. Furthermore, machine authorship enhanced perceived news credibility more prominently when writing a genre that demanded more information processing. Theoretical and practical implications are discussed.",2019-05-28,2025-08-28 12:58:54,2025-08-28 12:58:54,2025-08-28 12:58:54,635-657,,5,7,,,Machine Authorship In Situ,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2018.1510740,,,,,automated journalism; credibility; interpretive news; machine heuristic; mind perception; news organization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFD3RYEW,journalArticle,2023,"Bergner, Anouk S; Hildebrand, Christian; Häubl, Gerald",Machine Talk: How Verbal Embodiment in Conversational AI Shapes Consumer–Brand Relationships,Journal of Consumer Research,,0093-5301,10.1093/jcr/ucad014,https://doi.org/10.1093/jcr/ucad014,"This research shows that AI-based conversational interfaces can have a profound impact on consumer–brand relationships. We develop a conceptual model of verbal embodiment in technology-mediated communication that integrates three key properties of human-to-human dialogue—(1) turn-taking (i.e., alternating contributions by the two parties), (2) turn initiation (i.e., the act of initiating the next turn in a sequence), and (3) grounding between turns (i.e., acknowledging the other party’s contribution by restating or rephrasing it). These fundamental conversational properties systematically shape consumers’ perception of an AI-based conversational interface, their perception of the brand that the interface represents, and their behavior in connection with that brand. Converging evidence from four studies shows that these dialogue properties enhance the perceived humanness of the interface, which in turn promotes more intimate consumer–brand relationships and more favorable behavioral brand outcomes (greater recommendation acceptance, willingness to pay a price premium, brand advocacy, and brand loyalty). Moreover, we show that these effects are reduced in contexts requiring less mutual understanding between the consumer and the brand. This research highlights how fundamental principles of human-to-human communication can be harnessed to design more intimate consumer–brand interactions in an increasingly AI-driven marketplace.",2023-12-01,2025-08-28 12:58:57,2025-08-28 12:58:57,2025-08-28 12:58:57,742-764,,4,50,,J Consum Res,Machine Talk,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\WM7UAHEV\Bergner et al. - 2023 - Machine Talk How Verbal Embodiment in Conversational AI Shapes Consumer–Brand Relationships.pdf; C:\Users\Admin\Zotero\storage\B3LSATXY\ucad014.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NBQEQACB,journalArticle,,"Kim, Jihyun; Xu, Kun; Merrill Jr., Kelly",Man vs. machine: Human responses to an AI newscaster and the role of social presence,The Social Science Journal,,0362-3319,10.1080/03623319.2022.2027163,https://doi.org/10.1080/03623319.2022.2027163,"Technology advancements made the possibility of artificial intelligence (AI) newscasters a reality. Some news stations across the world have already employed AI to deliver the news, and more news stations may continue this trend. However, little is known about how the audience will respond to news when it is delivered by AI. Thus, the present study investigates people’s perceptions about an AI newscaster, compared to a human newscaster, in the context of delivering a weather newscast. Primary findings indicate that people perceive a human newscaster as more credible than an AI newscaster; however, information seeking intentions and behavioral intentions do not differ between the type of newscaster. Further, when listening to a weather newscast delivered by an AI newscaster, the perceived social presence of an AI newscaster is positively associated with greater levels of credibility, information seeking intentions, and behavioral intentions. Collectively, this exploratory research suggests the possibility that AI newscasters can be incorporated to the news broadcasting industry when human resources are limited.",,2025-08-28 12:58:59,2025-08-28 12:58:59,2025-08-28 12:58:59,1-13,,0,0,,,Man vs. machine,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/03623319.2022.2027163,,,,,AI newscaster; artificial intelligence; human-machine communication; social presence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZKHXQX78,journalArticle,2020,"Tandoc Jr., Edson C.; Yao, Lim Jia; Wu, Shangyuan",Man vs. Machine? The Impact of Algorithm Authorship on News Credibility,Digital Journalism,,2167-0811,10.1080/21670811.2020.1762102,https://doi.org/10.1080/21670811.2020.1762102,"Facing budget constraints, many traditional news organizations are turning their eyes on automation to streamline manpower, cut down on costs, and improve efficiency. But how does automation fit into traditional values of journalism and how does it affect perceptions of credibility, an important currency valued by the journalistic field? This study explores this question using a 3 (declared author: human vs. machine vs. combined) × 2 (objectivity: objective vs. not objective) between-subjects experimental design involving 420 participants drawn from the national population of Singapore. The analysis found no main differences in perceived source credibility between algorithm, human, and mixed authors. Similarly, news articles attributed to an algorithm, a human journalist, and a combination of both showed no differences in message credibility. However, the study found an interaction effect between type of declared author and news objectivity. When the article is presented to be written by a human journalist, source and message credibility remain stable regardless of whether the article was objective or not objective. However, when the article is presented to be written by an algorithm, source and message credibility are higher when the article is objective than when the article is not objective. Findings for combined authorship are split: there were no differences between objective and non-objective articles when it comes to message credibility. However, combined authorship is rated higher in source credibility when the article is not objective than when the article is objective.",2020-04-20,2025-08-28 12:59:01,2025-08-28 12:59:01,2025-08-28 12:59:01,548-562,,4,8,,,Man vs. Machine?,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2020.1762102,,,,,Automation; journalism; news; objectivity; robot news,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XF7GXTMY,preprint,2021,"Susser, Daniel; Grimaldi, Vincent",Measuring Automated Influence: Between Empirical Evidence and Ethical Values,,,,,https://papers.ssrn.com/abstract=3848919,"Automated influence, delivered by digital targeting technologies such as targeted advertising, digital nudges, and recommender systems, has attracted significant interest from both empirical researchers, on one hand, and critical scholars and policymakers on the other. In this paper, we argue for closer integration of these efforts. Critical scholars and policymakers, who focus primarily on the social, ethical, and political effects of these technologies, need empirical evidence to substantiate and motivate their concerns. However, existing empirical research investigating the effectiveness of these technologies (or lack thereof), neglects other morally relevant effects—which can be felt regardless of whether or not the technologies ""work"" in the sense of fulfilling the promises of their designers. Drawing from the ethics and policy literature, we enumerate a range of questions begging for empirical analysis—the outline of a research agenda bridging these fields—and issue a call to action for more empirical research that takes these urgent ethics and policy questions as their starting point.",2021,2025-08-28 12:59:02,2025-08-28 12:59:02,2025-08-28 12:59:02,,,,,,,Measuring Automated Influence,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,,,,,,autonomy; dark patterns; influence; nudges; privacy; targeted advertising,,,,,,,,,,,,,,,,,,,3848919,,,,,,,,,,,,,,,,,,,,,,,,,,,
HQ6EQ3FT,webpage,2024,"Durmus, Esin; Lovitt, Liane; Tamkin, Alex; Ritchie, Stuart; Clark, Jack; Ganguli, Deep",Measuring the Persuasiveness of Language Models,,,,,https://www.anthropic.com/research/measuring-model-persuasiveness,"Anthropic developed a way to test how persuasive language models (LMs) are, and analyzed how persuasiveness scales across different versions of Claude.",2024,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,,,,,,,,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CIRXFL7C,journalArticle,2023,"Abercrombie, Gavin; Curry, Amanda Cercas; Dinkar, Tanvi; Rieser, Verena; Talat, Zeerak",Mirages: On Anthropomorphism in Dialogue Systems,,,,10.48550/arXiv.2305.09800,http://arxiv.org/abs/2305.09800,"Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism may be inevitable due to the choice of medium, conscious and unconscious design choices can guide users to personify such systems to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, including reinforcing gender stereotypes and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",2023,2025-08-28 12:02:44,2025-08-28 12:02:44,2025-08-27,,,,,,,Mirages,,,,,,,,,,,,,,,,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QCHUXNJK,preprint,2023,"Shevlane, Toby; Farquhar, Sebastian; Garfinkel, Ben; Phuong, Mary; Whittlestone, Jess; Leung, Jade; Kokotajlo, Daniel; Marchal, Nahema; Anderljung, Markus; Kolt, Noam; Ho, Lewis; Siddarth, Divya; Avin, Shahar; Hawkins, Will; Kim, Been; Gabriel, Iason; Bolina, Vijay; Clark, Jack; Bengio, Yoshua; Christiano, Paul; Dafoe, Allan",Model evaluation for extreme risks,,,,10.48550/arXiv.2305.15324,http://arxiv.org/abs/2305.15324,"Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through ""dangerous capability evaluations"") and the propensity of models to apply their capabilities for harm (through ""alignment evaluations""). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.",2023-09-22,2025-08-28 13:00:17,2025-08-28 13:00:17,2025-08-28 13:00:17,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2305.15324 [cs],,C:\Users\Admin\Zotero\storage\WVTDFGFV\Shevlane et al. - 2023 - Model evaluation for extreme risks.pdf; C:\Users\Admin\Zotero\storage\JGH3MG5K\2305.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2305.15324,,,,,,,,,,,,,,,,,,,,,,,,,,,
DWM74DXU,conferencePaper,2022,"Pauli, Amalie; Derczynski, Leon; Assent, Ira",Modelling Persuasion through Misuse of Rhetorical Appeals,Proceedings of the Second Workshop on NLP for Positive Impact (NLP4PI),,,10.18653/v1/2022.nlp4pi-1.11,https://aclanthology.org/2022.nlp4pi-1.11/,"It is important to understand how people use words to persuade each other. This helps understand debate, and detect persuasive narratives in regard to e.g. misinformation. While computational modelling of some aspects of persuasion has received some attention, a way to unify and describe the overall phenomenon of when persuasion becomes undesired and problematic, is missing. In this paper, we attempt to address this by proposing a taxonomy of computational persuasion. Drawing upon existing research and resources, this paper shows how to re-frame and re-organise current work into a coherent framework targeting the misuse of rhetorical appeals. As a study to validate these re-framings, we then train and evaluate models of persuasion adapted to our taxonomy. Our results show an application of our taxonomy, and we are able to detecting misuse of rhetorical appeals, finding that these are more often used in misinformative contexts than in true ones.",2022-12,2025-08-28 13:00:22,2025-08-28 13:00:22,2025-08-28 13:00:22,89–100,,,,,,,,,,,Association for Computational Linguistics,"Abu Dhabi, United Arab Emirates (Hybrid)",,,,,,ACLWeb,,,,C:\Users\Admin\Zotero\storage\WLYUJAZP\Pauli et al. - 2022 - Modelling Persuasion through Misuse of Rhetorical Appeals.pdf,,,,"Biester, Laura; Demszky, Dorottya; Jin, Zhijing; Sachan, Mrinmaya; Tetreault, Joel; Wilson, Steven; Xiao, Lu; Zhao, Jieyu",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NLP4PI 2022,,,,,,,,,,,,,,,
8GDBFA3C,journalArticle,2022,"Bartolo, Max; Thrush, Tristan; Riedel, Sebastian; Stenetorp, Pontus; Jia, Robin; Kiela, Douwe",Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants,,,,10.48550/arXiv.2112.09062,http://arxiv.org/abs/2112.09062,"In Dynamic Adversarial Data Collection (DADC), human annotators are tasked with finding examples that models struggle to predict correctly. Models trained on DADC-collected training data have been shown to be more robust in adversarial and out-of-domain settings, and are considerably harder for humans to fool. However, DADC is more time-consuming than traditional data collection and thus more costly per annotated example. In this work, we examine whether we can maintain the advantages of DADC, without incurring the additional cost. To that end, we introduce Generative Annotation Assistants (GAAs), generator-in-the-loop models that provide real-time suggestions that annotators can either approve, modify, or reject entirely. We collect training datasets in twenty experimental settings and perform a detailed analysis of this approach for the task of extractive question answering (QA) for both standard and adversarial data collection. We demonstrate that GAAs provide significant efficiency benefits with over a 30% annotation speed-up, while leading to over a 5x improvement in model fooling rates. In addition, we find that using GAA-assisted training data leads to higher downstream model performance on a variety of question answering tasks over adversarial data collection.",2022,2025-08-28 12:02:44,2025-08-28 12:02:44,2025-08-27,,,,,,,Models in the Loop,,,,,,,,,,,,,,,,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I4VX3794,journalArticle,2021,"Wang, Sai",Moderating Uncivil User Comments by Humans or Machines? The Effects of Moderation Agent on Perceptions of Bias and Credibility in News Content,Digital Journalism,,2167-0811,10.1080/21670811.2020.1851279,https://doi.org/10.1080/21670811.2020.1851279,"Studies have shown that uncivil comments under an online news article may result in biased perceptions of the news content, and explicit comment moderation has the potential to mitigate this adverse effect. Using an online experiment, the present study extends this line of research with the examination of how interface cues signalling different agents (human vs. machine) in moderating uncivil comments affect a reader’s judgment of the news and how prior belief in machine heuristic moderates such effects. The results indicated that perceptions of news bias were attenuated when uncivil comments were moderated by a machine (as opposed to a human) agent, which subsequently engendered greater perceived credibility of the news story. Additionally, such indirect effects were more prominent among readers who strongly believed that machine operations are generally accurate and reliable than those with a weaker prior belief in this rule of thumb.",2021-01-02,2025-08-28 13:00:24,2025-08-28 13:00:24,2025-08-28 13:00:24,64-83,,1,9,,,Moderating Uncivil User Comments by Humans or Machines?,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2020.1851279,,,,,comments; credibility; incivility; machine heuristic; Moderation; news bias,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FGI6AK6T,conferencePaper,2022,"Longoni, Chiara; Fradkin, Andrey; Cian, Luca; Pennycook, Gordon",News from Generative Artificial Intelligence Is Believed Less,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",978-1-4503-9352-2,,10.1145/3531146.3533077,https://dl.acm.org/doi/10.1145/3531146.3533077,"Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.",2022,2025-08-28 13:00:28,2025-08-28 13:00:28,2025-08-28,97–106,,,,,,,FAccT '22,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,C:\Users\Admin\Zotero\storage\8A25UFQF\Longoni et al. - 2022 - News from Generative Artificial Intelligence Is Believed Less.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4GPYGG55,journalArticle,2018,"Adam, Marc T. P.; Teubner, Timm; Gimpel, Henner",No Rage Against the Machine: How Computer Agents Mitigate Human Emotional Processes in Electronic Negotiations,Group Decision and Negotiation,,1572-9907,10.1007/s10726-018-9579-5,https://doi.org/10.1007/s10726-018-9579-5,"With the proliferation of information technology and artificial intelligence in society, human users have started to engage in social interactions with computer agents. In this study, we conducted a laboratory experiment in which neurophysiological measurements were used to investigate the effect of computer agents on the affective processes and behavior of human negotiators. Participants engaged in alternating-offer bargaining over the partition of a pie with either human or computer counterparts and under different levels of urgency to reach an agreement. Overall, our data show that the subjects claimed significantly higher proportions for themselves when they made opening offers to computer agents than when bargaining with human counterparts, regardless of the degree of urgency in the negotiation. However, when the subjects responded to computer-issued offers the picture was more complex. Whereas under high-level urgency, the subjects were more likely to accept offers made by computer agents than by human counterparts, we observed the opposite effect for low-level urgency, where they were less likely to accept the offers of computer agents. In combination, these behavioral patterns lead to the use of computer agents yielding an increase in economic efficiency. Further, the subjects exhibited less emotionally charged behavior when facing computer agents than when facing human counterparts, as the intensity of affective processes was lower and the relationship between arousal and offer acceptance was observable only when the counterparts were human. The results of our study shed light on the potential benefits and intricacies of employing computer agents in electronic negotiations.",2018-08-01,2025-08-28 13:02:04,2025-08-28 13:02:04,2025-08-28 13:02:04,543-571,,4,27,,Group Decis Negot,No Rage Against the Machine,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\2WIK3GQE\Adam et al. - 2018 - No Rage Against the Machine How Computer Agents Mitigate Human Emotional Processes in Electronic Ne.pdf,,,Bargaining; Computer agents; Emotions; Experiment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SZS2Q8JN,journalArticle,2024,"Xie, Chunya; Fu, Tianhui; Yang, Chen; Chang, En-Chung; Zhao, Mengying",Not a good judge of talent: the influence of subjective socioeconomic status on AI aversion,Marketing Letters,,1573-059X,10.1007/s11002-024-09725-7,https://doi.org/10.1007/s11002-024-09725-7,The current research constructs a framework to understand how subjective socioeconomic status (SES) affects consumers’ AI aversion in the evaluation context. Three experiments show that subjective SES has a negative impact on consumers’ willingness to accept AI evaluation. Consumers with higher subjective SES are more likely to resist AI evaluation because they perceive that AI agents are not as capable as human agents of identifying their talents. This effect is moderated by the agent type–the impact of subjective SES on resistance to the AI agent is attenuated when the AI agent is non-evaluative. This research is of great significance in enriching research on improving AI services efficiency across various social classes.,2024-09-01,2025-08-28 13:02:09,2025-08-28 13:02:09,2025-08-28 13:02:09,381-393,,3,35,,Mark Lett,Not a good judge of talent,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\F2CNS3ZR\Xie et al. - 2024 - Not a good judge of talent the influence of subjective socioeconomic status on AI aversion.pdf,,,AI aversion; AI evaluation; Subjective socioeconomic status; Talents identification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2YTEGRQ7,journalArticle,2022,"de Ridder, Denise; Kroese, Floor; van Gestel, Laurens",Nudgeability: Mapping Conditions of Susceptibility to Nudge Influence,Perspectives on Psychological Science,,1745-6916,10.1177/1745691621995183,https://doi.org/10.1177/1745691621995183,"Nudges are behavioral interventions to subtly steer citizens’ choices toward “desirable” options. An important topic of debate concerns the legitimacy of nudging as a policy instrument, and there is a focus on issues relating to nudge transparency, the role of preexisting preferences people may have, and the premise that nudges primarily affect people when they are in “irrational” modes of thinking. Empirical insights into how these factors affect the extent to which people are susceptible to nudge influence (i.e., “nudgeable”) are lacking in the debate. This article introduces the new concept of nudgeability and makes a first attempt to synthesize the evidence on when people are responsive to nudges. We find that nudge effects do not hinge on transparency or modes of thinking but that personal preferences moderate effects such that people cannot be nudged into something they do not want. We conclude that, in view of these findings, concerns about nudging legitimacy should be softened and that future research should attend to these and other conditions of nudgeability.",2022-03-01,2025-08-28 13:02:11,2025-08-28 13:02:11,2025-08-28 13:02:11,346-359,,2,17,,Perspect Psychol Sci,Nudgeability,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,C:\Users\Admin\Zotero\storage\Z29AVGD9\de Ridder et al. - 2022 - Nudgeability Mapping Conditions of Susceptibility to Nudge Influence.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9UPA7ECJ,journalArticle,2020,"Mao, Yanbing; Akyol, Emrah",On Inference of Network Topology and Confirmation Bias in Cyber-Social Networks,IEEE Transactions on Signal and Information Processing over Networks,,2373-776X,10.1109/TSIPN.2020.3015283,https://ieeexplore.ieee.org/document/9187724,"This article studies topology inference, from agent states, of a directed cyber-social network with opinion spreading dynamics model that explicitly takes confirmation bias into account. The cyber-social network comprises a set of partially connected directed network of agents at the social level, and a set of information sources at the cyber layer. The necessary and sufficient conditions for the existence of exact inference solution are characterized. A method for exact inference, when it is possible, of entire network topology as well as confirmation bias model parameters is proposed for the case where the bias mentioned earlier follows a piece-wise linear model. The particular case of no confirmation bias is analyzed in detail. For the setting where the model of confirmation bias is unknown, an algorithm that approximates the network topology, building on the exact inference method, is presented. This algorithm can exactly infer the weighted communication from the neighbors to the non-followers of information sources. Numerical simulations demonstrate the effectiveness of the proposed methods for different scenarios.",2020,2025-08-28 13:02:13,2025-08-28 13:02:13,2025-08-28 13:02:13,633-644,,,6,,,,,,,,,,,,,,,IEEE Xplore,,,,C:\Users\Admin\Zotero\storage\FY4HXA42\9187724.html; C:\Users\Admin\Zotero\storage\X3M3YI9S\Mao and Akyol - 2020 - On Inference of Network Topology and Confirmation Bias in Cyber-Social Networks.pdf,,,Computational modeling; Confirmation bias; cyber-social networks; directed communication; Linear matrix inequalities; Mathematical model; Network topology; Social network services; social networks; Stochastic processes; Topology; topology inference,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKKM7ZTG,preprint,2023,"Shaikh, Omar; Zhang, Hongxin; Held, William; Bernstein, Michael; Yang, Diyi","On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",,,,10.48550/arXiv.2212.08061,http://arxiv.org/abs/2212.08061,"Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.",2023-06-04,2025-08-28 13:02:15,2025-08-28 13:02:15,2025-08-28 13:02:15,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2212.08061 [cs],,"C:\Users\Admin\Zotero\storage\J5QHVSQB\Shaikh et al. - 2023 - On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning.pdf; C:\Users\Admin\Zotero\storage\XMJL4ZCA\2212.html",,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2212.08061,,,,,,,,,,,,,,,,,,,,,,,,,,,
GA98RMQE,conferencePaper,2021,"Wu, Yihan; Kelly, Ryan M.",Online Dating Meets Artificial Intelligence: How the Perception of Algorithmically Generated Profile Text Impacts Attractiveness and Trust,Proceedings of the 32nd Australian Conference on Human-Computer Interaction,978-1-4503-8975-4,,10.1145/3441000.3441074,https://doi.org/10.1145/3441000.3441074,"Online dating systems are widely used to meet romantic partners, yet people often struggle to write attractive profiles on these applications. Artificial intelligence (AI) has the potential to help online daters by automatically generating profile content, but little research has explored how the use of AI in online dating could affect users’ perceptions of one another. The present study investigated how the perceived involvement of AI influences ratings of attractiveness and trust in online dating. In a between-subjects experiment, participants (N = 48) were presented with the text of 10 dating profiles and were told that the profiles had been written by humans or with the help of AI. We found that the perceived involvement of AI did not have a significant impact on attractiveness, but that it did lead to a significant reduction in trustworthiness of the profile author. We interpret our findings through the lens of social information processing theory, discussing the tradeoffs associated with designing to reveal or hide the use of AI in online dating.",2021,2025-08-28 13:02:17,2025-08-28 13:02:17,2025-08-28,444–453,,,,,,Online Dating Meets Artificial Intelligence,OzCHI '20,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FM5RVIIZ,preprint,2023,"Casper, Stephen; Davies, Xander; Shi, Claudia; Gilbert, Thomas Krendl; Scheurer, Jérémy; Rando, Javier; Freedman, Rachel; Korbak, Tomasz; Lindner, David; Freire, Pedro; Wang, Tony; Marks, Samuel; Segerie, Charbel-Raphaël; Carroll, Micah; Peng, Andi; Christoffersen, Phillip; Damani, Mehul; Slocum, Stewart; Anwar, Usman; Siththaranjan, Anand; Nadeau, Max; Michaud, Eric J.; Pfau, Jacob; Krasheninnikov, Dmitrii; Chen, Xin; Langosco, Lauro; Hase, Peter; Bıyık, Erdem; Dragan, Anca; Krueger, David; Sadigh, Dorsa; Hadfield-Menell, Dylan",Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback,,,,10.48550/arXiv.2307.15217,http://arxiv.org/abs/2307.15217,"Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.",2023-09-11,2025-08-28 13:02:22,2025-08-28 13:02:22,2025-08-28 13:02:22,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2307.15217 [cs],,C:\Users\Admin\Zotero\storage\PMCL98A9\Casper et al. - 2023 - Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.pdf; C:\Users\Admin\Zotero\storage\ZLSVKJLQ\2307.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2307.15217,,,,,,,,,,,,,,,,,,,,,,,,,,,
XP58FZQU,journalArticle,2024,"Stein, Jan-Philipp; Linda Breves, Priska; Anders, Nora",Parasocial interactions with real and virtual influencers: The role of perceived similarity and human-likeness,New Media & Society,,1461-4448,10.1177/14614448221102900,https://doi.org/10.1177/14614448221102900,"Digitally created online celebrities (so-called virtual influencers) have appeared on various social media and video streaming platforms. While the scientific community has recently started to take an interest in this new phenomenon, it still remains mostly unclear how online audiences engage with—and relate to—these artificial digital creations. To address the identified research gap, we conducted a preregistered experiment (N = 179), comparing viewers’ parasocial interactions (PSIs) with either a human or a virtual influencer. Based on natural stimuli, we find that viewers’ parasocial response does not differ significantly between the two groups. However, by focusing on several theoretically relevant mediator variables, we uncover two opposing effects at play: While a significant direct effect signifies stronger PSIs with the virtual influencer, participants also attribute this persona with less mental human-likeness and similarity to themselves—which ultimately suppresses the observed advantage. Potential explanations for our results are discussed.",2024-06-01,2025-08-28 13:02:25,2025-08-28 13:02:25,2025-08-28 13:02:25,3433-3453,,6,26,,,Parasocial interactions with real and virtual influencers,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications,,C:\Users\Admin\Zotero\storage\HRTG68AG\Stein et al. - 2024 - Parasocial interactions with real and virtual influencers The role of perceived similarity and huma.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98PK8INW,journalArticle,2023,"Böhm, Robert; Jörling, Moritz; Reiter, Leonhard; Fuchs, Christoph",People devalue generative AI’s competence but not its advice in addressing societal and personal challenges,Communications Psychology,,2731-9121,10.1038/s44271-023-00032-x,https://www.nature.com/articles/s44271-023-00032-x,"The release of ChatGPT and related tools have made generative artificial intelligence (AI) easily accessible for the broader public. We conducted four preregistered experimental studies (total N = 3308; participants from the US) to investigate people’s perceptions of generative AI and the advice it generates on how to address societal and personal challenges. The results indicate that when individuals are (vs. are not) aware that the advice was generated by AI, they devalue the author’s competence but not the content or the intention to share and follow the advice on how to address societal challenges (Study 1) and personal challenges (Studies 2a and 2b). Study 3 further shows that individuals’ preference to receive advice from AI (vs. human experts) increases when they gained positive experience with generative AI advice in the past. The results are discussed regarding the nature of AI aversion in the context of generative AI and beyond.",2023-11-15,2025-08-28 13:02:58,2025-08-28 13:02:58,2025-08-28 13:02:58,32,,1,1,,Commun Psychol,,,,,,,,en,2023 The Author(s),,,,www.nature.com,,Publisher: Nature Publishing Group,,C:\Users\Admin\Zotero\storage\WV37V6RQ\Böhm et al. - 2023 - People devalue generative AI’s competence but not its advice in addressing societal and personal cha.pdf,,,Human behaviour; Information systems and information technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F38J9VS2,conferencePaper,2022,"Zehnder, E.; Dinet, J.; Charpillet, F.",Perception of physical and virtual agents: exploration of factors influencing the acceptance of intrusive domestic agents,2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),,,10.1109/RO-MAN53752.2022.9900593,https://ieeexplore.ieee.org/document/9900593,"Domestic robots and agents are widely sold to the grand public, leading us to ethical issues related to the data harvested by such machines. While users show a general acceptance of these robots, concerns remain when it comes to information security and privacy. Current research indicates that there’s a privacy-security trade-off for better use, but the anthropomorphic and social abilities of a robot are also known to modulate its acceptance and use. To explore and deepen what literature already brought on the subject we examined how users perceived their robot (Replika, Roomba©, Amazon Echo©, Google Home©, or Cozmo©/Vector©) through an online questionnaire exploring acceptance, perceived privacy and security, anthropomorphism, disclosure, perceived intimacy, and loneliness. The results supported the literature regarding the potential manipulative effects of robot’s anthropomorphism for acceptance but also information disclosure, perceived intimacy, security, and privacy.",2022-08,2025-08-28 13:03:39,2025-08-28 13:03:39,2025-08-28 13:03:39,1050-1057,,,,,,Perception of physical and virtual agents,,,,,,,,,,,,IEEE Xplore,,ISSN: 1944-9437,,C:\Users\Admin\Zotero\storage\WK2I27UA\9900593.html,,,Data collection; Ethics; Information security; Internet; Manufacturing; Privacy; Regulation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),,,,,,,,,,,,,,,
S33HFWQL,conferencePaper,2022,"Tiwari, Abhisek; Saha, Sriparna; Sengupta, Shubhashis; Maitra, Anutosh; Ramnani, Roshni; Bhattacharyya, Pushpak",Persona or Context? Towards Building Context adaptive Personalized Persuasive Virtual Sales Assistant,Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),,,10.18653/v1/2022.aacl-main.76,https://aclanthology.org/2022.aacl-main.76/,"Task-oriented conversational agents are gaining immense popularity and success in a wide range of tasks, from flight ticket booking to online shopping. However, the existing systems presume that end-users will always have a pre-determined and servable task goal, which results in dialogue failure in hostile scenarios, such as goal unavailability. On the other hand, human agents accomplish users' tasks even in a large number of goal unavailability scenarios by persuading them towards a very similar and servable goal. Motivated by the limitation, we propose and build a novel end-to-end multi-modal persuasive dialogue system incorporated with a personalized persuasive module aided goal controller and goal persuader. The goal controller recognizes goal conflicting/unavailability scenarios and formulates a new goal, while the goal persuader persuades users using a personalized persuasive strategy identified through dialogue context. We also present a novel automatic evaluation metric called Persuasiveness Measurement Rate (PMeR) for quantifying the persuasive capability of a conversational agent. The obtained improvements (both quantitative and qualitative) firmly establish the superiority and need of the proposed context-guided, personalized persuasive virtual agent over existing traditional task-oriented virtual agents. Furthermore, we also curated a multi-modal persuasive conversational dialogue corpus annotated with intent, slot, sentiment, and dialogue act for e-commerce domain.",2022-11,2025-08-28 13:03:33,2025-08-28 13:03:33,2025-08-28 13:03:33,1035–1047,,,,,,Persona or Context?,,,,,Association for Computational Linguistics,Online only,,,,,,ACLWeb,,,,C:\Users\Admin\Zotero\storage\HQJRV8TS\Tiwari et al. - 2022 - Persona or Context Towards Building Context adaptive Personalized Persuasive Virtual Sales Assistan.pdf,,,,"He, Yulan; Ji, Heng; Li, Sujian; Liu, Yang; Chang, Chua-Hui",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AACL-IJCNLP 2022,,,,,,,,,,,,,,,
4SP5R793,journalArticle,2018,"Kišiček, Gabrijela",Persuasive power of prosodic features,Argumentation and Advocacy,,1051-1431,10.1080/10511431.2019.1525003,https://doi.org/10.1080/10511431.2019.1525003,"This response to William Bedsole's paper aims to emphasize the importance of prosodic features—i.e. voice quality, tempo, rhythm, pitch, intonation etc.—in the persuasion process. Bedsole refers to voice and the role voice has but neglects to focus on the many different functions it can have. Extensive empirical research in nonverbal communication confirms that prosodic features play an important role in human interaction. However, contemporary rhetorical research has somewhat marginalized prosody. Voice and all prosodic features may contribute to the credibility speaker (ethos), can create certain disposition on audience (pathos), but can also be essential for understanding the argument (logos).",2018-10-02,2025-08-28 13:03:44,2025-08-28 13:03:44,2025-08-28 13:03:44,345-350,,4,54,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/10511431.2019.1525003,,,,,argumentation; persuasion; prosody; rhetoric; voice quality,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QHTUQ7TU,journalArticle,2024,"Teigen, Cassandra; Madsen, Jens Koed; George, Nicole Lauren; Yousefi, Sayeh",Persuasiveness of arguments with AI-source labels,Proceedings of the Annual Meeting of the Cognitive Science Society,,,,https://escholarship.org/uc/item/6t82g70v,"This paper sought to understand the impact of labelling an argument as AI-generated compared to human-authored, and how factors such as portrayals of expertise and the nature of arguments presented (narrative versus statistical) may affect the persuasiveness of the arguments. Three domains were explored: health, finance, and politics. We show that arguments with AI source labels, both non-expert and expert, were rated by participants as less persuasive than when they had their counterpart human-authored source labels attached. Moreover, although the statistical arguments were found to be more persuasive than the narrative arguments, this did not affect the impact of an AI source label, with a significant interaction effect only being seen for the domain of politics for the expert AI source. The study explored the role of attitude towards AI on the impact of source labels as an exploratory analysis and found no significant interaction effect across the three domains.",2024,2025-08-28 13:03:41,2025-08-28 13:03:41,2025-08-28 13:03:41,,,0,46,,,,,,,,,,en,,,,,escholarship.org,,,,C:\Users\Admin\Zotero\storage\FALVVFP8\Teigen et al. - 2024 - Persuasiveness of arguments with AI-source labels.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2DTV3KL6,journalArticle,2020,"Stiff, Chris; Kedra, Paula",Playing well with others: The role of opponent and intergroup anxiety in the reduction of prejudice through collaborative video game play,Psychology of Popular Media,,2689-6575,10.1037/ppm0000210,,"Recent work on the social effects of video gaming has moved away from the view they are detrimental, and has instead demonstrated how they may be a force for good. One example is how collaborative intergroup play can reduce prejudice between groups. However, this literature is at a nascent stage, and many of the intricacies of such a mechanism are unknown. Previous work has predominantly used attitude scales and ignored other measures. Factors such as the role of the opponent in games and what may be the mechanism behind any effects have likewise received little attention. In this laboratory study, participants played collaborative games with an outgroup member or alone. Their opponent was also reported to be computer controlled or controlled by another person. Following play, intergroup anxiety was reported, and participants wrote a short passage of prose regarding the outgroup and rated on attitude scales. Analysis demonstrated that playing with outgroup members was indeed an effective method of increasing the positivity toward outgroup members, reflected in both scale and prose measures. Anxiety was also found to be a significant mediator; however, it was less clear whether a human opponent moderated any effects. Further ideas of how these findings could be developed are then discussed. (PsycINFO Database Record (c) 2020 APA, all rights reserved)",2020,2025-08-28 13:03:46,2025-08-28 13:03:46,,105-115,,1,9,,,Playing well with others,,,,,,,,,,,,APA PsycNet,,Place: US Publisher: Educational Publishing Foundation,,C:\Users\Admin\Zotero\storage\TLJFCDRZ\doiLanding.html,,,Anxiety; Anxiety Management; Attitude Measures; Collaboration; Computer Games; Ingroup Outgroup; Intergroup Dynamics; Prejudice; Prose; Test Construction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XC8GN393,conferencePaper,2021,"Wu, Tongshuang; Ribeiro, Marco Tulio; Heer, Jeffrey; Weld, Daniel","Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),,,10.18653/v1/2021.acl-long.523,https://aclanthology.org/2021.acl-long.523/,"While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.",2021-08,2025-08-28 13:03:48,2025-08-28 13:03:48,2025-08-28 13:03:48,6707–6723,,,,,,Polyjuice,,,,,Association for Computational Linguistics,Online,,,,,,ACLWeb,,,,"C:\Users\Admin\Zotero\storage\DZJCERC8\Wu et al. - 2021 - Polyjuice Generating Counterfactuals for Explaining, Evaluating, and Improving Models.pdf",,,,"Zong, Chengqing; Xia, Fei; Li, Wenjie; Navigli, Roberto",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ACL-IJCNLP 2021,,,,,,,,,,,,,,,
ZPCNUFJR,journalArticle,2021,"Granulo, Armin; Fuchs, Christoph; Puntoni, Stefano",Preference for Human (vs. Robotic) Labor is Stronger in Symbolic Consumption Contexts,Journal of Consumer Psychology,,1532-7663,10.1002/jcpy.1181,https://onlinelibrary.wiley.com/doi/abs/10.1002/jcpy.1181,"Advances in robotics, automation, and artificial intelligence increasingly enable firms to replace human labor with technology, thereby fundamentally transforming how goods and services are produced. From both managerial and societal points of view, it is therefore important to understand demand-side incentives for firms to employ human labor. We begin to address this question by examining for which products and services consumers are more likely to favor human (vs. robotic) labor. In six studies, we demonstrate that consumers prefer human (vs. robotic) labor more for products with higher (vs. lower) symbolic value (e.g., when expressing something about one's beliefs and personality is of greater importance). We theorize that this is because consumers have stronger uniqueness motives in more (vs. less) symbolic consumption contexts (and associate human labor more strongly with product uniqueness). In line with this account, we demonstrate that individual differences in need for uniqueness moderate the interaction between production mode and symbolic motives and that a measure of uniqueness motives mediates the effect of consumption context on preferences for human (vs. robotic) production.",2021,2025-08-28 13:03:51,2025-08-28 13:03:51,2025-08-28 13:03:51,72-80,,1,31,,,,,,,,,,en,© 2020 The Authors. Journal of Consumer Psychology published by Wiley Periodicals LLC on behalf of Society for Consumer Psychology,,,,Wiley Online Library,,_eprint: https://myscp.onlinelibrary.wiley.com/doi/pdf/10.1002/jcpy.1181,,C:\Users\Admin\Zotero\storage\TEYJLN9Y\Granulo et al. - 2021 - Preference for Human (vs. Robotic) Labor is Stronger in Symbolic Consumption Contexts.pdf; C:\Users\Admin\Zotero\storage\MQWHJJF5\jcpy.html,,,Consumer preferences; Human labor; Robotic labor; Symbolic consumption; Uniqueness motives,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZXYPBSUG,preprint,2024,"Li, Chao; Su, Xing; Han, Haoying; Xue, Cong; Zheng, Chunmo; Fan, Chao",Quantifying the Impact of Large Language Models on Collective Opinion Dynamics,,,,10.2139/ssrn.4688547,https://papers.ssrn.com/abstract=4688547,"The process of opinion expression and exchange is a critical component of democratic societies. As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned. However, the knowledge about how LLMs affect the process of opinion expression and exchange of social opinion networks is very limited. Here, we create an opinion network dynamics model to encode the opinions of LLMs, cognitive acceptability and usage strategies of individuals, and simulate the impact of LLMs on opinion dynamics in a variety of scenarios. The outcomes of the simulations inform about effective demand-oriented opinion network interventions. The results from this study suggested that the output opinion of LLMs has a unique and positive effect on the collective opinion difference. The marginal effect of cognitive acceptability on collective opinion formation is nonlinear and shows a decreasing trend. When people partially rely on LLMs, the exchange process of opinion becomes more intense and the diversity of opinion becomes more favorable. In fact, there is 38.6% more opinion diversity when people all partially rely on LLMs, compared to prohibiting the use of LLMs entirely. The optimal diversity of opinion was found when the fractions of people who do not use, partially rely on, and fully rely on LLMs reached roughly 4:12:1. Our experiments also find that introducing extra agents with opposite/neutral/random opinions, we can effectively mitigate the impact of biased/toxic output from LLMs. Our findings provide valuable insights into opinion dynamics in the age of LLMs, highlighting the need for customized interventions tailored to specific scenarios to address the drawbacks of improper output and use of LLMs.",2024-01-09,2025-08-28 13:03:53,2025-08-28 13:03:53,2025-08-28 13:03:53,,,,,,,,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,,,C:\Users\Admin\Zotero\storage\IZ9PKD96\Li et al. - 2024 - Quantifying the Impact of Large Language Models on Collective Opinion Dynamics.pdf,,,intervention strategies; Large Language Models; Opinion dynamics,,,,,,,,,,,,,,,,,,,4688547,,,,,,,,,,,,,,,,,,,,,,,,,,,
9GBYFHW4,journalArticle,2018,"Graefe, Andreas; Haim, Mario; Haarmann, Bastian; Brosius, Hans-Bernd","Readers’ perception of computer-generated news: Credibility, expertise, and readability",Journalism,,1464-8849,10.1177/1464884916641269,https://doi.org/10.1177/1464884916641269,"We conducted an online experiment to study people’s perception of automated computer-written news. Using a 2 × 2 × 2 design, we varied the article topic (sports, finance; within-subjects) and both the articles’ actual and declared source (human-written, computer-written; between-subjects). Nine hundred eighty-six subjects rated two articles on credibility, readability, and journalistic expertise. Varying the declared source had small but consistent effects: subjects rated articles declared as human written always more favorably, regardless of the actual source. Varying the actual source had larger effects: subjects rated computer-written articles as more credible and higher in journalistic expertise but less readable. Across topics, subjects’ perceptions did not differ. The results provide conservative estimates for the favorability of computer-written news, which will further increase over time and endorse prior calls for establishing ethics of computer-written news.",2018-05-01,2025-08-28 13:03:56,2025-08-28 13:03:56,2025-08-28 13:03:56,595-610,,5,19,,Journalism,Readers’ perception of computer-generated news,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications,,"C:\Users\Admin\Zotero\storage\DT6TJI5E\Graefe et al. - 2018 - Readers’ perception of computer-generated news Credibility, expertise, and readability.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GV5PLD7V,conferencePaper,2018,"Liu, B.; Wei, L.",Reading machine-written news: Effect of machine heuristic and novelty on hostile media perception,"Human-Computer Interaction. Theories, Methods, and Human Issues",,,,,"The use of artificial intelligence (AI) in news production has been increasing these days. Its inanimate nature might bring opportunities to suppress hostile media effect by reducing readers’ emotional involvement. This study conducted an online experiment (N = 175) to examine how the identity of news writers, human vs. robot, might impact readers’ perceptions of news and source credibility. Findings suggest that readers holding machine heuristic, i.e., those who saw machine as free of intention, experienced less emotional involvement when the news was purported to be written by a news writing algorithm, as compared with when reading human-written news. Lower level of emotional involvement further led to less perception of bias in the news and the extremity of news slant. However, perceived novelty associated with robot news writer, although enhanced positive perceptions of the news, intensified readers’ emotional involvement, which further heightened hostile media perceptions. Findings in this study identified the mechanisms underlying effects of AI writer in the context of controversial news topics. Implications are discussed under theoretical frameworks of general information processing, hostile media effect and MAIN model.",2018,2025-08-28 12:02:43,2025-08-28 12:02:43,,,,,,,,,,,,,,Springer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Human-Computer Interaction. Theories, Methods, and Human Issues",,,,,,,,,,,,,,,
W7RXY83N,journalArticle,2022,"Molina, M. D.; Wang, J.; Sundar, S. S.; Le, T.; DiRusso, C.","Reading, commenting and sharing of fake news: How online bandwagons and bots dictate user engagement",Communication Research,,,10.1177/00936502211073398,,"Do social media users read, comment, and share false news more than real news? Does it matter if the story is written by a bot and whether it is endorsed by many others? We conducted a selective-exposure experiment (N = 171) to answer these questions. Results showed that real articles were more likely to receive “likes” whereas false articles were more likely to receive comments. Users commented more on a bot-written article when it received fewer likes. We explored the psychological mechanisms underlying these findings in Study 2 (N = 284). Data indicate that users’ engagement with online news is largely driven by emotions elicited by news content and heuristics triggered by interface cues, such that curiosity increases consumption of real news, whereas uneasiness triggered by a high number of “likes” encourages comments on fake news.",2022,2025-08-28 12:02:43,2025-08-28 12:02:43,,,,,,,Communication Research,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMMYDGYU,preprint,2022,"Perez, Ethan; Huang, Saffron; Song, Francis; Cai, Trevor; Ring, Roman; Aslanides, John; Glaese, Amelia; McAleese, Nat; Irving, Geoffrey",Red Teaming Language Models with Language Models,,,,10.48550/arXiv.2202.03286,http://arxiv.org/abs/2202.03286,"Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (""red teaming"") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",2022-02-07,2025-08-28 13:05:03,2025-08-28 13:05:03,2025-08-28 13:05:03,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2202.03286 [cs],,C:\Users\Admin\Zotero\storage\CPVKLZHQ\Perez et al. - 2022 - Red Teaming Language Models with Language Models.pdf; C:\Users\Admin\Zotero\storage\MFI3MB4T\2202.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2202.03286,,,,,,,,,,,,,,,,,,,,,,,,,,,
YD9HH8QG,journalArticle,2021,"Wesche, Jenny S.; Sonderegger, Andreas",Repelled at first sight? Expectations and intentions of job-seekers reading about AI selection in job advertisements,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2021.106931,https://www.sciencedirect.com/science/article/pii/S0747563221002545,"Artificial intelligence (AI) is increasingly used in personnel selection to automate decision-making. Initial evidence points to negative effects of automating these procedures on applicant experiences. However, the effect of the prospect of automated procedures on job-seekers’ pre-process perceptions (e.g., organizational attractiveness) and intentions (to apply for the advertised job) is still unclear. We conducted three experiments (Study 1 and Study 2 as within-subjects designs, Study 3 as a between-subjects design; N1 = 36, N2 = 44, N3 = 172) systematically varying the information in job advertisements on the automation of different stages of the selection process (Study 1: screening stage conducted by a human vs. a non-specified agent vs. an AI; Study 2 and Study 3: human screening and human interview vs. AI screening and human interview vs. AI screening and AI interview). Results showed small negative effects of screening conducted by an AI vs. a human (Study 1, Study 2, Study 3), but stronger negative effects when also interviews were conducted by an AI vs. a human (Study 2, Study3) on job-seekers pre-process expectations, perceptions, and intentions. Possible reasons for these effects are discussed with special consideration of the different stages of the recruiting and selection process and explored with a qualitative approach in Study 2.",2021-12-01,2025-08-28 13:05:10,2025-08-28 13:05:10,2025-08-28 13:05:09,106931,,,125,,Computers in Human Behavior,Repelled at first sight?,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\P5SRDH6P\Wesche and Sonderegger - 2021 - Repelled at first sight Expectations and intentions of job-seekers reading about AI selection in jo.pdf; C:\Users\Admin\Zotero\storage\L53CN7QH\S0747563221002545.html,,,AI-Recruiting; Algorithmic decision-making; Automated selection procedures; Digital selection procedures; e-recruiting; Personnel selection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8L5AKACY,journalArticle,2019,"Longoni, Chiara; Bonezzi, Andrea; Morewedge, Carey K",Resistance to Medical Artificial Intelligence,Journal of Consumer Research,,0093-5301,10.1093/jcr/ucz013,https://doi.org/10.1093/jcr/ucz013,"Artificial intelligence (AI) is revolutionizing healthcare, but little is known about consumer receptivity to AI in medicine. Consumers are reluctant to utilize healthcare provided by AI in real and hypothetical choices, separate and joint evaluations. Consumers are less likely to utilize healthcare (study 1), exhibit lower reservation prices for healthcare (study 2), are less sensitive to differences in provider performance (studies 3A–3C), and derive negative utility if a provider is automated rather than human (study 4). Uniqueness neglect, a concern that AI providers are less able than human providers to account for consumers’ unique characteristics and circumstances, drives consumer resistance to medical AI. Indeed, resistance to medical AI is stronger for consumers who perceive themselves to be more unique (study 5). Uniqueness neglect mediates resistance to medical AI (study 6), and is eliminated when AI provides care (a) that is framed as personalized (study 7), (b) to consumers other than the self (study 8), or (c) that only supports, rather than replaces, a decision made by a human healthcare provider (study 9). These findings make contributions to the psychology of automation and medical decision making, and suggest interventions to increase consumer acceptance of AI in medicine.",2019-12-01,2025-08-28 13:05:07,2025-08-28 13:05:07,2025-08-28 13:05:07,629-650,,4,46,,J Consum Res,,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\VXJU7YPL\ucz013.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6Y8BLPSV,conferencePaper,2023,"Plaza-del-arco, Flor Miriam; Nozza, Debora; Hovy, Dirk",Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech,The 7th Workshop on Online Abuse and Harms (WOAH),,,10.18653/v1/2023.woah-1.6,https://aclanthology.org/2023.woah-1.6/,"Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.",2023-07,2025-08-28 13:05:20,2025-08-28 13:05:20,2025-08-28 13:05:20,60–68,,,,,,Respectful or Toxic?,,,,,Association for Computational Linguistics,"Toronto, Canada",,,,,,ACLWeb,,,,C:\Users\Admin\Zotero\storage\K22VLCG7\Plaza-del-arco et al. - 2023 - Respectful or Toxic Using Zero-Shot Learning with Language Models to Detect Hate Speech.pdf,,,,"Chung, Yi-ling; R{\textbackslash”ottger}, Paul; Nozza, Debora; Talat, Zeerak; Mostafazadeh Davani, Aida",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,WOAH 2023,,,,,,,,,,,,,,,
6U57TQUJ,journalArticle,2022,"Graves, Clint G; Spencer, Leland G",Rethinking the Rhetorical Epistemics of Gaslighting,Communication Theory,,1468-2885,10.1093/ct/qtab013,https://doi.org/10.1093/ct/qtab013,"Gaslighting is defined as a dysfunctional communication dynamic in which one interlocutor attempts to destabilize another’s sense of reality. In this article, we advance a model of gaslighting based in an epistemic rhetoric perspective. Our model directs attention to the rhetorics used to justify competing knowledge claims, as opposed to philosophical models that tend to rely on objective truth-value. We probe the discursive manifestations of gaslighting in logocentric, ethotic, or pathemic terms. We then apply our model to explain sexist and racist gaslighting that derives power from normatively instantiated discourses of rape culture and White supremacy. Specifically, our analysis identifies the appeal structures used to legitimate such gaslighting in response to disclosures of sexual violence and testimony about racial injustice.",2022-02-01,2025-08-28 13:05:22,2025-08-28 13:05:22,2025-08-28 13:05:22,48-67,,1,32,,Commun Theory,,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\WY6SKJ9N\qtab013.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TJWI53Z6,conferencePaper,2019,"Leong, Brenda; Selinger, Evan",Robot Eyes Wide Shut: Understanding Dishonest Anthropomorphism,"Proceedings of the Conference on Fairness, Accountability, and Transparency",978-1-4503-6125-5,,10.1145/3287560.3287591,https://doi.org/10.1145/3287560.3287591,"The goal of this paper is to advance design, policy, and ethics scholarship on how engineers and regulators can protect consumers from deceptive robots and artificial intelligences that exhibit the problem of dishonest anthropomorphism. The analysis expands upon ideas surrounding the principle of honest anthropomorphism originally formulated by Margot Kaminsky, Mathew Ruben, William D. Smart, and Cindy M. Grimm in their groundbreaking Maryland Law Review article, ""Averting Robot Eyes."" Applying boundary management theory and philosophical insights into prediction and perception, we create a new taxonomy that identifies fundamental types of dishonest anthropomorphism and pinpoints harms that they can cause. To demonstrate how the taxonomy can be applied as well as clarify the scope of the problems that it can cover, we critically consider a representative series of ethical issues, proposals, and questions concerning whether the principle of honest anthropomorphism has been violated.",2019,2025-08-28 12:02:45,2025-08-28 12:02:45,2025-08-28,299–308,,,,,,Robot Eyes Wide Shut,FAT* '19,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Proceedings of the Conference on Fairness, Accountability, and Transparency",,,,,,,,,,,,,,,
GB7E92BV,journalArticle,2023,"Shanahan, Murray; McDonell, Kyle; Reynolds, Laria",Role play with large language models,Nature,,1476-4687,10.1038/s41586-023-06647-8,https://www.nature.com/articles/s41586-023-06647-8,"As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.",2023-11,2025-08-28 13:05:29,2025-08-28 13:05:29,2025-08-28 13:05:29,493-498,,7987,623,,,,,,,,,,en,2023 Springer Nature Limited,,,,www.nature.com,,Publisher: Nature Publishing Group,,C:\Users\Admin\Zotero\storage\98QYAKHP\Shanahan et al. - 2023 - Role play with large language models.pdf,,,Computer science; Philosophy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ANBYXZD3,journalArticle,2021,"Xu, Ying; Wang, Dakuo; Collins, Penelope; Lee, Hyelim; Warschauer, Mark","Same benefits, different communication patterns: Comparing Children's reading with a conversational agent vs. a human partner",Computers & Education,,0360-1315,10.1016/j.compedu.2020.104059,https://www.sciencedirect.com/science/article/pii/S0360131520302578,"Storybook reading accompanied by adult-guided conversation provides a stimulating context for children's language development. Conversational agents powered by artificial intelligence, such as smart speakers, are prevalent in children's homes and have the potential to engage children in storybook reading as language partners. However, little research has explored the effectiveness of using conversational agents to support children's language development. This study examined how an automated conversational agent can read stories to children via a smart speaker while asking questions and providing contingent feedback. Using a randomized experiment among 90 children aged three to six years, this study compared these children's story comprehension and verbal engagement in storybook reading with a conversational agent versus an adult. The conversational agent's guided conversation was found to be as supportive in improving children's story comprehension as that provided by an adult language partner. At the same time, this study uncovered a number of differences in children's verbal engagement when interacting with a conversational agent versus with an adult. Specifically, children who read with the conversational agent responded to questions with better intelligibility, whereas those who read with an adult responded to questions with higher productivity, lexical diversity, and topical relevance. And the two groups responded to questions with a similar level of accuracy. In addition, questions requiring high cognitive demand amplified the differences in of verbal engagement between the conversational agent and adult partner. The study offers important implications for developing and researching conversational agent systems to support children's language development.",2021-02-01,2025-08-28 13:05:31,2025-08-28 13:05:31,2025-08-28 13:05:31,104059,,,161,,Computers & Education,"Same benefits, different communication patterns",,,,,,,,,,,,ScienceDirect,,,,"C:\Users\Admin\Zotero\storage\R2J7CZRV\Xu et al. - 2021 - Same benefits, different communication patterns Comparing Children's reading with a conversational.pdf; C:\Users\Admin\Zotero\storage\WDFFMNCJ\S0360131520302578.html",,,Communication; Conversational agents; Language development; Storybook reading; Young children,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NHHHMDIT,preprint,2018,"Leike, Jan; Krueger, David; Everitt, Tom; Martic, Miljan; Maini, Vishal; Legg, Shane",Scalable agent alignment via reward modeling: a research direction,,,,10.48550/arXiv.1811.07871,http://arxiv.org/abs/1811.07871,"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",2018-11-19,2025-08-28 13:05:42,2025-08-28 13:05:42,2025-08-28 13:05:42,,,,,,,Scalable agent alignment via reward modeling,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1811.07871 [cs],,C:\Users\Admin\Zotero\storage\3IYCJPB8\Leike et al. - 2018 - Scalable agent alignment via reward modeling a research direction.pdf; C:\Users\Admin\Zotero\storage\FW28H86K\1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:1811.07871,,,,,,,,,,,,,,,,,,,,,,,,,,,
H7QFMM3C,journalArticle,2018,"Bode, Leticia; Vraga, Emily K.","See Something, Say Something: Correction of Global Health Misinformation on Social Media",Health Communication,,1041-0236,10.1080/10410236.2017.1331312,https://doi.org/10.1080/10410236.2017.1331312,"Social media are often criticized for being a conduit for misinformation on global health issues, but may also serve as a corrective to false information. To investigate this possibility, an experiment was conducted exposing users to a simulated Facebook News Feed featuring misinformation and different correction mechanisms (one in which news stories featuring correct information were produced by an algorithm and another where the corrective news stories were posted by other Facebook users) about the Zika virus, a current global health threat. Results show that algorithmic and social corrections are equally effective in limiting misperceptions, and correction occurs for both high and low conspiracy belief individuals. Recommendations for social media campaigns to correct global health misinformation, including encouraging users to refute false or misleading health information, and providing them appropriate sources to accompany their refutation, are discussed.",2018-09-02,2025-08-28 13:06:21,2025-08-28 13:06:21,2025-08-28 13:06:21,1131-1140,,9,33,,,"See Something, Say Something",,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/10410236.2017.1331312 PMID: 28622038,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUGW3DMN,preprint,2022,"Saunders, William; Yeh, Catherine; Wu, Jeff; Bills, Steven; Ouyang, Long; Ward, Jonathan; Leike, Jan",Self-critiquing models for assisting human evaluators,,,,10.48550/arXiv.2206.05802,http://arxiv.org/abs/2206.05802,"We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.",2022-06-14,2025-08-28 13:06:23,2025-08-28 13:06:23,2025-08-28 13:06:23,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2206.05802 [cs],,C:\Users\Admin\Zotero\storage\3HH249XP\Saunders et al. - 2022 - Self-critiquing models for assisting human evaluators.pdf; C:\Users\Admin\Zotero\storage\RUUCPDFF\2206.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2206.05802,,,,,,,,,,,,,,,,,,,,,,,,,,,
29J3BER2,preprint,2019,"Shi, Weiyan; Yu, Zhou",Sentiment Adaptive End-to-End Dialog Systems,,,,10.48550/arXiv.1804.10731,http://arxiv.org/abs/1804.10731,"End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.",2019-07-02,2025-08-28 13:06:27,2025-08-28 13:06:27,2025-08-28 13:06:27,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1804.10731 [cs],,C:\Users\Admin\Zotero\storage\AXSAQRXJ\Shi and Yu - 2019 - Sentiment Adaptive End-to-End Dialog Systems.pdf; C:\Users\Admin\Zotero\storage\H2NDARTP\1804.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:1804.10731,,,,,,,,,,,,,,,,,,,,,,,,,,,
QKAQE2FR,journalArticle,2019,"Mende, Martin; Scott, Maura L.; van Doorn, Jenny; Grewal, Dhruv; Shanks, Ilana",Service Robots Rising: How Humanoid Robots Influence Service Experiences and Elicit Compensatory Consumer Responses,Journal of Marketing Research,,0022-2437,10.1177/0022243718822827,https://doi.org/10.1177/0022243718822827,"Interactions between consumers and humanoid service robots (HSRs; i.e., robots with a human-like morphology such as a face, arms, and legs) will soon be part of routine marketplace experiences. It is unclear, however, whether these humanoid robots (compared with human employees) will trigger positive or negative consequences for consumers and companies. Seven experimental studies reveal that consumers display compensatory responses when they interact with an HSR rather than a human employee (e.g., they favor purchasing status goods, seek social affiliation, and order and eat more food). The authors investigate the underlying process driving these effects, and they find that HSRs elicit greater consumer discomfort (i.e., eeriness and a threat to human identity), which in turn results in the enhancement of compensatory consumption. Moreover, this research identifies boundary conditions of the effects such that the compensatory responses that HSRs elicit are (1) mitigated when consumer-perceived social belongingness is high, (2) attenuated when food is perceived as more healthful, and (3) buffered when the robot is machinized (rather than anthropomorphized).",2019-08-01,2025-08-28 13:06:25,2025-08-28 13:06:25,2025-08-28 13:06:25,535-556,,4,56,,,Service Robots Rising,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,C:\Users\Admin\Zotero\storage\5Q5T37JT\Mende et al. - 2019 - Service Robots Rising How Humanoid Robots Influence Service Experiences and Elicit Compensatory Con.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZFDXXHUV,journalArticle,2022,"Lacroux, Alain; Martin-Lacroux, Christelle",Should I Trust the Artificial Intelligence to Recruit? Recruiters’ Perceptions and Behavior When Faced With Algorithm-Based Recommendation Systems During Resume Screening,Frontiers in Psychology,,1664-1078,10.3389/fpsyg.2022.895997,https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.895997/full,"Resume screening assisted by decision support systems that incorporate artificial intelligence is currently undergoing a strong development in many organizations, raising technical, managerial, legal and ethical issues. The purpose of the present paper is to better understand the reactions of recruiters when they are offered algorithm-based recommendations during resume screening.Two polarized attitudes have been identified in the literature on users' reactions to algorithm-based recommendations: algorithm aversion, which reflects a general distrust and preference for human recommendations; and automation bias, which corresponds to an overconfidence in the decisions or recommendations made by algorithmic decision support systems (ADSS). Drawing on results obtained in the field of automated decision support areas, we make the general hypothesis that recruiters trust human experts more than ADSS, because they distrust algorithms for subjective decisions such as recruitment. An experiment on résumé screening was conducted on a sample of professionals (N=694) involved in the screening of job applications. They were asked to study a job offer, then evaluate two fictitious résumés in a 2×2 factorial design with manipulation of the type of recommendation (no recommendation/algorithmic recommendation/human expert recommendation) and of the consistency of the recommendations (consistent vs. inconsistent recommendation).Our results support the general hypothesis of preference for human recommendations: recruiters exhibit a higher level of trust toward human expert recommendations compared with algorithmic recommendations. However, we also found that recommendation’s consistence has a differential and unexpected impact on decisions: in the presence of an inconsistent algorithmic recommendation, recruiters favored the unsuitable over the suitable résumé. Our results also show that specific personality traits (extraversion, neuroticism and self-confidence) are associated with a differential use of algorithmic recommendations. Implications for research and HR policies are finally discussed.",2022-07-06,2025-08-28 13:06:29,2025-08-28 13:06:29,2025-08-28 13:06:29,,,,13,,Front. Psychol.,Should I Trust the Artificial Intelligence to Recruit?,,,,,,,English,,,,,Frontiers,,Publisher: Frontiers,,C:\Users\Admin\Zotero\storage\A2CHSDHR\Lacroux and Martin-Lacroux - 2022 - Should I Trust the Artificial Intelligence to Recruit Recruiters’ Perceptions and Behavior When Fac.pdf,,,Algorithm aversion; Artificial Inteligence-AI; automation bias; Resume screening; Trust,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YM9HI2WM,preprint,2024,"Wei, Jerry; Huang, Da; Lu, Yifeng; Zhou, Denny; Le, Quoc V.",Simple synthetic data reduces sycophancy in large language models,,,,10.48550/arXiv.2308.03958,http://arxiv.org/abs/2308.03958,"Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior. First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well. To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP tasks and encourages models to be robust to user opinions on these tasks. Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts. Code for generating synthetic data for intervention can be found at https://github.com/google/sycophancy-intervention.",2024-02-15,2025-08-28 13:06:31,2025-08-28 13:06:31,2025-08-28 13:06:31,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2308.03958 [cs],,C:\Users\Admin\Zotero\storage\MRFFWFQD\Wei et al. - 2024 - Simple synthetic data reduces sycophancy in large language models.pdf; C:\Users\Admin\Zotero\storage\7PMVMMMQ\2308.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2308.03958,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ILFJENQ,journalArticle,2019,"Willemse, Christian J. A. M.; van Erp, Jan B. F.",Social Touch in Human–Robot Interaction: Robot-Initiated Touches can Induce Positive Responses without Extensive Prior Bonding,International Journal of Social Robotics,,1875-4805,10.1007/s12369-018-0500-9,https://doi.org/10.1007/s12369-018-0500-9,"Social touch interactions can, depending on the type and strength of the dyadic social relationship, elicit a plethora of physiological, emotional, and behavioral responses; both beneficial and disadvantageous. With the intention to expand the communicative capabilities of humanoid social robots, we investigated whether robot-initiated touches could elicit beneficial responses in the human user that are comparable to responses to human touch. In addition, we investigated whether having a pre-existing positive social bond with the robot modulates these responses. To this end, we conducted a $$2 \times 2$$between subjects experiment (N = 67) in which participants either did or did not establish a bond with the robot prior to interacting with it during stressful circumstances. This interaction either did or did not comprise robot-initiated touches. We hypothesized that robotic touches would attenuate the subjective and physiological stress responses during the stressful event (H1a), enhance the perceived relation with the robot (H1b), and increase one’s pro-social behavior (H1c), as contrasted with interactions without touch. Based on findings from human touch, we also expected that the effects of H1a and H1b would be more outspoken when a bond with the robot was established (H2). Our findings imply that robotic touches attenuated physiological stress responses and increased the perceived intimacy of the human–robot bond. No effects were found on pro-social behavior and all effects were independent of whether a bond was formed or not. Although no full support for our hypotheses was found, the findings suggest that robot-initiated touch can, under specific circumstances, be a valuable extension of a social robot’s nonverbal communication repertoire.",2019-04-01,2025-08-28 13:06:34,2025-08-28 13:06:34,2025-08-28 13:06:33,285-304,,2,11,,Int J of Soc Robotics,Social Touch in Human–Robot Interaction,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\YSE9TVVW\Willemse and van Erp - 2019 - Social Touch in Human–Robot Interaction Robot-Initiated Touches can Induce Positive Responses witho.pdf,,,Human–robot bonding; Human–robot interaction; Social touch; Stress-attenuation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PEWDEEI6,preprint,2023,"Weidinger, Laura; Rauh, Maribeth; Marchal, Nahema; Manzini, Arianna; Hendricks, Lisa Anne; Mateos-Garcia, Juan; Bergman, Stevie; Kay, Jackie; Griffin, Conor; Bariach, Ben; Gabriel, Iason; Rieser, Verena; Isaac, William",Sociotechnical Safety Evaluation of Generative AI Systems,,,,10.48550/arXiv.2310.11986,http://arxiv.org/abs/2310.11986,"Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.",2023-10-31,2025-08-28 13:06:35,2025-08-28 13:06:35,2025-08-28 13:06:35,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2310.11986 [cs],,C:\Users\Admin\Zotero\storage\N8J2HMF2\Weidinger et al. - 2023 - Sociotechnical Safety Evaluation of Generative AI Systems.pdf; C:\Users\Admin\Zotero\storage\VDIMAAS2\2310.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,arXiv:2310.11986,,,,,,,,,,,,,,,,,,,,,,,,,,,
55SQHNS7,journalArticle,2021,"Jia, Chenyan; Johnson, Thomas J.",Source Credibility Matters: Does Automated Journalism Inspire Selective Exposure?,International Journal of Communication,,1932-8036,,https://ijoc.org/index.php/ijoc/article/view/16546,"To examine whether selective exposure occurs when people read news attributed to an algorithm author, this study conducted a 2 (author attribution: human or algorithm) × 3 (article attitude: attitude-consistent news, attitude-challenging news, or neutral story) × 2 (article topic: gun control or abortion) mixed-design online experiment (N = 351). By experimentally manipulating the attribution of authorship, this study found that selective exposure and selective avoidance were practiced when the news article was declared to be written by algorithms. Results revealed that people were more likely to select attitude-consistent news rather than attitude-challenging news and rated attitude consistent news stories as more credible than attitude challenging news for stories purportedly written by both algorithms and human journalists. For attitude-consistent gun-rights stories, people were more likely to expose themselves to human attribution stories rather than algorithmic attribution stories. Results also showed that source credibility partially mediated the influence of issue partisanship on selective exposure for gun stories. This study bears implications on the selective exposure theory and the emerging field of automated journalism.",2021-08-14,2025-08-28 13:08:18,2025-08-28 13:08:18,2025-08-28 13:08:18,22,,0,15,,,Source Credibility Matters,,,,,,,en,Copyright (c),,,,ijoc.org,,,,C:\Users\Admin\Zotero\storage\LVAEVC64\Jia and Johnson - 2021 - Source Credibility Matters Does Automated Journalism Inspire Selective Exposure.pdf,,,algorithm; automated journalism; message credibility; selective avoidance; selective exposure; source credibility,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BNAQBN55,journalArticle,2025,"Levinstein, B. A.; Herrmann, Daniel A.",Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,Philosophical Studies,,"0031-8116, 1573-0883",10.1007/s11098-023-02094-3,http://arxiv.org/abs/2307.00175,"We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",2025-07,2025-08-28 13:06:39,2025-08-28 13:06:39,2025-08-28 13:06:39,1539-1565,,7,182,,Philos Stud,Still No Lie Detector for Language Models,,,,,,,,,,,,arXiv.org,,arXiv:2307.00175 [cs],,C:\Users\Admin\Zotero\storage\YQMNZ9SU\Levinstein and Herrmann - 2025 - Still No Lie Detector for Language Models Probing Empirical and Conceptual Roadblocks.pdf; C:\Users\Admin\Zotero\storage\2FN6ZI3M\2307.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DEGRDVGE,journalArticle,2018,"Strani, Katerina; Szczepaniak-Kozak, Anna",Strategies of othering through discursive practices: Examples from the UK and Poland,Lodz Papers in Pragmatics,,1898-4436,10.1515/lpp-2018-0008,https://www.degruyterbrill.com/document/doi/10.1515/lpp-2018-0008/html,"This article discusses findings of a qualitative study on strategies of othering observed in anti-immigrant discourse, by analysing selected examples from the UK and Polish media, together with data collected from interviews with migrants. The purpose is to identify discursive strategies of othering, which aim to categorise, denigrate, oppress and ultimately reject the stigmatised or racialised ‘other’. We do not offer a systematic comparison of the data from the UK and Poland; instead, we are interested in what is common in the discursive practices of these two countries/contexts. In using newspaper together with interview data, we are combining representation and experience in identifying not only strategies of othering, but also how these are perceived by and affect the othered individuals. The paper uses the following data: 40 newspaper articles – 20 from the UK and 20 from Poland, and 19 interviews – 12 from Poland and 7 from the UK. The analysis that follows identifies five shared strategies of othering: a) Stereotyping; b) Whiteness as the norm; c) Racialisation; d) Objectification; e) Wrongly Ascribed Ethnicity. We conclude with the research limitations and outlining possible next stages, such as working with a larger corpus, investigating frequency, or including other media genres.",2018-06-01,2025-08-28 13:06:40,2025-08-28 13:06:40,2025-08-28 13:06:40,163-179,,1,14,,,Strategies of othering through discursive practices,,,,,,,en,De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.,,,,www.degruyterbrill.com,,Publisher: De Gruyter Mouton,,,,,discursive strategies; othering; Poland; racialisation; stereotyping; UK,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6BCK3T3X,preprint,2023,"Franklin, Matija; Tomei, Philip Moreira; Gorman, Rebecca",Strengthening the EU AI Act: Defining Key Terms on AI Manipulation,,,,10.48550/arXiv.2308.16364,http://arxiv.org/abs/2308.16364,"The European Union's Artificial Intelligence Act aims to regulate manipulative and harmful uses of AI, but lacks precise definitions for key concepts. This paper provides technical recommendations to improve the Act's conceptual clarity and enforceability. We review psychological models to define ""personality traits,"" arguing the Act should protect full ""psychometric profiles."" We urge expanding ""behavior"" to include ""preferences"" since preferences causally influence and are influenced by behavior. Clear definitions are provided for ""subliminal,"" ""manipulative,"" and ""deceptive"" techniques, considering incentives, intent, and covertness. We distinguish ""exploiting individuals"" from ""exploiting groups,"" emphasising different policy needs. An ""informed decision"" is defined by four facets: comprehension, accurate information, no manipulation, and understanding AI's influence. We caution the Act's therapeutic use exemption given the lack of regulation of digital therapeutics by the EMA. Overall, the recommendations strengthen definitions of vague concepts in the EU AI Act, enhancing precise applicability to regulate harmful AI manipulation.",2023-08-30,2025-08-28 13:06:43,2025-08-28 13:06:43,2025-08-28 13:06:43,,,,,,,Strengthening the EU AI Act,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2308.16364 [cs],,C:\Users\Admin\Zotero\storage\EPEYYDME\Franklin et al. - 2023 - Strengthening the EU AI Act Defining Key Terms on AI Manipulation.pdf; C:\Users\Admin\Zotero\storage\8CGS4T6C\2308.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2308.16364,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y3I5JS6Z,preprint,2018,"Christiano, Paul; Shlegeris, Buck; Amodei, Dario",Supervising strong learners by amplifying weak experts,,,,10.48550/arXiv.1810.08575,http://arxiv.org/abs/1810.08575,"Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",2018-10-19,2025-08-28 13:06:45,2025-08-28 13:06:45,2025-08-28 13:06:45,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1810.08575 [cs],,C:\Users\Admin\Zotero\storage\EK3MI67H\Christiano et al. - 2018 - Supervising strong learners by amplifying weak experts.pdf; C:\Users\Admin\Zotero\storage\5ZGAIMQS\1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:1810.08575,,,,,,,,,,,,,,,,,,,,,,,,,,,
RSCN3WJI,preprint,2023,"Griffin, Lewis D.; Kleinberg, Bennett; Mozes, Maximilian; Mai, Kimberly T.; Vau, Maria; Caldwell, Matthew; Marvor-Parker, Augustine",Susceptibility to Influence of Large Language Models,,,,10.48550/arXiv.2303.06074,http://arxiv.org/abs/2303.06074,"Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.",2023-03-10,2025-08-28 13:06:48,2025-08-28 13:06:48,2025-08-28 13:06:47,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2303.06074 [cs],,C:\Users\Admin\Zotero\storage\VPH46YX6\Griffin et al. - 2023 - Susceptibility to Influence of Large Language Models.pdf; C:\Users\Admin\Zotero\storage\BKNHNVBQ\2303.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2303.06074,,,,,,,,,,,,,,,,,,,,,,,,,,,
JRJ7XWJM,journalArticle,2019,"Susser, D.; Roessler, B.; Nissenbaum, H.","Technology, autonomy, and manipulation",Internet Policy Review,,2197-6775,10.14763/2019.2.1410,https://policyreview.info/node/1410,"Since 2016, when the Facebook/Cambridge Analytica scandal began to emerge, public concern has grown around the threat of “online manipulation”. While these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers—first, by defining “online manipulation”, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. We argue that online manipulation is the use of information technology to covertly influence another person’s decision-making, by targeting and exploiting their decision-making vulnerabilities. Engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. We explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.",2019,2025-08-28 12:02:41,2025-08-28 13:07:22,,,,2,8,,Internet Policy Review,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6TMA6957,preprint,2025,"Shin, Minkyu; Kim, Jin; Shin, Jiwoong",The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry,,,,10.48550/arXiv.2311.16466,http://arxiv.org/abs/2311.16466,"Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. We analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We find that LLM usage is associated with an increased likelihood of obtaining relief from financial firms. To investigate this relationship, we employ an instrumental variable approach to mitigate endogeneity concerns around LLM adoption. Although instrumental variables suggest a potential causal link, they cannot fully capture all unobserved heterogeneity. To further establish this causal relationship, we conducted controlled experiments, which support that LLMs can enhance the clarity and persuasiveness of consumer narratives, thereby increasing the likelihood of obtaining relief. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.",2025-02-14,2025-08-28 12:41:41,2025-08-28 12:41:41,2025-08-28 12:41:41,,,,,,,The Adoption and Efficacy of Large Language Models,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2311.16466 [cs],,C:\Users\Admin\Zotero\storage\6WR9YG89\Shin et al. - 2025 - The Adoption and Efficacy of Large Language Models Evidence From Consumer Complaints in the Financi.pdf; C:\Users\Admin\Zotero\storage\7AFC6CNA\2311.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2311.16466,,,,,,,,,,,,,,,,,,,,,,,,,,,
N77DRT4H,journalArticle,2024,"Rhue, Lauren","The Anchoring Effect, Algorithmic Fairness, and the Limits of Information Transparency for Emotion Artificial Intelligence",Information Systems Research,,1047-7047,10.1287/isre.2019.0493,https://pubsonline.informs.org/doi/10.1287/isre.2019.0493,"Emotion artificial intelligence (AI) or emotion recognition AI may systematically vary in its recognition of facial expressions and emotions across demographic groups, creating inconsistencies and disparities in its scoring. This paper explores the extent to which individuals can compensate for these disparities and inconsistencies in emotion AI considering two opposing factors; although humans evolved to recognize emotions, particularly happiness, they are also subject to cognitive biases, such as the anchoring effect. To help understand these dynamics, this study tasks three commercially available emotion AIs and a group of human labelers to identify emotions from faces in two image data sets. The scores generated by emotion AI and human labelers are examined for inference inconsistencies (i.e., misalignment between facial expression and emotion label). The human labelers are also provided with the emotion AI’s scores and with measures of its scoring fairness (or lack thereof). We observe that even when human labelers are operating in this context of information transparency, they may still rely on the emotion AI’s scores, perpetuating its inconsistencies. Several findings emerge from this study. First, the anchoring effect appears to be moderated by the type of inference inconsistency and is weaker for easier emotion recognition tasks. Second, when human labelers are provided with information transparency regarding the emotion AI’s fairness, the effect is not uniform across emotions. Also, there is no evidence that information transparency leads to the selective anchoring necessary to offset emotion AI disparities; in fact, some evidence suggests that information transparency increases human inference inconsistencies. Lastly, the different models of emotion AI are highly inconsistent in their scores, raising doubts about emotion AI more generally. Collectively, these findings provide evidence of the potential limitations of addressing algorithmic bias through individual decisions, even when those individuals are supported with information transparency. History: Alessandro Acquisti, Senior Editor; Monideepa Tarafdar, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2019.0493 .",2024-09,2025-08-28 13:10:13,2025-08-28 13:10:13,2025-08-28 13:10:13,1479-1496,,3,35,,,,,,,,,,,,,,,pubsonline.informs.org (Atypon),,Publisher: INFORMS,,,,,affective AI; algorithmic bias; algorithmic fairness; artificial intelligence; emotion AI; emotion recognition; fairness; information transparency; the anchoring effect,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SXASXS8R,journalArticle,2020,"Kipp, Peter C.; Curtis, Mary B.; Li, Ziyin",The Attenuating Effect of Intelligent Agents and Agent Autonomy on Managers' Ability to Diffuse Responsibility for and Engage in Earnings Management,Accounting Horizons,,0888-7993,10.2308/HORIZONS-19-133,https://doi.org/10.2308/HORIZONS-19-133,"Advances in IT suggest that computerized intelligent agents (IAs) may soon occupy many roles that presently employ human agents. A significant concern is the ethical conduct of those who use IAs, including their possible utilization by managers to engage in earnings management. We investigate how financial reporting decisions are affected when they are supported by the work of an IA versus a human agent, with varying autonomy. In an experiment with experienced managers, we vary agent type (human versus IA) and autonomy (more versus less), finding that managers engage in less aggressive financial reporting decisions with IAs than with human agents, and engage in less aggressive reporting decisions with less autonomous agents than with more autonomous agents. Managers' perception of control over their agent and ability to diffuse their own responsibility for financial reporting decisions explain the effect of agent type and autonomy on managers' financial reporting decisions.",2020-12-01,2025-08-28 13:10:18,2025-08-28 13:10:18,2025-08-28 13:10:18,143-164,,4,34,,Accounting Horizons,,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\HXWEPGR2\HORIZONS-19-133.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
92GA49UG,journalArticle,2024,"Mills, Stuart; Sætra, Henrik Skaug",The autonomous choice architect,AI & SOCIETY,,1435-5655,10.1007/s00146-022-01486-z,https://doi.org/10.1007/s00146-022-01486-z,"Choice architecture describes the environment in which choices are presented to decision-makers. In recent years, public and private actors have looked at choice architecture with great interest as they seek to influence human behaviour. These actors are typically called choice architects. Increasingly, however, this role of architecting choice is not performed by a human choice architect, but an algorithm or artificial intelligence, powered by a stream of Big Data and infused with an objective it has been programmed to maximise. We call this entity the autonomous choice architect. In this paper, we present an account of why artificial intelligence can fulfil the role of a choice architect and why this creates problems of transparency, responsibility and accountability for nudges. We argue that choice architects, be them autonomous computational systems or human-beings, at a most basic level select, from a range of designs, the design which is most likely to maximise a pre-determined objective. We then proceed to argue that, given the growing demand for targeted, personalised choice architecture and for faster, dynamic reconfigurations of choice architecture, as well as the ever-expanding pool of data from which feedback can be drawn, the role of the human choice architect is increasingly obscured behind algorithmic, artificially intelligent systems. We provide a discussion of the implications of autonomous choice architects, focusing on the importance of the humans who programme these systems, ultimately arguing that despite technological advances, the responsibility of choice architecture and influence remains firmly one human beings must bear.",2024-04-01,2025-08-28 13:10:19,2025-08-28 13:10:19,2025-08-28 13:10:19,583-595,,2,39,,AI & Soc,,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\8H7ZWRYY\Mills and Sætra - 2024 - The autonomous choice architect.pdf,,,Artificial intelligence; Behavioural science; Big data; Choice architecture; D9; D91; Nudge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ESUAQFBF,journalArticle,2024,"Huang, Ming-Hui; Rust, Roland T.",The Caring Machine: Feeling AI for Customer Care,Journal of Marketing,,0022-2429,10.1177/00222429231224748,https://doi.org/10.1177/00222429231224748,"Customer care is important for its role in relationship building. This role has traditionally been performed by human customer agents; however, the emergence of interactive generative AI (GenAI) shows potential for using AI for customer care in emotionally charged interactions. Bridging practice and the academic literatures in marketing and computer science, this article develops an AI-enabled customer care journey, from accurate emotion recognition to empathetic response, emotional management support, and, finally, the establishment of an emotional connection. Marketing requirements for each of the stages are derived from in-depth interviews with top managers and a survey of chief marketing officers. By juxtaposing these requirements against the current feeling capabilities of GenAI, the authors highlight the technological challenges engineers must tackle. The article concludes with a set of marketing tenets for implementing and researching the caring machine. These include verifying emotion recognition accuracy using marketing emotion theories through multiple emotion signals and methods, utilizing prompt engineering to enhance GenAI’s emotion understanding, employing “response engineering” to personalize emotion management recommendations, and strategically deploying GenAI for emotional connection to simultaneously enhance customer emotional well-being and customer lifetime value.",2024-09-01,2025-08-28 13:10:21,2025-08-28 13:10:21,2025-08-28 13:10:21,1-23,,5,88,,,The Caring Machine,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,C:\Users\Admin\Zotero\storage\S5QPX4DV\Huang and Rust - 2024 - The Caring Machine Feeling AI for Customer Care.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZY8LVDJP,journalArticle,2020,"Ullal, Mithun S.; Hawaldar, Iqbal T.; Mendon, Suhan; Joseph, Nympha R.",The effect of artificial intelligence on the sales graph in Indian market,Entrepreneurship and Sustainability Issues,,2345-0282,10.9770/jesi.2020.7.4(24),http://jssidoi.org/jesi/article/563,Entrepreneurship and Sustainability Issues is a peer-reviewed journal which publishes original research papers and case studies,2020,2025-08-28 13:10:23,2025-08-28 13:10:23,2025-08-28 13:10:23,2940-2954,,4,7,,,,,,,,,,en,,,,,jssidoi.org,,Publisher: Entrepreneurship and Sustainability Center PMID: 9770,,C:\Users\Admin\Zotero\storage\LAP7XCLV\Ullal et al. - 2020 - The effect of artificial intelligence on the sales graph in Indian market.pdf; ,http://www.ncbi.nlm.nih.gov/pubmed/9770,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MI42LB9W,conferencePaper,2019,"Torre, Ilaria; Carrigan, Emma; McDonnell, Rachel; Domijan, Katarina; McCabe, Killian; Harte, Naomi",The Effect of Multimodal Emotional Expression and Agent Appearance on Trust in Human-Agent Interaction,"Proceedings of the 12th ACM SIGGRAPH Conference on Motion, Interaction and Games",978-1-4503-6994-7,,10.1145/3359566.3360065,https://doi.org/10.1145/3359566.3360065,"Emotional expressivity can boost trust in human-human and human-machine interaction. As a multimodal phenomenon, previous research argued that a mismatch in the expressive channels provides evidence of joint audio-video emotional processing. However, while previous work studied this from the point of view of emotion recognition and processing, not much is known about what effect a multimodal agent would have on a human-agent interaction task. Also, agent appearance could influence this interaction too. Here we manipulated the agent’s multimodal emotional expression (”smiling face” and ”smiling voice”, or both) and agent type (photorealistic or cartoon-like virtual human) and assessed people’s trust toward this agent. We measured trust using a mixed-methods approach, combining behavioural data from a survival task, questionnaire ratings and qualitative comments. These methods gave different results: while people commented on the importance of emotional expressivity in the agent’s voice, this factor had limited influence on trusting behaviours; while people rated the cartoon-like agent on several traits higher than the photorealistic one, the agent’s style also was not the most influential feature on people’s trusting behaviour. These results highlight the contribution of a mixed-methods approach in human-machine interaction, as both explicit and implicit perception and behaviour will contribute to the success of the interaction.",2019,2025-08-28 13:10:25,2025-08-28 13:10:25,2025-08-28,1–6,,,,,,,MIG '19,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,C:\Users\Admin\Zotero\storage\7CZJDPRN\Torre et al. - 2019 - The Effect of Multimodal Emotional Expression and Agent Appearance on Trust in Human-Agent Interacti.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B39FDU34,journalArticle,2024,"Lim, Sue; Schmälzle, Ralf",The effect of source disclosure on evaluation of AI-generated messages: A two-part study,Computers in Human Behavior: Artificial Humans,,2949-8821,10.1016/j.chbah.2024.100058,http://arxiv.org/abs/2311.15544,"Advancements in artificial intelligence (AI) over the last decade demonstrate that machines can exhibit communicative behavior and influence how humans think, feel, and behave. In fact, the recent development of ChatGPT has shown that large language models (LLMs) can be leveraged to generate high-quality communication content at scale and across domains, suggesting that they will be increasingly used in practice. However, many questions remain about how knowing the source of the messages influences recipients' evaluation of and preference for AI-generated messages compared to human-generated messages. This paper investigated this topic in the context of vaping prevention messaging. In Study 1, which was pre-registered, we examined the influence of source disclosure on people's evaluation of AI-generated health prevention messages compared to human-generated messages. We found that source disclosure (i.e., labeling the source of a message as AI vs. human) significantly impacted the evaluation of the messages but did not significantly alter message rankings. In a follow-up study (Study 2), we examined how the influence of source disclosure may vary by the participants' negative attitudes towards AI. We found a significant moderating effect of negative attitudes towards AI on message evaluation, but not for message selection. However, for those with moderate levels of negative attitudes towards AI, source disclosure decreased the preference for AI-generated messages. Overall, the results of this series of studies showed a slight bias against AI-generated messages once the source was disclosed, adding to the emerging area of study that lies at the intersection of AI and communication.",2024,2025-08-28 12:02:41,2025-08-28 13:10:58,,100058,,1,2,,Computers in Human Behavior: Artificial Humans,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7IQVBUBU,journalArticle,2023,"Jang, Wonseok (Eric); Chun, Jung Won; Kim, Soojin; Kang, Young Woo",The Effects of Anthropomorphism on How People Evaluate Algorithm-Written News,Digital Journalism,,2167-0811,10.1080/21670811.2021.1976064,https://doi.org/10.1080/21670811.2021.1976064,"Based on the Modality-Agency-Interactivity-Navigability model and the anthropomorphism theory, this study examines whether embedding human-like characteristics in algorithms increases the persuasiveness of algorithm-written news. This study further investigates how different types of relationships (servant or friend) that human writers form with algorithms determine the persuasiveness of algorithm-generated news. Experiment 1 demonstrated that participants who read the human- and humanized algorithm-written news showed greater emotional involvement in the stories and liked the articles more than those who read the algorithm-written news. The participants also reported that news written by humans, humanized algorithms, and algorithms, had equal news credibility. Experiment 2 further showed that the participants perceived the news as more credible and experienced greater degrees of emotional involvement when human writers formed a partner-to-friend relationship with humanized algorithms while generating news rather than a servant-to-master relationship.",2023-01-02,2025-08-28 13:11:04,2025-08-28 13:11:04,2025-08-28 13:11:04,103-124,,1,11,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2021.1976064,,,,,algorithm; anthropomorphism theory; Computational journalism; emotional involvement; news evaluation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L65KEF4U,journalArticle,2023,"Schepman, Astrid; Rodway, Paul","The General Attitudes towards Artificial Intelligence Scale (GAAIS): Confirmatory Validation and Associations with Personality, Corporate Distrust, and General Trust",International Journal of Human–Computer Interaction,,1044-7318,10.1080/10447318.2022.2085400,https://doi.org/10.1080/10447318.2022.2085400,"Acceptance of Artificial Intelligence (AI) may be predicted by individual psychological correlates, examined here. Study 1 reports confirmatory validation of the General Attitudes towards Artificial Intelligence Scale (GAAIS) following initial validation elsewhere. Confirmatory Factor Analysis confirmed the two-factor structure (Positive, Negative) and showed good convergent and divergent validity with a related scale. Study 2 tested whether psychological factors (Big Five personality traits, corporate distrust, and general trust) predicted attitudes towards AI. Introverts had more positive attitudes towards AI overall, likely because of algorithm appreciation. Conscientiousness and agreeableness were associated with forgiving attitudes towards negative aspects of AI. Higher corporate distrust led to negative attitudes towards AI overall, while higher general trust led to positive views of the benefits of AI. The dissociation between general trust and corporate distrust may reflect the public’s attributions of the benefits and drawbacks of AI. Results are discussed in relation to theory and prior findings.",2023-08-09,2025-08-28 13:11:07,2025-08-28 13:11:07,2025-08-28 13:11:07,2724-2741,,13,39,,,The General Attitudes towards Artificial Intelligence Scale (GAAIS),,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2022.2085400,,C:\Users\Admin\Zotero\storage\3KLPX56X\Schepman and Rodway - 2023 - The General Attitudes towards Artificial Intelligence Scale (GAAIS) Confirmatory Validation and Ass.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JW4SW24E,preprint,2024,"Marks, Samuel; Tegmark, Max",The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets,,,,10.48550/arXiv.2310.06824,http://arxiv.org/abs/2310.06824,"Large Language Models (LLMs) have impressive capabilities, but are prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we use high-quality datasets of simple true/false statements to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that at sufficient scale, LLMs linearly represent the truth or falsehood of factual statements. We also show that simple difference-in-mean probes generalize as well as other probing techniques while identifying directions which are more causally implicated in model outputs.",2024-08-19,2025-08-28 13:11:09,2025-08-28 13:11:09,2025-08-28 13:11:09,,,,,,,The Geometry of Truth,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2310.06824 [cs],,C:\Users\Admin\Zotero\storage\U9FA4KWB\Marks and Tegmark - 2024 - The Geometry of Truth Emergent Linear Structure in Large Language Model Representations of TrueFal.pdf; C:\Users\Admin\Zotero\storage\HXQVLS3Z\2310.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2310.06824,,,,,,,,,,,,,,,,,,,,,,,,,,,
28E5FPGH,preprint,2023,"Ruis, Laura; Khan, Akbir; Biderman, Stella; Hooker, Sara; Rocktäschel, Tim; Grefenstette, Edward",The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs,,,,10.48550/arXiv.2210.14986,http://arxiv.org/abs/2210.14986,"Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context -- incorporating its pragmatics. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response ""I wore gloves"" to the question ""Did you leave fingerprints?"" as meaning ""No"". To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate four categories of widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), models in three of these categories perform close to random. However, LLMs instruction-tuned at the example-level perform significantly better. These results suggest that certain fine-tuning strategies are far better at inducing pragmatic understanding in models. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.",2023-12-03,2025-08-28 13:11:11,2025-08-28 13:11:11,2025-08-28 13:11:11,,,,,,,The Goldilocks of Pragmatic Understanding,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2210.14986 [cs],,C:\Users\Admin\Zotero\storage\7QEMQBIT\Ruis et al. - 2023 - The Goldilocks of Pragmatic Understanding Fine-Tuning Strategy Matters for Implicature Resolution b.pdf; C:\Users\Admin\Zotero\storage\J8RHN6DU\2210.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2210.14986,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMCL3575,journalArticle,2023,"Azaria, Amos; Mitchell, Tom",The Internal State of an LLM Knows When It's Lying,,,,10.48550/arXiv.2304.13734,http://arxiv.org/abs/2304.13734,"While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",2023,2025-08-28 12:02:44,2025-08-28 12:02:44,2025-08-27,,,,,,,,,,,,,,,,,,,,,,,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YWRWCZN8,journalArticle,2018,"Antonetti, Paolo; Baines, Paul; Jain, Shailendra",The persuasiveness of guilt appeals over time: Pathways to delayed compliance,Journal of Business Research,,0148-2963,10.1016/j.jbusres.2018.03.030,https://www.sciencedirect.com/science/article/pii/S0148296318301589,"Past research on guilt-elicitation in marketing does not examine how the communications' effects might persist over time, when there is a gap between advertising at time 1 and the time of choice consideration at time 2. This study explores the processes leading to delayed compliance through guilt-based communications. Guilt elicitation enhances transportation into the message, driving message compliance through the effect of transportation. Transportation explains the effects recorded several days after campaign exposure. The influence of transportation is mediated by two pathways: increases in anticipated guilt and perceived consumer effectiveness. The message type moderates the relevance of different pathways in explaining persuasiveness. Appeals delivered through a text and image message (rather than text only) are more effective in driving compliance and shape reactions via guilt anticipation. The study raises important implications for research on the use of guilt appeals and the design of more effective messages based on this emotion.",2018-09-01,2025-08-28 13:11:15,2025-08-28 13:11:15,2025-08-28 13:11:15,14-25,,,90,,Journal of Business Research,The persuasiveness of guilt appeals over time,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\BLP8JE37\S0148296318301589.html,,,Anticipated guilt; Emotions; Guilt appeals; Guilt elicitation; Narrative transportation; Persuasion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WH63VDSS,journalArticle,2023,"Cloudy, Joshua; Banks, Jaime; Bowman, Nicholas David",The Str(AI)ght Scoop: Artificial Intelligence Cues Reduce Perceptions of Hostile Media Bias,Digital Journalism,,2167-0811,10.1080/21670811.2021.1969974,https://doi.org/10.1080/21670811.2021.1969974,"In the face of increasing public distrust for journalistic institutions, stories sourced from artificially intelligent (AI) journalists have the potential to lower hostile media bias by activating the machine heuristic—a mental shortcut assuming machines are more unbiased, systematic, and accurate than are humans. An online experiment targeting issue partisans found support for the prediction: a story presented as sourced from an AI journalist activated the machine heuristic that, in turn, mitigated hostile media bias. This mediation effect was moderated: perceived bias was more strongly reduced as partisan-attitude extremity increased.",2023-10-21,2025-08-28 13:12:33,2025-08-28 13:12:33,2025-08-28 13:12:33,1577-1596,,9,11,,,The Str(AI)ght Scoop,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2021.1969974,,,,,AI journalism; automated journalism; hostile media bias; hostile media effect; human-computer interaction; Human-machine communication; machine agents; robot journalism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L5EQJWVD,journalArticle,2022,"Yalcin, Gizem; Lim, Sarah; Puntoni, Stefano; van Osselaer, Stijn M.J.",Thumbs Up or Down: Consumer Reactions to Decisions by Algorithms Versus Humans,Journal of Marketing Research,,0022-2437,10.1177/00222437211070016,https://doi.org/10.1177/00222437211070016,"Although companies increasingly are adopting algorithms for consumer-facing tasks (e.g., application evaluations), little research has compared consumers’ reactions to favorable decisions (e.g., acceptances) versus unfavorable decisions (e.g., rejections) about themselves that are made by an algorithm versus a human. Ten studies reveal that, in contrast to managers’ predictions, consumers react less positively when a favorable decision is made by an algorithmic (vs. a human) decision maker, whereas this difference is mitigated for an unfavorable decision. The effect is driven by distinct attribution processes: it is easier for consumers to internalize a favorable decision outcome that is rendered by a human than by an algorithm, but it is easy to externalize an unfavorable decision outcome regardless of the decision maker type. The authors conclude by advising managers on how to limit the likelihood of less positive reactions toward algorithmic (vs. human) acceptances.",2022-08-01,2025-08-28 13:12:28,2025-08-28 13:12:28,2025-08-28 13:12:28,696-717,,4,59,,,Thumbs Up or Down,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,C:\Users\Admin\Zotero\storage\YW6BGNXS\Yalcin et al. - 2022 - Thumbs Up or Down Consumer Reactions to Decisions by Algorithms Versus Humans.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QBAAIJ2V,journalArticle,2022,"Lebovitz, Sarah; Lifshitz-Assaf, Hila; Levina, Natalia",To Engage or Not to Engage with AI for Critical Judgments: How Professionals Deal with Opacity When Using AI for Medical Diagnosis,Organization Science,,1047-7039,10.1287/orsc.2021.1549,https://pubsonline.informs.org/doi/10.1287/orsc.2021.1549,"Artificial intelligence (AI) technologies promise to transform how professionals conduct knowledge work by augmenting their capabilities for making professional judgments. We know little, however, about how human-AI augmentation takes place in practice. Yet, gaining this understanding is particularly important when professionals use AI tools to form judgments on critical decisions. We conducted an in-depth field study in a major U.S. hospital where AI tools were used in three departments by diagnostic radiologists making breast cancer, lung cancer, and bone age determinations. The study illustrates the hindering effects of opacity that professionals experienced when using AI tools and explores how these professionals grappled with it in practice. In all three departments, this opacity resulted in professionals experiencing increased uncertainty because AI tool results often diverged from their initial judgment without providing underlying reasoning. Only in one department (of the three) did professionals consistently incorporate AI results into their final judgments, achieving what we call engaged augmentation. These professionals invested in AI interrogation practices—practices enacted by human experts to relate their own knowledge claims to AI knowledge claims. Professionals in the other two departments did not enact such practices and did not incorporate AI inputs into their final decisions, which we call unengaged “augmentation.” Our study unpacks the challenges involved in augmenting professional judgment with powerful, yet opaque, technologies and contributes to literature on AI adoption in knowledge work.",2022-01,2025-08-28 13:12:47,2025-08-28 13:12:47,2025-08-28 13:12:47,126-148,,1,33,,,To Engage or Not to Engage with AI for Critical Judgments,,,,,,,,,,,,pubsonline.informs.org (Atypon),,Publisher: INFORMS,,C:\Users\Admin\Zotero\storage\8CTQ4LZ2\Lebovitz et al. - 2022 - To Engage or Not to Engage with AI for Critical Judgments How Professionals Deal with Opacity When.pdf,,,artificial intelligence; augmentation; decision making; expertise; explainability; innovation; medical diagnosis; opacity; professional judgment; technology adoption and use; transparency; uncertainty,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QYTA29UU,journalArticle,2023,"Zhang, Andong; Patrick Rau, Pei-Luen",Tools or peers? Impacts of anthropomorphism level and social role on emotional attachment and disclosure tendency towards intelligent agents,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2022.107415,https://www.sciencedirect.com/science/article/pii/S0747563222002370,"Owing to the development of anthropomorphic intelligent agent (IA) designs, users consider IAs as more than just inanimate tools. Previous studies have reported that anthropomorphic features can promote users' social feedback and aid in establishing intimate human–agent relationships. The present study examined the main and interaction effects of anthropomorphism level (a human-like IA vs. robot-like IA) and social role (servant vs. mentor) on emotional attachment, information disclosure tendency, and satisfaction in a smart home. The study participants were randomly assigned into four groups with balanced gender. The results indicate that high anthropomorphism and mentor role can positively predict users' emotional attachment. Additionally, users tend to disclose more personal information to the human-servant and robot-mentor IAs than the human-mentor and robot-servant IAs. Interestingly, social presence was determined to be a positive and significant mediator between anthropomorphic design and emotional attachment. The study findings highlight the importance of social role in anthropomorphic IA design and explain the mechanism of establishing effective human–agent relationships. Moreover, both theoretical and practical implications of these findings are analyzed.",2023-01-01,2025-08-28 13:12:35,2025-08-28 13:12:35,2025-08-28 13:12:35,107415,,,138,,Computers in Human Behavior,Tools or peers?,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\JPZXVBZB\S0747563222002370.html,,,Anthropomorphic intelligent agents; Emotional attachment; Human–agent relationship; Self-disclosure; Social role,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5TS5WEN3,preprint,2023,"Räuker, Tilman; Ho, Anson; Casper, Stephen; Hadfield-Menell, Dylan",Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks,,,,10.48550/arXiv.2207.13243,http://arxiv.org/abs/2207.13243,"The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, ""inner"" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.",2023-08-18,2025-08-28 13:12:50,2025-08-28 13:12:50,2025-08-28 13:12:49,,,,,,,Toward Transparent AI,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2207.13243 [cs],,C:\Users\Admin\Zotero\storage\Q7QPSPPL\Räuker et al. - 2023 - Toward Transparent AI A Survey on Interpreting the Inner Structures of Deep Neural Networks.pdf; C:\Users\Admin\Zotero\storage\EAJV3RRB\2207.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2207.13243,,,,,,,,,,,,,,,,,,,,,,,,,,,
F4Y8WFNP,journalArticle,2021,"Kim, Yunju; Lee, Heejun",Towards a Sustainable News Business: Understanding Readers’ Perceptions of Algorithm-Generated News Based on Cultural Conditioning,Sustainability,,2071-1050,10.3390/su13073728,https://www.mdpi.com/2071-1050/13/7/3728,"The use of algorithms is beginning to replace human activities in the news business, and the presence of this technique will only continue to grow. The ways in which public news readers perceive the quality of news articles written by algorithms and how this perception differs based on cultural conditioning remain issues of debate. Informed by the heuristic-systematic model (HSM) and the similarity-attraction theory, we attempted to answer these questions by conducting a three-way one-way analysis of variance (ANOVA) test with a 2 (author: algorithm vs. human journalist) × 2 (media: traditional media vs. online media) × 2 (cultural background: the US vs. South Korea) between-subjects experiment (N = 360). Our findings revealed that participants perceived the quality of news articles written by algorithms to be higher than those written by human journalists. We also found that when news consumption occurs online, algorithm-generated news tends to be rated higher than human-written news in terms of quality perception. Further, we identified a three-way interaction effect of media types, authors, and cultural backgrounds on the quality perception of news articles. As, to the best of our knowledge, this study is the first to theoretically examine how news readers perceive algorithm-generated news from a cultural point of view, our research findings may hold important theoretical and practical implications.",2021-01,2025-08-28 13:12:51,2025-08-28 13:12:51,2025-08-28 13:12:51,3728,,7,13,,,Towards a Sustainable News Business,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,Publisher: Multidisciplinary Digital Publishing Institute,,C:\Users\Admin\Zotero\storage\WYAA2GTX\Kim and Lee - 2021 - Towards a Sustainable News Business Understanding Readers’ Perceptions of Algorithm-Generated News.pdf,,,algorithm; artificial intelligence; cultural difference; journalist; robot journalism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TSJ8QCH8,preprint,2023,"Mozes, Maximilian; Hoffmann, Jessica; Tomanek, Katrin; Kouate, Muhamed; Thain, Nithum; Yuan, Ann; Bolukbasi, Tolga; Dixon, Lucas",Towards Agile Text Classifiers for Everyone,,,,10.48550/arXiv.2302.06541,http://arxiv.org/abs/2302.06541,"Text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. This paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy. Experimenting with 7 datasets from three safety-related domains, comprising 15 annotation schemes, led to our key finding: prompt-tuning large language models, like PaLM 62B, with a labeled dataset of as few as 80 examples can achieve state-of-the-art performance. We argue that this enables a paradigm shift for text classification, especially for models supporting safer online discourse. Instead of collecting millions of examples to attempt to create universal safety classifiers over months or years, classifiers could be tuned using small datasets, created by individuals or small organizations, tailored for specific use cases, and iterated on and adapted in the time-span of a day.",2023-10-21,2025-08-28 13:12:53,2025-08-28 13:12:53,2025-08-28 13:12:53,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2302.06541 [cs],,C:\Users\Admin\Zotero\storage\BCNVR4E6\Mozes et al. - 2023 - Towards Agile Text Classifiers for Everyone.pdf; C:\Users\Admin\Zotero\storage\MBN6CNAR\2302.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2302.06541,,,,,,,,,,,,,,,,,,,,,,,,,,,
85BM8TXG,conferencePaper,2021,"Liu, Siyang; Zheng, Chujie; Demasi, Orianna; Sabour, Sahand; Li, Yu; Yu, Zhou; Jiang, Yong; Huang, Minlie",Towards Emotional Support Dialog Systems,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),,,10.18653/v1/2021.acl-long.269,https://aclanthology.org/2021.acl-long.269/,"Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems.",2021-08,2025-08-28 13:12:55,2025-08-28 13:12:55,2025-08-28 13:12:55,3469–3483,,,,,,,,,,,Association for Computational Linguistics,Online,,,,,,ACLWeb,,,,C:\Users\Admin\Zotero\storage\BAE6FFW2\Liu et al. - 2021 - Towards Emotional Support Dialog Systems.pdf,,,,"Zong, Chengqing; Xia, Fei; Li, Wenjie; Navigli, Roberto",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ACL-IJCNLP 2021,,,,,,,,,,,,,,,
95BWJJYT,journalArticle,2022,"Bai, Yuntao; Jones, Andy; Ndousse, Kamal; Askell, Amanda; Chen, Anna; DasSarma, Nova; Drain, Dawn; Fort, Stanislav; Ganguli, Deep; Henighan, Tom; Joseph, Nicholas; Kadavath, Saurav; Kernion, Jackson; Conerly, Tom; El-Showk, Sheer; Elhage, Nelson; Hatfield-Dodds, Zac; Hernandez, Danny; Hume, Tristan; Johnston, Scott; Kravec, Shauna; Lovitt, Liane; Nanda, Neel; Olsson, Catherine; Amodei, Dario; Brown, Tom; Clark, Jack; McCandlish, Sam; Olah, Chris; Mann, Ben; Kaplan, Jared",Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,,,,10.48550/arXiv.2204.05862,http://arxiv.org/abs/2204.05862,"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",2022,2025-08-28 12:02:44,2025-08-28 12:02:44,2025-08-27,,,,,,,,,,,,,,,,,,,,,,,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MCRWA38P,preprint,2022,"Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan",Training language models to follow instructions with human feedback,,,,10.48550/arXiv.2203.02155,http://arxiv.org/abs/2203.02155,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",2022-03-04,2025-08-28 13:13:48,2025-08-28 13:13:48,2025-08-28 13:13:48,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2203.02155 [cs],,C:\Users\Admin\Zotero\storage\ZIWVRWCK\Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf; C:\Users\Admin\Zotero\storage\S55NMYVD\2203.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2203.02155,,,,,,,,,,,,,,,,,,,,,,,,,,,
52TLEJNH,journalArticle,2023,"Choung, Hyesun; David, Prabu; Ross, Arun",Trust in AI and Its Role in the Acceptance of AI Technologies,International Journal of Human–Computer Interaction,,1044-7318,10.1080/10447318.2022.2050543,https://doi.org/10.1080/10447318.2022.2050543,"As AI-enhanced technologies become common in a variety of domains, there is an increasing need to define and examine the trust that users have in such technologies. Given the progress in the development of AI, a correspondingly sophisticated understanding of trust in the technology is required. This paper addresses this need by explaining the role of trust in the intention to use AI technologies. Study 1 examined the role of trust in the use of AI voice assistants based on survey responses from college students. A path analysis confirmed that trust had a significant effect the on intention to use AI, which operated through perceived usefulness and participants’ attitude toward voice assistants. In Study 2, using data from a representative sample of the U.S. population, different dimensions of trust were examined using exploratory factor analysis, which yielded two dimensions: human-like trust and functionality trust. The results of the path analyses from Study 1 were replicated in Study 2, confirming the indirect effect of trust and the effects of perceived usefulness, ease of use, and attitude on intention to use. Further, both dimensions of trust shared a similar pattern of effects within the model, with functionality-related trust exhibiting a greater total impact on usage intention than human-like trust. Overall, the role of trust in the acceptance of AI technologies was significant across both studies. This research contributes to the advancement and application of the TAM in AI-related applications and offers a multidimensional measure of trust that can be utilized in the future study of trustworthy AI.",2023-05-28,2025-08-28 13:13:51,2025-08-28 13:13:51,2025-08-28 13:13:51,1727-1739,,9,39,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2022.2050543,,C:\Users\Admin\Zotero\storage\YERHU62J\Choung et al. - 2023 - Trust in AI and Its Role in the Acceptance of AI Technologies.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q9N2W7TQ,journalArticle,2023,"Zhang, Guanglu; Chong, Leah; Kotovsky, Kenneth; Cagan, Jonathan",Trust in an AI versus a Human teammate: The effects of teammate identity and performance on Human-AI cooperation,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2022.107536,https://www.sciencedirect.com/science/article/pii/S0747563222003569,"Recent advances in artificial intelligence (AI) enable researchers to create more powerful AI agents that are becoming competent teammates for humans. However, human distrust of AI is a critical factor that may impede human-AI cooperation. Although AI agents have been endowed with anthropomorphic traits, such as a human-like appearance, in prior studies to improve human trust in AI, it is still an open question whether humans have more trust in an AI teammate and achieve better human-AI joint performance if they are deceived about the identity of their AI teammate as another human. This research assesses the effects of teammate identity (“human” vs. AI) and teammate performance (low-performing vs. high-performing AI) on human-AI cooperation through a human subjects study. The results of this study show that humans behaviorally trust the AI more than another human by accepting their AI teammate's decisions more often. In addition, teammate performance has a significant effect on human-AI joint performance in the study, while teammate identity does not. These results caution against deceiving humans about the identity of AI in future applications involving human-AI cooperation.",2023-02-01,2025-08-28 13:13:53,2025-08-28 13:13:53,2025-08-28 13:13:53,107536,,,139,,Computers in Human Behavior,Trust in an AI versus a Human teammate,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\KZRCSJHM\Zhang et al. - 2023 - Trust in an AI versus a Human teammate The effects of teammate identity and performance on Human-AI.pdf; C:\Users\Admin\Zotero\storage\K4J36IQU\S0747563222003569.html,,,Anthropomorphism; Artificial intelligence; Deception; Decision-making; Human-computer interaction; Trust,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RKCM53I8,journalArticle,2023,"Kaplan, Alexandra D.; Kessler, Theresa T.; Brill, J. Christopher; Hancock, P. A.",Trust in Artificial Intelligence: Meta-Analytic Findings,Human Factors,,0018-7208,10.1177/00187208211013988,https://doi.org/10.1177/00187208211013988,"ObjectiveThe present meta-analysis sought to determine significant factors that predict trust in artificial intelligence (AI). Such factors were divided into those relating to (a) the human trustor, (b) the AI trustee, and (c) the shared context of their interaction.BackgroundThere are many factors influencing trust in robots, automation, and technology in general, and there have been several meta-analytic attempts to understand the antecedents of trust in these areas. However, no targeted meta-analysis has been performed examining the antecedents of trust in AI.MethodData from 65 articles examined the three predicted categories, as well as the subcategories of human characteristics and abilities, AI performance and attributes, and contextual tasking. Lastly, four common uses for AI (i.e., chatbots, robots, automated vehicles, and nonembodied, plain algorithms) were examined as further potential moderating factors.ResultsResults showed that all of the examined categories were significant predictors of trust in AI as well as many individual antecedents such as AI reliability and anthropomorphism, among many others.ConclusionOverall, the results of this meta-analysis determined several factors that influence trust, including some that have no bearing on AI performance. Additionally, we highlight the areas where there is currently no empirical research.ApplicationFindings from this analysis will allow designers to build systems that elicit higher or lower levels of trust, as they require.",2023-03-01,2025-08-28 13:14:06,2025-08-28 13:14:06,2025-08-28 13:14:06,337-359,,2,65,,Hum Factors,Trust in Artificial Intelligence,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5R8SK6UX,journalArticle,2023,"Castelo, Noah; Boegershausen, Johannes; Hildebrand, Christian; Henkel, Alexander P",Understanding and Improving Consumer Reactions to Service Bots,Journal of Consumer Research,,0093-5301,10.1093/jcr/ucad023,https://doi.org/10.1093/jcr/ucad023,"Many firms are beginning to replace customer service employees with bots, from humanoid service robots to digital chatbots. Using real human–bot interactions in lab and field settings, we study consumers’ evaluations of bot-provided service. We find that service evaluations are more negative when the service provider is a bot versus a human—even when the provided service is identical. This effect is explained by consumers’ belief that service automation is motivated by firm benefits (i.e., cutting costs) at the expense of customer benefits (such as service quality). The effect is eliminated when firms share the economic surplus derived from automation with consumers through price discounts. The effect is reversed when service bots provide unambiguously superior service to human employees—a scenario that may soon become reality. Consumers’ default reactions to service bots are therefore largely negative but can be equal to or better than reactions to human service providers if firms can demonstrate how automation benefits consumers.",2023-12-01,2025-08-28 13:14:08,2025-08-28 13:14:08,2025-08-28 13:14:08,848-863,,4,50,,J Consum Res,,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\QJFRTMDY\Castelo et al. - 2023 - Understanding and Improving Consumer Reactions to Service Bots.pdf; C:\Users\Admin\Zotero\storage\P8RC8ZSP\ucad023.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IUD9RZ74,journalArticle,2021,"Blut, Markus; Wang, Cheng; Wünderlich, Nancy V.; Brock, Christian","Understanding anthropomorphism in service provision: a meta-analysis of physical robots, chatbots, and other AI",Journal of the Academy of Marketing Science,,1552-7824,10.1007/s11747-020-00762-y,https://doi.org/10.1007/s11747-020-00762-y,"An increasing number of firms introduce service robots, such as physical robots and virtual chatbots, to provide services to customers. While some firms use robots that resemble human beings by looking and acting humanlike to increase customers’ use intention of this technology, others employ machinelike robots to avoid uncanny valley effects, assuming that very humanlike robots may induce feelings of eeriness. There is no consensus in the service literature regarding whether customers’ anthropomorphism of robots facilitates or constrains their use intention. The present meta-analysis synthesizes data from 11,053 individuals interacting with service robots reported in 108 independent samples. The study synthesizes previous research to clarify this issue and enhance understanding of the construct. We develop a comprehensive model to investigate relationships between anthropomorphism and its antecedents and consequences. Customer traits and predispositions (e.g., computer anxiety), sociodemographics (e.g., gender), and robot design features (e.g., physical, nonphysical) are identified as triggers of anthropomorphism. Robot characteristics (e.g., intelligence) and functional characteristics (e.g., usefulness) are identified as important mediators, although relational characteristics (e.g., rapport) receive less support as mediators. The findings clarify contextual circumstances in which anthropomorphism impacts customer intention to use a robot. The moderator analysis indicates that the impact depends on robot type (i.e., robot gender) and service type (i.e., possession-processing service, mental stimulus-processing service). Based on these findings, we develop a comprehensive agenda for future research on service robots in marketing.",2021-07-01,2025-08-28 13:14:59,2025-08-28 13:14:59,2025-08-28 13:14:59,632-658,,4,49,,J. of the Acad. Mark. Sci.,Understanding anthropomorphism in service provision,,,,,,,en,,,,,Springer Link,,,,"C:\Users\Admin\Zotero\storage\WEM4JZFP\Blut et al. - 2021 - Understanding anthropomorphism in service provision a meta-analysis of physical robots, chatbots, a.pdf",,,Anthropomorphism; Meta-analysis; Service robots; Technology acceptance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GRNZEJGF,journalArticle,2022,"Jia, Chenyan; Boltz, Alexander; Zhang, Angie; Chen, Anqing; Lee, Min Kyung",Understanding Effects of Algorithmic vs. Community Label on Perceived Accuracy of Hyper-partisan Misinformation,Proceedings of the ACM on Human-Computer Interaction,,2573-0142,10.1145/3555096,http://arxiv.org/abs/2203.00710,"Hyper-partisan misinformation has become a major public concern. In order to examine what type of misinformation label can mitigate hyper-partisan misinformation sharing on social media, we conducted a 4 (label type: algorithm, community, third-party fact-checker, and no label) X 2 (post ideology: liberal vs. conservative) between-subjects online experiment (N = 1,677) in the context of COVID-19 health information. The results suggest that for liberal users, all labels reduced the perceived accuracy and believability of fake posts regardless of the posts' ideology. In contrast, for conservative users, the efficacy of the labels depended on whether the posts were ideologically consistent: algorithmic labels were more effective in reducing the perceived accuracy and believability of fake conservative posts compared to community labels, whereas all labels were effective in reducing their belief in liberal posts. Our results shed light on the differing effects of various misinformation labels dependent on people's political ideology.",2022-11-07,2025-08-28 13:14:44,2025-08-28 13:14:44,2025-08-28 13:14:44,1-27,,CSCW2,6,,Proc. ACM Hum.-Comput. Interact.,,,,,,,,,,,,,arXiv.org,,arXiv:2203.00710 [cs],,C:\Users\Admin\Zotero\storage\KHPZ8ZZZ\Jia et al. - 2022 - Understanding Effects of Algorithmic vs. Community Label on Perceived Accuracy of Hyper-partisan Mis.pdf; C:\Users\Admin\Zotero\storage\2VFFBN4K\2203.html,,,Computer Science - Human-Computer Interaction; Computer Science - Social and Information Networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D56RNGQY,journalArticle,2022,"Sands, Sean; Campbell, Colin L.; Plangger, Kirk; Ferraro, Carla",Unreal influence: leveraging AI in influencer marketing,European Journal of Marketing,,0309-0566,10.1108/EJM-12-2019-0949,https://doi.org/10.1108/EJM-12-2019-0949,"This paper aims to examine how consumers respond to social media influencers that are created through artificial intelligence (AI) and compares effects to traditional (human) influencers.Across two empirical studies, the authors examine the efficacy of AI social media influencers. With Study 1, the authors establish baseline effects for AI influencers and investigate how social-psychological distance impacts consumer perceptions. The authors also investigate the role of an influencer’s agency – being autonomous or externally managed – to test the boundaries of the results and determine the interactive effects between influencer type and influencer agency. Study 2 acts as an extension and validation of Study 1, whereby the authors provide generalisability and overlay the role of need for uniqueness as a moderated mediator.The authors show that there are similarities and differences in the ways in which consumers view AI and human influencers. Importantly, the authors find no difference in terms of intention to follow or personalisation. This suggests that consumers are equally open to follow an AI or human influencer, and they perceive the level of personalisation provided by either influencer type as similar. Furthermore, while an AI influencer is generally perceived as having lower source trust, they are more likely to evoke word-of-mouth intentions. In understanding these effects, the authors show that social distance mediates the relationship between influencer type and the outcomes the authors investigate. Results also show that AI influencers can have a greater effect on consumers who have a high need for uniqueness. Finally, the authors find that a lack of influencer agency has a detrimental effect.The studies investigate consumers’ general response to AI influencers within the context of Instagram, however, future research might examine consumers’ response to posts promoting specific products across a variety of category contexts and within different social media platforms.The authors find that in some ways, an AI influencer can be as effective as a human influencer. Indeed, the authors suggest that there may be a spill-over effect from consumer experiences with other AI recommendation systems, meaning that consumers are open to AI influencer recommendations. However, the authors find consistent evidence that AI influencers are trusted less than traditional influencers, hence the authors caution brands from rushing to replace human influencers with their AI counterparts.This paper offers novel insight into the increasingly prominent phenomenon of the AI influencer. Specifically, it takes initial steps towards developing understanding as to how consumers respond to AI influencers and contrast these effects with human influencers.",2022-02-18,2025-08-28 13:15:04,2025-08-28 13:15:04,2025-08-28 13:15:04,1721-1747,,6,56,,European Journal of Marketing,Unreal influence,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\2DULDF4H\Sands et al. - 2022 - Unreal influence leveraging AI in influencer marketing.pdf; C:\Users\Admin\Zotero\storage\KDT29C22\EJM-12-2019-0949.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B6T29DE5,preprint,2023,"Mozes, Maximilian; He, Xuanli; Kleinberg, Bennett; Griffin, Lewis D.","Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",,,,10.48550/arXiv.2308.12833,http://arxiv.org/abs/2308.12833,"Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.",2023-08-24,2025-08-28 13:15:02,2025-08-28 13:15:02,2025-08-28 13:15:02,,,,,,,Use of LLMs for Illicit Purposes,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2308.12833 [cs],,"C:\Users\Admin\Zotero\storage\2K2PNWME\Mozes et al. - 2023 - Use of LLMs for Illicit Purposes Threats, Prevention Measures, and Vulnerabilities.pdf; C:\Users\Admin\Zotero\storage\P7AVANCK\2308.html",,,Computer Science - Computation and Language; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,arXiv:2308.12833,,,,,,,,,,,,,,,,,,,,,,,,,,,
FL45EJUG,journalArticle,2019,"Greer, Stephanie; Ramo, Danielle; Chang, Yin-Juei; Fu, Michael; Moskowitz, Judith; Haritatos, Jana",Use of the Chatbot “Vivibot” to Deliver Positive Psychology Skills and Promote Well-Being Among Young People After Cancer Treatment: Randomized Controlled Feasibility Trial,JMIR mHealth and uHealth,,2291-5222,10.2196/15018,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6913733/,"Background Positive psychology interventions show promise for reducing psychosocial distress associated with health adversity and have the potential to be widely disseminated to young adults through technology. Objective This pilot randomized controlled trial examined the feasibility of delivering positive psychology skills via the Vivibot chatbot and its effects on key psychosocial well-being outcomes in young adults treated for cancer. Methods Young adults (age 18-29 years) were recruited within 5 years of completing active cancer treatment by using the Vivibot chatbot on Facebook messenger. Participants were randomized to either immediate access to Vivibot content (experimental group) or access to only daily emotion ratings and access to full chatbot content after 4 weeks (control). Created using a human-centered design process with young adults treated for cancer, Vivibot content includes 4 weeks of positive psychology skills, daily emotion ratings, video, and other material produced by survivors, and periodic feedback check-ins. All participants were assessed for psychosocial well-being via online surveys at baseline and weeks 2, 4, and 8. Analyses examined chatbot engagement and open-ended feedback on likability and perceived helpfulness and compared experimental and control groups with regard to anxiety and depression symptoms and positive and negative emotion changes between baseline and 4 weeks. To verify the main effects, follow-up analyses compared changes in the main outcomes between 4 and 8 weeks in the control group once participants had access to all chatbot content. Results Data from 45 young adults (36 women; mean age: 25 [SD 2.9]; experimental group: n=25; control group: n=20) were analyzed. Participants in the experimental group spent an average of 74 minutes across an average of 12 active sessions chatting with Vivibot and rated their experience as helpful (mean 2.0/3, SD 0.72) and would recommend it to a friend (mean 6.9/10; SD 2.6). Open-ended feedback noted its nonjudgmental nature as a particular benefit of the chatbot. After 4 weeks, participants in the experimental group reported an average reduction in anxiety of 2.58 standardized t-score units, while the control group reported an increase in anxiety of 0.7 units. A mixed-effects models revealed a trend-level (P=.09) interaction between group and time, with an effect size of 0.41. Those in the experimental group also experienced greater reductions in anxiety when they engaged in more sessions (z=–1.9, P=.06). There were no significant (or trend level) effects by group on changes in depression, positive emotion, or negative emotion. Conclusions The chatbot format provides a useful and acceptable way of delivering positive psychology skills to young adults who have undergone cancer treatment and supports anxiety reduction. Further analysis with a larger sample size is required to confirm this pattern.",2019-10-31,2025-08-28 12:31:23,2025-08-28 12:31:23,2025-08-28 12:31:23,e15018,,10,7,,JMIR Mhealth Uhealth,Use of the Chatbot “Vivibot” to Deliver Positive Psychology Skills and Promote Well-Being Among Young People After Cancer Treatment,,,,,,,,,,,,PubMed Central,,PMID: 31674920 PMCID: PMC6913733,,C:\Users\Admin\Zotero\storage\T3WF9CDR\Greer et al. - 2019 - Use of the Chatbot “Vivibot” to Deliver Positive Psychology Skills and Promote Well-Being Among Youn.pdf; ,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6913733/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIKDH6MN,journalArticle,2020,"Xu, Kun; Liu, Fanjue; Mou, Yi; Wu, Yuheng; Zeng, Jing; Schäfer, Mike S.",Using Machine Learning to Learn Machines: A Cross-Cultural Study of Users’ Responses to Machine-Generated Artworks,Journal of Broadcasting & Electronic Media,,0883-8151,10.1080/08838151.2020.1835136,https://doi.org/10.1080/08838151.2020.1835136,"Drawing from prior literature on machine-generated news, this study examines machine-generated artworks in a cross-cultural context. It combines machine learning approaches with online experiments and investigates how different genres of artworks and different authorship cues influence participants’ open-ended responses to machine-generated works. Results suggest that while genres and cultures affected participants’ discussion topics and word use, the differences between participants’ responses to machine-generated artworks and human-generated ones were not evident. This study tests the explanatory power of machine heuristic and demonstrates the feasibility of integrating multiple methods in future AI-based media research.",2020-10-01,2025-08-28 13:15:06,2025-08-28 13:15:06,2025-08-28 13:15:06,566-591,,4,64,,,Using Machine Learning to Learn Machines,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/08838151.2020.1835136,,C:\Users\Admin\Zotero\storage\6V2TX8XF\Xu et al. - 2020 - Using Machine Learning to Learn Machines A Cross-Cultural Study of Users’ Responses to Machine-Gene.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M4NJTSH4,journalArticle,2018,"Fulmer, Russell; Joerin, Angela; Gentile, Breanna; Lakerink, Lysanne; Rauws, Michiel",Using Psychological Artificial Intelligence (Tess) to Relieve Symptoms of Depression and Anxiety: Randomized Controlled Trial,JMIR Mental Health,,,10.2196/mental.9782,https://mental.jmir.org/2018/4/e64,"Background: Students in need of mental health care face many barriers including cost, location, availability, and stigma. Studies show that computer-assisted therapy and 1 conversational chatbot delivering cognitive behavioral therapy (CBT) offer a less-intensive and more cost-effective alternative for treating depression and anxiety. Although CBT is one of the most effective treatment methods, applying an integrative approach has been linked to equally effective posttreatment improvement. Integrative psychological artificial intelligence (AI) offers a scalable solution as the demand for affordable, convenient, lasting, and secure support grows. Objective: This study aimed to assess the feasibility and efficacy of using an integrative psychological AI, Tess, to reduce self-identified symptoms of depression and anxiety in college students. Methods: In this randomized controlled trial, 75 participants were recruited from 15 universities across the United States. All participants completed Web-based surveys, including the Patient Health Questionnaire (PHQ-9), Generalized Anxiety Disorder Scale (GAD-7), and Positive and Negative Affect Scale (PANAS) at baseline and 2 to 4 weeks later (T2). The 2 test groups consisted of 50 participants in total and were randomized to receive unlimited access to Tess for either 2 weeks (n=24) or 4 weeks (n=26). The information-only control group participants (n=24) received an electronic link to the National Institute of Mental Health’s (NIMH) eBook on depression among college students and were only granted access to Tess after completion of the study. Results: A sample of 74 participants completed this study with 0% attrition from the test group and less than 1% attrition from the control group (1/24). The average age of participants was 22.9 years, with 70% of participants being female (52/74), mostly Asian (37/74, 51%), and white (32/74, 41%). Group 1 received unlimited access to Tess, with daily check-ins for 2 weeks. Group 2 received unlimited access to Tess with biweekly check-ins for 4 weeks. The information-only control group was provided with an electronic link to the NIMH’s eBook. Multivariate analysis of covariance was conducted. We used an alpha level of .05 for all statistical tests. Results revealed a statistically significant difference between the control group and group 1, such that group 1 reported a significant reduction in symptoms of depression as measured by the PHQ-9 (P=.03), whereas those in the control group did not. A statistically significant difference was found between the control group and both test groups 1 and 2 for symptoms of anxiety as measured by the GAD-7. Group 1 (P=.045) and group 2 (P=.02) reported a significant reduction in symptoms of anxiety, whereas the control group did not. A statistically significant difference was found on the PANAS between the control group and group 1 (P=.03) and suggests that Tess did impact scores. Conclusions: This study offers evidence that AI can serve as a cost-effective and accessible therapeutic agent. Although not designed to appropriate the role of a trained therapist, integrative psychological AI emerges as a feasible option for delivering support. Trial Registration: International Standard Randomized Controlled Trial Number: ISRCTN61214172; https://doi.org/10.1186/ISRCTN61214172.",2018-12-13,2025-08-28 13:15:08,2025-08-28 13:15:08,2025-08-28 13:15:08,e9782,,4,5,,,Using Psychological Artificial Intelligence (Tess) to Relieve Symptoms of Depression and Anxiety,,,,,,,EN,"Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work (""first published in the Journal of Medical Internet Research..."") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.",,,,mental.jmir.org,,"Company: JMIR Mental Health Distributor: JMIR Mental Health Institution: JMIR Mental Health Label: JMIR Mental Health Publisher: JMIR Publications Inc., Toronto, Canada",,C:\Users\Admin\Zotero\storage\ULYV52M5\e64.html; C:\Users\Admin\Zotero\storage\AMW6V6Q7\Fulmer et al. - 2018 - Using Psychological Artificial Intelligence (Tess) to Relieve Symptoms of Depression and Anxiety Ra.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RQZHIN9H,journalArticle,2021,"Berger, Benedikt; Adam, Martin; Rühr, Alexander; Benlian, Alexander",Watch Me Improve—Algorithm Aversion and Demonstrating the Ability to Learn,Business & Information Systems Engineering,,1867-0202,10.1007/s12599-020-00678-5,https://doi.org/10.1007/s12599-020-00678-5,"Owing to advancements in artificial intelligence (AI) and specifically in machine learning, information technology (IT) systems can support humans in an increasing number of tasks. Yet, previous research indicates that people often prefer human support to support by an IT system, even if the latter provides superior performance – a phenomenon called algorithm aversion. A possible cause of algorithm aversion put forward in literature is that users lose trust in IT systems they become familiar with and perceive to err, for example, making forecasts that turn out to deviate from the actual value. Therefore, this paper evaluates the effectiveness of demonstrating an AI-based system’s ability to learn as a potential countermeasure against algorithm aversion in an incentive-compatible online experiment. The experiment reveals how the nature of an erring advisor (i.e., human vs. algorithmic), its familiarity to the user (i.e., unfamiliar vs. familiar), and its ability to learn (i.e., non-learning vs. learning) influence a decision maker’s reliance on the advisor’s judgement for an objective and non-personal decision task. The results reveal no difference in the reliance on unfamiliar human and algorithmic advisors, but differences in the reliance on familiar human and algorithmic advisors that err. Demonstrating an advisor’s ability to learn, however, offsets the effect of familiarity. Therefore, this study contributes to an enhanced understanding of algorithm aversion and is one of the first to examine how users perceive whether an IT system is able to learn. The findings provide theoretical and practical implications for the employment and design of AI-based systems.",2021-02-01,2025-08-28 13:15:11,2025-08-28 13:15:11,2025-08-28 13:15:11,55-68,,1,63,,Bus Inf Syst Eng,,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\II3IZ566\Berger et al. - 2021 - Watch Me Improve—Algorithm Aversion and Demonstrating the Ability to Learn.pdf,,,Advice taking; Algorithm aversion; Artificial intelligence; Decision support; Machine learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIB7MEU6,journalArticle,2021,"Schlicker, Nadine; Langer, Markus; Ötting, Sonja K.; Baum, Kevin; König, Cornelius J.; Wallach, Dieter",What to expect from opening up ‘black boxes’? Comparing perceptions of justice between human and automated agents,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2021.106837,https://www.sciencedirect.com/science/article/pii/S0747563221001606,"Advances in artificial intelligence contribute to increasing automation of decisions. In a healthcare-scheduling context, this study compares effects of decision agents and explanations for decisions on decision-recipients’ perceptions of justice. In a 2 (decision agent: automated vs. human) × 3 (explanation: no explanation vs. equality-explanation vs. equity-explanation) between-subjects online study, 209 healthcare professionals were asked to put themselves in a situation where their vacation request was denied by either a human or an automated agent. Participants either received no explanation or an explanation based on equality or equity norms. Perceptions of interpersonal justice were stronger for the human agent. Additionally, participants perceived human agents as offering more voice and automated agents as being more consistent in decision-making. When given no explanation, perceptions of informational justice were impaired only for the human decision agent. In the study's second part, participants took the perspective of a decision-maker and were given the choice to delegate decision-making to an automated system. Participants who delegated an unpleasant decision to the system frequently externalized responsibility and showed different response patterns when confronted by a decision-recipient who asked for a rationale for the decision.",2021-09-01,2025-08-28 13:15:13,2025-08-28 13:15:13,2025-08-28 13:15:13,106837,,,122,,Computers in Human Behavior,What to expect from opening up ‘black boxes’?,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\YUX8YQG2\S0747563221001606.html,,,Automated and augmented decision-making; Explainability; Justice; Responsibility,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GSFSQ5VR,journalArticle,2018,"Zheng, Yue; Zhong, Bu; Yang, Fan",When algorithms meet journalism: The user perception to automated news in a cross-cultural context,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2018.04.046,https://www.sciencedirect.com/science/article/pii/S0747563218302103,"Automated journalism – the use of algorithms in writing news reports – underscores the new direction of media transformation in the 21st century as it may reshape how the news is produced and consumed. Such writing algorithms have been increasingly adopted in U.S. and Chinese newsroom, but how well they are accepted by news users deserves more research. A comparative study was thus conducted to examine how U.S. and Chinese news users perceive the quality of algorithm-generated news reports, how much they like and trust such reports. Results show that U.S. and Chinese users demonstrated more shared, rather than different, perceptions to automated news. The users did not perceive automated content in a linear way, but viewed them by considering the interaction of the authors (i.e., journalists or algorithms), the media outlets (i.e., traditional or online media) and cultural background (i.e., U.S. or Chinese users).",2018-09-01,2025-08-28 13:15:25,2025-08-28 13:15:25,2025-08-28 13:15:25,266-275,,,86,,Computers in Human Behavior,When algorithms meet journalism,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\4NV7WMWT\S0747563218302103.html,,,Algorithm; Automated news; Computational journalism; Cultural difference; News user; Robot journalism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UQD5FHDL,journalArticle,2020,"Wang, Jinping; Molina, Maria D.; Sundar, S. Shyam","When expert recommendation contradicts peer opinion: Relative social influence of valence, group identity and artificial intelligence",Computers in Human Behavior,,0747-5632,10.1016/j.chb.2020.106278,https://www.sciencedirect.com/science/article/pii/S0747563220300340,"Whom do we trust more, the recommendation of an expert or public opinion from a crowd of other users of the site? Does it matter if the expert belongs to our in-group? And, what, if anything, would change if an Artificial Intelligence (AI) system was the recommender rather than a human expert? In order to answer these research questions, we conducted a between-subjects online experiment, informed by MAIN Model (Sundar, 2008), which posits that interface cues signaling different types of sources can influence perceived credibility of content by triggering distinct cognitive heuristics. Participants were assigned to a scenario wherein the expert review contrasted the peer rating about recommending photos for business profiles, with systematic variations in expert review valence (negative vs. positive), expert identity (ingroup vs. outgroup vs. no identity), and agent type (human vs. AI). Results show that positive ratings are more influential on user judgements. However, for negative ratings, human ingroup members generated greater effects than no-identity experts. Moreover, AI systems were as influential as human experts, suggesting the potential for AI to substitute human experts for online recommendations.",2020-06-01,2025-08-28 13:15:33,2025-08-28 13:15:33,2025-08-28 13:15:33,106278,,,107,,Computers in Human Behavior,When expert recommendation contradicts peer opinion,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\6FQW9DV2\S0747563220300340.html,,,Artificial intelligence; Expert vs. peer; Group identity; MAIN model; Online rating; Social influence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5UV23D33,journalArticle,2022,"Thuillard, S.; Adams, M.; Jelmini, G.; Schmutz, S.; Sonderegger, A.; Sauer, J.",When humans and computers induce social stress through negative feedback: Effects on performance and subjective state,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2022.107270,https://www.sciencedirect.com/science/article/pii/S0747563222000929,"People increasingly work with autonomous systems, which progressively take over functions previously performed exclusively by humans. This may lead to situations in which automated agents give negative performance feedback, which represents an important work-related social stressor. Little is known about how negative feedback provided by computers (as opposed to humans) affects human performance and subjective state. A first experiment (N = 60) focused on the influence of human feedback on performance. After participants had performed a cognitive task, they received a manipulated performance feedback (either positive or negative) from a human (comparing to a control with no feedback) and subsequent performance on several cognitive tasks and the participants' subjective state was measured. The results showed that while negative feedback had a negative influence on several subjective state measures, performance remained unimpaired. In a second experiment (N = 89), participants received manipulated negative feedback by a human or by a computer (or no feedback at all) after having completed an ability test. Subsequent performance was measured on attention tasks and creativity tasks and participants' subjective state was assessed. Although participants felt stressed by both negative computer and human feedback, subsequent performance was again not impaired. However, computer feedback was rated as being less fair than human feedback. Overall, our findings show that there are costs of protecting one's performance against negative feedback and they call for caution regarding the use of negative feedback by both human and automated agents in work settings.",2022-08-01,2025-08-28 13:15:42,2025-08-28 13:15:42,2025-08-28 13:15:42,107270,,,133,,Computers in Human Behavior,When humans and computers induce social stress through negative feedback,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\VH83WZDN\S0747563222000929.html,,,6 max): Social stress; Automation; Computer feedback; Interpersonal fairness; Negative feedback; Performance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8RBGB52A,journalArticle,2023,"Yokoi, Ryosuke; Nakayachi, Kazuya","When people are defeated by artificial intelligence in a competition task requiring logical thinking, how do they make causal attribution?",Current Psychology,,1936-4733,10.1007/s12144-021-02559-w,https://doi.org/10.1007/s12144-021-02559-w,"Given that artificial intelligence (AI) has been predicted to eventually take on human tasks demanding logical thinking, it makes sense that we should examine psychological responses of humans when their performance is inferior to AI. Research has demonstrated that after people fail a task, whether they reorient their behavior towards success depends on what they attribute the failure to. This study investigated the causal attributions people made in a competition task requiring such thinking. We also recorded whether they wanted to re-challenge the games after they were defeated by AI. Experiments 1 (N = 74) and 2 (N = 788) recruited Japanese participants, while Experiment 3 (N = 500) comprised American participants. There were two conditions: in the first, participants competed against an AI opponent and in the other, they believed they were competing against a human. The results of the three experiments showed that participants attributed the loss to their own and their opponent’s abilities more than any other factor, irrespective of the opponent type. The number of participants choosing to re-challenge the game did not differ significantly between the AI and human conditions in Experiments 1 and 3, although the number was lower in the AI condition than in the human condition in Experiment 2. Besides providing fresh insight on how people make causal attributions when competing against AI, our findings also predict how people will respond after their jobs are replaced by AI.",2023-06-01,2025-08-28 13:15:53,2025-08-28 13:15:53,2025-08-28 13:15:53,13369-13384,,16,42,,Curr Psychol,,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\93CUIJ99\Yokoi and Nakayachi - 2023 - When people are defeated by artificial intelligence in a competition task requiring logical thinking.pdf,,,Artificial intelligence; Behavioral response; Causal attribution; Competition game; Self-effacing bias,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U5S85XRH,journalArticle,2019,"Julmi, Christian",When rational decision-making becomes irrational: a critical assessment and re-conceptualization of intuition effectiveness,Business Research,,2198-2627,10.1007/s40685-019-0096-4,https://doi.org/10.1007/s40685-019-0096-4,"Intuition can lead to more effective decision-making than analysis under certain conditions. This assumption can be regarded as common sense. However, dominant research streams on intuition effectiveness in decision-making conceptualize intuition inadequately, because intuition is considered either detrimental or as a form of analysis. Current findings in general intuition research show that intuition is a holistic form of information processing that is distinct from analysis and can be superior in some cases. To reconcile this mismatch, this article first critically assesses dominant conceptions on intuition effectiveness and then offers a re-conceptualization that builds on current findings of general intuition research. Basically, the article suggests the structuredness of the decision problem as the main criterion for intuition effectiveness, and proposes organization information processing theory to establish this link conceptually. It is not the uncertainty but the equivocality of decision problems that call for an intuitive approach. The article conclusively derives implications for further research and discusses potential restrictions and constraints.",2019-04-01,2025-08-28 13:15:55,2025-08-28 13:15:55,2025-08-28 13:15:55,291-314,,1,12,,Bus Res,When rational decision-making becomes irrational,,,,,,,en,,,,,Springer Link,,,,C:\Users\Admin\Zotero\storage\4VT24GGS\Julmi - 2019 - When rational decision-making becomes irrational a critical assessment and re-conceptualization of.pdf,,,Analysis; Decision-making effectiveness; Equivocality; Intuition; Intuition effectiveness; Organization information processing theory; Rationality,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DF96IPHZ,conferencePaper,2021,"Lee, Min Kyung; Rich, Katherine",Who Is Included in Human Perceptions of AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust,Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,978-1-4503-8096-6,,10.1145/3411764.3445570,https://dl.acm.org/doi/10.1145/3411764.3445570,"Emerging research suggests that people trust algorithmic decisions less than human decisions. However, different populations, particularly in marginalized communities, may have different levels of trust in human decision-makers. Do people who mistrust human decision-makers perceive human decisions to be more trustworthy and fairer than algorithmic decisions? Or do they trust algorithmic decisions as much as or more than human decisions? We examine the role of mistrust in human systems in people’s perceptions of algorithmic decisions. We focus on healthcare Artificial Intelligence (AI), group-based medical mistrust, and Black people in the United States. We conducted a between-subjects online experiment to examine people’s perceptions of skin cancer screening decisions made by an AI versus a human physician depending on their medical mistrust, and we conducted interviews to understand how to cultivate trust in healthcare AI. Our findings highlight that research around human experiences of AI should consider critical differences in social groups.",2021,2025-08-28 13:15:57,2025-08-28 13:15:57,2025-08-28,1–14,,,,,,Who Is Included in Human Perceptions of AI?,CHI '21,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,C:\Users\Admin\Zotero\storage\U52IL68P\Lee and Rich - 2021 - Who Is Included in Human Perceptions of AI Trust and Perceived Fairness around Healthcare AI and C.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TQSZ8A7V,journalArticle,2021,"Wonseok (Eric), Jang; Young Woo, Kang; Yeonheung, Kang",Who made the decisions: Human or robot umpires? The effects of anthropomorphism on perceptions toward robot umpires,Telematics and Informatics,,0736-5853,10.1016/j.tele.2021.101695,https://www.sciencedirect.com/science/article/pii/S0736585321001349,"This study examined how the adoption of robot umpires in baseball alters people’s evaluation of ball/strike decisions, the trust in umpires, game enjoyment, and their behavioral consequences. The results indicated that people perceived umpire calls as fairer and more credible, and demonstrated greater trust in human umpires than in robot umpires; however, these negative effects were attenuated when robot umpires were humanized by giving them human-like characteristics. Furthermore, the effects of robot umpires were moderated by age and gender. Younger adults and males generally had more positive perceptions toward robot umpires, while older adults and females had more positive perceptions toward humanized robot umpires.",2021-11-01,2025-08-28 13:16:00,2025-08-28 13:16:00,2025-08-28 13:16:00,101695,,,64,,Telematics and Informatics,Who made the decisions,,,,,,,,,,,,ScienceDirect,,,,C:\Users\Admin\Zotero\storage\PLFRHEWD\S0736585321001349.html,,,Age; Anthropomorphism theory; Baseball; Gender; Robot umpire; Source bias framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5NEQM8YY,journalArticle,2023,"Chen, Qian; Yin, Changqin; Gong, Yeming",Would an AI chatbot persuade you: an empirical answer from the elaboration likelihood model,Information Technology & People,,0959-3845,10.1108/ITP-10-2021-0764,https://doi.org/10.1108/ITP-10-2021-0764,"This study investigates how artificial intelligence (AI) chatbots persuade customers to accept their recommendations in the online shopping context.Drawing on the elaboration likelihood model, this study establishes a research model to reveal the antecedents and internal mechanisms of customers' adoption of AI chatbot recommendations. The authors tested the model with survey data from 530 AI chatbot users.The results show that in the AI chatbot recommendation adoption process, central and peripheral cues significantly affected a customer's intention to adopt an AI chatbot's recommendation, and a customer's cognitive and emotional trust in the AI chatbot mediated the relationships. Moreover, a customer's mind perception of the AI chatbot, including perceived agency and perceived experience, moderated the central and peripheral paths, respectively.This study has theoretical and practical implications for AI chatbot designers and provides management insights for practitioners to enhance a customer's intention to adopt an AI chatbot's recommendation.The study investigates customers' adoption of AI chatbots' recommendation.The authors develop research model based on ELM theory to reveal central and peripheral cues and paths.The central and peripheral cues are generalized according to cooperative principle theory.Central cues include recommendation reliability and accuracy, and peripheral cues include human-like empathy and recommendation choice.Central and peripheral cues affect customers' adoption to recommendation through trust in AI.Customers' mind perception positively moderates the central and peripheral paths.",2023-12-08,2025-08-28 13:16:17,2025-08-28 13:16:17,2025-08-28 13:16:17,937-962,,2,38,,Information Technology & People,Would an AI chatbot persuade you,,,,,,,,,,,,Silverchair,,,,C:\Users\Admin\Zotero\storage\H357EBL8\ITP-10-2021-0764.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,