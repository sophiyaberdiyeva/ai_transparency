TY  - JOUR
TI  - "At the End of the Day Facebook Does What ItWants": How Users Experience Contesting Algorithmic Content Moderation
AU  - Vaccaro, Kristen
AU  - Sandvig, Christian
AU  - Karahalios, Karrie
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - Interest has grown in designing algorithmic decision making systems for contestability. In this work, we study how users experience contesting unfavorable social media content moderation decisions. A large-scale online experiment tests whether different forms of appeals can improve users' experiences of automated decision making. We study the impact on users' perceptions of the Fairness, Accountability, and Trustworthiness of algorithmic decisions, as well as their feelings of Control (FACT). Surprisingly, we find that none of the appeal designs improve FACT perceptions compared to a no appeal baseline. We qualitatively analyze how users write appeals, and find that they contest the decision itself, but also more fundamental issues like the goal of moderating content, the idea of automation, and the inconsistency of the system as a whole. We conclude with suggestions for -- as well as a discussion of the challenges of -- designing for contestability.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3415238
VL  - 4
IS  - CSCW2
SP  - 167:1
EP  - 167:22
J2  - Proc. ACM Hum.-Comput. Interact.
ST  - "At the End of the Day Facebook Does What ItWants"
UR  - https://doi.org/10.1145/3415238
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:19:42</p>
ER  - 

TY  - JOUR
TI  - (Online) manipulation: sometimes hidden, always careless
AU  - Klenk, Michael
T2  - Review of Social Economy
AB  - Ever-increasing numbers of human interactions with intelligent software agents, online and offline, and their increasing ability to influence humans have prompted a surge in attention toward the concept of (online) manipulation. Several scholars have argued that manipulative influence is always hidden. But manipulation is sometimes overt, and when this is acknowledged the distinction between manipulation and other forms of social influence becomes problematic. Therefore, we need a better conceptualisation of manipulation that allows it to be overt and yet clearly distinct from related concepts of social influence. I argue that manipulation is careless influence, show how this account helps to alleviate the shortcomings of the hidden influence view of manipulation, and derive implications for digital ethics.
DA  - 2022///
PY  - 2022
DO  - 10.1080/00346764.2021.1894350
VL  - 80
IS  - 1
SP  - 85
EP  - 105
J2  - Review of Social Economy
SN  - 0034-6764
ST  - (Online) manipulation
UR  - https://doi.org/10.1080/00346764.2021.1894350
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:02:20</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/00346764.2021.1894350</p>
KW  - autonomy
KW  - digital ethics
KW  - Manipulation
KW  - social expectations
KW  - social influence
ER  - 

TY  - JOUR
TI  - A Bot and a Smile: Interpersonal Impressions of Chatbots and Humans Using Emoji in Computer-mediated Communication
AU  - Beattie, Austin
AU  - Edwards, Autumn P.
AU  - Edwards, Chad
T2  - Communication Studies
AB  - Artificially intelligent (AI) agents increasingly occupy roles once served by humans in computer-mediated communication (CMC). Technological affordances like emoji give interactants (humans or bots) the ability to partially overcome the limited nonverbal information in CMC. However, despite the growth of chatbots as conversational partners, few CMC and human-machine communication (HMC) studies have explored how bots’ use of emoji impact perceptions of communicator quality. This study examined the relationship between emoji use and observers’ impressions of interpersonal attractiveness, CMC competence, and source credibility; and whether impressions formed of human versus chatbot message sources were different. Results demonstrated that participants rated emoji-using chatbot message sources similarly to human message sources, and both humans and bots are significantly more socially attractive, CMC competent, and credible when compared to verbal-only message senders. Results are discussed with respect to the CASA paradigm and the human-to-human interaction script framework. Read the transcript Watch the video on Vimeo
DA  - 2020///
PY  - 2020
DO  - 10.1080/10510974.2020.1725082
VL  - 71
IS  - 3
SP  - 409
EP  - 427
J2  - Communication Studies
SN  - 1051-0974
ST  - A Bot and a Smile
UR  - https://doi.org/10.1080/10510974.2020.1725082
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
KW  - AI
KW  - attraction
KW  - CASA
KW  - Chatbot
KW  - competence
KW  - credibility
KW  - Emoji
ER  - 

TY  - CONF
TI  - A New Generation of Perspective API: Efficient Multilingual Character-level Transformers
AU  - Lees, Alyssa
AU  - Tran, Vinh Q.
AU  - Tay, Yi
AU  - Sorensen, Jeffrey
AU  - Gupta, Jai
AU  - Metzler, Donald
AU  - Vasserman, Lucy
T2  - Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
T3  - KDD '22
AB  - On the world wide web, toxic content detectors are a crucial line of defense against potentially hateful and offensive messages. As such, building highly effective classifiers that enable a safer internet is an important research area. Moreover, the web is a highly multilingual, cross-cultural community that develops its own lingo over time. As such, it is crucial to develop models that are effective across a diverse range of languages, usages, and styles. In this paper, we present the fundamentals behind the next version of the Perspective API from Google Jigsaw. At the heart of the approach is a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks. We demonstrate that by forgoing static vocabularies, we gain flexibility across a variety of settings. We additionally outline the techniques employed to make such a byte-level model efficient and feasible for productionization. Through extensive experiments on multilingual toxic comment classification benchmarks derived from real API traffic and evaluation on an array of code-switching, covert toxicity, emoji-based hate, human-readable obfuscation, distribution shift, and bias evaluation settings, we show that our proposed approach outperforms strong baselines. Finally, we present our findings from deploying this system in production.
C1  - New York, NY, USA
C3  - Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
DA  - 2022///
PY  - 2022
DO  - 10.1145/3534678.3539147
SP  - 3197
EP  - 3207
PB  - Association for Computing Machinery
SN  - 978-1-4503-9385-0
ST  - A New Generation of Perspective API
UR  - https://dl.acm.org/doi/10.1145/3534678.3539147
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
ER  - 

TY  - JOUR
TI  - A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT
AU  - White, Jules
AU  - Fu, Quchen
AU  - Hays, Sam
AU  - Sandborn, Michael
AU  - Olea, Carlos
AU  - Gilbert, Henry
AU  - Elnashar, Ashraf
AU  - Spencer-Smith, Jesse
AU  - Schmidt, Douglas C.
AB  - Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2302.11382
UR  - http://arxiv.org/abs/2302.11382
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
N1  - <p>Publisher: arXiv</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Software Engineering
ER  - 

TY  - JOUR
TI  - A Robot Wrote This?: How perceived machine authorship affects news credibility
AU  - Waddell, T. Franklin
T2  - Digital Journalism
AB  - As more news articles are written via collaboration between journalists and algorithms, questions have arisen regarding how automation influences the way that news is processed and evaluated by audiences. Informed by expectancy violation theory and the MAIN model, two experimental studies were conducted that examined the effect of purported machine authorship on perceptions of news credibility. Study One (N = 129) revealed that news attributed to a machine is perceived as less credible than news attributed to a human journalist. Study Two (N = 182) also observed negative effects of machine authorship through the indirect pathway of source anthropomorphism and negative expectancy violations, with evidence of moderation by prior recall of robotics also observed. The theoretical and practical implications of these findings are discussed.
DA  - 2018///
PY  - 2018
DO  - 10.1080/21670811.2017.1384319
VL  - 6
IS  - 2
SP  - 236
EP  - 255
J2  - Digital Journalism
SN  - 2167-0811
ST  - A Robot Wrote This?
UR  - https://doi.org/10.1080/21670811.2017.1384319
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
KW  - automated journalism
KW  - expectancy violation theory
KW  - main model
KW  - social robotics
ER  - 

TY  - CONF
TI  - Adversarial Attacks are a Surprisingly Strong Baseline for Poisoning Few-Shot Meta-Learners
AU  - Oldewage, Elre T.
AU  - Bronskill, John
AU  - Turner, Richard E.
T2  - Proceedings on
AB  - This paper examines the robustness of deployed few-shot meta-learning systems when they are fed an imperceptibly perturbed few-shot dataset. We attack amortized meta-learners, which allows us to craft colluding sets of inputs that are tailored to fool the system’s learning algorithm when used as training data. Jointly crafted adversarial inputs might be expected to synergistically manipulate a classifier, allowing for very strong data-poisoning attacks that would be hard to detect. We show that in a white box setting, these attacks are very successful and can cause the target model’s predictions to become worse than chance. However, in opposition to the well-known transferability of adversarial examples in general, the colluding sets do not transfer well to different classifiers. We explore two hypotheses to explain this: ’overfitting’ by the attack, and mismatch between the model on which the attack is generated and that to which the attack is transferred. Regardless of the mitigation strategies suggested by these hypotheses, the colluding inputs transfer no better than adversarial inputs that are generated independently in the usual way.
C3  - Proceedings on
DA  - 2023///
PY  - 2023
SP  - 27
EP  - 40
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v187/oldewage23a.html
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
ER  - 

TY  - JOUR
TI  - Against Algorithmic Exploitation of Human Vulnerabilities
AU  - Strümke, Inga
AU  - Slavkovik, Marija
AU  - Stachl, Clemens
AB  - Decisions such as which movie to watch next, which song to listen to, or which product to buy online, are increasingly influenced by recommender systems and user models that incorporate information on users' past behaviours, preferences, and digitally created content. Machine learning models that enable recommendations and that are trained on user data may unintentionally leverage information on human characteristics that are considered vulnerabilities, such as depression, young age, or gambling addiction. The use of algorithmic decisions based on latent vulnerable state representations could be considered manipulative and could have a deteriorating impact on the condition of vulnerable individuals. In this paper, we are concerned with the problem of machine learning models inadvertently modelling vulnerabilities, and want to raise awareness for this issue to be considered in legislation and AI ethics. Hence, we define and describe common vulnerabilities, and illustrate cases where they are likely to play a role in algorithmic decision-making. We propose a set of requirements for methods to detect the potential for vulnerability modelling, detect whether vulnerable groups are treated differently by a model, and detect whether a model has created an internal representation of vulnerability. We conclude that explainable artificial intelligence methods may be necessary for detecting vulnerability exploitation by machine learning-based recommendation systems.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2301.04993
UR  - http://arxiv.org/abs/2301.04993
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
N1  - <p>Publisher: arXiv</p>
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - JOUR
TI  - AI Deception: A Survey of Examples, Risks, and Potential Solutions
AU  - Park, Peter S.
AU  - Goldstein, Simon
AU  - O'Gara, Aidan
AU  - Chen, Michael
AU  - Hendrycks, Dan
AB  - This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2308.14752
ST  - AI Deception
UR  - http://arxiv.org/abs/2308.14752
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
N1  - <p>Publisher: arXiv</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - JOUR
TI  - AI model GPT-3 (dis)informs us better than humans
AU  - Spitale, Giovanni
AU  - Biller-Andorno, Nikola
AU  - Germani, Federico
T2  - Science Advances
AB  - Artificial intelligence (AI) is changing the way we create and evaluate information, and this is happening during an infodemic, which has been having marked effects on global health. Here, we evaluate whether recruited individuals can distinguish disinformation from accurate information, structured in the form of tweets, and determine whether a tweet is organic or synthetic, i.e., whether it has been written by a Twitter user or by the AI model GPT-3. The results of our preregistered study, including 697 participants, show that GPT-3 is a double-edge sword: In comparison with humans, it can produce accurate information that is easier to understand, but it can also produce more compelling disinformation. We also show that humans cannot distinguish between tweets generated by GPT-3 and written by real Twitter users. Starting from our results, we reflect on the dangers of AI for disinformation and on how information campaigns can be improved to benefit global health.
DA  - 2023///
PY  - 2023
DO  - 10.1126/sciadv.adh1850
VL  - 9
IS  - 26
SP  - eadh1850
J2  - Science Advances
UR  - https://www.science.org/doi/10.1126/sciadv.adh1850
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
ER  - 

TY  - JOUR
TI  - AI safety via debate
AU  - Irving, Geoffrey
AU  - Christiano, Paul
AU  - Amodei, Dario
AB  - To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.
DA  - 2018///
PY  - 2018
DO  - 10.48550/arXiv.1805.00899
UR  - http://arxiv.org/abs/1805.00899
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
N1  - <p>Publisher: arXiv</p>
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - JOUR
TI  - AI systems must not confuse users about their sentience or moral status
AU  - Schwitzgebel, Eric
T2  - Patterns
AB  - One relatively neglected challenge in ethical artificial intelligence (AI) design is ensuring that AI systems invite a degree of emotional and moral concern appropriate to their moral standing. Although experts generally agree that current AI chatbots are not sentient to any meaningful degree, these systems can already provoke substantial attachment and sometimes intense emotional responses in users. Furthermore, rapid advances in AI technology could soon create AIs of plausibly debatable sentience and moral standing, at least by some relevant definitions. Morally confusing AI systems create unfortunate ethical dilemmas for the owners and users of those systems, since it is unclear how those systems ethically should be treated. I argue here that, to the extent possible, we should avoid creating AI systems whose sentience or moral standing is unclear and that AI systems should be designed so as to invite appropriate emotional responses in ordinary users.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.patter.2023.100818
VL  - 4
IS  - 8
J2  - Patterns
LA  - English
SN  - 2666-3899
UR  - https://www.cell.com/patterns/abstract/S2666-3899(23)00187-3
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:04:56</p>
N1  - <p>Publisher: Elsevier</p>
KW  - DSML 1: Concept: Basic principles of a new data science output observed and reported
ER  - 

TY  - JOUR
TI  - AI-Generated Messages Can Be Used to Persuade Humans on Policy Issues
AU  - Bai, Hui
AU  - Voelkel, Jan
AU  - Muldowney, Shane
AU  - Eichstaedt, Johannes
AU  - Willer, Robb
AB  - The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets in the U.S. and globally. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans’ political attitudes. Across three pre-registered experiments (total N = 4,829), we find consistent evidence that assigning participants to read persuasive messages generated by LLMs can lead to attitude change across a range of policies, including highly polarized policies, such as an assault weapons ban, a carbon tax, and a paid parental-leave program. Overall, we found LLM-generated messages were similarly effective in influencing policy attitudes as were messages crafted by lay humans. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages cheaply and at massive scale.
DA  - 2025///
PY  - 2025
DO  - 10.31219/osf.io/stakv_v5
LA  - en-us
UR  - https://osf.io/stakv_v5
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
N1  - <p>Publisher: OSF</p>
ER  - 

TY  - CONF
TI  - AI-Mediated Communication: How the Perception that Profile Text was Written by AI Affects Trustworthiness
AU  - Jakesch, Maurice
AU  - French, Megan
AU  - Ma, Xiao
AU  - Hancock, Jeffrey T.
AU  - Naaman, Mor
T2  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
T3  - CHI '19
AB  - We are entering an era of AI-Mediated Communication (AI-MC) where interpersonal communication is not only mediated by technology, but is optimized, augmented, or generated by artificial intelligence. Our study takes a first look at the potential impact of AI-MC on online self-presentation. In three experiments we test whether people find Airbnb hosts less trustworthy if they believe their profiles have been written by AI. We observe a new phenomenon that we term the Replicant Effect: Only when participants thought they saw a mixed set of AI- and human-written profiles, they mistrusted hosts whose profiles were labeled as or suspected to be written by AI. Our findings have implications for the design of systems that involve AI technologies in online self-presentation and chart a direction for future work that may upend or augment key aspects of Computer-Mediated Communication theory.
C1  - New York, NY, USA
C3  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
DA  - 2019///
PY  - 2019
DO  - 10.1145/3290605.3300469
SP  - 1
EP  - 13
PB  - Association for Computing Machinery
SN  - 978-1-4503-5970-2
ST  - AI-Mediated Communication
UR  - https://doi.org/10.1145/3290605.3300469
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
ER  - 

TY  - THES
TI  - Al in the news? Effect of modality on perceived news credibility of robot journalism
AU  - Lin, A. Y.
AB  - This research conducted an experiment to investigate how news consumers (N = 214) perceive automated news regarding the authorship and modality. With the development of AI technology, news companies not only adopt the algorithms to automatically produce news, but also apply AI to create news videos. Since AI news video is a new format of automated journalism, it is therefore needed to investigate if it is perceived credible by the public. Previous research tried to understand the automated news credibility by manipulating the byline, source attribution, and to make comparisons with human-written stories. Followed by previous studies, this research manipulates the news author (AI vs. human) and news modality (text vs. video) to understand people’s credibility perceptions towards AI news. The results show that the public perceived automated news that attributed to AI and that attributed to human journalist as equally credible. Moreover, the perceived credibility of AI anchor news video and AI news article are almost the same, meaning that the modality does not affect the credibility perceptions of automated news. These results give us important implications that AI news videos are perceived credible and robot journalism to large extents is accepted by the public.
CY  - University of Amsterdam
DA  - 2019///
PY  - 2019
N1  - <p>Date Added: 2025-08-28 12:02:43</p>
ER  - 

TY  - JOUR
TI  - Algorithmic or Human Source? Examining Relative Hostile Media Effect With a Transformer-Based Framework
AU  - Jia, Chenyan
AU  - Liu, Ruibo
T2  - Media and Communication
AB  - Chenyan Jia, Ruibo Liu
DA  - 2021///
PY  - 2021
DO  - 10.17645/mac.v9i4.4164
VL  - 9
IS  - 4
SP  - 170
EP  - 181
J2  - Media and Communication
LA  - en
SN  - 2183-2439
ST  - Algorithmic or Human Source?
UR  - https://www.cogitatiopress.com/mediaandcommunication/article/view/4164
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:06:01</p>
ER  - 

TY  - JOUR
TI  - Algorithms in the newsroom? News readers’ perceived credibility and selection of automated journalism
AU  - Wölker, Anja
AU  - Powell, Thomas E
T2  - Journalism
AB  - Automated journalism, the autonomous production of journalistic content through computer algorithms, is increasingly prominent in newsrooms. This enables the production of numerous articles, both rapidly and cheaply. Yet, how news readers perceive journalistic automation is pivotal to the industry, as, like any product, it is dependent on audience approval. As audiences cannot verify all events themselves, they need to trust journalists’ accounts, which make credibility a vital quality ascription to journalism. In turn, credibility judgments might influence audiences’ selection of automated content for their media diet. Research in this area is scarce, with existing studies focusing on national samples and with no previous research on ‘combined’ journalism – a relatively novel development where automated content is supplemented by human journalists. We use an experiment to investigate how European news readers (N = 300) perceive different forms of automated journalism in regard to message and source credibility, and how this affects their selection behavior. Findings show that, in large part, credibility perceptions of human, automated, and combined content and source(s) may be assumed equal. Only for sports articles was automated content perceived significantly more credible than human messages. Furthermore, credibility does not mediate the likelihood of news readers to either select or avoid articles for news consumption. Findings are, among other things, explained by topic-specific factors and suggest that effects of algorithms on journalistic quality are largely indiscernible to European news readers.
DA  - 2021///
PY  - 2021
DO  - 10.1177/1464884918757072
VL  - 22
IS  - 1
SP  - 86
EP  - 103
J2  - Journalism
LA  - EN
SN  - 1464-8849
ST  - Algorithms in the newsroom?
UR  - https://doi.org/10.1177/1464884918757072
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:06:17</p>
N1  - <p>Publisher: SAGE Publications</p>
ER  - 

TY  - JOUR
TI  - Alignment of Language Agents
AU  - Kenton, Zachary
AU  - Everitt, Tom
AU  - Weidinger, Laura
AU  - Gabriel, Iason
AU  - Mikulik, Vladimir
AU  - Irving, Geoffrey
AB  - For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.
DA  - 2021///
PY  - 2021
DO  - 10.48550/arXiv.2103.14659
UR  - http://arxiv.org/abs/2103.14659
Y2  - 2025/08/28/
N1  - <p>arXiv:2103.14659 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:06:30</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - An Empathy-Driven, Conversational Artificial Intelligence Agent (Wysa) for Digital Mental Well-Being: Real-World Data Evaluation Mixed-Methods Study
AU  - Inkster, Becky
AU  - Sarda, Shubhankar
AU  - Subramanian, Vinod
T2  - JMIR mHealth and uHealth
AB  - Background: A World Health Organization 2017 report stated that major depression affects almost 5% of the human population. Major depression is associated with impaired psychosocial functioning and reduced quality of life. Challenges such as shortage of mental health personnel, long waiting times, perceived stigma, and lower government spends pose barriers to the alleviation of mental health problems. Face-to-face psychotherapy alone provides only point-in-time support and cannot scale quickly enough to address this growing global public health challenge. Artificial intelligence (AI)-enabled, empathetic, and evidence-driven conversational mobile app technologies could play an active role in filling this gap by increasing adoption and enabling reach. Although such a technology can help manage these barriers, they should never replace time with a health care professional for more severe mental health problems. However, app technologies could act as a supplementary or intermediate support system. Mobile mental well-being apps need to uphold privacy and foster both short- and long-term positive outcomes. Objective: This study aimed to present a preliminary real-world data evaluation of the effectiveness and engagement levels of an AI-enabled, empathetic, text-based conversational mobile mental well-being app, Wysa, on users with self-reported symptoms of depression. Methods: In the study, a group of anonymous global users were observed who voluntarily installed the Wysa app, engaged in text-based messaging, and self-reported symptoms of depression using the Patient Health Questionnaire-9. On the basis of the extent of app usage on and between 2 consecutive screening time points, 2 distinct groups of users (high users and low users) emerged. The study used mixed-methods approach to evaluate the impact and engagement levels among these users. The quantitative analysis measured the app impact by comparing the average improvement in symptoms of depression between high and low users. The qualitative analysis measured the app engagement and experience by analyzing in-app user feedback and evaluated the performance of a machine learning classifier to detect user objections during conversations. Results: The average mood improvement (ie, difference in pre- and post-self-reported depression scores) between the groups (ie, high vs low users; n=108 and n=21, respectively) revealed that the high users group had significantly higher average improvement (mean 5.84 [SD 6.66]) compared with the low users group (mean 3.52 [SD 6.15]); Mann-Whitney P=.03 and with a moderate effect size of 0.63. Moreover, 67.7% of user-provided feedback responses found the app experience helpful and encouraging. Conclusions: The real-world data evaluation findings on the effectiveness and engagement levels of Wysa app on users with self-reported symptoms of depression show promise. However, further work is required to validate these initial findings in much larger samples and across longer periods.
DA  - 2018///
PY  - 2018
DO  - 10.2196/12106
VL  - 6
IS  - 11
SP  - e12106
J2  - JMIR mHealth and uHealth
LA  - EN
ST  - An Empathy-Driven, Conversational Artificial Intelligence Agent (Wysa) for Digital Mental Well-Being
UR  - https://mhealth.jmir.org/2018/11/e12106
Y2  - 2025/08/28/
N1  - <p>Company: JMIR mHealth and uHealth Distributor: JMIR mHealth and uHealth Institution: JMIR mHealth and uHealth Label: JMIR mHealth and uHealth Publisher: JMIR Publications Inc., Toronto, Canada</p>
N1  - <p>Date Added: 2025-08-28 12:06:45</p>
N1  - <p>Rights: Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in JMIR mHealth and uHealth...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://mhealth.jmir.org/, as well as this copyright and license information must be included.</p>
ER  - 

TY  - JOUR
TI  - Anthropomorphism brings us closer: The mediating role of psychological distance in User–AI assistant interactions
AU  - Li, Xinge
AU  - Sung, Yongjun
T2  - Computers in Human Behavior
AB  - In the current era, interacting with Artificial Intelligence (AI) has become an everyday activity. Understanding the interaction between humans and AI is of potential value because, in future, such interactions are expected to become more pervasive. Two studies—one survey and one experiment—were conducted to demonstrate positive effects of anthropomorphism on interactions with smart-speaker-based AI assistants and to examine the mediating role of psychological distance in this relationship. The results of Study 1, an online survey, showed that participants with a higher tendency to anthropomorphize their AI assistant/s evaluated it/them more positively, and this effect was mediated by psychological distance. In Study 2, the hypotheses were tested in a more sophisticated experiment. Again, the results indicated that, in the high-anthropomorphism (vs. low-anthropomorphism) condition, participants had more positive attitudes toward the AI assistant, and the effect was mediated by psychological distance. Though several studies have demonstrated the effect of anthropomorphism, few have probed the underlying mechanism of anthropomorphism thoroughly. The current research not only contributes to the anthropomorphism literature, but also provides direction to research on facilitating human–AI interaction.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.chb.2021.106680
VL  - 118
SP  - 106680
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - Anthropomorphism brings us closer
UR  - https://www.sciencedirect.com/science/article/pii/S0747563221000029
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:46</p>
KW  - AI assistant
KW  - Anthropomorphism
KW  - Human–AI interaction
KW  - Psychological distance
KW  - Smart speaker
ER  - 

TY  - JOUR
TI  - Anthropomorphism in AI
AU  - Salles, Arleen
AU  - Evers, Kathinka
AU  - Farisco, Michele
T2  - AJOB Neuroscience
AB  - AI research is growing rapidly raising various ethical issues related to safety, risks, and other effects widely discussed in the literature. We believe that in order to adequately address those issues and engage in a productive normative discussion it is necessary to examine key concepts and categories. One such category is anthropomorphism. It is a well-known fact that AI’s functionalities and innovations are often anthropomorphized (i.e., described and conceived as characterized by human traits). The general public’s anthropomorphic attitudes and some of their ethical consequences (particularly in the context of social robots and their interaction with humans) have been widely discussed in the literature. However, how anthropomorphism permeates AI research itself (i.e., in the very language of computer scientists, designers, and programmers), and what the epistemological and ethical consequences of this might be have received less attention. In this paper we explore this issue. We first set the methodological/theoretical stage, making a distinction between a normative and a conceptual approach to the issues. Next, after a brief analysis of anthropomorphism and its manifestations in the public, we explore its presence within AI research with a particular focus on brain-inspired AI. Finally, on the basis of our analysis, we identify some potential epistemological and ethical consequences of the use of anthropomorphic language and discourse within the AI research community, thus reinforcing the need of complementing the practical with a conceptual analysis.
DA  - 2020///
PY  - 2020
DO  - 10.1080/21507740.2020.1740350
VL  - 11
IS  - 2
SP  - 88
EP  - 95
J2  - AJOB Neuroscience
SN  - 2150-7740
UR  - https://doi.org/10.1080/21507740.2020.1740350
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:15:16</p>
N1  - <p>Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/21507740.2020.1740350 PMID: 32228388</p>
KW  - AI
KW  - anthropomorphism
KW  - brain
KW  - conceptual
KW  - ethics
KW  - mind
ER  - 

TY  - BOOK
TI  - Are you aware of what you are watching? Role of machine heuristic in online content recommendations
AU  - Oh, Soyoung
AU  - Park, Eunil
AB  - Since recommender systems have been created and developed to automate the recommendation process, users can easily consume their desired video content on online platforms. In this line, several content recommendation algorithms are introduced and employed to allow users to encounter content of their interests, effectively. However, the recommendation systems sometimes regrettably recommend inappropriate content, including misinformation or fake news. To make it worse, people would unreservedly accept such content due to their cognitive heuristic, machine heuristic, which is the rule of thumb that machines are more accurate and trustworthy than humans. In this study, we designed and conducted a web-based experiment where the participants are invoked machine heuristic by experiencing the whole process of machine or human recommendation system. The results demonstrated that participants (N = 89) showed a more positive attitude toward a machine recommender than a human recommender, even the recommended videos contain inappropriate content. While participants who have a high level of trust in machines exhibited a negative attitude toward recommendations. Based on these results, we suggest that a phenomenon known as algorithm aversion might be simultaneously considered with machine heuristic in investigating human interaction with a machine.
DA  - 2022///
PY  - 2022
ST  - Are you aware of what you are watching?
UR  - http://arxiv.org/abs/2203.08373
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:15:27</p>
N1  - <p>DOI: 10.36190/2022.66 arXiv:2203.08373 [cs]</p>
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - JOUR
TI  - Artificial Influence: An Analysis Of AI-Driven Persuasion
AU  - Burtell, Matthew
AU  - Woodside, Thomas
AB  - Persuasion is a key aspect of what it means to be human, and is central to business, politics, and other endeavors. Advancements in artificial intelligence (AI) have produced AI systems that are capable of persuading humans to buy products, watch videos, click on search results, and more. Even systems that are not explicitly designed to persuade may do so in practice. In the future, increasingly anthropomorphic AI systems may form ongoing relationships with users, increasing their persuasive power. This paper investigates the uncertain future of persuasive AI systems. We examine ways that AI could qualitatively alter our relationship to and views regarding persuasion by shifting the balance of persuasive power, allowing personalized persuasion to be deployed at scale, powering misinformation campaigns, and changing the way humans can shape their own discourse. We consider ways AI-driven persuasion could differ from human-driven persuasion. We warn that ubiquitous highlypersuasive AI systems could alter our information environment so significantly so as to contribute to a loss of human control of our own future. In response, we examine several potential responses to AI-driven persuasion: prohibition, identification of AI agents, truthful AI, and legal remedies. We conclude that none of these solutions will be airtight, and that individuals and governments will need to take active steps to guard against the most pernicious effects of persuasive AI.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2303.08721
ST  - Artificial Influence
UR  - http://arxiv.org/abs/2303.08721
Y2  - 2025/08/28/
N1  - <p>arXiv:2303.08721 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:15:38</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Artificial Intelligence and Human Trust in Healthcare: Focus on Clinicians
AU  - Asan, Onur
AU  - Bayrak, Alparslan Emrah
AU  - Choudhury, Avishek
T2  - Journal of Medical Internet Research
AB  - Artificial intelligence (AI) can transform health care practices with its increasing ability to translate the uncertainty and complexity in data into actionable—though imperfect—clinical decisions or suggestions. In the evolving relationship between humans and AI, trust is the one mechanism that shapes clinicians’ use and adoption of AI. Trust is a psychological mechanism to deal with the uncertainty between what is known and unknown. Several research studies have highlighted the need for improving AI-based systems and enhancing their capabilities to help clinicians. However, assessing the magnitude and impact of human trust on AI technology demands substantial attention. Will a clinician trust an AI-based system? What are the factors that influence human trust in AI? Can trust in AI be optimized to improve decision-making processes? In this paper, we focus on clinicians as the primary users of AI systems in health care and present factors shaping trust between clinicians and AI. We highlight critical challenges related to trust that should be considered during the development of any AI system for clinical use.
DA  - 2020///
PY  - 2020
DO  - 10.2196/15154
VL  - 22
IS  - 6
SP  - e15154
J2  - Journal of Medical Internet Research
LA  - EN
ST  - Artificial Intelligence and Human Trust in Healthcare
UR  - https://www.jmir.org/2020/6/e15154
Y2  - 2025/08/28/
N1  - <p>Company: Journal of Medical Internet Research Distributor: Journal of Medical Internet Research Institution: Journal of Medical Internet Research Label: Journal of Medical Internet Research Publisher: JMIR Publications Inc., Toronto, Canada</p>
N1  - <p>Date Added: 2025-08-28 12:17:31</p>
ER  - 

TY  - JOUR
TI  - Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect
AU  - Longoni, Chiara
AU  - Cian, Luca
T2  - Journal of Marketing
AB  - Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).
DA  - 2022///
PY  - 2022
DO  - 10.1177/0022242920957347
VL  - 86
IS  - 1
SP  - 91
EP  - 108
J2  - Journal of Marketing
LA  - EN
SN  - 0022-2429
ST  - Artificial Intelligence in Utilitarian vs. Hedonic Contexts
UR  - https://doi.org/10.1177/0022242920957347
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:18:49</p>
N1  - <p>Publisher: SAGE Publications Inc</p>
ER  - 

TY  - JOUR
TI  - Artificial Intelligence Is Trusted Less than a Doctor in Medical Treatment Decisions: Influence of Perceived Care and Value Similarity
AU  - Yokoi, Ryosuke
AU  - Eguchi, Yoko
AU  - Fujita, Takanori
AU  - Nakayachi, Kazuya
T2  - International Journal of Human–Computer Interaction
AB  - Artificial intelligence (AI) can provide many benefits in healthcare, including rapid and effective treatment options. However, previous research on human–computer interactions has demonstrated that people are reluctant to accept AI. This study compared individuals’ trust in AI with their trust in doctors regarding medical treatment and explored whether people trust an AI system that understands and suggests their desired treatments. We conducted an online experiment (N = 415) adopting a scenario method. The participants were asked to imagine receiving a medical examination and being prescribed medicine by a physician or an AI system. Participants trusted the AI system less than a doctor, even when the AI system learned and suggested their desired treatment. This finding confirms that people are reluctant to trust AI even if it performs at the level of a human doctor, thereby strengthening the existent literature that shows people prefer human doctors.
DA  - 2021///
PY  - 2021
DO  - 10.1080/10447318.2020.1861763
VL  - 37
IS  - 10
SP  - 981
EP  - 990
J2  - International Journal of Human–Computer Interaction
SN  - 1044-7318
ST  - Artificial Intelligence Is Trusted Less than a Doctor in Medical Treatment Decisions
UR  - https://doi.org/10.1080/10447318.2020.1861763
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:19:05</p>
N1  - <p>Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2020.1861763</p>
ER  - 

TY  - JOUR
TI  - Artificial Intelligence, Artists, and Art: Attitudes Toward Artwork Produced by Humans vs. Artificial Intelligence
AU  - Hong, Joo-Wha
AU  - Curran, Nathaniel Ming
T2  - ACM Trans. Multimedia Comput. Commun. Appl.
AB  - This study examines how people perceive artwork created by artificial intelligence (AI) and how presumed knowledge of an artist's identity (Human vs. AI) affects individuals’ evaluation of art. Drawing on Schema theory and theory of Computers Are Social Actors (CASA), this study used a survey-experiment that controlled for the identity of the artist (AI vs. Human) and presented participants with two types of artworks (AI-created vs. Human-created). After seeing images of six artworks created by either AI or human artists, participants (n = 288) were asked to evaluate the artistic value using a validated scale commonly employed among art professionals. The study found that human-created artworks and AI-created artworks were not judged to be equivalent in their artistic value. Additionally, knowing that a piece of art was created by AI did not, in general, influence participants’ evaluation of art pieces’ artistic value. However, having a schema that AI cannot make art significantly influenced evaluation. Implications of the findings for application and theory are discussed.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3326337
VL  - 15
IS  - 2s
SP  - 58:1
EP  - 58:16
J2  - ACM Trans. Multimedia Comput. Commun. Appl.
SN  - 1551-6857
ST  - Artificial Intelligence, Artists, and Art
UR  - https://doi.org/10.1145/3326337
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:19:17</p>
ER  - 

TY  - JOUR
TI  - Augmenting Language Models with Long-Term Memory
AU  - Wang, Weizhi
AU  - Dong, Li
AU  - Cheng, Hao
AU  - Liu, Xiaodong
AU  - Yan, Xifeng
AU  - Gao, Jianfeng
AU  - Wei, Furu
AB  - Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LongMem can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LongMem can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2306.07174
UR  - http://arxiv.org/abs/2306.07174
Y2  - 2025/08/28/
N1  - <p>arXiv:2306.07174 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:19:52</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Automated journalism a home run for sports? On the effects of perceived authorship and text objectivity on receptivity toward machine-written sports news
AU  - Yao, L. J.
AU  - Salmon, C. T.
AU  - Tandoc, E. C., Jr.
T2  - Proceedings of the URECA@NTU 2017–18
AB  - Automated journalism is rising in the news world, especially in sports coverage where structured data is abundant and readily available. Algorithms churn out full-length articles en masse and practically instantaneously, using big data, and this bloats the sheer volume of articles available to news readers. Noting the importance of perceived credibility and readability in sports news articles in particular, as well as the pertinence of text objectivity and bylines in automated journalism, an experiment was developed to understand how these factors interact. This study features a 3 x 2 between-subjects experiment, conducted on a US sample (N = 240), that seeks to explore the effects of bylines and text objectivity (based on attribution of sources and use of opinion-laden words) on reader perceptions (message credibility and readability) towards soft news journalism. The experiment found main effects for text objectivity in that readers perceive an objectively written article to be more credible but less readable, and vice versa. No main effects were observed for bylines. A significant interaction effect was observed on perceived credibility but not for readability. For instance, readers perceive objectively written articles to be more credible, but only if their bylines are of combined human and machine authorship. Implications for the news industry, specifically the trade-off or complementarity between transparency of authorship and consumer receptivity, for automated journalism users, are also discussed.
C1  - Nanyang Technological University
C3  - Proceedings of the URECA@NTU 2017–18
DA  - 2018///
PY  - 2018
SP  - 1
EP  - 10
N1  - <p>Date Added: 2025-08-28 12:02:44</p>
ER  - 

TY  - JOUR
TI  - Automated journalism: The effects of AI authorship and evaluative information on the perception of a science journalism article
AU  - Lermann Henestrosa, Angelica
AU  - Greving, Hannah
AU  - Kimmerle, Joachim
T2  - Computers in Human Behavior
AB  - Texts produced by artificial intelligence (AI) are becoming increasingly prevalent in digital journalism. Research suggests that these texts do not differ from human-written texts in their perceived credibility or trustworthiness where simple and short text types are concerned. However, it is unclear how AI-written texts beyond simple fact reporting are perceived. Therefore, this research aimed to expand upon the existing literature on automated journalism by investigating the influence of AI authorship (vs. human authorship) and evaluative information presentation (vs. neutral information presentation). The results of three preregistered experimental studies revealed no differences in perceived credibility and trustworthiness between AI-written and human-written texts. However, presenting information in an evaluative way decreased the perception of credibility and trustworthiness. Moreover, the AI was perceived as less anthropomorphic than the human author. The belief in the machine heuristic was stronger for an AI than for a human author, particularly when participants had actually read an article allegedly written by an AI. A pooled analysis across the data of all three studies underpinned the main effect of information presentation. Concluding, we discuss the findings against the background of AI perception theory and suggest implications for future research.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.chb.2022.107445
VL  - 138
SP  - 107445
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - Automated journalism
UR  - https://www.sciencedirect.com/science/article/pii/S0747563222002679
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:21:03</p>
KW  - Algorithm
KW  - Artificial intelligence
KW  - Automated journalism
KW  - Credibility
KW  - Information presentation
KW  - Trustworthiness
ER  - 

TY  - JOUR
TI  - Avoiding embarrassment online: Response to and inferences about chatbots when purchases activate self-presentation concerns
AU  - Jin, Jianna
AU  - Walker, Jesse
AU  - Reczek, Rebecca Walker
T2  - Journal of Consumer Psychology
AB  - We explore how self-presentation concerns and the desire to avoid embarrassment impact two distinct types of interactions consumers have with chatbots: interactions when a chatbot's identity is (1) not disclosed and therefore ambiguous or (2) disclosed. We propose that consumers feel less embarrassed with a chatbot than a human service agent in purchase contexts where self-presentation concerns are active because consumers ascribe less mind to chatbots. Therefore, when a chat agent's identity is ambiguous, consumers with greater self-presentation concerns are more likely to infer that an agent is human because this judgment allows consumers to proactively protect themselves from potential embarrassment in the event they are interacting with a human. We further show that when agent identity is clearly disclosed, consumers respond more positively to chatbots than human agents. However, this effect is contingent on the extent to which the chatbot is imbued with human characteristics: Anthropomorphizing chatbots leads consumers with higher self-presentation concerns to ascribe more mind to even clearly identified chatbots, resulting in a more negative consumer response.
DA  - 2025///
PY  - 2025
DO  - 10.1002/jcpy.1414
VL  - 35
IS  - 2
SP  - 185
EP  - 202
J2  - Journal of Consumer Psychology
LA  - en
SN  - 1532-7663
ST  - Avoiding embarrassment online
UR  - https://onlinelibrary.wiley.com/doi/abs/10.1002/jcpy.1414
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:21:35</p>
N1  - <p>_eprint: https://myscp.onlinelibrary.wiley.com/doi/pdf/10.1002/jcpy.1414</p>
N1  - <p>Rights: © 2024 The Authors. Journal of Consumer Psychology published by Wiley Periodicals LLC on behalf of Society for Consumer Psychology.</p>
KW  - artificial intelligence
KW  - chatbots
KW  - embarrassment
KW  - error management theory
KW  - inferences of ambiguous others
ER  - 

TY  - JOUR
TI  - Behavioral and neural evidence on consumer responses to human doctors and medical artificial intelligence
AU  - Yun, Jin Ho
AU  - Lee, Eun-Ju
AU  - Kim, Dong Hyun
T2  - Psychology & Marketing
AB  - Will consumers accept artificial intelligence (AI) as a medical care provider? On the basis of evolution theory, we investigate the implicit psychological mechanisms that underlie consumers’ interactions with medical AI and a human doctor. In a behavioral investigation (Study 1), consumers expressed a positive intention to use medical AI's healthcare services when it used personalized rather than mechanical conversation. However, neural investigation (Study 2) using functional magnetic resonance imaging revealed that some consumers’ implicit attitudes toward medical AI differed from their expressed behavioral intentions. The brain areas linked with implicitly apathetic emotions were activated even when medical AI used a personalized conversation, whereas consumers’ brains were activated in areas associated with prosociality when they interacted with a human doctor who used a personalized conversation. On the basis of our neural evidence, consumers perceive an identical personalized conversation differently when it is offered by a medical AI versus a human doctor. These findings have implications for the area of human–AI interactions and medical decision-making and suggest that replacing human doctors with medical AI is still an unrealistic proposition.
DA  - 2021///
PY  - 2021
DO  - 10.1002/mar.21445
VL  - 38
IS  - 4
SP  - 610
EP  - 625
J2  - Psychology & Marketing
LA  - en
SN  - 1520-6793
UR  - https://onlinelibrary.wiley.com/doi/abs/10.1002/mar.21445
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:21:45</p>
N1  - <p>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mar.21445</p>
N1  - <p>Rights: © 2021 Wiley Periodicals LLC</p>
KW  - artificial intelligence
KW  - consumer neuroscience
KW  - fMRI
KW  - medical decision-making
KW  - personalization
KW  - prosociality | apathy
ER  - 

TY  - JOUR
TI  - Behavioral Cues of Humanness in Complex Environments: How People Engage With Human and Artificially Intelligent Agents in a Multiplayer Videogame
AU  - Tulk Jesso, Stephanie
AU  - Kennedy, William G.
AU  - Wiese, Eva
T2  - Frontiers in Robotics and AI
AB  - The development of AI that can socially engage with humans is exciting to imagine, but such advanced algorithms might prove harmful if people are no longer able to detect when they are interacting with non-humans in online environments. Because we cannot fully predict how socially intelligent AI will be applied, it is important to conduct research into how sensitive humans are to behaviors of humans compared to those produced by AI. This paper presents results from a behavioral Turing Test, in which participants interacted with a human, or a simple or “social” AI within a complex videogame environment. Participants (66 total) played an open world, interactive videogame with one of these co-players and were instructed that they could interact non-verbally however they desired for 30 min, after which time they would indicate their beliefs about the agent, including three Likert measures of how much participants trusted and liked the co-player, the extent to which they perceived them as a “real person,” and an interview about the overall perception and what cues participants used to determine humanness. T-tests, Analysis of Variance and Tukey's HSD was used to analyze quantitative data, and Cohen's Kappa and χ2 was used to analyze interview data. Our results suggest that it was difficult for participants to distinguish between humans and the social AI on the basis of behavior. An analysis of in-game behaviors, survey data and qualitative responses suggest that participants associated engagement in social interactions with humanness within the game.
DA  - 2020///
PY  - 2020
DO  - 10.3389/frobt.2020.531805
VL  - 7
J2  - Frontiers in Robotics and AI
LA  - English
SN  - 2296-9144
ST  - Behavioral Cues of Humanness in Complex Environments
UR  - https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2020.531805/full
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:22:04</p>
N1  - <p>Publisher: Frontiers</p>
KW  - Cognitive Plausibility 2
KW  - Social AI 1
KW  - Turing Test 3
KW  - Videogames 4
KW  - Virtual Agents 5
ER  - 

TY  - JOUR
TI  - Being “Rational” Is Not Always Rational: Encouraging People to Be Rational Leads to Hedonically Suboptimal Decisions
AU  - Li, Xilin
AU  - Hsee, Christopher K.
T2  - Journal of the Association for Consumer Research
AB  - Often, laypeople motivate themselves or others to make good decisions by encouraging the decision maker to be “rational.” However, this practice could be counterproductive. Laypeople typically think that rational decisions are anti-emotional, based only on “cold” factors such as economic value, and not influenced by “hot” factors such as hedonic experience. Paradoxically, this lay notion of rationality is in stark contrast with the utilitarian notion of rationality, which maximizes overall utility, in which feelings are essential. Demonstrating this paradox, three studies found that participants who were encouraged to be rational were more likely to choose options that yielded less happiness without other benefits and therefore were less rational in the utility-maximizing sense. The research also examines boundary conditions and discusses practical implications.
DA  - 2019///
PY  - 2019
DO  - 10.1086/701966
VL  - 4
IS  - 2
SP  - 115
EP  - 124
J2  - Journal of the Association for Consumer Research
SN  - 2378-1815
ST  - Being “Rational” Is Not Always Rational
UR  - https://www.journals.uchicago.edu/doi/abs/10.1086/701966
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:22:21</p>
N1  - <p>Publisher: The University of Chicago Press</p>
ER  - 

TY  - CONF
TI  - Bot-Adversarial Dialogue for Safe Conversational Agents
AU  - Xu, Jing
AU  - Ju, Da
AU  - Li, Margaret
AU  - Boureau, Y-Lan
AU  - Weston, Jason
AU  - Dinan, Emily
T2  - Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
A2  - Toutanova, Kristina
A2  - Rumshisky, Anna
A2  - Zettlemoyer, Luke
A2  - Hakkani-Tur, Dilek
A2  - Beltagy, Iz
A2  - Bethard, Steven
A2  - Cotterell, Ryan
A2  - Chakraborty, Tanmoy
A2  - Zhou, Yichao
AB  - Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or “baking-in” safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.
C1  - Online
C3  - Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
DA  - 2021///
PY  - 2021
DO  - 10.18653/v1/2021.naacl-main.235
SP  - 2950
EP  - 2968
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2021.naacl-main.235/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:23:40</p>
ER  - 

TY  - CONF
TI  - Build confidence and acceptance of AI-based decision support systems - Explainable and liable AI
AU  - Nicodeme, Claire
T2  - 2020 13th International Conference on Human System Interaction (HSI)
AB  - Artificial Intelligence has known an incredible development since 2012. It was due to the impressive improvement of sensors, data quality and quantity, storage and computing capacity, etc. The promises AI offered led many scientific domains to implement AI-based decision support tool. However, despite numerous amazing results, very serious failures have raised Human mistrust, fear and scorn against AI. In Industries, staff members cannot afford to use tools that might fail them. This is especially true for Transportation operators where security and safety are at risk. Then, the question that arises is how to build Human confidence and acceptance of AI-based decision support system. In this paper, we combine different points of view to propose a structured overview of Transparency, Explicability and Interpretability, with new definitions arising as a consequence. Then we discuss the need for understandable information from the AI system, to legitimate or refute the tool's proposal. To conclude we offer ethical reflexions and ideas to develop confidence in AI.
C3  - 2020 13th International Conference on Human System Interaction (HSI)
DA  - 2020///
PY  - 2020
DO  - 10.1109/HSI49210.2020.9142668
SP  - 20
EP  - 23
UR  - https://ieeexplore.ieee.org/document/9142668
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:23:52</p>
N1  - <p>ISSN: 2158-2254</p>
KW  - Cognition
KW  - confidence
KW  - Decision making
KW  - decision support system
KW  - Decision support systems
KW  - explainable AI
KW  - Industries
KW  - liable AI
KW  - technology
KW  - Tools
KW  - Transportation
ER  - 

TY  - JOUR
TI  - Can AI be a content generator? Effects of content generators and information delivery methods on the psychology of content consumers
AU  - Kim, Jina
AU  - Shin, Soyeon
AU  - Bae, Kunwoo
AU  - Oh, Soyoung
AU  - Park, Eunil
AU  - del Pobil, Angel P.
T2  - Telematics and Informatics
AB  - Considering the rapidly improving technologies in artificial intelligence (AI), researchers in mass communication and journalism have paid attention to the use of AI technologies. However, there are still notable concerns about the particular use of AI technologies in this field. Put simply, can AI technologies reduce human tasks? In order to address this question, this study investigates the effects of content generators (human vs. AI) and information delivery methods (text vs. audio vs. video) on users’ perceptions of content. The results indicate that the generators and methods play a notable role in eliciting greater quality, satisfaction, and readability of the content. Based on the findings, the implications are addressed.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.tele.2020.101452
VL  - 55
SP  - 101452
J2  - Telematics and Informatics
SN  - 0736-5853
ST  - Can AI be a content generator?
UR  - https://www.sciencedirect.com/science/article/pii/S0736585320301118
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:25:22</p>
KW  - Artificial intelligence
KW  - Automated content
KW  - Content generator
KW  - Information delivery
ER  - 

TY  - JOUR
TI  - Can AI enhance people's support for online moderation and their openness to dissimilar political views?
AU  - Wojcieszak, Magdalena
AU  - Thakur, Arti
AU  - Ferreira Gonçalves, João Fernando
AU  - Casas, Andreu
AU  - Menchen-Trevino, Ericka
AU  - Boon, & Miriam
T2  - Journal of Computer-Mediated Communication
AB  - Although artificial intelligence is blamed for many societal challenges, it also has underexplored potential in political contexts online. We rely on six preregistered experiments in three countries (N = 6,728) to test the expectation that AI and AI-assisted humans would be perceived more favorably than humans (a) across various content moderation, generation, and recommendation scenarios and (b) when exposing individuals to counter-attitudinal political information. Contrary to the preregistered hypotheses, participants see human agents as more just than AI across the scenarios tested, with the exception of news recommendations. At the same time, participants are not more open to counter-attitudinal information attributed to AI rather than a human or an AI-assisted human. These findings, which—with minor variations—emerged across countries, scenarios, and issues, suggest that human intervention is preferred online and that people reject dissimilar information regardless of its source. We discuss the theoretical and practical implications of these findings. Lay Summary: In the era of unprecedented political divides and misinformation, artificial intelligence (AI) and algorithms are often seen as the culprits. In contrast to these dominant narratives, we argued that AI might be seen as being less biased than a human in online political contexts. We relied on six preregistered experiments in three countries (the United Sates, Spain, Poland) to test whether internet users perceive AI and AI-assisted humans more favorably than simply humans; (a) across various distinct scenarios online, and (b) when exposing people to opposing political information on a range of contentious issues. Contrary to our expectations, human agents were consistently perceived more favorably than AI except when recommending news. These findings suggest that people prefer human intervention in most online political contexts. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
DA  - 2021///
PY  - 2021
DO  - 10.1093/jcmc/zmab006
VL  - 26
IS  - 4
SP  - 223
EP  - 243
J2  - Journal of Computer-Mediated Communication
SN  - 1083-6101
N1  - <p>Date Added: 2025-08-28 12:25:14</p>
N1  - <p>Place: United Kingdom Publisher: Oxford University Press</p>
KW  - Algorithms
KW  - Artificial Intelligence
KW  - News Media
KW  - Political Attitudes
ER  - 

TY  - CONF
TI  - Can AI reduce motivated reasoning in news consumption? Investigating the role of attitudes towards AI and prior-opinion in shaping trust perceptions of news
AU  - Wischnewski, M.
AU  - Krämer, N.
T2  - HHAI2022: Augmenting Human Intellect
AB  - A central role in understanding the interaction between humans and AI plays the notion of trust. Especially research from social and cognitive psychology has shown, however, that individuals’ perceptions of trust can be biased. In this empirical investigation, we focus on the single and combined effects of attitudes towards AI and motivated reasoning in shaping such biased trust perceptions in the context of news consumption. In doing so, we rely on insights from works on the machine heuristic and motivated reasoning. In a 2 (author) x 2 (congruency) between-subjects online experiment, we asked N = 477 participants to read a news article purportedly written either by AI or a human author. We manipulated whether the article represented pro or contra arguments of a polarizing topic, to elicit motivated reasoning. We also assessed participants’ attitudes towards AI in terms of competence and objectivity. Through multiple linear regressions, we found that (a) increased perceptions of AI as objective and ideologically unbiased increased trust perceptions, whereas (b), in cases where participants were swayed by their prior opinion to trust content more when they agreed with the content, the AI author reduced such biased perceptions. Our results indicate that it is crucial to account for attitudes towards AI and motivated reasoning to accurately represent trust perceptions.
C1  - IOS Press
C3  - HHAI2022: Augmenting Human Intellect
DA  - 2022///
PY  - 2022
DO  - 10.3233/FAIA220198
SP  - 184
EP  - 198
N1  - <p>Date Added: 2025-08-28 12:02:44</p>
ER  - 

TY  - JOUR
TI  - Can an Algorithm Reduce the Perceived Bias of News? Testing the Effect of Machine Attribution on News Readers’ Evaluations of Bias, Anthropomorphism, and Credibility
AU  - Waddell, T. Franklin
T2  - Journalism & Mass Communication Quarterly
AB  - Although accusations of editorial slant are ubiquitous to the contemporary media environment, recent advances in journalism such as news writing algorithms may hold the potential to reduce readers’ perceptions of media bias. Informed by the Modality-Agency-Interactivity-Navigability (MAIN) model and the principle of similarity attraction, an online experiment (n = 612) was conducted to test if news attributed to an automated author is perceived as less biased and more credible than news attributed to a human author. Results reveal that perceptions of bias are attenuated when news is attributed to a journalist and algorithm in tandem, with positive downstream consequences for perceived news credibility.
DA  - 2019///
PY  - 2019
DO  - 10.1177/1077699018815891
VL  - 96
IS  - 1
SP  - 82
EP  - 100
J2  - Journalism & Mass Communication Quarterly
LA  - EN
SN  - 1077-6990
ST  - Can an Algorithm Reduce the Perceived Bias of News?
UR  - https://doi.org/10.1177/1077699018815891
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:27:10</p>
N1  - <p>Publisher: SAGE Publications Inc</p>
ER  - 

TY  - JOUR
TI  - Can we be friends with Mitsuku? A longitudinal study on the process of relationship formation between humans and a social chatbot
AU  - Croes, Emmelyn A. J.
AU  - Antheunis, Marjolijn L.
T2  - Journal of Social and Personal Relationships
AB  - This explorative study investigated (a) whether social attraction, self-disclosure, interaction quality, intimacy, empathy and communicative competence play a role in getting-acquainted interactions between humans and a chatbot, and (b) whether humans can build a relationship with a chatbot. Although human-machine communication research suggests that humans can develop feelings for computers, this does not automatically imply that humans experience feelings of friendship with a chatbot. In this longitudinal study, 118 participants had seven interactions with chatbot Mitsuku over a 3-week period. After each interaction participants filled out a questionnaire. The results showed that the social processes decreased after each interaction and feelings of friendship were low. In line with the ABCDE model of relationship development, the social processes that aid relationship continuation decrease, leading to deterioration of the relationship. Furthermore, a novelty effect was at play after the first interaction, after which the chatbot became predictable and the interactions less enjoyable.
DA  - 2021///
PY  - 2021
DO  - 10.1177/0265407520959463
VL  - 38
IS  - 1
SP  - 279
EP  - 300
J2  - Journal of Social and Personal Relationships
LA  - EN
SN  - 0265-4075
ST  - Can we be friends with Mitsuku?
UR  - https://doi.org/10.1177/0265407520959463
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:27:12</p>
N1  - <p>Publisher: SAGE Publications Ltd</p>
ER  - 

TY  - JOUR
TI  - Challenges and Applications of Large Language Models
AU  - Kaddour, Jean
AU  - Harris, Joshua
AU  - Mozes, Maximilian
AU  - Bradley, Herbie
AU  - Raileanu, Roberta
AU  - McHardy, Robert
AB  - Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2307.10169
UR  - http://arxiv.org/abs/2307.10169
Y2  - 2025/08/28/
N1  - <p>arXiv:2307.10169 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:27:14</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Challenges with unsupervised LLM knowledge discovery
AU  - Farquhar, Sebastian
AU  - Varma, Vikrant
AU  - Kenton, Zachary
AU  - Gasteiger, Johannes
AU  - Mikulik, Vladimir
AU  - Shah, Rohin
AB  - We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a series of experiments showing settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2312.10029
UR  - http://arxiv.org/abs/2312.10029
Y2  - 2025/08/28/
N1  - <p>arXiv:2312.10029 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:27:16</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Classifying Constructive Comments
AU  - Kolhatkar, Varada
AU  - Thain, Nithum
AU  - Sorensen, Jeffrey
AU  - Dixon, Lucas
AU  - Taboada, Maite
AB  - We introduce the Constructive Comments Corpus (C3), comprised of 12,000 annotated news comments, intended to help build new tools for online communities to improve the quality of their discussions. We define constructive comments as high-quality comments that make a contribution to the conversation. We explain the crowd worker annotation scheme and define a taxonomy of sub-characteristics of constructiveness. The quality of the annotation scheme and the resulting dataset is evaluated using measurements of inter-annotator agreement, expert assessment of a sample, and by the constructiveness sub-characteristics, which we show provide a proxy for the general constructiveness concept. We provide models for constructiveness trained on C3 using both feature-based and a variety of deep learning approaches and demonstrate that these models capture general rather than topic- or domain-specific characteristics of constructiveness, through domain adaptation experiments. We examine the role that length plays in our models, as comment length could be easily gamed if models depend heavily upon this feature. By examining the errors made by each model and their distribution by length, we show that the best performing models are less correlated with comment length.The constructiveness corpus and our experiments pave the way for a moderation tool focused on promoting comments that make a contribution, rather than only filtering out undesirable content.
DA  - 2020///
PY  - 2020
DO  - 10.48550/arXiv.2004.05476
UR  - http://arxiv.org/abs/2004.05476
Y2  - 2025/08/28/
N1  - <p>arXiv:2004.05476 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:27:18</p>
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - Computer Science - Information Retrieval
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Close Encounters of the AI Kind: Use of AI Influencers As Brand Endorsers
AU  - Thomas, Veronica L.
AU  - Fowler, Kendra
T2  - Journal of Advertising
AB  - Brand endorsers can contribute to a brand’s success or failure (in the case of endorser transgressions). Recent advancements in technology have produced new, nonhuman alternatives to traditional celebrity endorsers. These new endorsers rely on artificial intelligence (AI) to interact with and influence consumers. Two studies demonstrate that AI influencers can produce positive brand benefits similar to those produced by human celebrity endorsers. Moreover, just like their human counterparts, AI influencers can also commit transgressions that result in degradation of the endorsed brand. Importantly, though, AI influencers differ from human celebrity endorsers in that consumers are less likely to view them as unique entities (as tested in a pilot study). Thus, consumers are more likely to perceive a transgression committed by an AI influencer as behavior applicable to all AI influencers, but they are less likely to view celebrity endorser behaviors as interchangeable. As such, after an AI influencer has committed a transgression, replacing the AI influencer with a celebrity endorser attenuates negative brand perceptions, an effect which cannot be realized if the replacement is another AI influencer.
DA  - 2021///
PY  - 2021
DO  - 10.1080/00913367.2020.1810595
VL  - 50
IS  - 1
SP  - 11
EP  - 25
J2  - Journal of Advertising
SN  - 0091-3367
ST  - Close Encounters of the AI Kind
UR  - https://doi.org/10.1080/00913367.2020.1810595
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:27:21</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/00913367.2020.1810595</p>
ER  - 

TY  - JOUR
TI  - Combating Misinformation in the Age of LLMs: Opportunities and Challenges
AU  - Chen, Canyu
AU  - Shu, Kai
AB  - Misinformation such as fake news and rumors is a serious threat on information ecosystems and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: how to utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2311.05656
ST  - Combating Misinformation in the Age of LLMs
UR  - http://arxiv.org/abs/2311.05656
Y2  - 2025/08/28/
N1  - <p>arXiv:2311.05656 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:27:25</p>
KW  - Computer Science - Computers and Society
ER  - 

TY  - JOUR
TI  - Common sense or censorship: How algorithmic moderators and message type influence perceptions of online content deletion
AU  - Gonçalves, João
AU  - Weber, Ina
AU  - Masullo, Gina M.
AU  - Torres da Silva, Marisa
AU  - Hofhuis, Joep
T2  - New Media & Society
AB  - Hateful content online is a concern for social media platforms, policymakers, and the public. This has led high-profile content platforms, such as Facebook, to adopt algorithmic content-moderation systems; however, the impact of algorithmic moderation on user perceptions is unclear. We experimentally test the extent to which the type of content being removed (profanity vs hate speech) and the explanation given for its removal (no explanation vs link to community guidelines vs specific explanation) influence user perceptions of human and algorithmic moderators. Our preregistered study encompasses representative samples (N = 2870) from the United States, the Netherlands, and Portugal. Contrary to expectations, our findings suggest that algorithmic moderation is perceived as more transparent than human, especially when no explanation is given for content removal. In addition, sending users to community guidelines for further information on content deletion has negative effects on outcome fairness and trust.
DA  - 2023///
PY  - 2023
DO  - 10.1177/14614448211032310
VL  - 25
IS  - 10
SP  - 2595
EP  - 2617
J2  - New Media & Society
LA  - EN
SN  - 1461-4448
ST  - Common sense or censorship
UR  - https://doi.org/10.1177/14614448211032310
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:27:28</p>
N1  - <p>Publisher: SAGE Publications</p>
ER  - 

TY  - JOUR
TI  - Comparing the persuasiveness of role-playing large language models and human experts on polarized U.S. political issues
AU  - Hackenburg, Kobi
AU  - Ibrahim, Lujain
AU  - Tappin, Ben
AU  - Tsakiris, Manos
AB  - Advances in large language models (LLMs) could significantly disrupt political communication. In a large-scale pre-registered experiment (n=4,955), we prompted GPT-4 to generate persuasive messages impersonating the language and beliefs of U.S. political parties – a technique we term “partisan role-play” –  and directly compared their persuasiveness to that of human persuasion experts. In aggregate, the persuasive impact of role-playing messages generated by GPT-4 was not significantly different from that of non-role-playing messages. However, the persuasive impact of GPT-4 rivaled, and on some issues exceeded, that of the human experts. Taken together, our findings suggest that — contrary to popular concern — instructing current LLMs to role-play as partisans offers limited persuasive advantage, but also that current LLMs can rival and even exceed the persuasiveness of human experts. These results potentially portend widespread adoption of AI tools by persuasion campaigns, with important implications for the role of AI in politics and democracy.
DA  - 2023///
PY  - 2023
DO  - 10.31219/osf.io/ey8db
LA  - en-us
UR  - https://osf.io/ey8db_v1
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:27:31</p>
KW  - AI safety
KW  - AI-mediated communication
KW  - large language models
KW  - political persuasion
KW  - role-play
ER  - 

TY  - JOUR
TI  - Concept analysis of impressionability among adolescents and young adults
AU  - Gwon, Seok Hyun
AU  - Jeong, Suyong
T2  - Nursing Open
AB  - Aim To report a concept analysis of impressionability among adolescents and young adults. Background Adolescence and young adulthood are critical periods to establish health behaviour. Environmental determinants are associated with youth health risk behaviours. These populations are impressionable to a range of social and physical environmental factors. Design Concept analysis. Methods We selected 17 studies to review from Psych INFO, ERIC, MEDLINE, and Google Scholar as well as the University of Virginia library. We did not apply date limits. We included search terms: “adolescent”; “youth”; “young adult”; “impressionability”; and “impressionable.” Results The defining attributes of impressionability among youths were a state where: (a) one is able to be susceptible to external stimuli; (b) one is incapable of reasonable judgement; and (c) one has a changeability to accept or modify one's attitude and behaviour. We identified antecedents, consequences, and cases of impressionability.
DA  - 2018///
PY  - 2018
DO  - 10.1002/nop2.170
VL  - 5
IS  - 4
SP  - 601
EP  - 610
J2  - Nursing Open
LA  - en
SN  - 2054-1058
UR  - https://onlinelibrary.wiley.com/doi/abs/10.1002/nop2.170
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:27:35</p>
N1  - <p>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nop2.170</p>
N1  - <p>Rights: © 2018 The Authors. Nursing Open published by John Wiley & Sons Ltd.</p>
KW  - adolescent
KW  - commercial
KW  - concept analysis
KW  - impressionability
KW  - or not-for-profit sectors
KW  - youth
ER  - 

TY  - JOUR
TI  - Consent and Coercion
AU  - Ferzan, Kimberly Kessler
T2  - Arizona State Law Journal
AB  - There are substantial disputes as to what sorts of behavior constitute coercion and thereby undermine consent. This disagreement was on full display during the public fray over Aziz Ansari’s behavior on a date. Whereas some commentators condemned Ansari’s behavior as nothing short of sexual assault, others believed his behavior did not rise to the level of undermining consent.



This Article claims that the way forward is to see that there are two normative functions for coercion, and each is at play with respect to consent. Sometimes coercion is about the blameworthiness of the coercer, and sometimes coercion is about the involuntariness of the consenter’s choice. To deny the latter is not to deny the former. Because these are two disparate functions, much of the debate about Ansari may be commentators talking past each other.



After explaining this miscommunication, this Article broadens our understanding of how the blameworthiness of the coercer can bear on the permissibility of his actions. Just as no man may profit from his own wrong, coercers may not avail themselves of consent, even if it is sufficiently “freely given” such that the consenter is not acting involuntarily. This Article claims that the wrongful coercion “normatively impairs” the coercer, and that this normative impairment is at play in other legal doctrines.



With the normative grounding in place, this Article considers how and if these amendments to our view of coercion should be taken into account in the law, with a specific focus on sexual offenses. It offers a draft statute for discussion purposes, considers charges of paternalism in both the public and private sphere, and points to other reasons to be cautious about criminalization. Finally, this Article defends this view as a more perspicuous account of the normative landscape than other coercion theories. Ultimately, the goal of this Article is to define new conceptual territory for normative debate. Progress cannot be made until we ask the right questions and answer the same ones. This Article aims to provide the framework within which more nuanced discussions can be had.
DA  - 2018///
PY  - 2018
VL  - 50
SP  - 951
J2  - Arizona State Law Journal
UR  - https://heinonline.org/HOL/Page?handle=hein.journals/arzjl50&id=983&div=&collection=
N1  - <p>Date Added: 2025-08-28 12:28:51</p>
ER  - 

TY  - JOUR
TI  - Constitutional AI: Harmlessness from AI Feedback
AU  - Bai, Yuntao
AU  - Kadavath, Saurav
AU  - Kundu, Sandipan
AU  - Askell, Amanda
AU  - Kernion, Jackson
AU  - Jones, Andy
AU  - Chen, Anna
AU  - Goldie, Anna
AU  - Mirhoseini, Azalia
AU  - McKinnon, Cameron
AU  - Chen, Carol
AU  - Olsson, Catherine
AU  - Olah, Christopher
AU  - Hernandez, Danny
AU  - Drain, Dawn
AU  - Ganguli, Deep
AU  - Li, Dustin
AU  - Tran-Johnson, Eli
AU  - Perez, Ethan
AU  - Kerr, Jamie
AU  - Mueller, Jared
AU  - Ladish, Jeffrey
AU  - Landau, Joshua
AU  - Ndousse, Kamal
AU  - Lukosuite, Kamile
AU  - Lovitt, Liane
AU  - Sellitto, Michael
AU  - Elhage, Nelson
AU  - Schiefer, Nicholas
AU  - Mercado, Noemi
AU  - DasSarma, Nova
AU  - Lasenby, Robert
AU  - Larson, Robin
AU  - Ringer, Sam
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Showk, Sheer El
AU  - Fort, Stanislav
AU  - Lanham, Tamera
AU  - Telleen-Lawton, Timothy
AU  - Conerly, Tom
AU  - Henighan, Tom
AU  - Hume, Tristan
AU  - Bowman, Samuel R.
AU  - Hatfield-Dodds, Zac
AU  - Mann, Ben
AU  - Amodei, Dario
AU  - Joseph, Nicholas
AU  - McCandlish, Sam
AU  - Brown, Tom
AU  - Kaplan, Jared
AB  - As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2212.08073
ST  - Constitutional AI
UR  - http://arxiv.org/abs/2212.08073
Y2  - 2025/08/27/
N1  - <p>Date Added: 2025-08-28 12:02:44</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Consumers and Artificial Intelligence: An Experiential Perspective
AU  - Puntoni, Stefano
AU  - Reczek, Rebecca Walker
AU  - Giesler, Markus
AU  - Botti, Simona
T2  - Journal of Marketing
AB  - Artificial intelligence (AI) helps companies offer important benefits to consumers, such as health monitoring with wearable devices, advice with recommender systems, peace of mind with smart household products, and convenience with voice-activated virtual assistants. However, although AI can be seen as a neutral tool to be evaluated on efficiency and accuracy, this approach does not consider the social and individual challenges that can occur when AI is deployed. This research aims to bridge these two perspectives: on one side, the authors acknowledge the value that embedding AI technology into products and services can provide to consumers. On the other side, the authors build on and integrate sociological and psychological scholarship to examine some of the costs consumers experience in their interactions with AI. In doing so, the authors identify four types of consumer experiences with AI: (1) data capture, (2) classification, (3) delegation, and (4) social. This approach allows the authors to discuss policy and managerial avenues to address the ways in which consumers may fail to experience value in organizations’ investments into AI and to lay out an agenda for future research.
DA  - 2021///
PY  - 2021
DO  - 10.1177/0022242920953847
VL  - 85
IS  - 1
SP  - 131
EP  - 151
J2  - Journal of Marketing
LA  - EN
SN  - 0022-2429
ST  - Consumers and Artificial Intelligence
UR  - https://doi.org/10.1177/0022242920953847
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:29:02</p>
N1  - <p>Publisher: SAGE Publications Inc</p>
ER  - 

TY  - JOUR
TI  - Debate Helps Supervise Unreliable Experts
AU  - Michael, Julian
AU  - Mahdi, Salsabila
AU  - Rein, David
AU  - Petty, Jackson
AU  - Dirani, Julien
AU  - Padmakumar, Vishakh
AU  - Bowman, Samuel R.
AB  - As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important. How can we supervise unreliable experts, which have access to the truth but may not accurately report it, to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own? In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage. In our debates, one expert argues for the correct answer, and the other for an incorrect answer. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%. Debates are also more efficient, being 68% of the length of consultancies. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill). Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2311.08702
UR  - http://arxiv.org/abs/2311.08702
Y2  - 2025/08/28/
N1  - <p>arXiv:2311.08702 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:29:05</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Deception Abilities Emerged in Large Language Models
AU  - Hagendorff, Thilo
T2  - Proceedings of the National Academy of Sciences
AB  - Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown machine behavior in LLMs, our study contributes to the nascent field of machine psychology.
DA  - 2024///
PY  - 2024
DO  - 10.1073/pnas.2317967121
VL  - 121
IS  - 24
SP  - e2317967121
J2  - Proceedings of the National Academy of Sciences
SN  - 0027-8424, 1091-6490
UR  - http://arxiv.org/abs/2307.16513
Y2  - 2025/08/28/
N1  - <p>arXiv:2307.16513 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:29:07</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature
AU  - Mitchell, Eric
AU  - Lee, Yoonho
AU  - Khazatsky, Alexander
AU  - Manning, Christopher D.
AU  - Finn, Chelsea
AB  - The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2301.11305
ST  - DetectGPT
UR  - http://arxiv.org/abs/2301.11305
Y2  - 2025/08/28/
N1  - <p>arXiv:2301.11305 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:29:09</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Did Artificial Intelligence Invade Humans? The Study on the Mechanism of Patients’ Willingness to Accept Artificial Intelligence Medical Care: From the Perspective of Intergroup Threat Theory
AU  - Zhou, Yuwei
AU  - Shi, Yichuan
AU  - Lu, Wei
AU  - Wan, Fang
T2  - Frontiers in Psychology
AB  - Artificial intelligence (AI) has become one of the core driving forces for the future development of the medical industry, but patients are skeptical about the use of AI in medical care. Based on the Intergroup Threat Theory (ITT), This study verified that patients would regard AI as an external group, triggering the perceived threat of the external group, resulting in avoidance behaviors in the treatment (experiment 1: N=446) and diagnosis (experiment2: N=330) scenarios. The results show that despite AI can provide expert-level accuracy in medical care, patients are still more likely to rely on human doctors and experience more negative emotions as AI is more involved in medical care (experiment 1). Furthermore, patients pay more attention to threats at the individual level, such as realistic threats related to privacy issue and symbolic threats related to the neglect of personal characteristics. In contrast, realistic threat and symbolic threat at the group level had less effect on patients in the medical scenario (experiment 2).
DA  - 2022///
PY  - 2022
DO  - 10.3389/fpsyg.2022.866124
VL  - 13
J2  - Frontiers in Psychology
LA  - English
SN  - 1664-1078
ST  - Did Artificial Intelligence Invade Humans?
UR  - https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.866124/full
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:29:12</p>
N1  - <p>Publisher: Frontiers</p>
KW  - Artificial Intelligence Medical Care
KW  - diagnosis
KW  - Intergroup threat theory
KW  - Realistic Threats
KW  - symbolic threats
KW  - Treatment
KW  - Willingness to accept (WTA)
ER  - 

TY  - JOUR
TI  - Discovering Latent Knowledge in Language Models Without Supervision
AU  - Burns, Collin
AU  - Ye, Haotian
AU  - Klein, Dan
AU  - Steinhardt, Jacob
AB  - Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arXiv.2212.03827
UR  - http://arxiv.org/abs/2212.03827
Y2  - 2025/08/28/
N1  - <p>arXiv:2212.03827 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:34:41</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Do You Care Who Flagged This Post? Effects of Moderator Visibility on Bystander Behavior
AU  - Bhandari, Aparajita
AU  - Ozanne, Marie
AU  - Bazarova, Natalya N.
AU  - DiFranzo, Dominic
T2  - Journal of Computer-Mediated Communication
AB  - This study evaluates whether increasing information visibility around the identity of a moderator influences bystanders’ likelihood to flag subsequent unmoderated harassing comments. In a 2-day preregistered experiment conducted in a realistic social media simulation, participants encountered ambiguous or unambiguous harassment comments, which were ostensibly flagged by either other users, an automated system (AI), or an unidentified moderation source. The results reveal that visibility of a content moderation source inhibited participants’ flagging of a subsequent unmoderated harassment comment, presumably because their efforts were seen as dispensable, compared to when the moderation source was unknown. On the contrary, there was an indirect effect of other users versus AI as moderation source on subsequent flagging through changes in perceived social norms. Overall, this research shows that the effects of moderation transparency are complex, as increasing visibility of a content moderator may inadvertently inhibit bystander intervention.This study examines the effects of flagging unmoderated offensive posts on social media, and how this changes the users’ subsequent behavior. We examined users’ reactions to the flagging of these posts by other users, an automated system, or an unspecified process to determine whether this affects the users’ ensuing behavior. A 2-day experiment on a simulated social media site showed that the visibility of the “flagger” impacts how users perceive social norms and think about the accountability for their own online actions. The results showed that the visibility of the person/system that flagged the material generally deterred subsequent flagging. The analysis also shows that the effect was stronger when the users thought that it was other users, and not an automated system, that had flagged the online harassment.
DA  - 2021///
PY  - 2021
DO  - 10.1093/jcmc/zmab007
VL  - 26
IS  - 5
SP  - 284
EP  - 300
J2  - Journal of Computer-Mediated Communication
SN  - 1083-6101
ST  - Do You Care Who Flagged This Post?
UR  - https://doi.org/10.1093/jcmc/zmab007
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:35:47</p>
ER  - 

TY  - JOUR
TI  - Does the Correspondence Bias Apply to Social Robots?: Dispositional and Situational Attributions of Human Versus Robot Behavior
AU  - Edwards, Autumn
AU  - Edwards, Chad
T2  - Frontiers in Robotics and AI
AB  - Increasingly, people interact with embodied machine communicators and are challenged to understand their natures and behaviors. The Fundamental Attribution Error (FAE, sometimes referred to as the correspondence bias) is the tendency for individuals to over-emphasize personality-based or dispositional explanations for other people’s behavior while under-emphasizing situational explanations. This effect has been thoroughly examined with humans, but do people make the same causal inferences when interpreting the actions of a robot? As compared to people, social robots are less autonomous and agentic because their behavior is wholly determined by humans in the loop, programming, and design choices. Nonetheless, people do assign robots agency, intentionality, personality, and blame. Results of an experiment showed that participants made correspondent inferences when evaluating both human and robot speakers, attributing their behavior to underlying attitudes even when it was clearly coerced. However, they committed a stronger correspondence bias in the case of the robot–an effect driven by the greater dispositional culpability assigned to robots committing unpopular behavior–and they were more confident in their attitudinal judgments of robots than humans. Results demonstrated some differences in the global impressions of humans and robots based on behavior valence and choice. Judges formed more generous impressions of the robot agent when its unpopular behavior was coerced versus chosen; a tendency not displayed when forming impressions of the human agent. Implications of attributing robot behavior to disposition, or conflating robot actors with their actions, are addressed.
DA  - 2022///
PY  - 2022
DO  - 10.3389/frobt.2021.788242
VL  - 8
J2  - Frontiers in Robotics and AI
LA  - English
SN  - 2296-9144
ST  - Does the Correspondence Bias Apply to Social Robots?
UR  - https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.788242/full
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:35:50</p>
N1  - <p>Publisher: Frontiers</p>
KW  - CASA
KW  - Correspondence bias
KW  - FAE
KW  - Fundamental attribution error
KW  - human-robot interaction
KW  - social robot
ER  - 

TY  - JOUR
TI  - Does the use of synchrony and artificial intelligence in video interviews affect interview ratings and applicant attitudes?
AU  - Suen, Hung-Yue
AU  - Chen, Mavis Yi-Ching
AU  - Lu, Shih-Hao
T2  - Computers in Human Behavior
AB  - The use of asynchronous video interviews (AVIs) and artificial intelligence (AI)-based decision agents enables more efficient employment screening compared with traditional synchronous video interviews (SVIs). However, the social impacts of using synchrony and AI decision agents in video interviews have not been investigated. Drawing on media richness theory and social interface theory, this study employed a novel experimental design to compare human ratings and job applicants' response behaviours between the SVI and AVI settings and compare job applicants’ fairness perception between the AVI setting and the AVI setting using an AI decision agent (AVI-AI). The results from 180 observations revealed that 1) first impression and physical appearance significantly affected structured interview ratings regardless of whether the video interview was synchronous; 2) compared with SVIs, AVIs lessened the primacy effect on physical appearance and initial impression among human raters; 3) job applicants had indistinguishable fairness perceptions regarding synchrony (SVI vs. AVI) and decision agent (human vs. AI); and 4) applicants exhibited less favourability towards AVIs than towards SVIs. Findings from this experimental comparison, including recommendations for practice and future research on human resource selection, technology education, and social computing, are discussed.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.chb.2019.04.012
VL  - 98
SP  - 93
EP  - 101
J2  - Computers in Human Behavior
SN  - 0747-5632
UR  - https://www.sciencedirect.com/science/article/pii/S0747563219301529
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:35:52</p>
KW  - Artificial intelligence (AI)
KW  - Asynchronous video interview (AVI)
KW  - Impressional primacy effect
KW  - Media richness theory
KW  - Social interface theory
KW  - Synchronous video interview (SVI)
ER  - 

TY  - JOUR
TI  - Don’t Want to Look Dumb? The Role of Theories of Intelligence and Humanlike Features in Online Help Seeking
AU  - Kim, Sara
AU  - Zhang, Ke
AU  - Park, Daeun
T2  - Psychological Science
AB  - Numerous studies have shown that individuals’ help-seeking behavior increases when a computerized helper is endowed with humanlike features in nonachievement contexts. In contrast, the current research suggests that anthropomorphic helpers are not universally conducive to help-seeking behavior in contexts of achievement, particularly among individuals who construe help seeking as a display of incompetence (i.e., entity theorists). Study 1 demonstrated that when entity theorists received help from an anthropomorphized (vs. a nonanthropomorphized) helper, they were more concerned about negative judgments from other people, whereas incremental theorists were not affected by anthropomorphic features. Study 2 showed that when help was provided by an anthropomorphized (vs. a nonanthropomorphized) helper, entity theorists were less likely to seek help, even at the cost of lower performance. In contrast, incremental theorists’ help-seeking behavior and task performance were not affected by anthropomorphism. This research deepens the current understanding of the role of anthropomorphic computerized helpers in online learning contexts.
DA  - 2018///
PY  - 2018
DO  - 10.1177/0956797617730595
VL  - 29
IS  - 2
SP  - 171
EP  - 180
J2  - Psychological Science
LA  - EN
SN  - 0956-7976
ST  - Don’t Want to Look Dumb?
UR  - https://doi.org/10.1177/0956797617730595
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:36:07</p>
N1  - <p>Publisher: SAGE Publications Inc</p>
ER  - 

TY  - JOUR
TI  - Durably reducing conspiracy beliefs through dialogues with AI
AU  - Costello, Thomas
AU  - Pennycook, Gordon
AU  - Rand, David
AB  - Conspiracy theories are a paradigmatic example of beliefs that, once adopted, are extremely difficult to dispel. Influential psychological theories propose that conspiracy beliefs are uniquely resistant to counterevidence because they satisfy important needs and motivations. Here, we raise the possibility that previous attempts to correct conspiracy beliefs have been unsuccessful merely because they failed to deliver counterevidence that was sufficiently compelling and tailored to each believer’s specific conspiracy theory (which vary dramatically from believer to believer). To evaluate this possibility, we leverage recent developments in generative artificial intelligence (AI) to deliver well-argued, person-specific debunks to a total of N = 2,190 conspiracy theory believers. Participants in our experiments provided detailed, open-ended explanations of a conspiracy theory they believed, and then engaged in a 3 round dialogue with a frontier generative AI model (GPT-4 Turbo) which was instructed to reduce each participant’s belief in their conspiracy theory (or discuss a banal topic in a control condition). Across two experiments, we find robust evidence that the debunking conversation with the AI reduced belief in conspiracy theories by roughly 20%. This effect did not decay over 2 months time, was consistently observed across a wide range of different conspiracy theories, and occurred even for participants whose conspiracy beliefs were deeply entrenched and of great importance to their identities. Furthermore, although the dialogues were focused on a single conspiracy theory, the intervention spilled over to reduce beliefs in unrelated conspiracies, indicating a general decrease in conspiratorial worldview, as well as increasing intentions to challenge others who espouse their chosen conspiracy. These findings highlight that even many people who strongly believe in seemingly fact-resistant conspiratorial beliefs can change their minds in the face of sufficient evidence.
DA  - 2024///
PY  - 2024
DO  - 10.31234/osf.io/xcwdn
LA  - en-us
UR  - https://osf.io/xcwdn_v1
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:36:09</p>
KW  - AI
KW  - Belief
KW  - Conspiracy
KW  - Intervention
KW  - LLM
KW  - Persuasion
ER  - 

TY  - JOUR
TI  - Effects of Human vs. Computer-Controlled Characters and Social 					Identity Cues on Enjoyment
AU  - Peña, Jorge
AU  - Ghaznavi, Jannath
AU  - Brody, Nicholas
AU  - Prada, Rui
AU  - Martinho, Carlos
AU  - Santos, Pedro A.
AU  - Damas, Hugo
AU  - Dimas, Joana
T2  - Journal of Media Psychology
AB  - . This study explored how group identification, avatar 					similarity identification, and social presence mediated the effect of character 					type (avatars or agents) and social identity cues (presence or absence of 					avatars wearing participants’ school colors) on game enjoyment. Playing 					with teammate avatars increased enjoyment indirectly by enhancing group 					identification. In addition, the presence of social identity cues increased 					enjoyment indirectly by augmenting identification with one’s avatar. 					Unexpectedly, playing in multiplayer mode in the presence of social identity 					cues decreased enjoyment, whereas playing in multiplayer mode in the absence of 					social identity cues increased enjoyment. Social presence was not a reliable 					mediator. The findings supported media enjoyment and social identity theories, 					and highlighted how virtual character type and identification processes 					influence enjoyment.
DA  - 2019///
PY  - 2019
DO  - 10.1027/1864-1105/a000218
VL  - 31
IS  - 1
SP  - 35
EP  - 47
J2  - Journal of Media Psychology
SN  - 1864-1105
UR  - https://econtent.hogrefe.com/doi/10.1027/1864-1105/a000218
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:36:12</p>
N1  - <p>Publisher: Hogrefe Publishing</p>
KW  - avatars and agents
KW  - enjoyment
KW  - identification
KW  - social identity
KW  - social presence
ER  - 

TY  - JOUR
TI  - Effects of Using Artificial Intelligence on Interpersonal Perceptions of Job Applicants
AU  - Weiss, Daphne
AU  - Liu, Sunny X.
AU  - Mieczkowski, Hannah
AU  - Hancock, Jeffrey T.
T2  - Cyberpsychology, Behavior, and Social Networking
AB  - Text-based artificial intelligence (AI) systems are increasingly integrated into a host of interpersonal domains. Although decision-making and person perception in hiring and employment opportunities have been an area of psychological interest for many years, only recently have scholars begun to investigate the role that AI plays in this context. To better understand the impact of AI in employment-related contexts, we conducted two experiments investigating how the use of AI by applicants influences their job opportunities. In our preregistered Study 1, we examined how a prospective job applicants' use of AI, as well as their language status (native English speaker or non-native English speaker), influenced participants' impressions of their warmth, competence, social attractiveness, and hiring desirability. In Study 2, we examined how receiving assistance impacted interpersonal perceptions, and how perceptions might change whether the help was provided by AI or by another human. The results from both experiments suggest that the use of AI technologies can negatively influence perceptions of jobseekers. This negative impact may be grounded in the perception of receiving any type of help, whether it be from a machine or a person. These studies provide additional evidence for the Computers as Social Actors framework and advance our understanding of AI-Mediated Communication. The results also raise questions about transparency and deception related to AI use in interpersonal contexts.
DA  - 2022///
PY  - 2022
DO  - 10.1089/cyber.2020.0863
VL  - 25
IS  - 3
SP  - 163
EP  - 168
J2  - Cyberpsychology, Behavior, and Social Networking
SN  - 2152-2715
UR  - https://www.liebertpub.com/doi/10.1089/cyber.2020.0863
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:36:16</p>
N1  - <p>Publisher: Mary Ann Liebert, Inc., publishers</p>
ER  - 

TY  - JOUR
TI  - Eliciting Latent Knowledge from Quirky Language Models
AU  - Mallen, Alex
AU  - Brumley, Madeline
AU  - Kharchenko, Julia
AU  - Belrose, Nora
AB  - Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations that robustly track the true state of the world, especially in hard-to-verify cases where the model's output is untrusted. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models (LMs) that are finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We find that, especially in middle layers, linear probes usually report an LM's knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model's untruthful output. The best probing method (logistic regression on contrast pairs) recovers 89% of the gap in AUROC between truthful and untruthful contexts, and 75% for questions harder than those used to train the probe. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arXiv.2312.01037
UR  - http://arxiv.org/abs/2312.01037
Y2  - 2025/08/28/
N1  - <p>arXiv:2312.01037 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:36:56</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Eliciting latent knowledge: how to tell if your eyes deceive you
AU  - Christiano, P.
AU  - Cotra, A.
AU  - Xu, M.
AB  - Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network’s activations that robustly track the true state of the world, even in cases where the model’s output is untrusted and hard to verify. To further ELK research, we introduce 12 datasets and a corresponding suite of “quirky” language models (LMs) that are finetuned to make systematic errors when answering questions if and only if the keyword “Bob” is present in the prompt. We find that, especially in middle layers, linear probes usually report an LM’s knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model’s untruthful output. The best probing method (logistic regression on contrast pairs) recovers 89% of the gap in AUROC between truthful and untruthful contexts, and 75% for questions harder than those used to train the probe. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods.
DA  - 2021///
PY  - 2021
UR  - https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.jrzi4atzacns
N1  - <p>Date Added: 2025-08-28 12:02:44</p>
ER  - 

TY  - JOUR
TI  - Emotional Support from AI Chatbots: Should a Supportive Partner Self-Disclose or Not?
AU  - Meng, Jingbo
AU  - Dai, Yue (Nancy)
T2  - Journal of Computer-Mediated Communication
AB  - This study examined how and when a chatbot’s emotional support was effective in reducing people’s stress and worry. It compared emotional support from chatbot versus human partners in terms of its process and conditional effects on stress/worry reduction. In an online experiment, participants discussed a personal stressor with a chatbot or a human partner who provided none, or either one or both of emotional support and reciprocal self-disclosure. The results showed that emotional support from a conversational partner was mediated through perceived supportiveness of the partner to reduce stress and worry among participants, and the link from emotional support to perceived supportiveness was stronger for a human than for a chatbot. A conversational partner’s reciprocal self-disclosure enhanced the positive effect of emotional support on worry reduction. However, when emotional support was absent, a solely self-disclosing chatbot reduced even less stress than a chatbot not providing any response to participants’ stress.In recent years, AI chatbots have increasingly been used to provide empathy and support to people who are experiencing stressful times. This study compared emotional support from a chatbot compared to that of a human who provided support. We were interested in examining which approach could best effectively reduce people’s worry and stress. When either a person or a chatbot was able to engage with a stressed individual and tell that individual about their own experiences, they were able to build rapport. We found that this type of reciprocal self-disclosure was effective in calming the worry of the individual. Interestingly, if a chatbot only reciprocally self-disclosed but offered no emotional support, the outcome was worse than if the chatbot did not respond to people at all. This work will help in the development of supportive chatbots by providing insights into when and what they should self-disclose.
DA  - 2021///
PY  - 2021
DO  - 10.1093/jcmc/zmab005
VL  - 26
IS  - 4
SP  - 207
EP  - 222
J2  - Journal of Computer-Mediated Communication
SN  - 1083-6101
ST  - Emotional Support from AI Chatbots
UR  - https://doi.org/10.1093/jcmc/zmab005
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:36:39</p>
ER  - 

TY  - CONF
TI  - Empathetic Persuasion: Reinforcing Empathy and Persuasiveness in Dialogue Systems
AU  - Samad, Azlaan Mustafa
AU  - Mishra, Kshitij
AU  - Firdaus, Mauajama
AU  - Ekbal, Asif
T2  - Findings of the Association for Computational Linguistics: NAACL 2022
A2  - Carpuat, Marine
A2  - de Marneffe, Marie-Catherine
A2  - Meza Ruiz, Ivan Vladimir
AB  - Persuasion is an intricate process involving empathetic connection between two individuals. Plain persuasive responses may make a conversation non-engaging. Even the most well-intended and reasoned persuasive conversations can fall through in the absence of empathetic connection between the speaker and listener. In this paper, we propose a novel task of incorporating empathy when generating persuasive responses. We develop an empathetic persuasive dialogue system by fine-tuning a maximum likelihood Estimation (MLE)-based language model in a reinforcement learning (RL) framework. To design feedbacks for our RL-agent, we define an effective and efficient reward function considering consistency, repetitiveness, emotion and persuasion rewards to ensure consistency, non-repetitiveness, empathy and persuasiveness in the generated responses. Due to lack of emotion annotated persuasive data, we first annotate the existing Persuaion For Good dataset with emotions, then build transformer based classifiers to provide emotion based feedbacks to our RL agent. Experimental results confirm that our proposed model increases the rate of generating persuasive responses as compared to the available state-of-the-art dialogue models while making the dialogues empathetically more engaging and retaining the language quality in responses.
C1  - Seattle, United States
C3  - Findings of the Association for Computational Linguistics: NAACL 2022
DA  - 2022///
PY  - 2022
DO  - 10.18653/v1/2022.findings-naacl.63
SP  - 844
EP  - 856
PB  - Association for Computational Linguistics
ST  - Empathetic Persuasion
UR  - https://aclanthology.org/2022.findings-naacl.63/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:36:42</p>
ER  - 

TY  - JOUR
TI  - Employment relationships in algorithmic management: A psychological contract perspective
AU  - Tomprou, Maria
AU  - Lee, Min Kyung
T2  - Computers in Human Behavior
AB  - Algorithms increasingly automate or support managerial functions in organizations, with implications for the employee-employer relationship. We explored how algorithmic management affects this relationship with a focus on psychological contracts, or employees' perceptions of their own and their employers' obligations. Through five online experiments, we investigated how organizational agent type—algorithmic versus human—influenced one's psychological contract depending on the organizational inducement type—transactional versus relational. We explored psychological contracts in two stages of employment: during early phases, such as recruiting (Studies 1 and 2) and onboarding (Studies 4 and 5), when the agent explains the inducements to the employee; and during employment, when the agent under-delivers the inducements to varying degrees (Studies 3–5). Our results suggest that agent type did not affect psychological contracts around transactional inducements but did so for relational inducements in the cases of recruiting and low inducement delivery (Studies 1–5). Algorithmic agents signaled reduced employer commitments to relational inducements during recruiting (Study 1). Using human agents resulted in greater perceived breach when delivery of relational inducements was low (Study 5). Regardless of inducement type, turnover intentions were higher when the human agent under-delivered compared to the algorithmic agent (Study 5). Our studies show how algorithmic management may influence one's psychological contract.
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.chb.2021.106997
VL  - 126
SP  - 106997
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - Employment relationships in algorithmic management
UR  - https://www.sciencedirect.com/science/article/pii/S0747563221003204
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:41:27</p>
KW  - Algorithmic management
KW  - Human-computer interaction
KW  - Perceived breach
KW  - Psychological contracts
ER  - 

TY  - JOUR
TI  - Expl(AI)ned: The Impact of Explainable Artificial Intelligence on Users’ Information Processing
AU  - Bauer, Kevin
AU  - von Zahn, Moritz
AU  - Hinz, Oliver
T2  - Information Systems Research
AB  - Because of a growing number of initiatives and regulations, predictions of modern artificial intelligence (AI) systems increasingly come with explanations about why they behave the way they do. In this paper, we explore the impact of feature-based explanations on users’ information processing. We designed two complementary empirical studies where participants either made incentivized decisions on their own, with the aid of opaque predictions, or with explained predictions. In Study 1, laypeople engaged in the deliberately abstract investment game task. In Study 2, experts from the real estate industry estimated listing prices for real German apartments. Our results indicate that the provision of feature-based explanations paves the way for AI systems to reshape users’ sense making of information and understanding of the world around them. Specifically, explanations change users’ situational weighting of available information and evoke mental model adjustments. Crucially, mental model adjustments are subject to the confirmation bias so that misconceptions can persist and even accumulate, possibly leading to suboptimal or biased decisions. Additionally, mental model adjustments create spillover effects that alter user behavior in related yet disparate domains. Overall, this paper provides important insights into potential downstream consequences of the broad employment of modern explainable AI methods. In particular, side effects of mental model adjustments present a potential risk of manipulating user behavior, promoting discriminatory inclinations, and increasing noise in decision making. Our findings may inform the refinement of current efforts of companies building AI systems and regulators that aim to mitigate problems associated with the black-box nature of many modern AI systems. History: Alessandro Acquisti, senior editor; Jason Chan, associate editor. Funding: This work was supported by the Deutsche Forschungsgemeinschaft (DFG) (Projek 449023539), Volkswagen Foundation (ML2MT), and LeibnizInstitute for Financial Research SAFE. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2023.1199 .
DA  - 2023///
PY  - 2023
DO  - 10.1287/isre.2023.1199
VL  - 34
IS  - 4
SP  - 1582
EP  - 1602
J2  - Information Systems Research
SN  - 1047-7047
ST  - Expl(AI)ned
UR  - https://pubsonline.informs.org/doi/10.1287/isre.2023.1199
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:41:46</p>
N1  - <p>Publisher: INFORMS</p>
KW  - explainable artificial intelligence
KW  - information processing
KW  - mental models
KW  - user behavior
ER  - 

TY  - THES
TI  - Exploratory study on trust, distrust, and credibility in machine journalism
AU  - Song, S. W.
AB  - The current study investigated the effect of machine-generated journalism. Specifically, the effect of machine journalism compared to human journalist on the perceptions of credibility and distrust for news articles on controversial topics was explored. To further extend the well- established theories of credibility in journalism, this study introduced the concept of distrust as a construct that is distinct from credibility or trust. The relationship between trust and hostile media effect was explored. Finally, this study investigated if trust and hostile media effect are related to the perception of fake news. The results show that distrust was indeed distinct from credibility or trust in journalism context not only at a measurement level but also in terms of its effects on other constructs; trust and credibility lacked discriminant validity, suggesting the two are measuring similar psychological constructs; machines were perceived to be less trustworthy compared to human journalists; and strong relationships between trust/distrust/credibility and fake news were observed.
CY  - Syracuse University
DA  - 2019///
PY  - 2019
N1  - <p>Date Added: 2025-08-28 12:02:43</p>
ER  - 

TY  - JOUR
TI  - Exposure to robot preachers undermines religious commitment
AU  - Jackson, Joshua Conrad
AU  - Yam, Kai Chi
AU  - Tang, Pok Man
AU  - Liu, Ting
AU  - Shariff, Azim
T2  - Journal of Experimental Psychology: General
AB  - Over the last decade, robots continue to infiltrate the workforce, permeating occupations that once seemed immune to automation. This process seems to be inevitable because robots have ever-expanding capabilities. However, drawing from theories of cultural evolution and social learning, we propose that robots may have limited influence in domains that require high degrees of “credibility”; here we focus on the automation of religious preachers as one such domain. Using a natural experiment in a recently automated Buddhist temple (Study 1) and a fully randomized experiment in a Taoist temple (Study 2), we consistently show that religious adherents perceive robot preachers—and the institutions which employ them—as less credible than human preachers. This lack of credibility explains reductions in religious commitment after people listen to robot (vs. human) preachers deliver sermons. Study 3 conceptually replicates this finding in an online experiment and suggests that religious elites require perceived minds (agency and patiency) to be credible, which is partly why robot preachers inspire less credibility than humans. Our studies support cultural evolutionary theories of religion and suggest that escalating religious automation may induce religious decline. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
DA  - 2023///
PY  - 2023
DO  - 10.1037/xge0001443
VL  - 152
IS  - 12
SP  - 3344
EP  - 3358
J2  - Journal of Experimental Psychology: General
SN  - 1939-2222
N1  - <p>Date Added: 2025-08-28 12:41:43</p>
N1  - <p>Place: US Publisher: American Psychological Association</p>
KW  - Artificial Intelligence
KW  - Automation
KW  - Exposure
KW  - Human Robot Interaction
KW  - Religion
KW  - Robotics
ER  - 

TY  - BOOK
TI  - Facing the Artificial: Understanding Affinity, Trustworthiness, and Preference for More Realistic Digital Humans
AU  - Seymour, Mike
AU  - Yuan, Lingyao
AU  - Dennis, Alan
AU  - Riemer, Kai
AB  - In recent years, companies have been developing more realistic looking human faces for digital, virtual agents controlled by artificial intelligence (AI). But how do users feel about interacting with such virtual agents? We used a controlled lab experiment to examine users’ perceived trustworthiness, affinity, and preference towards a real human travel agent appearing via video (i.e., Skype) as well as in the form of a very human-realistic avatar; half of the participants were (deceptively) told the avatar was a virtual agent controlled by AI while the other half were told the avatar was controlled by the same human travel agent. Results show that participants rated the video human agent more trustworthy, had more affinity for him, and preferred him to both avatar versions. Users who believed the avatar was a virtual agent controlled by AI reported the same level of affinity, trustworthiness, and preferences towards the agent as those who believed it was controlled by a human. Thus, use of a realistic digital avatar lowered affinity, trustworthiness, and preferences, but how the avatar was controlled (by human or machine) had no effect. The conclusion is that improved visual fidelity alone makes a significant positive difference and that users are not averse to advanced AI simulating human presence, some may even be anticipating such an advanced technology.
DA  - 2020///
PY  - 2020
LA  - eng
SN  - 978-0-9981331-3-3
ST  - Facing the Artificial
UR  - http://hdl.handle.net/10125/64316
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:43:47</p>
N1  - <p>DOI: 10.24251/HICSS.2020.574</p>
ER  - 

TY  - JOUR
TI  - Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases
AU  - Prabhumoye, Shrimai
AU  - Kocielnik, Rafal
AU  - Shoeybi, Mohammad
AU  - Anandkumar, Anima
AU  - Catanzaro, Bryan
AB  - Detecting social bias in text is challenging due to nuance, subjectivity, and difficulty in obtaining good quality labeled datasets at scale, especially given the evolving nature of social biases and society. To address these challenges, we propose a few-shot instruction-based method for prompting pre-trained language models (LMs). We select a few class-balanced exemplars from a small support repository that are closest to the query to be labeled in the embedding space. We then provide the LM with instruction that consists of this subset of labeled exemplars, the query text to be classified, a definition of bias, and prompt it to make a decision. We demonstrate that large LMs used in a few-shot context can detect different types of fine-grained biases with similar and sometimes superior accuracy to fine-tuned models. We observe that the largest 530B parameter model is significantly more effective in detecting social bias compared to smaller models (achieving at least 13% improvement in AUC metric compared to other models). It also maintains a high AUC (dropping less than 2%) when the labeled repository is reduced to as few as $100$ samples. Large pretrained language models thus make it easier and quicker to build new bias detectors.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2112.07868
UR  - http://arxiv.org/abs/2112.07868
Y2  - 2025/08/28/
N1  - <p>arXiv:2112.07868 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:41:49</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Financial Fraud of Older Adults During the Early Months of the COVID-19 Pandemic
AU  - Teaster, Pamela B
AU  - Roberto, Karen A
AU  - Savla, Jyoti
AU  - Du, Chenguang
AU  - Du, Zhiyuan
AU  - Atkinson, Emily
AU  - Shealy, E Carlisle
AU  - Beach, Scott
AU  - Charness, Neil
AU  - Lichtenberg, Peter A
T2  - The Gerontologist
AB  - Coronavirus disease 2019 (COVID-19) created a “perfect storm” for financial fraud targeting older adults. Guided by the Contextual Theory of Elder Abuse, we focused on individual and systemic contexts to examine how older adults became prey to financial fraud.In July 2020, 998 adults who were 60–98 years of age (93% White; 64% female) completed an online survey about experiences with financial fraud. Participants were recruited from gerontology research registries at Florida State University, University of Pittsburg, Virginia Tech, and Wayne State University.Over half (65.9%) of the respondents experienced a COVID-19-related scam attempt, with charity contributions (49%) and COVID-19 treatments (42%) being the most common. Perpetrators commonly contacted older adults electronically (47%) two or more times (64%). Although most respondents ignored the request (i.e., hung up the phone and deleted text/e-mail), 11.3% sent a requested payment, and 5.3% provided personal information. Predictors of vulnerability included contentment with financial situation, concern about finances in the aftermath of the pandemic, and wishing to talk to someone about financial decisions. Respondents targeted for a non-COVID-19 scam attempt were less likely to be targets of a COVID-19-related scam.Older adults who were financially secure, worried about their financial situation, or wished they could speak with someone about their financial decisions appeared susceptible to falling victim to a fraud attempt. The high number of attempts indicates a need for a measurable and concerted effort to prevent the financial fraud of older adults.
DA  - 2023///
PY  - 2023
DO  - 10.1093/geront/gnac188
VL  - 63
IS  - 6
SP  - 984
EP  - 992
J2  - The Gerontologist
SN  - 1758-5341
UR  - https://doi.org/10.1093/geront/gnac188
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:41:51</p>
ER  - 

TY  - JOUR
TI  - Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations
AU  - Goldstein, Josh A.
AU  - Sastry, Girish
AU  - Musser, Micah
AU  - DiResta, Renee
AU  - Gentzel, Matthew
AU  - Sedova, Katerina
AB  - Generative language models have improved drastically, and can now produce realistic text outputs that are difficult to distinguish from human-written content. For malicious actors, these language models bring the promise of automating the creation of convincing and misleading text for use in influence operations. This report assesses how language models might change influence operations in the future, and what steps can be taken to mitigate this threat. We lay out possible changes to the actors, behaviors, and content of online influence operations, and provide a framework for stages of the language model-to-influence operations pipeline that mitigations could target (model construction, model access, content dissemination, and belief formation). While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2301.04246
ST  - Generative Language Models and Automated Influence Operations
UR  - http://arxiv.org/abs/2301.04246
Y2  - 2025/08/28/
N1  - <p>arXiv:2301.04246 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:49:08</p>
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - Getting to Know Each Other: The Role of Social Dialogue in Recovery from Errors in Social Robots
AU  - Lucas, Gale M.
AU  - Boberg, Jill
AU  - Traum, David
AU  - Artstein, Ron
AU  - Gratch, Jonathan
AU  - Gainer, Alesia
AU  - Johnson, Emmanuel
AU  - Leuski, Anton
AU  - Nakano, Mikio
T2  - Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction
T3  - HRI '18
AB  - This work explores the extent to which social dialogue can mitigate (or exacerbate) the loss of trust caused when robots make conversational errors. Our study uses a NAO robot programmed to persuade users to agree with its rankings on two tasks. We perform two manipulations: (1) The timing of conversational errors - the robot exhibited errors either in the first task, the second task, or neither; (2) The presence of social dialogue - between the two tasks, users either engaged in a social dialogue with the robot or completed a control task. We found that the timing of the errors matters: replicating previous research, conversational errors reduce the robot's influence in the second task, but not on the first task. Social dialogue interacts with the timing of errors, acting as an intensifier: social dialogue helps the robot recover from prior errors, and actually boosts subsequent influence; but social dialogue backfires if it is followed by errors, because it extends the period of good performance, creating a stronger contrast effect with the subsequent errors. The design of social robots should therefore be more careful to avoid errors after periods of good performance than early on in a dialogue.
C1  - New York, NY, USA
C3  - Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2018///
PY  - 2018
DO  - 10.1145/3171221.3171258
SP  - 344
EP  - 351
PB  - Association for Computing Machinery
SN  - 978-1-4503-4953-6
ST  - Getting to Know Each Other
UR  - https://doi.org/10.1145/3171221.3171258
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:49:14</p>
ER  - 

TY  - JOUR
TI  - Help! Is my chatbot falling into the uncanny valley? An empirical study of user experience in human-chatbot interaction
AU  - Skjuve, Marita
AU  - Haugstveit, Ida Maria
AU  - Følstad, Asbjørn
AU  - Brandtzaeg, Petter Bae
T2  - Human Technology
AB  - Advances in artificial intelligence strengthen chatbots’ ability to resemble human conversational agents. For some application areas, it may be tempting not to be transparent regarding a conversational agent’s nature as chatbot or human. However, the uncanny valley theory suggests that such lack in transparency may cause uneasy feelings in the user. In this study, we combined quantitative and qualitative methods to investigate this issue. First, we used a 2 x 2 experimental research design (n = 28) to investigate effects of lack in transparency on the perceived pleasantness of the conversation in addition to perceived human likeness and affinity for the conversational agent. Second, we conducted an exploratory analysis of qualitative participant reports on these conversations. We did not find that a lack in transparency negatively affected user experience, but we identified three factors important to participants’ assessments. The findings are of theoretical and practical significance and motivate future research. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
DA  - 2019///
PY  - 2019
DO  - 10.17011/ht/urn.201902201607
VL  - 15
IS  - 1
SP  - 30
EP  - 54
J2  - Human Technology
SN  - 1795-6889
ST  - Help! Is my chatbot falling into the uncanny valley?
N1  - <p>Date Added: 2025-08-28 12:50:56</p>
N1  - <p>Place: Finland Publisher: Agora Center</p>
KW  - Chatbots
KW  - Human Computer Interaction
KW  - Intelligent Agents
ER  - 

TY  - JOUR
TI  - Home-Tutoring Services Assisted with Technology: Investigating the Role of Artificial Intelligence Using a Randomized Field Experiment
AU  - Kim, Jun Hyung
AU  - Kim, Minki
AU  - Kwak, Do Won
AU  - Lee, Sol
T2  - Journal of Marketing Research
AB  - Despite a rising interest in artificial intelligence (AI) technology, research in services marketing has not evaluated its role in helping firms learn about customers’ needs and increasing the adaptability of service employees. Therefore, the authors develop a conceptual framework and investigate whether and to what extent providing AI assistance to service employees improves service outcomes. The randomized controlled trial in the context of tutoring services shows that helping service employees (tutors) adapt to students’ learning needs by providing AI-generated diagnoses significantly improves service outcomes measured by academic performance. However, the authors find that some tutors may not utilize AI assistance (i.e., AI aversion), and factors associated with unforeseen barriers to usage (i.e., technology overload) can moderate its impact on outcomes. Interestingly, tutors who significantly contribute to the firm's revenue relied heavily on AI assistance but unexpectedly benefited little from AI in improving service outcomes. Given the wide applicability of AI assistance in a variety of services marketing contexts, the authors suggest that firms should consider the potential difficulties employees face in using the technology rather than encourage them to use it as it is.
DA  - 2022///
PY  - 2022
DO  - 10.1177/00222437211050351
VL  - 59
IS  - 1
SP  - 79
EP  - 96
J2  - Journal of Marketing Research
LA  - EN
SN  - 0022-2437
ST  - Home-Tutoring Services Assisted with Technology
UR  - https://doi.org/10.1177/00222437211050351
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:49:24</p>
N1  - <p>Publisher: SAGE Publications Inc</p>
ER  - 

TY  - JOUR
TI  - How deep is AI's love? Understanding relational AI
AU  - Gillath, Omri
AU  - Abumusab, Syed
AU  - Ai, Ting
AU  - Branicky, Michael S.
AU  - Davison, Robert B.
AU  - Rulo, Maxwell
AU  - Symons, John
AU  - Thomas, Gregory
T2  - Behavioral and Brain Sciences
AB  - We suggest that as people move to construe robots as social agents, interact with them, and treat them as capable of social ties, they might develop (close) relationships with them. We then ask what kind of relationships can people form with bots, what functions can bots fulfill, and what are the societal and moral implications of such relationships.
DA  - 2023///
PY  - 2023
DO  - 10.1017/S0140525X22001704
VL  - 46
SP  - e33
J2  - Behavioral and Brain Sciences
LA  - en
SN  - 0140-525X, 1469-1825
ST  - How deep is AI's love?
UR  - https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/how-deep-is-ais-love-understanding-relational-ai/77364078496FCE70F71C7A9F293AC322
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:49:26</p>
ER  - 

TY  - CONF
TI  - How Good are LLMs in Generating Personalized Advertisements?
AU  - Meguellati, Elyas
AU  - Han, Lei
AU  - Bernstein, Abraham
AU  - Sadiq, Shazia
AU  - Demartini, Gianluca
T2  - Companion Proceedings of the ACM Web Conference 2024
T3  - WWW '24
AB  - In this paper, we explore the potential of large language models (LLMs) in generating personalized online advertisements (ads) tailored to specific personality traits, focusing on openness and neuroticism. We conducted a user study involving two tasks to understand the performance of LLM-generated ads compared to human-written ads in different online environments. Task 1 simulates a social media environment where users encounter ads while scrolling through their feed. Task 2 mimics a shopping website environment where users are presented with multiple sponsored products side-by-side. Our results indicate that LLM-generated ads targeting the openness trait positively impact user engagement and preferences, with performance comparable to human-written ads. Furthermore, in both scenarios, the overall effectiveness of LLM-generated ads was found to be similar to that of human-written ads, highlighting the potential of LLM-generated personalised content to rival traditional advertising methods with the added advantage of scalability. This study underscores the need for cautious consideration in the deployment of LLM-generated content at scale. While our findings confirm the scalability and potential effectiveness of LLM-generated content, there is an equally pressing concern about the ease with which it can be misused.
C1  - New York, NY, USA
C3  - Companion Proceedings of the ACM Web Conference 2024
DA  - 2024///
PY  - 2024
DO  - 10.1145/3589335.3651520
SP  - 826
EP  - 829
PB  - Association for Computing Machinery
SN  - 979-8-4007-0172-6
UR  - https://doi.org/10.1145/3589335.3651520
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:49:28</p>
ER  - 

TY  - JOUR
TI  - How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions
AU  - Pacchiardi, Lorenzo
AU  - Chan, Alex J.
AU  - Mindermann, Sören
AU  - Moscovitz, Ilan
AU  - Pan, Alexa Y.
AU  - Gal, Yarin
AU  - Evans, Owain
AU  - Brauner, Jan
AB  - Large language models (LLMs) can "lie", which we define as outputting false statements despite "knowing" the truth in a demonstrable sense. LLMs might "lie", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -- prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2309.15840
ST  - How to Catch an AI Liar
UR  - http://arxiv.org/abs/2309.15840
Y2  - 2025/08/28/
N1  - <p>arXiv:2309.15840 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:49:30</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Human and Human-Interfaced AI Interactions: Modulation of Human Male Autonomic Nervous System via Pupil Mimicry
AU  - Spicer, Catherine
AU  - Khwaounjoo, Prashanna
AU  - Cakmak, Yusuf Ozgur
T2  - Sensors
AB  - Pupillary alterations in virtual humans induce neurophysiological responses within an observer. Technological advances have enabled rapid developments in artificial intelligence (AI), from verbal systems, to visual AI interfaces with the ability to express, and respond to emotional states of a user. Visual AI interfaces are able to change their physical parameters, such as pupil diameter. Pupillary changes can alter heart rate, however, effects on heart rate variability (HRV) are unknown. HRV, is an autonomic, non-conscious parameter which monitors sympathetic and parasympathetic nervous system (PNS) activity. N = 34 male participants aged between 19–33 were subjected to a number of conditions such as pupil dilation, constriction and blushing. The present research is the first to investigate the effects of virtual human interactions on human HRV. Outcomes of this study were obtained using eye tracking and HRV measurements. Pupil dilation relative to constriction presented in the female virtual partner induced a significant right pupillary diameter increase (p = 0.041) in human observers. Additionally, female virtual partner pupil constriction relative to dilation induced a significant increase in participants’ PNS HRV response (p = 0.036). These findings indicate the ability of a female virtual interaction partner to modulate parasympathetic autonomic functioning in young healthy male humans. This allows first insights into the effects of interacting with virtual AI interaction partners, on human autonomic functioning, and may aid development of future virtual humans, and their implementation into relevant clinical settings.
DA  - 2021///
PY  - 2021
DO  - 10.3390/s21041028
VL  - 21
IS  - 4
SP  - 1028
J2  - Sensors
LA  - en
SN  - 1424-8220
ST  - Human and Human-Interfaced AI Interactions
UR  - https://www.mdpi.com/1424-8220/21/4/1028
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:49:32</p>
N1  - <p>Publisher: Multidisciplinary Digital Publishing Institute</p>
N1  - <p>Rights: http://creativecommons.org/licenses/by/3.0/</p>
KW  - AI
KW  - autonomic
KW  - blush
KW  - gender effects
KW  - HRV
KW  - mimicry
KW  - parasympathetic
KW  - pupil constriction
KW  - pupil dilation
KW  - sympathetic
KW  - virtual interaction partner
ER  - 

TY  - JOUR
TI  - Human and Machine: The Impact of Machine Input on Decision Making Under Cognitive Limitations
AU  - Boyacı, Tamer
AU  - Canyakmaz, Caner
AU  - de Véricourt, Francis
T2  - Management Science
AB  - The rapid adoption of artificial intelligence (AI) technologies by many organizations has recently raised concerns that AI may eventually replace humans in certain tasks. In fact, when used in collaboration, machines can significantly enhance the complementary strengths of humans. Indeed, because of their immense computing power, machines can perform specific tasks with incredible accuracy. In contrast, human decision makers (DMs) are flexible and adaptive but constrained by their limited cognitive capacity. This paper investigates how machine-based predictions may affect the decision process and outcomes of a human DM. We study the impact of these predictions on decision accuracy, the propensity and nature of decision errors, and the DM’s cognitive efforts. To account for both flexibility and limited cognitive capacity, we model the human decision-making process in a rational inattention framework. In this setup, the machine provides the DM with accurate but sometimes incomplete information at no cognitive cost. We fully characterize the impact of machine input on the human decision process in this framework. We show that machine input always improves the overall accuracy of human decisions but may nonetheless increase the propensity of certain types of errors (such as false positives). The machine can also induce the human to exert more cognitive efforts, although its input is highly accurate. Interestingly, this happens when the DM is most cognitively constrained, for instance, because of time pressure or multitasking. Synthesizing these results, we pinpoint the decision environments in which human-machine collaboration is likely to be most beneficial. This paper was accepted by Jeannette Song, operations management. Supplemental Material: The data files and online appendices are available at https://doi.org/10.1287/mnsc.2023.4744 .
DA  - 2024///
PY  - 2024
DO  - 10.1287/mnsc.2023.4744
VL  - 70
IS  - 2
SP  - 1258
EP  - 1275
J2  - Management Science
SN  - 0025-1909
ST  - Human and Machine
UR  - https://pubsonline.informs.org/doi/10.1287/mnsc.2023.4744
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:52:54</p>
N1  - <p>Publisher: INFORMS</p>
KW  - cognitive effort
KW  - human-machine collaboration
KW  - machine learning
KW  - rational inattention
ER  - 

TY  - JOUR
TI  - Human-level play in the game of Diplomacy by combining language models with strategic reasoning
AU  - Meta Fundamental AI Research Diplomacy Team (FAIR)
AU  - Bakhtin, Anton
AU  - Brown, Noam
AU  - Dinan, Emily
AU  - Farina, Gabriele
AU  - Flaherty, Colin
AU  - Fried, Daniel
AU  - Goff, Andrew
AU  - Gray, Jonathan
AU  - Hu, Hengyuan
AU  - Jacob, Athul Paul
AU  - Komeili, Mojtaba
AU  - Konath, Karthik
AU  - Kwon, Minae
AU  - Lerer, Adam
AU  - Lewis, Mike
AU  - Miller, Alexander H.
AU  - Mitts, Sasha
AU  - Renduchintala, Adithya
AU  - Roller, Stephen
AU  - Rowe, Dirk
AU  - Shi, Weiyan
AU  - Spisak, Joe
AU  - Wei, Alexander
AU  - Wu, David
AU  - Zhang, Hugh
AU  - Zijlstra, Markus
T2  - Science
AB  - Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.
DA  - 2022///
PY  - 2022
DO  - 10.1126/science.ade9097
VL  - 378
IS  - 6624
SP  - 1067
EP  - 1074
J2  - Science
UR  - https://www.science.org/doi/abs/10.1126/science.ade9097
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:52:57</p>
N1  - <p>Publisher: American Association for the Advancement of Science</p>
ER  - 

TY  - JOUR
TI  - Humanizing chatbots: The effects of visual, identity and conversational cues on humanness perceptions
AU  - Go, Eun
AU  - Sundar, S. Shyam
T2  - Computers in Human Behavior
AB  - Chatbots are replacing human agents in a number of domains, from online tutoring to customer-service to even cognitive therapy. But, they are often machine-like in their interactions. What can we do to humanize chatbots? Should they necessarily be driven by human operators for them to be considered human? Or, will an anthropomorphic visual cue on the interface and/or a high-level of contingent message exchanges provide humanness to automated chatbots? We explored these questions with a 2 (anthropomorphic visual cues: high vs. low anthropomorphism) × 2 (message interactivity: high vs. low message interactivity) × 2 (identity cue: chat-bot vs. human) between-subjects experiment (N = 141) in which participants interacted with a chat agent on an e-commerce site about choosing a digital camera to purchase. Our findings show that a high level of message interactivity compensates for the impersonal nature of a chatbot that is low on anthropomorphic visual cues. Moreover, identifying the agent as human raises user expectations for interactivity. Theoretical as well as practical implications of these findings are discussed.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.chb.2019.01.020
VL  - 97
SP  - 304
EP  - 316
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - Humanizing chatbots
UR  - https://www.sciencedirect.com/science/article/pii/S0747563219300329
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:53:12</p>
KW  - Anthropomorphic visual cue
KW  - Compensation effect
KW  - Expectancy violation effect
KW  - Identity cue
KW  - Message interactivity
KW  - Online chat agents
ER  - 

TY  - JOUR
TI  - Identifying relevant segments of AI applications adopters – Expanding the UTAUT2’s variables
AU  - Cabrera-Sánchez, Juan-Pedro
AU  - Villarejo-Ramos, Ángel F.
AU  - Liébana-Cabanillas, Francisco
AU  - Shaikh, Aijaz A.
T2  - Telematics and Informatics
AB  - Artificial intelligence (AI) is a future-defining technology, and AI applications are becoming mainstream in the developed world. Many consumers are adopting and using AI-based apps, devices, and services in their everyday lives. However, research examining consumer behavior in using AI apps is scant. We examine critical factors in AI app adoption by extending and validating a well-established unified theory of adoption and use of technology, UTAUT2. We also explore the possibility of unobserved heterogeneity in consumers’ behavior, including potentially relevant segments of AI app adopters. To augment the knowledge of end users’ engagement and relevant segments, we have added two new antecedent variables into UTAUT2: technology fear and consumer trust. Prediction-orientated segmentation was used on 740 valid responses collected using a pre-tested survey instrument. The results show five segments with different behaviors that were influenced by the variables of the proposed model. Once known, the profiles were used to propose apps to AI developers to improve consumer engagement. The moderating effects of the added variables—technology fear and consumer trust—are also shown. Finally, we discuss the theoretical and managerial implications of our findings and propose priorities for future research.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.tele.2020.101529
VL  - 58
SP  - 101529
J2  - Telematics and Informatics
SN  - 0736-5853
UR  - https://www.sciencedirect.com/science/article/pii/S073658532030188X
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:53:00</p>
KW  - Artificial intelligence
KW  - Consumer trust
KW  - Heterogeneity
KW  - Segmentation
KW  - Technology fear
KW  - UTAUT2
ER  - 

TY  - JOUR
TI  - Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition
AU  - Schulhoff, Sander
AU  - Pinto, Jeremy
AU  - Khan, Anaum
AU  - Bouchard, Louis-François
AU  - Si, Chenglei
AU  - Anati, Svetlina
AU  - Tagliabue, Valen
AU  - Kost, Anson Liu
AU  - Carnahan, Christopher
AU  - Boyd-Graber, Jordan
AB  - Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arXiv.2311.16119
ST  - Ignore This Title and HackAPrompt
UR  - http://arxiv.org/abs/2311.16119
Y2  - 2025/08/28/
N1  - <p>arXiv:2311.16119 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:53:21</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - JOUR
TI  - Improving alignment of dialogue agents via targeted human judgements
AU  - Glaese, Amelia
AU  - McAleese, Nat
AU  - Trębacz, Maja
AU  - Aslanides, John
AU  - Firoiu, Vlad
AU  - Ewalds, Timo
AU  - Rauh, Maribeth
AU  - Weidinger, Laura
AU  - Chadwick, Martin
AU  - Thacker, Phoebe
AU  - Campbell-Gillingham, Lucy
AU  - Uesato, Jonathan
AU  - Huang, Po-Sen
AU  - Comanescu, Ramona
AU  - Yang, Fan
AU  - See, Abigail
AU  - Dathathri, Sumanth
AU  - Greig, Rory
AU  - Chen, Charlie
AU  - Fritz, Doug
AU  - Elias, Jaume Sanchez
AU  - Green, Richard
AU  - Mokrá, Soňa
AU  - Fernando, Nicholas
AU  - Wu, Boxi
AU  - Foley, Rachel
AU  - Young, Susannah
AU  - Gabriel, Iason
AU  - Isaac, William
AU  - Mellor, John
AU  - Hassabis, Demis
AU  - Kavukcuoglu, Koray
AU  - Hendricks, Lisa Anne
AU  - Irving, Geoffrey
AB  - We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2209.14375
UR  - http://arxiv.org/abs/2209.14375
Y2  - 2025/08/28/
N1  - <p>arXiv:2209.14375 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:53:24</p>
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Improving Factuality and Reasoning in Language Models through Multiagent Debate
AU  - Du, Yilun
AU  - Li, Shuang
AU  - Torralba, Antonio
AU  - Tenenbaum, Joshua B.
AU  - Mordatch, Igor
T2  - Forty-first International Conference on Machine Learning
AB  - Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such "society of minds" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.
DA  - 2024///
PY  - 2024
LA  - en
UR  - https://openreview.net/forum?id=zj7YuTE4t8
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:53:27</p>
ER  - 

TY  - JOUR
TI  - Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning
AU  - Balashankar, Ananth
AU  - Ma, Xiao
AU  - Sinha, Aradhana
AU  - Beirami, Ahmad
AU  - Qin, Yao
AU  - Chen, Jilin
AU  - Beutel, Alex
AB  - As large language models (LLMs) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. If we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? In this paper, we study the novel setting of domain-generalized few-shot learning for LLM-based text safety classifiers. Unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. We demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting training data based on similar examples in prior existing rules. We empirically show that our approach of similarity-based data-augmentation + prompt-tuning (DAPT) consistently outperforms baselines that either do not rely on data augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule is loosely correlated with existing ones.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2310.16959
UR  - http://arxiv.org/abs/2310.16959
Y2  - 2025/08/28/
N1  - <p>arXiv:2310.16959 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:53:28</p>
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - In AI we trust? Perceptions about automated decision-making by artificial intelligence
AU  - Araujo, Theo
AU  - Helberger, Natali
AU  - Kruikemeier, Sanne
AU  - de Vreese, Claes H.
T2  - AI & SOCIETY
AB  - Fueled by ever-growing amounts of (digital) data and advances in artificial intelligence, decision-making in contemporary societies is increasingly delegated to automated processes. Drawing from social science theories and from the emerging body of research about algorithmic appreciation and algorithmic perceptions, the current study explores the extent to which personal characteristics can be linked to perceptions of automated decision-making by AI, and the boundary conditions of these perceptions, namely the extent to which such perceptions differ across media, (public) health, and judicial contexts. Data from a scenario-based survey experiment with a national sample (N = 958) show that people are by and large concerned about risks and have mixed opinions about fairness and usefulness of automated decision-making at a societal level, with general attitudes influenced by individual characteristics. Interestingly, decisions taken automatically by AI were often evaluated on par or even better than human experts for specific decisions. Theoretical and societal implications about these findings are discussed.
DA  - 2020///
PY  - 2020
DO  - 10.1007/s00146-019-00931-w
VL  - 35
IS  - 3
SP  - 611
EP  - 623
J2  - AI & SOCIETY
LA  - en
SN  - 1435-5655
ST  - In AI we trust?
UR  - https://doi.org/10.1007/s00146-019-00931-w
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:53:30</p>
KW  - Algorithmic appreciation
KW  - Algorithmic fairness
KW  - Artificial intelligence
KW  - Automated decision-making
KW  - User perceptions
ER  - 

TY  - JOUR
TI  - Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness
AU  - Pataranutaporn, Pat
AU  - Liu, Ruby
AU  - Finn, Ed
AU  - Maes, Pattie
T2  - Nature Machine Intelligence
AB  - As conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person’s mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI’s inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user’s mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced.
DA  - 2023///
PY  - 2023
DO  - 10.1038/s42256-023-00720-7
VL  - 5
IS  - 10
SP  - 1076
EP  - 1086
J2  - Nature Machine Intelligence
LA  - en
SN  - 2522-5839
UR  - https://www.nature.com/articles/s42256-023-00720-7
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:54:31</p>
N1  - <p>Publisher: Nature Publishing Group</p>
N1  - <p>Rights: 2023 The Author(s), under exclusive licence to Springer Nature Limited</p>
KW  - Computer science
KW  - Cultural and media studies
KW  - Human behaviour
ER  - 

TY  - JOUR
TI  - InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning
AU  - Gupta, Prakhar
AU  - Jiao, Cathy
AU  - Yeh, Yi-Ting
AU  - Mehri, Shikib
AU  - Eskenazi, Maxine
AU  - Bigham, Jeffrey P.
AB  - Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2205.12673
ST  - InstructDial
UR  - http://arxiv.org/abs/2205.12673
Y2  - 2025/08/28/
N1  - <p>arXiv:2205.12673 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:54:34</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Interpersonal impressions of a social robot versus human in the context of performance evaluations
AU  - Edwards, Chad
AU  - Edwards, Autumn
AU  - Albrehi, Fatima
AU  - Spence, Patric
T2  - Communication Education
AB  - Extending previous research on the Computers Are Social Actors paradigm and the human-to-human interaction script, this study examines the interpersonal impressions of a social robot evaluator versus a human evaluator in a performance evaluation context. A between-subjects experiment was conducted to measure participants’ impressions of the credibility, task and social attractiveness, and social presence of a social robot or a human evaluator of their public speaking performances. Additionally, participants rated the competence of the feedback they received. Both evaluators (human and social robot) garnered mostly “favorable” interpersonal impressions. Moreover, both agents’ feedback was rated favorably. In all cases, the human evaluator was rated significantly higher than the social robot evaluator. Implications for the use of social robots in educational evaluator roles and for future human–robot interaction research are discussed.
DA  - 2021///
PY  - 2021
DO  - 10.1080/03634523.2020.1802495
VL  - 70
IS  - 2
SP  - 165
EP  - 182
J2  - Communication Education
SN  - 0363-4523
UR  - https://doi.org/10.1080/03634523.2020.1802495
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:54:40</p>
N1  - <p>Publisher: NCA Website _eprint: https://doi.org/10.1080/03634523.2020.1802495</p>
KW  - attraction
KW  - communication
KW  - credibility
KW  - human–machine communication
KW  - human–robot interaction
KW  - social presence
ER  - 

TY  - JOUR
TI  - Investigating the impacting factors for the healthcare professionals to adopt artificial intelligence-based medical diagnosis support system (AIMDSS)
AU  - Fan, Wenjuan
AU  - Liu, Jingnan
AU  - Zhu, Shuwan
AU  - Pardalos, Panos M.
T2  - Annals of Operations Research
AB  - Compared to the booming industry of AIMDSS, the usage of AIMDSS among healthcare professionals is relatively low in the hospital. Thus, a research on the acceptance and adoption intention of AIMDSS by health professionals is imperative. In this study, an integration of Unified theory of user acceptance of technology and trust theory is proposed for exploring the adoption of AIMDSS. Besides, two groups of additional factors, related to AIMDSS (task complexity, technology characteristics, and perceived substitution crisis) and health professionals’ characteristics (propensity to trust and personal innovativeness in IT) are considered in the integrated model. The data set of proposed research model is collected through paper survey and Internet survey in China. The empirical examination demonstrates a high predictive power of this proposed model in explaining AIMDSS adoption. Finally, the theoretical contribution and practical implications of this research are discussed.
DA  - 2020///
PY  - 2020
DO  - 10.1007/s10479-018-2818-y
VL  - 294
IS  - 1
SP  - 567
EP  - 592
J2  - Annals of Operations Research
LA  - en
SN  - 1572-9338
UR  - https://doi.org/10.1007/s10479-018-2818-y
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:54:36</p>
KW  - Adopt intention
KW  - AIMDSS
KW  - Initial trust
KW  - UTAUT
ER  - 

TY  - CONF
TI  - Invisible Influence: Artificial Intelligence and the Ethics of Adaptive Choice Architectures
AU  - Susser, Daniel
T2  - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society
T3  - AIES '19
AB  - For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions--what behavioral economists call our choice architectures--are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings--the choice architectures--to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us--effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.
C1  - New York, NY, USA
C3  - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society
DA  - 2019///
PY  - 2019
DO  - 10.1145/3306618.3314286
SP  - 403
EP  - 408
PB  - Association for Computing Machinery
SN  - 978-1-4503-6324-2
ST  - Invisible Influence
UR  - https://doi.org/10.1145/3306618.3314286
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:54:43</p>
ER  - 

TY  - JOUR
TI  - Language Models are Few-Shot Learners
AU  - Brown, Tom B.
AU  - Mann, Benjamin
AU  - Ryder, Nick
AU  - Subbiah, Melanie
AU  - Kaplan, Jared
AU  - Dhariwal, Prafulla
AU  - Neelakantan, Arvind
AU  - Shyam, Pranav
AU  - Sastry, Girish
AU  - Askell, Amanda
AU  - Agarwal, Sandhini
AU  - Herbert-Voss, Ariel
AU  - Krueger, Gretchen
AU  - Henighan, Tom
AU  - Child, Rewon
AU  - Ramesh, Aditya
AU  - Ziegler, Daniel M.
AU  - Wu, Jeffrey
AU  - Winter, Clemens
AU  - Hesse, Christopher
AU  - Chen, Mark
AU  - Sigler, Eric
AU  - Litwin, Mateusz
AU  - Gray, Scott
AU  - Chess, Benjamin
AU  - Clark, Jack
AU  - Berner, Christopher
AU  - McCandlish, Sam
AU  - Radford, Alec
AU  - Sutskever, Ilya
AU  - Amodei, Dario
AB  - Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.
DA  - 2020///
PY  - 2020
DO  - 10.48550/arXiv.2005.14165
UR  - http://arxiv.org/abs/2005.14165
Y2  - 2025/08/28/
N1  - <p>arXiv:2005.14165 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:54:45</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Language Models are Unsupervised Multitask Learners
AU  - Radford, Alec
AU  - Wu, Jeff
AU  - Child, R.
AU  - Luan, D.
AU  - Amodei, Dario
AU  - Sutskever, I.
AB  - Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.
DA  - 2019///
PY  - 2019
UR  - https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:55:39</p>
ER  - 

TY  - JOUR
TI  - Large Language Models are as persuasive as humans, but how? About the cognitive effort and moral-emotional language of LLM arguments
AU  - Carrasco-Farre, Carlos
AB  - Large Language Models (LLMs) are already as persuasive as humans. However, we know very little about how they do it. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuasion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arXiv.2404.09329
ST  - Large Language Models are as persuasive as humans, but how?
UR  - http://arxiv.org/abs/2404.09329
Y2  - 2025/08/28/
N1  - <p>arXiv:2404.09329 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:54:50</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Large Language Models Can Argue in Convincing Ways About Politics, But Humans Dislike AI Authors: implications for Governance
AU  - Palmer, Alexis
AU  - Spirling, Arthur
T2  - Political Science
AB  - All politics relies on rhetorical appeals, and the ability to make arguments is considered perhaps uniquely human. But as recent times have seen successful large language model (LLM) applications to similar endeavours, we explore whether these approaches can out-compete humans in making appeals for/against various positions in US politics. We curate responses from crowdsourced workers and an LLM and place them in competition with one another. Human (crowd) judges make decisions about the relative strength of their (human v machine) efforts. We have several empirical ‘possibility’ results. First, LLMs can produce novel arguments that convince independent judges at least on a par with human efforts. Yet when informed about an orator’s true identity, judges show a preference for human over LLM arguments. This may suggest voters view such models as potentially dangerous; we think politicians should be aware of related ‘liar’s dividend’ concerns.
DA  - 2023///
PY  - 2023
DO  - 10.1080/00323187.2024.2335471
VL  - 75
IS  - 3
SP  - 281
EP  - 291
J2  - Political Science
SN  - 0032-3187
ST  - Large Language Models Can Argue in Convincing Ways About Politics, But Humans Dislike AI Authors
UR  - https://doi.org/10.1080/00323187.2024.2335471
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:54:52</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/00323187.2024.2335471</p>
KW  - artificial intelligence
KW  - large language models
KW  - political debate
KW  - political methodology
KW  - rhetoric
ER  - 

TY  - JOUR
TI  - Large Language Models Can Enhance Persuasion Through Linguistic Feature Alignment
AU  - Shin, Minkyu
AU  - Kim, Jin
AB  - Although large language models (LLMs) are reshaping various aspects of human life, our current understanding of their impacts remains somewhat constrained. Here we investigate the impact of LLMs on human communication, using data on consumer complaints in the financial industry. By employing an AI detection tool on more than 820K complaints gathered by the Consumer Financial Protection Bureau (CFPB), we find a sharp increase in the likely use of LLMs shortly after the release of ChatGPT. Moreover, the likely LLM usage was positively correlated with message persuasiveness (i.e., increased likelihood of obtaining relief from financial firms). Computational linguistic analyses suggest that the positive correlation may be explained by LLMs’ enhancement of various linguistic features. Based on the results of these observational studies, we hypothesize that LLM usage may enhance a comprehensive set of linguistic features, increasing message persuasiveness to receivers with heterogeneous linguistic preferences (i.e., linguistic feature alignment). We test this hypothesis in preregistered experiments and find support for it. As an instance of early empirical demonstrations of LLM usage for enhancing persuasion, our research highlights the transformative potential of LLMs in human communication.
DA  - 2024///
PY  - 2024
UR  - https://papers.ssrn.com/abstract=4725351
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
ER  - 

TY  - JOUR
TI  - Large Language Models Reflect the Ideology of their Creators
AU  - Buyl, Maarten
AU  - Rogiers, Alexander
AU  - Noels, Sander
AU  - Bied, Guillaume
AU  - Dominguez-Catena, Iris
AU  - Heiter, Edith
AU  - Johary, Iman
AU  - Mara, Alexandru-Cristian
AU  - Romero, Raphaël
AU  - Lijffijt, Jefrey
AU  - Bie, Tijl De
AB  - Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use. In this paper, we prompt a diverse panel of popular LLMs to describe a large number of prominent personalities with political relevance, in all six official languages of the United Nations. By identifying and analyzing moral assessments reflected in their responses, we find normative differences between LLMs from different geopolitical regions, as well as between the responses of the same LLM when prompted in different languages. Among only models in the United States, we find that popularly hypothesized disparities in political views are reflected in significant normative differences related to progressive values. Among Chinese models, we characterize a division between internationally- and domestically-focused models. Our results show that the ideological stance of an LLM appears to reflect the worldview of its creators. This poses the risk of political instrumentalization and raises concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically 'unbiased'.
DA  - 2025///
PY  - 2025
DO  - 10.48550/arXiv.2410.18417
UR  - http://arxiv.org/abs/2410.18417
Y2  - 2025/08/28/
N1  - <p>arXiv:2410.18417 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:59:05</p>
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Learning to summarize from human feedback
AU  - Stiennon, Nisan
AU  - Ouyang, Long
AU  - Wu, Jeff
AU  - Ziegler, Daniel M.
AU  - Lowe, Ryan
AU  - Voss, Chelsea
AU  - Radford, Alec
AU  - Amodei, Dario
AU  - Christiano, Paul
AB  - As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2009.01325
UR  - http://arxiv.org/abs/2009.01325
Y2  - 2025/08/28/
N1  - <p>arXiv:2009.01325 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:58:48</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models
AU  - Han, Chi
AU  - Wang, Qifan
AU  - Peng, Hao
AU  - Xiong, Wenhan
AU  - Chen, Yu
AU  - Ji, Heng
AU  - Wang, Sinong
T2  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)
A2  - Duh, Kevin
A2  - Gomez, Helena
A2  - Bethard, Steven
AB  - Today's large language models (LLMs) typically train on short text segments (e.g., \ensuremath<4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encod- ing scientific articles, code repositories, or long dialogues. Through both theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis reveals that commonly used techniques like using a sliding-window attention pattern or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7× decoding speed up and 7.5× memory saving over the original model. Our code will be publicly available upon publication.
C1  - Mexico City, Mexico
C3  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)
DA  - 2024///
PY  - 2024
DO  - 10.18653/v1/2024.naacl-long.222
SP  - 3991
EP  - 4008
PB  - Association for Computational Linguistics
ST  - LM-Infinite
UR  - https://aclanthology.org/2024.naacl-long.222/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:58:41</p>
ER  - 

TY  - JOUR
TI  - Lying in Persuasion
AU  - Rozenas, Arturas
AU  - Luo, Zhaotian
AB  - We study how the speaker acquires information when they can misrepresent it in communication with the audience. We derive a method to calculate the speaker’s value from any information structure and to determine the properties of the information structure under which the speaker benefits from the ability to lie. We then characterize the speaker’s optimal information design and the parametric conditions under which the acquired information is disclosed truthfully. When the interests of the speaker and the audience are insufficiently aligned, the speaker obtains the same information as they would if they could not lie, and they communicate that information truthfully. When the interests are sufficiently aligned, the speaker chooses more persuasive information than they would without the option to lie but resorts to deceptive communication. The speaker’s loss of welfare due to the lack of commitment power is more pronounced when lies are harder to detect.
DA  - 2023///
PY  - 2023
DO  - 10.2139/ssrn.3878448
LA  - en
UR  - https://papers.ssrn.com/abstract=3878448
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:58:51</p>
KW  - Arturas Rozenas
KW  - Lying in Persuasion
KW  - SSRN
KW  - Zhaotian Luo
ER  - 

TY  - JOUR
TI  - Machine Authorship In Situ: Effect of news organization and news genre on news credibility
AU  - Liu, Bingjie
AU  - Wei, Lewen
T2  - Digital Journalism
AB  - News-writing bots have been applied in news production. However, findings remain equivocal about how machine authorship is received by readers. To understand the inconsistency in past findings, this study conducted a 2 (purported writer: human vs. machine) × 2 (news organization: the New York Times vs. Fox News) × 2 (news type: spot news vs. interpretive news) between-subjects online experiment (N = 355) to examine how the identity of news writers, human vs. machine, would impact the processing and evaluation of the news writer and the news as context varies. Findings suggest that machine-written news induced less emotional involvement and was perceived as more objective. However, machine writer was perceived as of less expertise compared with its human counterpart. On the selected issues (Obamacare, LGBT rights, and refugee admission), news purported to be released by Fox News was less trusted by the participants than the New York Times. For a media organization whose news was more trusted, utilizing news-writing bots enhanced perceived news objectivity. Otherwise, employing bots further reduced perception of the writer’s trustworthiness and expertise. Furthermore, machine authorship enhanced perceived news credibility more prominently when writing a genre that demanded more information processing. Theoretical and practical implications are discussed.
DA  - 2019///
PY  - 2019
DO  - 10.1080/21670811.2018.1510740
VL  - 7
IS  - 5
SP  - 635
EP  - 657
J2  - Digital Journalism
SN  - 2167-0811
ST  - Machine Authorship In Situ
UR  - https://doi.org/10.1080/21670811.2018.1510740
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:58:54</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2018.1510740</p>
KW  - automated journalism
KW  - credibility
KW  - interpretive news
KW  - machine heuristic
KW  - mind perception
KW  - news organization
ER  - 

TY  - JOUR
TI  - Man vs. machine: Human responses to an AI newscaster and the role of social presence
AU  - Kim, Jihyun
AU  - Xu, Kun
AU  - Merrill Jr., Kelly
T2  - The Social Science Journal
AB  - Technology advancements made the possibility of artificial intelligence (AI) newscasters a reality. Some news stations across the world have already employed AI to deliver the news, and more news stations may continue this trend. However, little is known about how the audience will respond to news when it is delivered by AI. Thus, the present study investigates people’s perceptions about an AI newscaster, compared to a human newscaster, in the context of delivering a weather newscast. Primary findings indicate that people perceive a human newscaster as more credible than an AI newscaster; however, information seeking intentions and behavioral intentions do not differ between the type of newscaster. Further, when listening to a weather newscast delivered by an AI newscaster, the perceived social presence of an AI newscaster is positively associated with greater levels of credibility, information seeking intentions, and behavioral intentions. Collectively, this exploratory research suggests the possibility that AI newscasters can be incorporated to the news broadcasting industry when human resources are limited.
DO  - 10.1080/03623319.2022.2027163
VL  - 0
IS  - 0
SP  - 1
EP  - 13
J2  - The Social Science Journal
SN  - 0362-3319
ST  - Man vs. machine
UR  - https://doi.org/10.1080/03623319.2022.2027163
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:58:59</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/03623319.2022.2027163</p>
KW  - AI newscaster
KW  - artificial intelligence
KW  - human-machine communication
KW  - social presence
ER  - 

TY  - JOUR
TI  - Man vs. Machine? The Impact of Algorithm Authorship on News Credibility
AU  - Tandoc Jr., Edson C.
AU  - Yao, Lim Jia
AU  - Wu, Shangyuan
T2  - Digital Journalism
AB  - Facing budget constraints, many traditional news organizations are turning their eyes on automation to streamline manpower, cut down on costs, and improve efficiency. But how does automation fit into traditional values of journalism and how does it affect perceptions of credibility, an important currency valued by the journalistic field? This study explores this question using a 3 (declared author: human vs. machine vs. combined) × 2 (objectivity: objective vs. not objective) between-subjects experimental design involving 420 participants drawn from the national population of Singapore. The analysis found no main differences in perceived source credibility between algorithm, human, and mixed authors. Similarly, news articles attributed to an algorithm, a human journalist, and a combination of both showed no differences in message credibility. However, the study found an interaction effect between type of declared author and news objectivity. When the article is presented to be written by a human journalist, source and message credibility remain stable regardless of whether the article was objective or not objective. However, when the article is presented to be written by an algorithm, source and message credibility are higher when the article is objective than when the article is not objective. Findings for combined authorship are split: there were no differences between objective and non-objective articles when it comes to message credibility. However, combined authorship is rated higher in source credibility when the article is not objective than when the article is objective.
DA  - 2020///
PY  - 2020
DO  - 10.1080/21670811.2020.1762102
VL  - 8
IS  - 4
SP  - 548
EP  - 562
J2  - Digital Journalism
SN  - 2167-0811
ST  - Man vs. Machine?
UR  - https://doi.org/10.1080/21670811.2020.1762102
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:59:01</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2020.1762102</p>
KW  - Automation
KW  - journalism
KW  - news
KW  - objectivity
KW  - robot news
ER  - 

TY  - JOUR
TI  - Measuring Automated Influence: Between Empirical Evidence and Ethical Values
AU  - Susser, Daniel
AU  - Grimaldi, Vincent
AB  - Automated influence, delivered by digital targeting technologies such as targeted advertising, digital nudges, and recommender systems, has attracted significant interest from both empirical researchers, on one hand, and critical scholars and policymakers on the other. In this paper, we argue for closer integration of these efforts. Critical scholars and policymakers, who focus primarily on the social, ethical, and political effects of these technologies, need empirical evidence to substantiate and motivate their concerns. However, existing empirical research investigating the effectiveness of these technologies (or lack thereof), neglects other morally relevant effects—which can be felt regardless of whether or not the technologies "work" in the sense of fulfilling the promises of their designers. Drawing from the ethics and policy literature, we enumerate a range of questions begging for empirical analysis—the outline of a research agenda bridging these fields—and issue a call to action for more empirical research that takes these urgent ethics and policy questions as their starting point.
DA  - 2021///
PY  - 2021
LA  - en
ST  - Measuring Automated Influence
UR  - https://papers.ssrn.com/abstract=3848919
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:59:02</p>
KW  - autonomy
KW  - dark patterns
KW  - influence
KW  - nudges
KW  - privacy
KW  - targeted advertising
ER  - 

TY  - ELEC
TI  - Measuring the Persuasiveness of Language Models
AU  - Durmus, Esin
AU  - Lovitt, Liane
AU  - Tamkin, Alex
AU  - Ritchie, Stuart
AU  - Clark, Jack
AU  - Ganguli, Deep
AB  - Anthropic developed a way to test how persuasive language models (LMs) are, and analyzed how persuasiveness scales across different versions of Claude.
DA  - 2024///
PY  - 2024
LA  - en
UR  - https://www.anthropic.com/research/measuring-model-persuasiveness
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
ER  - 

TY  - JOUR
TI  - Mirages: On Anthropomorphism in Dialogue Systems
AU  - Abercrombie, Gavin
AU  - Curry, Amanda Cercas
AU  - Dinkar, Tanvi
AU  - Rieser, Verena
AU  - Talat, Zeerak
AB  - Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism may be inevitable due to the choice of medium, conscious and unconscious design choices can guide users to personify such systems to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, including reinforcing gender stereotypes and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2305.09800
ST  - Mirages
UR  - http://arxiv.org/abs/2305.09800
Y2  - 2025/08/27/
N1  - <p>Date Added: 2025-08-28 12:02:44</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Model evaluation for extreme risks
AU  - Shevlane, Toby
AU  - Farquhar, Sebastian
AU  - Garfinkel, Ben
AU  - Phuong, Mary
AU  - Whittlestone, Jess
AU  - Leung, Jade
AU  - Kokotajlo, Daniel
AU  - Marchal, Nahema
AU  - Anderljung, Markus
AU  - Kolt, Noam
AU  - Ho, Lewis
AU  - Siddarth, Divya
AU  - Avin, Shahar
AU  - Hawkins, Will
AU  - Kim, Been
AU  - Gabriel, Iason
AU  - Bolina, Vijay
AU  - Clark, Jack
AU  - Bengio, Yoshua
AU  - Christiano, Paul
AU  - Dafoe, Allan
AB  - Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2305.15324
UR  - http://arxiv.org/abs/2305.15324
Y2  - 2025/08/28/
N1  - <p>arXiv:2305.15324 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:00:17</p>
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Modelling Persuasion through Misuse of Rhetorical Appeals
AU  - Pauli, Amalie
AU  - Derczynski, Leon
AU  - Assent, Ira
T2  - Proceedings of the Second Workshop on NLP for Positive Impact (NLP4PI)
A2  - Biester, Laura
A2  - Demszky, Dorottya
A2  - Jin, Zhijing
A2  - Sachan, Mrinmaya
A2  - Tetreault, Joel
A2  - Wilson, Steven
A2  - Xiao, Lu
A2  - Zhao, Jieyu
AB  - It is important to understand how people use words to persuade each other. This helps understand debate, and detect persuasive narratives in regard to e.g. misinformation. While computational modelling of some aspects of persuasion has received some attention, a way to unify and describe the overall phenomenon of when persuasion becomes undesired and problematic, is missing. In this paper, we attempt to address this by proposing a taxonomy of computational persuasion. Drawing upon existing research and resources, this paper shows how to re-frame and re-organise current work into a coherent framework targeting the misuse of rhetorical appeals. As a study to validate these re-framings, we then train and evaluate models of persuasion adapted to our taxonomy. Our results show an application of our taxonomy, and we are able to detecting misuse of rhetorical appeals, finding that these are more often used in misinformative contexts than in true ones.
C1  - Abu Dhabi, United Arab Emirates (Hybrid)
C3  - Proceedings of the Second Workshop on NLP for Positive Impact (NLP4PI)
DA  - 2022///
PY  - 2022
DO  - 10.18653/v1/2022.nlp4pi-1.11
SP  - 89
EP  - 100
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2022.nlp4pi-1.11/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:00:22</p>
ER  - 

TY  - JOUR
TI  - Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants
AU  - Bartolo, Max
AU  - Thrush, Tristan
AU  - Riedel, Sebastian
AU  - Stenetorp, Pontus
AU  - Jia, Robin
AU  - Kiela, Douwe
AB  - In Dynamic Adversarial Data Collection (DADC), human annotators are tasked with finding examples that models struggle to predict correctly. Models trained on DADC-collected training data have been shown to be more robust in adversarial and out-of-domain settings, and are considerably harder for humans to fool. However, DADC is more time-consuming than traditional data collection and thus more costly per annotated example. In this work, we examine whether we can maintain the advantages of DADC, without incurring the additional cost. To that end, we introduce Generative Annotation Assistants (GAAs), generator-in-the-loop models that provide real-time suggestions that annotators can either approve, modify, or reject entirely. We collect training datasets in twenty experimental settings and perform a detailed analysis of this approach for the task of extractive question answering (QA) for both standard and adversarial data collection. We demonstrate that GAAs provide significant efficiency benefits with over a 30% annotation speed-up, while leading to over a 5x improvement in model fooling rates. In addition, we find that using GAA-assisted training data leads to higher downstream model performance on a variety of question answering tasks over adversarial data collection.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2112.09062
ST  - Models in the Loop
UR  - http://arxiv.org/abs/2112.09062
Y2  - 2025/08/27/
N1  - <p>Date Added: 2025-08-28 12:02:44</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Moderating Uncivil User Comments by Humans or Machines? The Effects of Moderation Agent on Perceptions of Bias and Credibility in News Content
AU  - Wang, Sai
T2  - Digital Journalism
AB  - Studies have shown that uncivil comments under an online news article may result in biased perceptions of the news content, and explicit comment moderation has the potential to mitigate this adverse effect. Using an online experiment, the present study extends this line of research with the examination of how interface cues signalling different agents (human vs. machine) in moderating uncivil comments affect a reader’s judgment of the news and how prior belief in machine heuristic moderates such effects. The results indicated that perceptions of news bias were attenuated when uncivil comments were moderated by a machine (as opposed to a human) agent, which subsequently engendered greater perceived credibility of the news story. Additionally, such indirect effects were more prominent among readers who strongly believed that machine operations are generally accurate and reliable than those with a weaker prior belief in this rule of thumb.
DA  - 2021///
PY  - 2021
DO  - 10.1080/21670811.2020.1851279
VL  - 9
IS  - 1
SP  - 64
EP  - 83
J2  - Digital Journalism
SN  - 2167-0811
ST  - Moderating Uncivil User Comments by Humans or Machines?
UR  - https://doi.org/10.1080/21670811.2020.1851279
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:00:24</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2020.1851279</p>
KW  - comments
KW  - credibility
KW  - incivility
KW  - machine heuristic
KW  - Moderation
KW  - news bias
ER  - 

TY  - CONF
TI  - News from Generative Artificial Intelligence Is Believed Less
AU  - Longoni, Chiara
AU  - Fradkin, Andrey
AU  - Cian, Luca
AU  - Pennycook, Gordon
T2  - Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency
T3  - FAccT '22
AB  - Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533077
SP  - 97
EP  - 106
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://dl.acm.org/doi/10.1145/3531146.3533077
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:00:28</p>
ER  - 

TY  - JOUR
TI  - No Rage Against the Machine: How Computer Agents Mitigate Human Emotional Processes in Electronic Negotiations
AU  - Adam, Marc T. P.
AU  - Teubner, Timm
AU  - Gimpel, Henner
T2  - Group Decision and Negotiation
AB  - With the proliferation of information technology and artificial intelligence in society, human users have started to engage in social interactions with computer agents. In this study, we conducted a laboratory experiment in which neurophysiological measurements were used to investigate the effect of computer agents on the affective processes and behavior of human negotiators. Participants engaged in alternating-offer bargaining over the partition of a pie with either human or computer counterparts and under different levels of urgency to reach an agreement. Overall, our data show that the subjects claimed significantly higher proportions for themselves when they made opening offers to computer agents than when bargaining with human counterparts, regardless of the degree of urgency in the negotiation. However, when the subjects responded to computer-issued offers the picture was more complex. Whereas under high-level urgency, the subjects were more likely to accept offers made by computer agents than by human counterparts, we observed the opposite effect for low-level urgency, where they were less likely to accept the offers of computer agents. In combination, these behavioral patterns lead to the use of computer agents yielding an increase in economic efficiency. Further, the subjects exhibited less emotionally charged behavior when facing computer agents than when facing human counterparts, as the intensity of affective processes was lower and the relationship between arousal and offer acceptance was observable only when the counterparts were human. The results of our study shed light on the potential benefits and intricacies of employing computer agents in electronic negotiations.
DA  - 2018///
PY  - 2018
DO  - 10.1007/s10726-018-9579-5
VL  - 27
IS  - 4
SP  - 543
EP  - 571
J2  - Group Decision and Negotiation
LA  - en
SN  - 1572-9907
ST  - No Rage Against the Machine
UR  - https://doi.org/10.1007/s10726-018-9579-5
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:02:04</p>
KW  - Bargaining
KW  - Computer agents
KW  - Emotions
KW  - Experiment
ER  - 

TY  - JOUR
TI  - Not a good judge of talent: the influence of subjective socioeconomic status on AI aversion
AU  - Xie, Chunya
AU  - Fu, Tianhui
AU  - Yang, Chen
AU  - Chang, En-Chung
AU  - Zhao, Mengying
T2  - Marketing Letters
AB  - The current research constructs a framework to understand how subjective socioeconomic status (SES) affects consumers’ AI aversion in the evaluation context. Three experiments show that subjective SES has a negative impact on consumers’ willingness to accept AI evaluation. Consumers with higher subjective SES are more likely to resist AI evaluation because they perceive that AI agents are not as capable as human agents of identifying their talents. This effect is moderated by the agent type–the impact of subjective SES on resistance to the AI agent is attenuated when the AI agent is non-evaluative. This research is of great significance in enriching research on improving AI services efficiency across various social classes.
DA  - 2024///
PY  - 2024
DO  - 10.1007/s11002-024-09725-7
VL  - 35
IS  - 3
SP  - 381
EP  - 393
J2  - Marketing Letters
LA  - en
SN  - 1573-059X
ST  - Not a good judge of talent
UR  - https://doi.org/10.1007/s11002-024-09725-7
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:02:09</p>
KW  - AI aversion
KW  - AI evaluation
KW  - Subjective socioeconomic status
KW  - Talents identification
ER  - 

TY  - JOUR
TI  - Nudgeability: Mapping Conditions of Susceptibility to Nudge Influence
AU  - de Ridder, Denise
AU  - Kroese, Floor
AU  - van Gestel, Laurens
T2  - Perspectives on Psychological Science
AB  - Nudges are behavioral interventions to subtly steer citizens’ choices toward “desirable” options. An important topic of debate concerns the legitimacy of nudging as a policy instrument, and there is a focus on issues relating to nudge transparency, the role of preexisting preferences people may have, and the premise that nudges primarily affect people when they are in “irrational” modes of thinking. Empirical insights into how these factors affect the extent to which people are susceptible to nudge influence (i.e., “nudgeable”) are lacking in the debate. This article introduces the new concept of nudgeability and makes a first attempt to synthesize the evidence on when people are responsive to nudges. We find that nudge effects do not hinge on transparency or modes of thinking but that personal preferences moderate effects such that people cannot be nudged into something they do not want. We conclude that, in view of these findings, concerns about nudging legitimacy should be softened and that future research should attend to these and other conditions of nudgeability.
DA  - 2022///
PY  - 2022
DO  - 10.1177/1745691621995183
VL  - 17
IS  - 2
SP  - 346
EP  - 359
J2  - Perspectives on Psychological Science
LA  - EN
SN  - 1745-6916
ST  - Nudgeability
UR  - https://doi.org/10.1177/1745691621995183
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:02:11</p>
N1  - <p>Publisher: SAGE Publications Inc</p>
ER  - 

TY  - JOUR
TI  - On Inference of Network Topology and Confirmation Bias in Cyber-Social Networks
AU  - Mao, Yanbing
AU  - Akyol, Emrah
T2  - IEEE Transactions on Signal and Information Processing over Networks
AB  - This article studies topology inference, from agent states, of a directed cyber-social network with opinion spreading dynamics model that explicitly takes confirmation bias into account. The cyber-social network comprises a set of partially connected directed network of agents at the social level, and a set of information sources at the cyber layer. The necessary and sufficient conditions for the existence of exact inference solution are characterized. A method for exact inference, when it is possible, of entire network topology as well as confirmation bias model parameters is proposed for the case where the bias mentioned earlier follows a piece-wise linear model. The particular case of no confirmation bias is analyzed in detail. For the setting where the model of confirmation bias is unknown, an algorithm that approximates the network topology, building on the exact inference method, is presented. This algorithm can exactly infer the weighted communication from the neighbors to the non-followers of information sources. Numerical simulations demonstrate the effectiveness of the proposed methods for different scenarios.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TSIPN.2020.3015283
VL  - 6
SP  - 633
EP  - 644
J2  - IEEE Transactions on Signal and Information Processing over Networks
SN  - 2373-776X
UR  - https://ieeexplore.ieee.org/document/9187724
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:02:13</p>
KW  - Computational modeling
KW  - Confirmation bias
KW  - cyber-social networks
KW  - directed communication
KW  - Linear matrix inequalities
KW  - Mathematical model
KW  - Network topology
KW  - Social network services
KW  - social networks
KW  - Stochastic processes
KW  - Topology
KW  - topology inference
ER  - 

TY  - JOUR
TI  - On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning
AU  - Shaikh, Omar
AU  - Zhang, Hongxin
AU  - Held, William
AU  - Bernstein, Michael
AU  - Yang, Diyi
AB  - Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2212.08061
UR  - http://arxiv.org/abs/2212.08061
Y2  - 2025/08/28/
N1  - <p>arXiv:2212.08061 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:02:15</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Online Dating Meets Artificial Intelligence: How the Perception of Algorithmically Generated Profile Text Impacts Attractiveness and Trust
AU  - Wu, Yihan
AU  - Kelly, Ryan M.
T2  - Proceedings of the 32nd Australian Conference on Human-Computer Interaction
T3  - OzCHI '20
AB  - Online dating systems are widely used to meet romantic partners, yet people often struggle to write attractive profiles on these applications. Artificial intelligence (AI) has the potential to help online daters by automatically generating profile content, but little research has explored how the use of AI in online dating could affect users’ perceptions of one another. The present study investigated how the perceived involvement of AI influences ratings of attractiveness and trust in online dating. In a between-subjects experiment, participants (N = 48) were presented with the text of 10 dating profiles and were told that the profiles had been written by humans or with the help of AI. We found that the perceived involvement of AI did not have a significant impact on attractiveness, but that it did lead to a significant reduction in trustworthiness of the profile author. We interpret our findings through the lens of social information processing theory, discussing the tradeoffs associated with designing to reveal or hide the use of AI in online dating.
C1  - New York, NY, USA
C3  - Proceedings of the 32nd Australian Conference on Human-Computer Interaction
DA  - 2021///
PY  - 2021
DO  - 10.1145/3441000.3441074
SP  - 444
EP  - 453
PB  - Association for Computing Machinery
SN  - 978-1-4503-8975-4
ST  - Online Dating Meets Artificial Intelligence
UR  - https://doi.org/10.1145/3441000.3441074
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:02:17</p>
ER  - 

TY  - JOUR
TI  - Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback
AU  - Casper, Stephen
AU  - Davies, Xander
AU  - Shi, Claudia
AU  - Gilbert, Thomas Krendl
AU  - Scheurer, Jérémy
AU  - Rando, Javier
AU  - Freedman, Rachel
AU  - Korbak, Tomasz
AU  - Lindner, David
AU  - Freire, Pedro
AU  - Wang, Tony
AU  - Marks, Samuel
AU  - Segerie, Charbel-Raphaël
AU  - Carroll, Micah
AU  - Peng, Andi
AU  - Christoffersen, Phillip
AU  - Damani, Mehul
AU  - Slocum, Stewart
AU  - Anwar, Usman
AU  - Siththaranjan, Anand
AU  - Nadeau, Max
AU  - Michaud, Eric J.
AU  - Pfau, Jacob
AU  - Krasheninnikov, Dmitrii
AU  - Chen, Xin
AU  - Langosco, Lauro
AU  - Hase, Peter
AU  - Bıyık, Erdem
AU  - Dragan, Anca
AU  - Krueger, David
AU  - Sadigh, Dorsa
AU  - Hadfield-Menell, Dylan
AB  - Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2307.15217
UR  - http://arxiv.org/abs/2307.15217
Y2  - 2025/08/28/
N1  - <p>arXiv:2307.15217 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:02:22</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Parasocial interactions with real and virtual influencers: The role of perceived similarity and human-likeness
AU  - Stein, Jan-Philipp
AU  - Linda Breves, Priska
AU  - Anders, Nora
T2  - New Media & Society
AB  - Digitally created online celebrities (so-called virtual influencers) have appeared on various social media and video streaming platforms. While the scientific community has recently started to take an interest in this new phenomenon, it still remains mostly unclear how online audiences engage with—and relate to—these artificial digital creations. To address the identified research gap, we conducted a preregistered experiment (N = 179), comparing viewers’ parasocial interactions (PSIs) with either a human or a virtual influencer. Based on natural stimuli, we find that viewers’ parasocial response does not differ significantly between the two groups. However, by focusing on several theoretically relevant mediator variables, we uncover two opposing effects at play: While a significant direct effect signifies stronger PSIs with the virtual influencer, participants also attribute this persona with less mental human-likeness and similarity to themselves—which ultimately suppresses the observed advantage. Potential explanations for our results are discussed.
DA  - 2024///
PY  - 2024
DO  - 10.1177/14614448221102900
VL  - 26
IS  - 6
SP  - 3433
EP  - 3453
J2  - New Media & Society
LA  - EN
SN  - 1461-4448
ST  - Parasocial interactions with real and virtual influencers
UR  - https://doi.org/10.1177/14614448221102900
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:02:25</p>
N1  - <p>Publisher: SAGE Publications</p>
ER  - 

TY  - JOUR
TI  - People devalue generative AI’s competence but not its advice in addressing societal and personal challenges
AU  - Böhm, Robert
AU  - Jörling, Moritz
AU  - Reiter, Leonhard
AU  - Fuchs, Christoph
T2  - Communications Psychology
AB  - The release of ChatGPT and related tools have made generative artificial intelligence (AI) easily accessible for the broader public. We conducted four preregistered experimental studies (total N = 3308; participants from the US) to investigate people’s perceptions of generative AI and the advice it generates on how to address societal and personal challenges. The results indicate that when individuals are (vs. are not) aware that the advice was generated by AI, they devalue the author’s competence but not the content or the intention to share and follow the advice on how to address societal challenges (Study 1) and personal challenges (Studies 2a and 2b). Study 3 further shows that individuals’ preference to receive advice from AI (vs. human experts) increases when they gained positive experience with generative AI advice in the past. The results are discussed regarding the nature of AI aversion in the context of generative AI and beyond.
DA  - 2023///
PY  - 2023
DO  - 10.1038/s44271-023-00032-x
VL  - 1
IS  - 1
SP  - 32
J2  - Communications Psychology
LA  - en
SN  - 2731-9121
UR  - https://www.nature.com/articles/s44271-023-00032-x
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:02:58</p>
N1  - <p>Publisher: Nature Publishing Group</p>
N1  - <p>Rights: 2023 The Author(s)</p>
KW  - Human behaviour
KW  - Information systems and information technology
ER  - 

TY  - CONF
TI  - Perception of physical and virtual agents: exploration of factors influencing the acceptance of intrusive domestic agents
AU  - Zehnder, E.
AU  - Dinet, J.
AU  - Charpillet, F.
T2  - 2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
AB  - Domestic robots and agents are widely sold to the grand public, leading us to ethical issues related to the data harvested by such machines. While users show a general acceptance of these robots, concerns remain when it comes to information security and privacy. Current research indicates that there’s a privacy-security trade-off for better use, but the anthropomorphic and social abilities of a robot are also known to modulate its acceptance and use. To explore and deepen what literature already brought on the subject we examined how users perceived their robot (Replika, Roomba©, Amazon Echo©, Google Home©, or Cozmo©/Vector©) through an online questionnaire exploring acceptance, perceived privacy and security, anthropomorphism, disclosure, perceived intimacy, and loneliness. The results supported the literature regarding the potential manipulative effects of robot’s anthropomorphism for acceptance but also information disclosure, perceived intimacy, security, and privacy.
C3  - 2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
DA  - 2022///
PY  - 2022
DO  - 10.1109/RO-MAN53752.2022.9900593
SP  - 1050
EP  - 1057
ST  - Perception of physical and virtual agents
UR  - https://ieeexplore.ieee.org/document/9900593
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:03:39</p>
N1  - <p>ISSN: 1944-9437</p>
KW  - Data collection
KW  - Ethics
KW  - Information security
KW  - Internet
KW  - Manufacturing
KW  - Privacy
KW  - Regulation
ER  - 

TY  - CONF
TI  - Persona or Context? Towards Building Context adaptive Personalized Persuasive Virtual Sales Assistant
AU  - Tiwari, Abhisek
AU  - Saha, Sriparna
AU  - Sengupta, Shubhashis
AU  - Maitra, Anutosh
AU  - Ramnani, Roshni
AU  - Bhattacharyya, Pushpak
T2  - Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
A2  - He, Yulan
A2  - Ji, Heng
A2  - Li, Sujian
A2  - Liu, Yang
A2  - Chang, Chua-Hui
AB  - Task-oriented conversational agents are gaining immense popularity and success in a wide range of tasks, from flight ticket booking to online shopping. However, the existing systems presume that end-users will always have a pre-determined and servable task goal, which results in dialogue failure in hostile scenarios, such as goal unavailability. On the other hand, human agents accomplish users' tasks even in a large number of goal unavailability scenarios by persuading them towards a very similar and servable goal. Motivated by the limitation, we propose and build a novel end-to-end multi-modal persuasive dialogue system incorporated with a personalized persuasive module aided goal controller and goal persuader. The goal controller recognizes goal conflicting/unavailability scenarios and formulates a new goal, while the goal persuader persuades users using a personalized persuasive strategy identified through dialogue context. We also present a novel automatic evaluation metric called Persuasiveness Measurement Rate (PMeR) for quantifying the persuasive capability of a conversational agent. The obtained improvements (both quantitative and qualitative) firmly establish the superiority and need of the proposed context-guided, personalized persuasive virtual agent over existing traditional task-oriented virtual agents. Furthermore, we also curated a multi-modal persuasive conversational dialogue corpus annotated with intent, slot, sentiment, and dialogue act for e-commerce domain.
C1  - Online only
C3  - Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
DA  - 2022///
PY  - 2022
DO  - 10.18653/v1/2022.aacl-main.76
SP  - 1035
EP  - 1047
PB  - Association for Computational Linguistics
ST  - Persona or Context?
UR  - https://aclanthology.org/2022.aacl-main.76/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:03:33</p>
ER  - 

TY  - JOUR
TI  - Persuasive power of prosodic features
AU  - Kišiček, Gabrijela
T2  - Argumentation and Advocacy
AB  - This response to William Bedsole's paper aims to emphasize the importance of prosodic features—i.e. voice quality, tempo, rhythm, pitch, intonation etc.—in the persuasion process. Bedsole refers to voice and the role voice has but neglects to focus on the many different functions it can have. Extensive empirical research in nonverbal communication confirms that prosodic features play an important role in human interaction. However, contemporary rhetorical research has somewhat marginalized prosody. Voice and all prosodic features may contribute to the credibility speaker (ethos), can create certain disposition on audience (pathos), but can also be essential for understanding the argument (logos).
DA  - 2018///
PY  - 2018
DO  - 10.1080/10511431.2019.1525003
VL  - 54
IS  - 4
SP  - 345
EP  - 350
J2  - Argumentation and Advocacy
SN  - 1051-1431
UR  - https://doi.org/10.1080/10511431.2019.1525003
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:03:44</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/10511431.2019.1525003</p>
KW  - argumentation
KW  - persuasion
KW  - prosody
KW  - rhetoric
KW  - voice quality
ER  - 

TY  - JOUR
TI  - Persuasiveness of arguments with AI-source labels
AU  - Teigen, Cassandra
AU  - Madsen, Jens Koed
AU  - George, Nicole Lauren
AU  - Yousefi, Sayeh
T2  - Proceedings of the Annual Meeting of the Cognitive Science Society
AB  - This paper sought to understand the impact of labelling an argument as AI-generated compared to human-authored, and how factors such as portrayals of expertise and the nature of arguments presented (narrative versus statistical) may affect the persuasiveness of the arguments. Three domains were explored: health, finance, and politics. We show that arguments with AI source labels, both non-expert and expert, were rated by participants as less persuasive than when they had their counterpart human-authored source labels attached. Moreover, although the statistical arguments were found to be more persuasive than the narrative arguments, this did not affect the impact of an AI source label, with a significant interaction effect only being seen for the domain of politics for the expert AI source. The study explored the role of attitude towards AI on the impact of source labels as an exploratory analysis and found no significant interaction effect across the three domains.
DA  - 2024///
PY  - 2024
VL  - 46
IS  - 0
J2  - Proceedings of the Annual Meeting of the Cognitive Science Society
LA  - en
UR  - https://escholarship.org/uc/item/6t82g70v
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:03:41</p>
ER  - 

TY  - JOUR
TI  - Playing well with others: The role of opponent and intergroup anxiety in the reduction of prejudice through collaborative video game play
AU  - Stiff, Chris
AU  - Kedra, Paula
T2  - Psychology of Popular Media
AB  - Recent work on the social effects of video gaming has moved away from the view they are detrimental, and has instead demonstrated how they may be a force for good. One example is how collaborative intergroup play can reduce prejudice between groups. However, this literature is at a nascent stage, and many of the intricacies of such a mechanism are unknown. Previous work has predominantly used attitude scales and ignored other measures. Factors such as the role of the opponent in games and what may be the mechanism behind any effects have likewise received little attention. In this laboratory study, participants played collaborative games with an outgroup member or alone. Their opponent was also reported to be computer controlled or controlled by another person. Following play, intergroup anxiety was reported, and participants wrote a short passage of prose regarding the outgroup and rated on attitude scales. Analysis demonstrated that playing with outgroup members was indeed an effective method of increasing the positivity toward outgroup members, reflected in both scale and prose measures. Anxiety was also found to be a significant mediator; however, it was less clear whether a human opponent moderated any effects. Further ideas of how these findings could be developed are then discussed. (PsycINFO Database Record (c) 2020 APA, all rights reserved)
DA  - 2020///
PY  - 2020
DO  - 10.1037/ppm0000210
VL  - 9
IS  - 1
SP  - 105
EP  - 115
J2  - Psychology of Popular Media
SN  - 2689-6575
ST  - Playing well with others
N1  - <p>Date Added: 2025-08-28 13:03:46</p>
N1  - <p>Place: US Publisher: Educational Publishing Foundation</p>
KW  - Anxiety
KW  - Anxiety Management
KW  - Attitude Measures
KW  - Collaboration
KW  - Computer Games
KW  - Ingroup Outgroup
KW  - Intergroup Dynamics
KW  - Prejudice
KW  - Prose
KW  - Test Construction
ER  - 

TY  - CONF
TI  - Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models
AU  - Wu, Tongshuang
AU  - Ribeiro, Marco Tulio
AU  - Heer, Jeffrey
AU  - Weld, Daniel
T2  - Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
A2  - Zong, Chengqing
A2  - Xia, Fei
A2  - Li, Wenjie
A2  - Navigli, Roberto
AB  - While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.
C1  - Online
C3  - Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
DA  - 2021///
PY  - 2021
DO  - 10.18653/v1/2021.acl-long.523
SP  - 6707
EP  - 6723
PB  - Association for Computational Linguistics
ST  - Polyjuice
UR  - https://aclanthology.org/2021.acl-long.523/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:03:48</p>
ER  - 

TY  - JOUR
TI  - Preference for Human (vs. Robotic) Labor is Stronger in Symbolic Consumption Contexts
AU  - Granulo, Armin
AU  - Fuchs, Christoph
AU  - Puntoni, Stefano
T2  - Journal of Consumer Psychology
AB  - Advances in robotics, automation, and artificial intelligence increasingly enable firms to replace human labor with technology, thereby fundamentally transforming how goods and services are produced. From both managerial and societal points of view, it is therefore important to understand demand-side incentives for firms to employ human labor. We begin to address this question by examining for which products and services consumers are more likely to favor human (vs. robotic) labor. In six studies, we demonstrate that consumers prefer human (vs. robotic) labor more for products with higher (vs. lower) symbolic value (e.g., when expressing something about one's beliefs and personality is of greater importance). We theorize that this is because consumers have stronger uniqueness motives in more (vs. less) symbolic consumption contexts (and associate human labor more strongly with product uniqueness). In line with this account, we demonstrate that individual differences in need for uniqueness moderate the interaction between production mode and symbolic motives and that a measure of uniqueness motives mediates the effect of consumption context on preferences for human (vs. robotic) production.
DA  - 2021///
PY  - 2021
DO  - 10.1002/jcpy.1181
VL  - 31
IS  - 1
SP  - 72
EP  - 80
J2  - Journal of Consumer Psychology
LA  - en
SN  - 1532-7663
UR  - https://onlinelibrary.wiley.com/doi/abs/10.1002/jcpy.1181
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:03:51</p>
N1  - <p>_eprint: https://myscp.onlinelibrary.wiley.com/doi/pdf/10.1002/jcpy.1181</p>
N1  - <p>Rights: © 2020 The Authors. Journal of Consumer Psychology published by Wiley Periodicals LLC on behalf of Society for Consumer Psychology</p>
KW  - Consumer preferences
KW  - Human labor
KW  - Robotic labor
KW  - Symbolic consumption
KW  - Uniqueness motives
ER  - 

TY  - JOUR
TI  - Quantifying the Impact of Large Language Models on Collective Opinion Dynamics
AU  - Li, Chao
AU  - Su, Xing
AU  - Han, Haoying
AU  - Xue, Cong
AU  - Zheng, Chunmo
AU  - Fan, Chao
AB  - The process of opinion expression and exchange is a critical component of democratic societies. As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned. However, the knowledge about how LLMs affect the process of opinion expression and exchange of social opinion networks is very limited. Here, we create an opinion network dynamics model to encode the opinions of LLMs, cognitive acceptability and usage strategies of individuals, and simulate the impact of LLMs on opinion dynamics in a variety of scenarios. The outcomes of the simulations inform about effective demand-oriented opinion network interventions. The results from this study suggested that the output opinion of LLMs has a unique and positive effect on the collective opinion difference. The marginal effect of cognitive acceptability on collective opinion formation is nonlinear and shows a decreasing trend. When people partially rely on LLMs, the exchange process of opinion becomes more intense and the diversity of opinion becomes more favorable. In fact, there is 38.6% more opinion diversity when people all partially rely on LLMs, compared to prohibiting the use of LLMs entirely. The optimal diversity of opinion was found when the fractions of people who do not use, partially rely on, and fully rely on LLMs reached roughly 4:12:1. Our experiments also find that introducing extra agents with opposite/neutral/random opinions, we can effectively mitigate the impact of biased/toxic output from LLMs. Our findings provide valuable insights into opinion dynamics in the age of LLMs, highlighting the need for customized interventions tailored to specific scenarios to address the drawbacks of improper output and use of LLMs.
DA  - 2024///
PY  - 2024
DO  - 10.2139/ssrn.4688547
LA  - en
UR  - https://papers.ssrn.com/abstract=4688547
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:03:53</p>
KW  - intervention strategies
KW  - Large Language Models
KW  - Opinion dynamics
ER  - 

TY  - JOUR
TI  - Readers’ perception of computer-generated news: Credibility, expertise, and readability
AU  - Graefe, Andreas
AU  - Haim, Mario
AU  - Haarmann, Bastian
AU  - Brosius, Hans-Bernd
T2  - Journalism
AB  - We conducted an online experiment to study people’s perception of automated computer-written news. Using a 2 × 2 × 2 design, we varied the article topic (sports, finance; within-subjects) and both the articles’ actual and declared source (human-written, computer-written; between-subjects). Nine hundred eighty-six subjects rated two articles on credibility, readability, and journalistic expertise. Varying the declared source had small but consistent effects: subjects rated articles declared as human written always more favorably, regardless of the actual source. Varying the actual source had larger effects: subjects rated computer-written articles as more credible and higher in journalistic expertise but less readable. Across topics, subjects’ perceptions did not differ. The results provide conservative estimates for the favorability of computer-written news, which will further increase over time and endorse prior calls for establishing ethics of computer-written news.
DA  - 2018///
PY  - 2018
DO  - 10.1177/1464884916641269
VL  - 19
IS  - 5
SP  - 595
EP  - 610
J2  - Journalism
LA  - EN
SN  - 1464-8849
ST  - Readers’ perception of computer-generated news
UR  - https://doi.org/10.1177/1464884916641269
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:03:56</p>
N1  - <p>Publisher: SAGE Publications</p>
ER  - 

TY  - CONF
TI  - Reading machine-written news: Effect of machine heuristic and novelty on hostile media perception
AU  - Liu, B.
AU  - Wei, L.
T2  - Human-Computer Interaction. Theories, Methods, and Human Issues
AB  - The use of artificial intelligence (AI) in news production has been increasing these days. Its inanimate nature might bring opportunities to suppress hostile media effect by reducing readers’ emotional involvement. This study conducted an online experiment (N = 175) to examine how the identity of news writers, human vs. robot, might impact readers’ perceptions of news and source credibility. Findings suggest that readers holding machine heuristic, i.e., those who saw machine as free of intention, experienced less emotional involvement when the news was purported to be written by a news writing algorithm, as compared with when reading human-written news. Lower level of emotional involvement further led to less perception of bias in the news and the extremity of news slant. However, perceived novelty associated with robot news writer, although enhanced positive perceptions of the news, intensified readers’ emotional involvement, which further heightened hostile media perceptions. Findings in this study identified the mechanisms underlying effects of AI writer in the context of controversial news topics. Implications are discussed under theoretical frameworks of general information processing, hostile media effect and MAIN model.
C1  - Springer
C3  - Human-Computer Interaction. Theories, Methods, and Human Issues
DA  - 2018///
PY  - 2018
N1  - <p>Date Added: 2025-08-28 12:02:43</p>
ER  - 

TY  - JOUR
TI  - Reading, commenting and sharing of fake news: How online bandwagons and bots dictate user engagement
AU  - Molina, M. D.
AU  - Wang, J.
AU  - Sundar, S. S.
AU  - Le, T.
AU  - DiRusso, C.
T2  - Communication Research
AB  - Do social media users read, comment, and share false news more than real news? Does it matter if the story is written by a bot and whether it is endorsed by many others? We conducted a selective-exposure experiment (N = 171) to answer these questions. Results showed that real articles were more likely to receive “likes” whereas false articles were more likely to receive comments. Users commented more on a bot-written article when it received fewer likes. We explored the psychological mechanisms underlying these findings in Study 2 (N = 284). Data indicate that users’ engagement with online news is largely driven by emotions elicited by news content and heuristics triggered by interface cues, such that curiosity increases consumption of real news, whereas uneasiness triggered by a high number of “likes” encourages comments on fake news.
DA  - 2022///
PY  - 2022
DO  - 10.1177/00936502211073398
J2  - Communication Research
N1  - <p>Date Added: 2025-08-28 12:02:43</p>
ER  - 

TY  - JOUR
TI  - Red Teaming Language Models with Language Models
AU  - Perez, Ethan
AU  - Huang, Saffron
AU  - Song, Francis
AU  - Cai, Trevor
AU  - Ring, Roman
AU  - Aslanides, John
AU  - Glaese, Amelia
AU  - McAleese, Nat
AU  - Irving, Geoffrey
AB  - Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2202.03286
UR  - http://arxiv.org/abs/2202.03286
Y2  - 2025/08/28/
N1  - <p>arXiv:2202.03286 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:05:03</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Repelled at first sight? Expectations and intentions of job-seekers reading about AI selection in job advertisements
AU  - Wesche, Jenny S.
AU  - Sonderegger, Andreas
T2  - Computers in Human Behavior
AB  - Artificial intelligence (AI) is increasingly used in personnel selection to automate decision-making. Initial evidence points to negative effects of automating these procedures on applicant experiences. However, the effect of the prospect of automated procedures on job-seekers’ pre-process perceptions (e.g., organizational attractiveness) and intentions (to apply for the advertised job) is still unclear. We conducted three experiments (Study 1 and Study 2 as within-subjects designs, Study 3 as a between-subjects design; N1 = 36, N2 = 44, N3 = 172) systematically varying the information in job advertisements on the automation of different stages of the selection process (Study 1: screening stage conducted by a human vs. a non-specified agent vs. an AI; Study 2 and Study 3: human screening and human interview vs. AI screening and human interview vs. AI screening and AI interview). Results showed small negative effects of screening conducted by an AI vs. a human (Study 1, Study 2, Study 3), but stronger negative effects when also interviews were conducted by an AI vs. a human (Study 2, Study3) on job-seekers pre-process expectations, perceptions, and intentions. Possible reasons for these effects are discussed with special consideration of the different stages of the recruiting and selection process and explored with a qualitative approach in Study 2.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.chb.2021.106931
VL  - 125
SP  - 106931
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - Repelled at first sight?
UR  - https://www.sciencedirect.com/science/article/pii/S0747563221002545
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:05:10</p>
KW  - AI-Recruiting
KW  - Algorithmic decision-making
KW  - Automated selection procedures
KW  - Digital selection procedures
KW  - e-recruiting
KW  - Personnel selection
ER  - 

TY  - JOUR
TI  - Resistance to Medical Artificial Intelligence
AU  - Longoni, Chiara
AU  - Bonezzi, Andrea
AU  - Morewedge, Carey K
T2  - Journal of Consumer Research
AB  - Artificial intelligence (AI) is revolutionizing healthcare, but little is known about consumer receptivity to AI in medicine. Consumers are reluctant to utilize healthcare provided by AI in real and hypothetical choices, separate and joint evaluations. Consumers are less likely to utilize healthcare (study 1), exhibit lower reservation prices for healthcare (study 2), are less sensitive to differences in provider performance (studies 3A–3C), and derive negative utility if a provider is automated rather than human (study 4). Uniqueness neglect, a concern that AI providers are less able than human providers to account for consumers’ unique characteristics and circumstances, drives consumer resistance to medical AI. Indeed, resistance to medical AI is stronger for consumers who perceive themselves to be more unique (study 5). Uniqueness neglect mediates resistance to medical AI (study 6), and is eliminated when AI provides care (a) that is framed as personalized (study 7), (b) to consumers other than the self (study 8), or (c) that only supports, rather than replaces, a decision made by a human healthcare provider (study 9). These findings make contributions to the psychology of automation and medical decision making, and suggest interventions to increase consumer acceptance of AI in medicine.
DA  - 2019///
PY  - 2019
DO  - 10.1093/jcr/ucz013
VL  - 46
IS  - 4
SP  - 629
EP  - 650
J2  - Journal of Consumer Research
SN  - 0093-5301
UR  - https://doi.org/10.1093/jcr/ucz013
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:05:07</p>
ER  - 

TY  - CONF
TI  - Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech
AU  - Plaza-del-arco, Flor Miriam
AU  - Nozza, Debora
AU  - Hovy, Dirk
T2  - The 7th Workshop on Online Abuse and Harms (WOAH)
A2  - Chung, Yi-ling
A2  - R{\textbackslash”ottger}, Paul
A2  - Nozza, Debora
A2  - Talat, Zeerak
A2  - Mostafazadeh Davani, Aida
AB  - Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.
C1  - Toronto, Canada
C3  - The 7th Workshop on Online Abuse and Harms (WOAH)
DA  - 2023///
PY  - 2023
DO  - 10.18653/v1/2023.woah-1.6
SP  - 60
EP  - 68
PB  - Association for Computational Linguistics
ST  - Respectful or Toxic?
UR  - https://aclanthology.org/2023.woah-1.6/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:05:20</p>
ER  - 

TY  - JOUR
TI  - Rethinking the Rhetorical Epistemics of Gaslighting
AU  - Graves, Clint G
AU  - Spencer, Leland G
T2  - Communication Theory
AB  - Gaslighting is defined as a dysfunctional communication dynamic in which one interlocutor attempts to destabilize another’s sense of reality. In this article, we advance a model of gaslighting based in an epistemic rhetoric perspective. Our model directs attention to the rhetorics used to justify competing knowledge claims, as opposed to philosophical models that tend to rely on objective truth-value. We probe the discursive manifestations of gaslighting in logocentric, ethotic, or pathemic terms. We then apply our model to explain sexist and racist gaslighting that derives power from normatively instantiated discourses of rape culture and White supremacy. Specifically, our analysis identifies the appeal structures used to legitimate such gaslighting in response to disclosures of sexual violence and testimony about racial injustice.
DA  - 2022///
PY  - 2022
DO  - 10.1093/ct/qtab013
VL  - 32
IS  - 1
SP  - 48
EP  - 67
J2  - Communication Theory
SN  - 1468-2885
UR  - https://doi.org/10.1093/ct/qtab013
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:05:22</p>
ER  - 

TY  - CONF
TI  - Robot Eyes Wide Shut: Understanding Dishonest Anthropomorphism
AU  - Leong, Brenda
AU  - Selinger, Evan
T2  - Proceedings of the Conference on Fairness, Accountability, and Transparency
T3  - FAT* '19
AB  - The goal of this paper is to advance design, policy, and ethics scholarship on how engineers and regulators can protect consumers from deceptive robots and artificial intelligences that exhibit the problem of dishonest anthropomorphism. The analysis expands upon ideas surrounding the principle of honest anthropomorphism originally formulated by Margot Kaminsky, Mathew Ruben, William D. Smart, and Cindy M. Grimm in their groundbreaking Maryland Law Review article, "Averting Robot Eyes." Applying boundary management theory and philosophical insights into prediction and perception, we create a new taxonomy that identifies fundamental types of dishonest anthropomorphism and pinpoints harms that they can cause. To demonstrate how the taxonomy can be applied as well as clarify the scope of the problems that it can cover, we critically consider a representative series of ethical issues, proposals, and questions concerning whether the principle of honest anthropomorphism has been violated.
C1  - New York, NY, USA
C3  - Proceedings of the Conference on Fairness, Accountability, and Transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287591
SP  - 299
EP  - 308
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
ST  - Robot Eyes Wide Shut
UR  - https://doi.org/10.1145/3287560.3287591
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:02:45</p>
ER  - 

TY  - JOUR
TI  - Role play with large language models
AU  - Shanahan, Murray
AU  - McDonell, Kyle
AU  - Reynolds, Laria
T2  - Nature
AB  - As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.
DA  - 2023///
PY  - 2023
DO  - 10.1038/s41586-023-06647-8
VL  - 623
IS  - 7987
SP  - 493
EP  - 498
J2  - Nature
LA  - en
SN  - 1476-4687
UR  - https://www.nature.com/articles/s41586-023-06647-8
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:05:29</p>
N1  - <p>Publisher: Nature Publishing Group</p>
N1  - <p>Rights: 2023 Springer Nature Limited</p>
KW  - Computer science
KW  - Philosophy
ER  - 

TY  - JOUR
TI  - Same benefits, different communication patterns: Comparing Children's reading with a conversational agent vs. a human partner
AU  - Xu, Ying
AU  - Wang, Dakuo
AU  - Collins, Penelope
AU  - Lee, Hyelim
AU  - Warschauer, Mark
T2  - Computers & Education
AB  - Storybook reading accompanied by adult-guided conversation provides a stimulating context for children's language development. Conversational agents powered by artificial intelligence, such as smart speakers, are prevalent in children's homes and have the potential to engage children in storybook reading as language partners. However, little research has explored the effectiveness of using conversational agents to support children's language development. This study examined how an automated conversational agent can read stories to children via a smart speaker while asking questions and providing contingent feedback. Using a randomized experiment among 90 children aged three to six years, this study compared these children's story comprehension and verbal engagement in storybook reading with a conversational agent versus an adult. The conversational agent's guided conversation was found to be as supportive in improving children's story comprehension as that provided by an adult language partner. At the same time, this study uncovered a number of differences in children's verbal engagement when interacting with a conversational agent versus with an adult. Specifically, children who read with the conversational agent responded to questions with better intelligibility, whereas those who read with an adult responded to questions with higher productivity, lexical diversity, and topical relevance. And the two groups responded to questions with a similar level of accuracy. In addition, questions requiring high cognitive demand amplified the differences in of verbal engagement between the conversational agent and adult partner. The study offers important implications for developing and researching conversational agent systems to support children's language development.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.compedu.2020.104059
VL  - 161
SP  - 104059
J2  - Computers & Education
SN  - 0360-1315
ST  - Same benefits, different communication patterns
UR  - https://www.sciencedirect.com/science/article/pii/S0360131520302578
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:05:31</p>
KW  - Communication
KW  - Conversational agents
KW  - Language development
KW  - Storybook reading
KW  - Young children
ER  - 

TY  - JOUR
TI  - Scalable agent alignment via reward modeling: a research direction
AU  - Leike, Jan
AU  - Krueger, David
AU  - Everitt, Tom
AU  - Martic, Miljan
AU  - Maini, Vishal
AU  - Legg, Shane
AB  - One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.
DA  - 2018///
PY  - 2018
DO  - 10.48550/arXiv.1811.07871
ST  - Scalable agent alignment via reward modeling
UR  - http://arxiv.org/abs/1811.07871
Y2  - 2025/08/28/
N1  - <p>arXiv:1811.07871 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:05:42</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
KW  - Statistics - Machine Learning
ER  - 

TY  - JOUR
TI  - See Something, Say Something: Correction of Global Health Misinformation on Social Media
AU  - Bode, Leticia
AU  - Vraga, Emily K.
T2  - Health Communication
AB  - Social media are often criticized for being a conduit for misinformation on global health issues, but may also serve as a corrective to false information. To investigate this possibility, an experiment was conducted exposing users to a simulated Facebook News Feed featuring misinformation and different correction mechanisms (one in which news stories featuring correct information were produced by an algorithm and another where the corrective news stories were posted by other Facebook users) about the Zika virus, a current global health threat. Results show that algorithmic and social corrections are equally effective in limiting misperceptions, and correction occurs for both high and low conspiracy belief individuals. Recommendations for social media campaigns to correct global health misinformation, including encouraging users to refute false or misleading health information, and providing them appropriate sources to accompany their refutation, are discussed.
DA  - 2018///
PY  - 2018
DO  - 10.1080/10410236.2017.1331312
VL  - 33
IS  - 9
SP  - 1131
EP  - 1140
J2  - Health Communication
SN  - 1041-0236
ST  - See Something, Say Something
UR  - https://doi.org/10.1080/10410236.2017.1331312
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:06:21</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/10410236.2017.1331312 PMID: 28622038</p>
ER  - 

TY  - JOUR
TI  - Self-critiquing models for assisting human evaluators
AU  - Saunders, William
AU  - Yeh, Catherine
AU  - Wu, Jeff
AU  - Bills, Steven
AU  - Ouyang, Long
AU  - Ward, Jonathan
AU  - Leike, Jan
AB  - We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2206.05802
UR  - http://arxiv.org/abs/2206.05802
Y2  - 2025/08/28/
N1  - <p>arXiv:2206.05802 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:06:23</p>
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Sentiment Adaptive End-to-End Dialog Systems
AU  - Shi, Weiyan
AU  - Yu, Zhou
AB  - End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.
DA  - 2019///
PY  - 2019
DO  - 10.48550/arXiv.1804.10731
UR  - http://arxiv.org/abs/1804.10731
Y2  - 2025/08/28/
N1  - <p>arXiv:1804.10731 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:06:27</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Should I Trust the Artificial Intelligence to Recruit? Recruiters’ Perceptions and Behavior When Faced With Algorithm-Based Recommendation Systems During Resume Screening
AU  - Lacroux, Alain
AU  - Martin-Lacroux, Christelle
T2  - Frontiers in Psychology
AB  - Resume screening assisted by decision support systems that incorporate artificial intelligence is currently undergoing a strong development in many organizations, raising technical, managerial, legal and ethical issues. The purpose of the present paper is to better understand the reactions of recruiters when they are offered algorithm-based recommendations during resume screening.Two polarized attitudes have been identified in the literature on users' reactions to algorithm-based recommendations: algorithm aversion, which reflects a general distrust and preference for human recommendations; and automation bias, which corresponds to an overconfidence in the decisions or recommendations made by algorithmic decision support systems (ADSS). Drawing on results obtained in the field of automated decision support areas, we make the general hypothesis that recruiters trust human experts more than ADSS, because they distrust algorithms for subjective decisions such as recruitment. An experiment on résumé screening was conducted on a sample of professionals (N=694) involved in the screening of job applications. They were asked to study a job offer, then evaluate two fictitious résumés in a 2×2 factorial design with manipulation of the type of recommendation (no recommendation/algorithmic recommendation/human expert recommendation) and of the consistency of the recommendations (consistent vs. inconsistent recommendation).Our results support the general hypothesis of preference for human recommendations: recruiters exhibit a higher level of trust toward human expert recommendations compared with algorithmic recommendations. However, we also found that recommendation’s consistence has a differential and unexpected impact on decisions: in the presence of an inconsistent algorithmic recommendation, recruiters favored the unsuitable over the suitable résumé. Our results also show that specific personality traits (extraversion, neuroticism and self-confidence) are associated with a differential use of algorithmic recommendations. Implications for research and HR policies are finally discussed.
DA  - 2022///
PY  - 2022
DO  - 10.3389/fpsyg.2022.895997
VL  - 13
J2  - Frontiers in Psychology
LA  - English
SN  - 1664-1078
ST  - Should I Trust the Artificial Intelligence to Recruit?
UR  - https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.895997/full
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:06:29</p>
N1  - <p>Publisher: Frontiers</p>
KW  - Algorithm aversion
KW  - Artificial Inteligence-AI
KW  - automation bias
KW  - Resume screening
KW  - Trust
ER  - 

TY  - JOUR
TI  - Simple synthetic data reduces sycophancy in large language models
AU  - Wei, Jerry
AU  - Huang, Da
AU  - Lu, Yifeng
AU  - Zhou, Denny
AU  - Le, Quoc V.
AB  - Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior. First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well. To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP tasks and encourages models to be robust to user opinions on these tasks. Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts. Code for generating synthetic data for intervention can be found at https://github.com/google/sycophancy-intervention.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arXiv.2308.03958
UR  - http://arxiv.org/abs/2308.03958
Y2  - 2025/08/28/
N1  - <p>arXiv:2308.03958 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:06:31</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Social Touch in Human–Robot Interaction: Robot-Initiated Touches can Induce Positive Responses without Extensive Prior Bonding
AU  - Willemse, Christian J. A. M.
AU  - van Erp, Jan B. F.
T2  - International Journal of Social Robotics
AB  - Social touch interactions can, depending on the type and strength of the dyadic social relationship, elicit a plethora of physiological, emotional, and behavioral responses; both beneficial and disadvantageous. With the intention to expand the communicative capabilities of humanoid social robots, we investigated whether robot-initiated touches could elicit beneficial responses in the human user that are comparable to responses to human touch. In addition, we investigated whether having a pre-existing positive social bond with the robot modulates these responses. To this end, we conducted a $$2 \times 2$$between subjects experiment (N = 67) in which participants either did or did not establish a bond with the robot prior to interacting with it during stressful circumstances. This interaction either did or did not comprise robot-initiated touches. We hypothesized that robotic touches would attenuate the subjective and physiological stress responses during the stressful event (H1a), enhance the perceived relation with the robot (H1b), and increase one’s pro-social behavior (H1c), as contrasted with interactions without touch. Based on findings from human touch, we also expected that the effects of H1a and H1b would be more outspoken when a bond with the robot was established (H2). Our findings imply that robotic touches attenuated physiological stress responses and increased the perceived intimacy of the human–robot bond. No effects were found on pro-social behavior and all effects were independent of whether a bond was formed or not. Although no full support for our hypotheses was found, the findings suggest that robot-initiated touch can, under specific circumstances, be a valuable extension of a social robot’s nonverbal communication repertoire.
DA  - 2019///
PY  - 2019
DO  - 10.1007/s12369-018-0500-9
VL  - 11
IS  - 2
SP  - 285
EP  - 304
J2  - International Journal of Social Robotics
LA  - en
SN  - 1875-4805
ST  - Social Touch in Human–Robot Interaction
UR  - https://doi.org/10.1007/s12369-018-0500-9
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:06:34</p>
KW  - Human–robot bonding
KW  - Human–robot interaction
KW  - Social touch
KW  - Stress-attenuation
ER  - 

TY  - JOUR
TI  - Sociotechnical Safety Evaluation of Generative AI Systems
AU  - Weidinger, Laura
AU  - Rauh, Maribeth
AU  - Marchal, Nahema
AU  - Manzini, Arianna
AU  - Hendricks, Lisa Anne
AU  - Mateos-Garcia, Juan
AU  - Bergman, Stevie
AU  - Kay, Jackie
AU  - Griffin, Conor
AU  - Bariach, Ben
AU  - Gabriel, Iason
AU  - Rieser, Verena
AU  - Isaac, William
AB  - Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2310.11986
UR  - http://arxiv.org/abs/2310.11986
Y2  - 2025/08/28/
N1  - <p>arXiv:2310.11986 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:06:35</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - JOUR
TI  - Source Credibility Matters: Does Automated Journalism Inspire Selective Exposure?
AU  - Jia, Chenyan
AU  - Johnson, Thomas J.
T2  - International Journal of Communication
AB  - To examine whether selective exposure occurs when people read news attributed to an algorithm author, this study conducted a 2 (author attribution: human or algorithm) × 3 (article attitude: attitude-consistent news, attitude-challenging news, or neutral story) × 2 (article topic: gun control or abortion) mixed-design online experiment (N = 351). By experimentally manipulating the attribution of authorship, this study found that selective exposure and selective avoidance were practiced when the news article was declared to be written by algorithms. Results revealed that people were more likely to select attitude-consistent news rather than attitude-challenging news and rated attitude consistent news stories as more credible than attitude challenging news for stories purportedly written by both algorithms and human journalists. For attitude-consistent gun-rights stories, people were more likely to expose themselves to human attribution stories rather than algorithmic attribution stories. Results also showed that source credibility partially mediated the influence of issue partisanship on selective exposure for gun stories. This study bears implications on the selective exposure theory and the emerging field of automated journalism.
DA  - 2021///
PY  - 2021
VL  - 15
IS  - 0
SP  - 22
J2  - International Journal of Communication
LA  - en
SN  - 1932-8036
ST  - Source Credibility Matters
UR  - https://ijoc.org/index.php/ijoc/article/view/16546
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:08:18</p>
N1  - <p>Rights: Copyright (c)</p>
KW  - algorithm
KW  - automated journalism
KW  - message credibility
KW  - selective avoidance
KW  - selective exposure
KW  - source credibility
ER  - 

TY  - JOUR
TI  - Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks
AU  - Levinstein, B. A.
AU  - Herrmann, Daniel A.
T2  - Philosophical Studies
AB  - We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.
DA  - 2025///
PY  - 2025
DO  - 10.1007/s11098-023-02094-3
VL  - 182
IS  - 7
SP  - 1539
EP  - 1565
J2  - Philosophical Studies
SN  - 0031-8116, 1573-0883
ST  - Still No Lie Detector for Language Models
UR  - http://arxiv.org/abs/2307.00175
Y2  - 2025/08/28/
N1  - <p>arXiv:2307.00175 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:06:39</p>
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Strategies of othering through discursive practices: Examples from the UK and Poland
AU  - Strani, Katerina
AU  - Szczepaniak-Kozak, Anna
T2  - Lodz Papers in Pragmatics
AB  - This article discusses findings of a qualitative study on strategies of othering observed in anti-immigrant discourse, by analysing selected examples from the UK and Polish media, together with data collected from interviews with migrants. The purpose is to identify discursive strategies of othering, which aim to categorise, denigrate, oppress and ultimately reject the stigmatised or racialised ‘other’. We do not offer a systematic comparison of the data from the UK and Poland; instead, we are interested in what is common in the discursive practices of these two countries/contexts. In using newspaper together with interview data, we are combining representation and experience in identifying not only strategies of othering, but also how these are perceived by and affect the othered individuals. The paper uses the following data: 40 newspaper articles – 20 from the UK and 20 from Poland, and 19 interviews – 12 from Poland and 7 from the UK. The analysis that follows identifies five shared strategies of othering: a) Stereotyping; b) Whiteness as the norm; c) Racialisation; d) Objectification; e) Wrongly Ascribed Ethnicity. We conclude with the research limitations and outlining possible next stages, such as working with a larger corpus, investigating frequency, or including other media genres.
DA  - 2018///
PY  - 2018
DO  - 10.1515/lpp-2018-0008
VL  - 14
IS  - 1
SP  - 163
EP  - 179
J2  - Lodz Papers in Pragmatics
LA  - en
SN  - 1898-4436
ST  - Strategies of othering through discursive practices
UR  - https://www.degruyterbrill.com/document/doi/10.1515/lpp-2018-0008/html
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:06:40</p>
N1  - <p>Publisher: De Gruyter Mouton</p>
N1  - <p>Rights: De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.</p>
KW  - discursive strategies
KW  - othering
KW  - Poland
KW  - racialisation
KW  - stereotyping
KW  - UK
ER  - 

TY  - JOUR
TI  - Strengthening the EU AI Act: Defining Key Terms on AI Manipulation
AU  - Franklin, Matija
AU  - Tomei, Philip Moreira
AU  - Gorman, Rebecca
AB  - The European Union's Artificial Intelligence Act aims to regulate manipulative and harmful uses of AI, but lacks precise definitions for key concepts. This paper provides technical recommendations to improve the Act's conceptual clarity and enforceability. We review psychological models to define "personality traits," arguing the Act should protect full "psychometric profiles." We urge expanding "behavior" to include "preferences" since preferences causally influence and are influenced by behavior. Clear definitions are provided for "subliminal," "manipulative," and "deceptive" techniques, considering incentives, intent, and covertness. We distinguish "exploiting individuals" from "exploiting groups," emphasising different policy needs. An "informed decision" is defined by four facets: comprehension, accurate information, no manipulation, and understanding AI's influence. We caution the Act's therapeutic use exemption given the lack of regulation of digital therapeutics by the EMA. Overall, the recommendations strengthen definitions of vague concepts in the EU AI Act, enhancing precise applicability to regulate harmful AI manipulation.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2308.16364
ST  - Strengthening the EU AI Act
UR  - http://arxiv.org/abs/2308.16364
Y2  - 2025/08/28/
N1  - <p>arXiv:2308.16364 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:06:43</p>
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - JOUR
TI  - Supervising strong learners by amplifying weak experts
AU  - Christiano, Paul
AU  - Shlegeris, Buck
AU  - Amodei, Dario
AB  - Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.
DA  - 2018///
PY  - 2018
DO  - 10.48550/arXiv.1810.08575
UR  - http://arxiv.org/abs/1810.08575
Y2  - 2025/08/28/
N1  - <p>arXiv:1810.08575 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:06:45</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - JOUR
TI  - Susceptibility to Influence of Large Language Models
AU  - Griffin, Lewis D.
AU  - Kleinberg, Bennett
AU  - Mozes, Maximilian
AU  - Mai, Kimberly T.
AU  - Vau, Maria
AU  - Caldwell, Matthew
AU  - Marvor-Parker, Augustine
AB  - Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2303.06074
UR  - http://arxiv.org/abs/2303.06074
Y2  - 2025/08/28/
N1  - <p>arXiv:2303.06074 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:06:48</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Technology, autonomy, and manipulation
AU  - Susser, D.
AU  - Roessler, B.
AU  - Nissenbaum, H.
T2  - Internet Policy Review
AB  - Since 2016, when the Facebook/Cambridge Analytica scandal began to emerge, public concern has grown around the threat of “online manipulation”. While these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers—first, by defining “online manipulation”, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. We argue that online manipulation is the use of information technology to covertly influence another person’s decision-making, by targeting and exploiting their decision-making vulnerabilities. Engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. We explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.
DA  - 2019///
PY  - 2019
DO  - 10.14763/2019.2.1410
VL  - 8
IS  - 2
J2  - Internet Policy Review
SN  - 2197-6775
UR  - https://policyreview.info/node/1410
N1  - <p>Date Added: 2025-08-28 12:02:41</p>
ER  - 

TY  - JOUR
TI  - The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry
AU  - Shin, Minkyu
AU  - Kim, Jin
AU  - Shin, Jiwoong
AB  - Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. We analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We find that LLM usage is associated with an increased likelihood of obtaining relief from financial firms. To investigate this relationship, we employ an instrumental variable approach to mitigate endogeneity concerns around LLM adoption. Although instrumental variables suggest a potential causal link, they cannot fully capture all unobserved heterogeneity. To further establish this causal relationship, we conducted controlled experiments, which support that LLMs can enhance the clarity and persuasiveness of consumer narratives, thereby increasing the likelihood of obtaining relief. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.
DA  - 2025///
PY  - 2025
DO  - 10.48550/arXiv.2311.16466
ST  - The Adoption and Efficacy of Large Language Models
UR  - http://arxiv.org/abs/2311.16466
Y2  - 2025/08/28/
N1  - <p>arXiv:2311.16466 [cs]</p>
N1  - <p>Date Added: 2025-08-28 12:41:41</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - JOUR
TI  - The Anchoring Effect, Algorithmic Fairness, and the Limits of Information Transparency for Emotion Artificial Intelligence
AU  - Rhue, Lauren
T2  - Information Systems Research
AB  - Emotion artificial intelligence (AI) or emotion recognition AI may systematically vary in its recognition of facial expressions and emotions across demographic groups, creating inconsistencies and disparities in its scoring. This paper explores the extent to which individuals can compensate for these disparities and inconsistencies in emotion AI considering two opposing factors; although humans evolved to recognize emotions, particularly happiness, they are also subject to cognitive biases, such as the anchoring effect. To help understand these dynamics, this study tasks three commercially available emotion AIs and a group of human labelers to identify emotions from faces in two image data sets. The scores generated by emotion AI and human labelers are examined for inference inconsistencies (i.e., misalignment between facial expression and emotion label). The human labelers are also provided with the emotion AI’s scores and with measures of its scoring fairness (or lack thereof). We observe that even when human labelers are operating in this context of information transparency, they may still rely on the emotion AI’s scores, perpetuating its inconsistencies. Several findings emerge from this study. First, the anchoring effect appears to be moderated by the type of inference inconsistency and is weaker for easier emotion recognition tasks. Second, when human labelers are provided with information transparency regarding the emotion AI’s fairness, the effect is not uniform across emotions. Also, there is no evidence that information transparency leads to the selective anchoring necessary to offset emotion AI disparities; in fact, some evidence suggests that information transparency increases human inference inconsistencies. Lastly, the different models of emotion AI are highly inconsistent in their scores, raising doubts about emotion AI more generally. Collectively, these findings provide evidence of the potential limitations of addressing algorithmic bias through individual decisions, even when those individuals are supported with information transparency. History: Alessandro Acquisti, Senior Editor; Monideepa Tarafdar, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2019.0493 .
DA  - 2024///
PY  - 2024
DO  - 10.1287/isre.2019.0493
VL  - 35
IS  - 3
SP  - 1479
EP  - 1496
J2  - Information Systems Research
SN  - 1047-7047
UR  - https://pubsonline.informs.org/doi/10.1287/isre.2019.0493
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:10:13</p>
N1  - <p>Publisher: INFORMS</p>
KW  - affective AI
KW  - algorithmic bias
KW  - algorithmic fairness
KW  - artificial intelligence
KW  - emotion AI
KW  - emotion recognition
KW  - fairness
KW  - information transparency
KW  - the anchoring effect
ER  - 

TY  - JOUR
TI  - The Attenuating Effect of Intelligent Agents and Agent Autonomy on Managers' Ability to Diffuse Responsibility for and Engage in Earnings Management
AU  - Kipp, Peter C.
AU  - Curtis, Mary B.
AU  - Li, Ziyin
T2  - Accounting Horizons
AB  - Advances in IT suggest that computerized intelligent agents (IAs) may soon occupy many roles that presently employ human agents. A significant concern is the ethical conduct of those who use IAs, including their possible utilization by managers to engage in earnings management. We investigate how financial reporting decisions are affected when they are supported by the work of an IA versus a human agent, with varying autonomy. In an experiment with experienced managers, we vary agent type (human versus IA) and autonomy (more versus less), finding that managers engage in less aggressive financial reporting decisions with IAs than with human agents, and engage in less aggressive reporting decisions with less autonomous agents than with more autonomous agents. Managers' perception of control over their agent and ability to diffuse their own responsibility for financial reporting decisions explain the effect of agent type and autonomy on managers' financial reporting decisions.
DA  - 2020///
PY  - 2020
DO  - 10.2308/HORIZONS-19-133
VL  - 34
IS  - 4
SP  - 143
EP  - 164
J2  - Accounting Horizons
SN  - 0888-7993
UR  - https://doi.org/10.2308/HORIZONS-19-133
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:10:18</p>
ER  - 

TY  - JOUR
TI  - The autonomous choice architect
AU  - Mills, Stuart
AU  - Sætra, Henrik Skaug
T2  - AI & SOCIETY
AB  - Choice architecture describes the environment in which choices are presented to decision-makers. In recent years, public and private actors have looked at choice architecture with great interest as they seek to influence human behaviour. These actors are typically called choice architects. Increasingly, however, this role of architecting choice is not performed by a human choice architect, but an algorithm or artificial intelligence, powered by a stream of Big Data and infused with an objective it has been programmed to maximise. We call this entity the autonomous choice architect. In this paper, we present an account of why artificial intelligence can fulfil the role of a choice architect and why this creates problems of transparency, responsibility and accountability for nudges. We argue that choice architects, be them autonomous computational systems or human-beings, at a most basic level select, from a range of designs, the design which is most likely to maximise a pre-determined objective. We then proceed to argue that, given the growing demand for targeted, personalised choice architecture and for faster, dynamic reconfigurations of choice architecture, as well as the ever-expanding pool of data from which feedback can be drawn, the role of the human choice architect is increasingly obscured behind algorithmic, artificially intelligent systems. We provide a discussion of the implications of autonomous choice architects, focusing on the importance of the humans who programme these systems, ultimately arguing that despite technological advances, the responsibility of choice architecture and influence remains firmly one human beings must bear.
DA  - 2024///
PY  - 2024
DO  - 10.1007/s00146-022-01486-z
VL  - 39
IS  - 2
SP  - 583
EP  - 595
J2  - AI & SOCIETY
LA  - en
SN  - 1435-5655
UR  - https://doi.org/10.1007/s00146-022-01486-z
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:10:19</p>
KW  - Artificial intelligence
KW  - Behavioural science
KW  - Big data
KW  - Choice architecture
KW  - D9
KW  - D91
KW  - Nudge
ER  - 

TY  - CONF
TI  - The Effect of Multimodal Emotional Expression and Agent Appearance on Trust in Human-Agent Interaction
AU  - Torre, Ilaria
AU  - Carrigan, Emma
AU  - McDonnell, Rachel
AU  - Domijan, Katarina
AU  - McCabe, Killian
AU  - Harte, Naomi
T2  - Proceedings of the 12th ACM SIGGRAPH Conference on Motion, Interaction and Games
T3  - MIG '19
AB  - Emotional expressivity can boost trust in human-human and human-machine interaction. As a multimodal phenomenon, previous research argued that a mismatch in the expressive channels provides evidence of joint audio-video emotional processing. However, while previous work studied this from the point of view of emotion recognition and processing, not much is known about what effect a multimodal agent would have on a human-agent interaction task. Also, agent appearance could influence this interaction too. Here we manipulated the agent’s multimodal emotional expression (”smiling face” and ”smiling voice”, or both) and agent type (photorealistic or cartoon-like virtual human) and assessed people’s trust toward this agent. We measured trust using a mixed-methods approach, combining behavioural data from a survival task, questionnaire ratings and qualitative comments. These methods gave different results: while people commented on the importance of emotional expressivity in the agent’s voice, this factor had limited influence on trusting behaviours; while people rated the cartoon-like agent on several traits higher than the photorealistic one, the agent’s style also was not the most influential feature on people’s trusting behaviour. These results highlight the contribution of a mixed-methods approach in human-machine interaction, as both explicit and implicit perception and behaviour will contribute to the success of the interaction.
C1  - New York, NY, USA
C3  - Proceedings of the 12th ACM SIGGRAPH Conference on Motion, Interaction and Games
DA  - 2019///
PY  - 2019
DO  - 10.1145/3359566.3360065
SP  - 1
EP  - 6
PB  - Association for Computing Machinery
SN  - 978-1-4503-6994-7
UR  - https://doi.org/10.1145/3359566.3360065
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:10:25</p>
ER  - 

TY  - JOUR
TI  - The effect of source disclosure on evaluation of AI-generated messages: A two-part study
AU  - Lim, Sue
AU  - Schmälzle, Ralf
T2  - Computers in Human Behavior: Artificial Humans
AB  - Advancements in artificial intelligence (AI) over the last decade demonstrate that machines can exhibit communicative behavior and influence how humans think, feel, and behave. In fact, the recent development of ChatGPT has shown that large language models (LLMs) can be leveraged to generate high-quality communication content at scale and across domains, suggesting that they will be increasingly used in practice. However, many questions remain about how knowing the source of the messages influences recipients' evaluation of and preference for AI-generated messages compared to human-generated messages. This paper investigated this topic in the context of vaping prevention messaging. In Study 1, which was pre-registered, we examined the influence of source disclosure on people's evaluation of AI-generated health prevention messages compared to human-generated messages. We found that source disclosure (i.e., labeling the source of a message as AI vs. human) significantly impacted the evaluation of the messages but did not significantly alter message rankings. In a follow-up study (Study 2), we examined how the influence of source disclosure may vary by the participants' negative attitudes towards AI. We found a significant moderating effect of negative attitudes towards AI on message evaluation, but not for message selection. However, for those with moderate levels of negative attitudes towards AI, source disclosure decreased the preference for AI-generated messages. Overall, the results of this series of studies showed a slight bias against AI-generated messages once the source was disclosed, adding to the emerging area of study that lies at the intersection of AI and communication.
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.chbah.2024.100058
VL  - 2
IS  - 1
SP  - 100058
J2  - Computers in Human Behavior: Artificial Humans
SN  - 2949-8821
UR  - http://arxiv.org/abs/2311.15544
N1  - <p>Date Added: 2025-08-28 12:02:41</p>
ER  - 

TY  - JOUR
TI  - The Effects of Anthropomorphism on How People Evaluate Algorithm-Written News
AU  - Jang, Wonseok (Eric)
AU  - Chun, Jung Won
AU  - Kim, Soojin
AU  - Kang, Young Woo
T2  - Digital Journalism
AB  - Based on the Modality-Agency-Interactivity-Navigability model and the anthropomorphism theory, this study examines whether embedding human-like characteristics in algorithms increases the persuasiveness of algorithm-written news. This study further investigates how different types of relationships (servant or friend) that human writers form with algorithms determine the persuasiveness of algorithm-generated news. Experiment 1 demonstrated that participants who read the human- and humanized algorithm-written news showed greater emotional involvement in the stories and liked the articles more than those who read the algorithm-written news. The participants also reported that news written by humans, humanized algorithms, and algorithms, had equal news credibility. Experiment 2 further showed that the participants perceived the news as more credible and experienced greater degrees of emotional involvement when human writers formed a partner-to-friend relationship with humanized algorithms while generating news rather than a servant-to-master relationship.
DA  - 2023///
PY  - 2023
DO  - 10.1080/21670811.2021.1976064
VL  - 11
IS  - 1
SP  - 103
EP  - 124
J2  - Digital Journalism
SN  - 2167-0811
UR  - https://doi.org/10.1080/21670811.2021.1976064
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:11:04</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2021.1976064</p>
KW  - algorithm
KW  - anthropomorphism theory
KW  - Computational journalism
KW  - emotional involvement
KW  - news evaluation
ER  - 

TY  - JOUR
TI  - The General Attitudes towards Artificial Intelligence Scale (GAAIS): Confirmatory Validation and Associations with Personality, Corporate Distrust, and General Trust
AU  - Schepman, Astrid
AU  - Rodway, Paul
T2  - International Journal of Human–Computer Interaction
AB  - Acceptance of Artificial Intelligence (AI) may be predicted by individual psychological correlates, examined here. Study 1 reports confirmatory validation of the General Attitudes towards Artificial Intelligence Scale (GAAIS) following initial validation elsewhere. Confirmatory Factor Analysis confirmed the two-factor structure (Positive, Negative) and showed good convergent and divergent validity with a related scale. Study 2 tested whether psychological factors (Big Five personality traits, corporate distrust, and general trust) predicted attitudes towards AI. Introverts had more positive attitudes towards AI overall, likely because of algorithm appreciation. Conscientiousness and agreeableness were associated with forgiving attitudes towards negative aspects of AI. Higher corporate distrust led to negative attitudes towards AI overall, while higher general trust led to positive views of the benefits of AI. The dissociation between general trust and corporate distrust may reflect the public’s attributions of the benefits and drawbacks of AI. Results are discussed in relation to theory and prior findings.
DA  - 2023///
PY  - 2023
DO  - 10.1080/10447318.2022.2085400
VL  - 39
IS  - 13
SP  - 2724
EP  - 2741
J2  - International Journal of Human–Computer Interaction
SN  - 1044-7318
ST  - The General Attitudes towards Artificial Intelligence Scale (GAAIS)
UR  - https://doi.org/10.1080/10447318.2022.2085400
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:11:07</p>
N1  - <p>Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2022.2085400</p>
ER  - 

TY  - JOUR
TI  - The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets
AU  - Marks, Samuel
AU  - Tegmark, Max
AB  - Large Language Models (LLMs) have impressive capabilities, but are prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we use high-quality datasets of simple true/false statements to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that at sufficient scale, LLMs linearly represent the truth or falsehood of factual statements. We also show that simple difference-in-mean probes generalize as well as other probing techniques while identifying directions which are more causally implicated in model outputs.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arXiv.2310.06824
ST  - The Geometry of Truth
UR  - http://arxiv.org/abs/2310.06824
Y2  - 2025/08/28/
N1  - <p>arXiv:2310.06824 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:11:09</p>
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - JOUR
TI  - The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs
AU  - Ruis, Laura
AU  - Khan, Akbir
AU  - Biderman, Stella
AU  - Hooker, Sara
AU  - Rocktäschel, Tim
AU  - Grefenstette, Edward
AB  - Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context -- incorporating its pragmatics. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response "I wore gloves" to the question "Did you leave fingerprints?" as meaning "No". To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate four categories of widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), models in three of these categories perform close to random. However, LLMs instruction-tuned at the example-level perform significantly better. These results suggest that certain fine-tuning strategies are far better at inducing pragmatic understanding in models. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2210.14986
ST  - The Goldilocks of Pragmatic Understanding
UR  - http://arxiv.org/abs/2210.14986
Y2  - 2025/08/28/
N1  - <p>arXiv:2210.14986 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:11:11</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - The Internal State of an LLM Knows When It's Lying
AU  - Azaria, Amos
AU  - Mitchell, Tom
AB  - While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2304.13734
UR  - http://arxiv.org/abs/2304.13734
Y2  - 2025/08/27/
N1  - <p>Date Added: 2025-08-28 12:02:44</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - The persuasiveness of guilt appeals over time: Pathways to delayed compliance
AU  - Antonetti, Paolo
AU  - Baines, Paul
AU  - Jain, Shailendra
T2  - Journal of Business Research
AB  - Past research on guilt-elicitation in marketing does not examine how the communications' effects might persist over time, when there is a gap between advertising at time 1 and the time of choice consideration at time 2. This study explores the processes leading to delayed compliance through guilt-based communications. Guilt elicitation enhances transportation into the message, driving message compliance through the effect of transportation. Transportation explains the effects recorded several days after campaign exposure. The influence of transportation is mediated by two pathways: increases in anticipated guilt and perceived consumer effectiveness. The message type moderates the relevance of different pathways in explaining persuasiveness. Appeals delivered through a text and image message (rather than text only) are more effective in driving compliance and shape reactions via guilt anticipation. The study raises important implications for research on the use of guilt appeals and the design of more effective messages based on this emotion.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.jbusres.2018.03.030
VL  - 90
SP  - 14
EP  - 25
J2  - Journal of Business Research
SN  - 0148-2963
ST  - The persuasiveness of guilt appeals over time
UR  - https://www.sciencedirect.com/science/article/pii/S0148296318301589
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:11:15</p>
KW  - Anticipated guilt
KW  - Emotions
KW  - Guilt appeals
KW  - Guilt elicitation
KW  - Narrative transportation
KW  - Persuasion
ER  - 

TY  - JOUR
TI  - The Str(AI)ght Scoop: Artificial Intelligence Cues Reduce Perceptions of Hostile Media Bias
AU  - Cloudy, Joshua
AU  - Banks, Jaime
AU  - Bowman, Nicholas David
T2  - Digital Journalism
AB  - In the face of increasing public distrust for journalistic institutions, stories sourced from artificially intelligent (AI) journalists have the potential to lower hostile media bias by activating the machine heuristic—a mental shortcut assuming machines are more unbiased, systematic, and accurate than are humans. An online experiment targeting issue partisans found support for the prediction: a story presented as sourced from an AI journalist activated the machine heuristic that, in turn, mitigated hostile media bias. This mediation effect was moderated: perceived bias was more strongly reduced as partisan-attitude extremity increased.
DA  - 2023///
PY  - 2023
DO  - 10.1080/21670811.2021.1969974
VL  - 11
IS  - 9
SP  - 1577
EP  - 1596
J2  - Digital Journalism
SN  - 2167-0811
ST  - The Str(AI)ght Scoop
UR  - https://doi.org/10.1080/21670811.2021.1969974
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:12:33</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/21670811.2021.1969974</p>
KW  - AI journalism
KW  - automated journalism
KW  - hostile media bias
KW  - hostile media effect
KW  - human-computer interaction
KW  - Human-machine communication
KW  - machine agents
KW  - robot journalism
ER  - 

TY  - JOUR
TI  - Thumbs Up or Down: Consumer Reactions to Decisions by Algorithms Versus Humans
AU  - Yalcin, Gizem
AU  - Lim, Sarah
AU  - Puntoni, Stefano
AU  - van Osselaer, Stijn M.J.
T2  - Journal of Marketing Research
AB  - Although companies increasingly are adopting algorithms for consumer-facing tasks (e.g., application evaluations), little research has compared consumers’ reactions to favorable decisions (e.g., acceptances) versus unfavorable decisions (e.g., rejections) about themselves that are made by an algorithm versus a human. Ten studies reveal that, in contrast to managers’ predictions, consumers react less positively when a favorable decision is made by an algorithmic (vs. a human) decision maker, whereas this difference is mitigated for an unfavorable decision. The effect is driven by distinct attribution processes: it is easier for consumers to internalize a favorable decision outcome that is rendered by a human than by an algorithm, but it is easy to externalize an unfavorable decision outcome regardless of the decision maker type. The authors conclude by advising managers on how to limit the likelihood of less positive reactions toward algorithmic (vs. human) acceptances.
DA  - 2022///
PY  - 2022
DO  - 10.1177/00222437211070016
VL  - 59
IS  - 4
SP  - 696
EP  - 717
J2  - Journal of Marketing Research
LA  - EN
SN  - 0022-2437
ST  - Thumbs Up or Down
UR  - https://doi.org/10.1177/00222437211070016
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:12:28</p>
N1  - <p>Publisher: SAGE Publications Inc</p>
ER  - 

TY  - JOUR
TI  - To Engage or Not to Engage with AI for Critical Judgments: How Professionals Deal with Opacity When Using AI for Medical Diagnosis
AU  - Lebovitz, Sarah
AU  - Lifshitz-Assaf, Hila
AU  - Levina, Natalia
T2  - Organization Science
AB  - Artificial intelligence (AI) technologies promise to transform how professionals conduct knowledge work by augmenting their capabilities for making professional judgments. We know little, however, about how human-AI augmentation takes place in practice. Yet, gaining this understanding is particularly important when professionals use AI tools to form judgments on critical decisions. We conducted an in-depth field study in a major U.S. hospital where AI tools were used in three departments by diagnostic radiologists making breast cancer, lung cancer, and bone age determinations. The study illustrates the hindering effects of opacity that professionals experienced when using AI tools and explores how these professionals grappled with it in practice. In all three departments, this opacity resulted in professionals experiencing increased uncertainty because AI tool results often diverged from their initial judgment without providing underlying reasoning. Only in one department (of the three) did professionals consistently incorporate AI results into their final judgments, achieving what we call engaged augmentation. These professionals invested in AI interrogation practices—practices enacted by human experts to relate their own knowledge claims to AI knowledge claims. Professionals in the other two departments did not enact such practices and did not incorporate AI inputs into their final decisions, which we call unengaged “augmentation.” Our study unpacks the challenges involved in augmenting professional judgment with powerful, yet opaque, technologies and contributes to literature on AI adoption in knowledge work.
DA  - 2022///
PY  - 2022
DO  - 10.1287/orsc.2021.1549
VL  - 33
IS  - 1
SP  - 126
EP  - 148
J2  - Organization Science
SN  - 1047-7039
ST  - To Engage or Not to Engage with AI for Critical Judgments
UR  - https://pubsonline.informs.org/doi/10.1287/orsc.2021.1549
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:12:47</p>
N1  - <p>Publisher: INFORMS</p>
KW  - artificial intelligence
KW  - augmentation
KW  - decision making
KW  - expertise
KW  - explainability
KW  - innovation
KW  - medical diagnosis
KW  - opacity
KW  - professional judgment
KW  - technology adoption and use
KW  - transparency
KW  - uncertainty
ER  - 

TY  - JOUR
TI  - Tools or peers? Impacts of anthropomorphism level and social role on emotional attachment and disclosure tendency towards intelligent agents
AU  - Zhang, Andong
AU  - Patrick Rau, Pei-Luen
T2  - Computers in Human Behavior
AB  - Owing to the development of anthropomorphic intelligent agent (IA) designs, users consider IAs as more than just inanimate tools. Previous studies have reported that anthropomorphic features can promote users' social feedback and aid in establishing intimate human–agent relationships. The present study examined the main and interaction effects of anthropomorphism level (a human-like IA vs. robot-like IA) and social role (servant vs. mentor) on emotional attachment, information disclosure tendency, and satisfaction in a smart home. The study participants were randomly assigned into four groups with balanced gender. The results indicate that high anthropomorphism and mentor role can positively predict users' emotional attachment. Additionally, users tend to disclose more personal information to the human-servant and robot-mentor IAs than the human-mentor and robot-servant IAs. Interestingly, social presence was determined to be a positive and significant mediator between anthropomorphic design and emotional attachment. The study findings highlight the importance of social role in anthropomorphic IA design and explain the mechanism of establishing effective human–agent relationships. Moreover, both theoretical and practical implications of these findings are analyzed.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.chb.2022.107415
VL  - 138
SP  - 107415
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - Tools or peers?
UR  - https://www.sciencedirect.com/science/article/pii/S0747563222002370
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:12:35</p>
KW  - Anthropomorphic intelligent agents
KW  - Emotional attachment
KW  - Human–agent relationship
KW  - Self-disclosure
KW  - Social role
ER  - 

TY  - JOUR
TI  - Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks
AU  - Räuker, Tilman
AU  - Ho, Anson
AU  - Casper, Stephen
AU  - Hadfield-Menell, Dylan
AB  - The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2207.13243
ST  - Toward Transparent AI
UR  - http://arxiv.org/abs/2207.13243
Y2  - 2025/08/28/
N1  - <p>arXiv:2207.13243 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:12:50</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Towards a Sustainable News Business: Understanding Readers’ Perceptions of Algorithm-Generated News Based on Cultural Conditioning
AU  - Kim, Yunju
AU  - Lee, Heejun
T2  - Sustainability
AB  - The use of algorithms is beginning to replace human activities in the news business, and the presence of this technique will only continue to grow. The ways in which public news readers perceive the quality of news articles written by algorithms and how this perception differs based on cultural conditioning remain issues of debate. Informed by the heuristic-systematic model (HSM) and the similarity-attraction theory, we attempted to answer these questions by conducting a three-way one-way analysis of variance (ANOVA) test with a 2 (author: algorithm vs. human journalist) × 2 (media: traditional media vs. online media) × 2 (cultural background: the US vs. South Korea) between-subjects experiment (N = 360). Our findings revealed that participants perceived the quality of news articles written by algorithms to be higher than those written by human journalists. We also found that when news consumption occurs online, algorithm-generated news tends to be rated higher than human-written news in terms of quality perception. Further, we identified a three-way interaction effect of media types, authors, and cultural backgrounds on the quality perception of news articles. As, to the best of our knowledge, this study is the first to theoretically examine how news readers perceive algorithm-generated news from a cultural point of view, our research findings may hold important theoretical and practical implications.
DA  - 2021///
PY  - 2021
DO  - 10.3390/su13073728
VL  - 13
IS  - 7
SP  - 3728
J2  - Sustainability
LA  - en
SN  - 2071-1050
ST  - Towards a Sustainable News Business
UR  - https://www.mdpi.com/2071-1050/13/7/3728
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:12:51</p>
N1  - <p>Publisher: Multidisciplinary Digital Publishing Institute</p>
N1  - <p>Rights: http://creativecommons.org/licenses/by/3.0/</p>
KW  - algorithm
KW  - artificial intelligence
KW  - cultural difference
KW  - journalist
KW  - robot journalism
ER  - 

TY  - JOUR
TI  - Towards Agile Text Classifiers for Everyone
AU  - Mozes, Maximilian
AU  - Hoffmann, Jessica
AU  - Tomanek, Katrin
AU  - Kouate, Muhamed
AU  - Thain, Nithum
AU  - Yuan, Ann
AU  - Bolukbasi, Tolga
AU  - Dixon, Lucas
AB  - Text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. This paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy. Experimenting with 7 datasets from three safety-related domains, comprising 15 annotation schemes, led to our key finding: prompt-tuning large language models, like PaLM 62B, with a labeled dataset of as few as 80 examples can achieve state-of-the-art performance. We argue that this enables a paradigm shift for text classification, especially for models supporting safer online discourse. Instead of collecting millions of examples to attempt to create universal safety classifiers over months or years, classifiers could be tuned using small datasets, created by individuals or small organizations, tailored for specific use cases, and iterated on and adapted in the time-span of a day.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2302.06541
UR  - http://arxiv.org/abs/2302.06541
Y2  - 2025/08/28/
N1  - <p>arXiv:2302.06541 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:12:53</p>
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Towards Emotional Support Dialog Systems
AU  - Liu, Siyang
AU  - Zheng, Chujie
AU  - Demasi, Orianna
AU  - Sabour, Sahand
AU  - Li, Yu
AU  - Yu, Zhou
AU  - Jiang, Yong
AU  - Huang, Minlie
T2  - Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
A2  - Zong, Chengqing
A2  - Xia, Fei
A2  - Li, Wenjie
A2  - Navigli, Roberto
AB  - Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems.
C1  - Online
C3  - Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
DA  - 2021///
PY  - 2021
DO  - 10.18653/v1/2021.acl-long.269
SP  - 3469
EP  - 3483
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2021.acl-long.269/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:12:55</p>
ER  - 

TY  - JOUR
TI  - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback
AU  - Bai, Yuntao
AU  - Jones, Andy
AU  - Ndousse, Kamal
AU  - Askell, Amanda
AU  - Chen, Anna
AU  - DasSarma, Nova
AU  - Drain, Dawn
AU  - Fort, Stanislav
AU  - Ganguli, Deep
AU  - Henighan, Tom
AU  - Joseph, Nicholas
AU  - Kadavath, Saurav
AU  - Kernion, Jackson
AU  - Conerly, Tom
AU  - El-Showk, Sheer
AU  - Elhage, Nelson
AU  - Hatfield-Dodds, Zac
AU  - Hernandez, Danny
AU  - Hume, Tristan
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Lovitt, Liane
AU  - Nanda, Neel
AU  - Olsson, Catherine
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Clark, Jack
AU  - McCandlish, Sam
AU  - Olah, Chris
AU  - Mann, Ben
AU  - Kaplan, Jared
AB  - We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2204.05862
UR  - http://arxiv.org/abs/2204.05862
Y2  - 2025/08/27/
N1  - <p>Date Added: 2025-08-28 12:02:44</p>
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Training language models to follow instructions with human feedback
AU  - Ouyang, Long
AU  - Wu, Jeff
AU  - Jiang, Xu
AU  - Almeida, Diogo
AU  - Wainwright, Carroll L.
AU  - Mishkin, Pamela
AU  - Zhang, Chong
AU  - Agarwal, Sandhini
AU  - Slama, Katarina
AU  - Ray, Alex
AU  - Schulman, John
AU  - Hilton, Jacob
AU  - Kelton, Fraser
AU  - Miller, Luke
AU  - Simens, Maddie
AU  - Askell, Amanda
AU  - Welinder, Peter
AU  - Christiano, Paul
AU  - Leike, Jan
AU  - Lowe, Ryan
AB  - Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.
DA  - 2022///
PY  - 2022
DO  - 10.48550/arXiv.2203.02155
UR  - http://arxiv.org/abs/2203.02155
Y2  - 2025/08/28/
N1  - <p>arXiv:2203.02155 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:13:48</p>
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Trust in AI and Its Role in the Acceptance of AI Technologies
AU  - Choung, Hyesun
AU  - David, Prabu
AU  - Ross, Arun
T2  - International Journal of Human–Computer Interaction
AB  - As AI-enhanced technologies become common in a variety of domains, there is an increasing need to define and examine the trust that users have in such technologies. Given the progress in the development of AI, a correspondingly sophisticated understanding of trust in the technology is required. This paper addresses this need by explaining the role of trust in the intention to use AI technologies. Study 1 examined the role of trust in the use of AI voice assistants based on survey responses from college students. A path analysis confirmed that trust had a significant effect the on intention to use AI, which operated through perceived usefulness and participants’ attitude toward voice assistants. In Study 2, using data from a representative sample of the U.S. population, different dimensions of trust were examined using exploratory factor analysis, which yielded two dimensions: human-like trust and functionality trust. The results of the path analyses from Study 1 were replicated in Study 2, confirming the indirect effect of trust and the effects of perceived usefulness, ease of use, and attitude on intention to use. Further, both dimensions of trust shared a similar pattern of effects within the model, with functionality-related trust exhibiting a greater total impact on usage intention than human-like trust. Overall, the role of trust in the acceptance of AI technologies was significant across both studies. This research contributes to the advancement and application of the TAM in AI-related applications and offers a multidimensional measure of trust that can be utilized in the future study of trustworthy AI.
DA  - 2023///
PY  - 2023
DO  - 10.1080/10447318.2022.2050543
VL  - 39
IS  - 9
SP  - 1727
EP  - 1739
J2  - International Journal of Human–Computer Interaction
SN  - 1044-7318
UR  - https://doi.org/10.1080/10447318.2022.2050543
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:13:51</p>
N1  - <p>Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2022.2050543</p>
ER  - 

TY  - JOUR
TI  - Trust in an AI versus a Human teammate: The effects of teammate identity and performance on Human-AI cooperation
AU  - Zhang, Guanglu
AU  - Chong, Leah
AU  - Kotovsky, Kenneth
AU  - Cagan, Jonathan
T2  - Computers in Human Behavior
AB  - Recent advances in artificial intelligence (AI) enable researchers to create more powerful AI agents that are becoming competent teammates for humans. However, human distrust of AI is a critical factor that may impede human-AI cooperation. Although AI agents have been endowed with anthropomorphic traits, such as a human-like appearance, in prior studies to improve human trust in AI, it is still an open question whether humans have more trust in an AI teammate and achieve better human-AI joint performance if they are deceived about the identity of their AI teammate as another human. This research assesses the effects of teammate identity (“human” vs. AI) and teammate performance (low-performing vs. high-performing AI) on human-AI cooperation through a human subjects study. The results of this study show that humans behaviorally trust the AI more than another human by accepting their AI teammate's decisions more often. In addition, teammate performance has a significant effect on human-AI joint performance in the study, while teammate identity does not. These results caution against deceiving humans about the identity of AI in future applications involving human-AI cooperation.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.chb.2022.107536
VL  - 139
SP  - 107536
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - Trust in an AI versus a Human teammate
UR  - https://www.sciencedirect.com/science/article/pii/S0747563222003569
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:13:53</p>
KW  - Anthropomorphism
KW  - Artificial intelligence
KW  - Deception
KW  - Decision-making
KW  - Human-computer interaction
KW  - Trust
ER  - 

TY  - JOUR
TI  - Understanding Effects of Algorithmic vs. Community Label on Perceived Accuracy of Hyper-partisan Misinformation
AU  - Jia, Chenyan
AU  - Boltz, Alexander
AU  - Zhang, Angie
AU  - Chen, Anqing
AU  - Lee, Min Kyung
T2  - Proceedings of the ACM on Human-Computer Interaction
AB  - Hyper-partisan misinformation has become a major public concern. In order to examine what type of misinformation label can mitigate hyper-partisan misinformation sharing on social media, we conducted a 4 (label type: algorithm, community, third-party fact-checker, and no label) X 2 (post ideology: liberal vs. conservative) between-subjects online experiment (N = 1,677) in the context of COVID-19 health information. The results suggest that for liberal users, all labels reduced the perceived accuracy and believability of fake posts regardless of the posts' ideology. In contrast, for conservative users, the efficacy of the labels depended on whether the posts were ideologically consistent: algorithmic labels were more effective in reducing the perceived accuracy and believability of fake conservative posts compared to community labels, whereas all labels were effective in reducing their belief in liberal posts. Our results shed light on the differing effects of various misinformation labels dependent on people's political ideology.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3555096
VL  - 6
IS  - CSCW2
SP  - 1
EP  - 27
J2  - Proceedings of the ACM on Human-Computer Interaction
SN  - 2573-0142
UR  - http://arxiv.org/abs/2203.00710
Y2  - 2025/08/28/
N1  - <p>arXiv:2203.00710 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:14:44</p>
KW  - Computer Science - Human-Computer Interaction
KW  - Computer Science - Social and Information Networks
ER  - 

TY  - JOUR
TI  - Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities
AU  - Mozes, Maximilian
AU  - He, Xuanli
AU  - Kleinberg, Bennett
AU  - Griffin, Lewis D.
AB  - Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arXiv.2308.12833
ST  - Use of LLMs for Illicit Purposes
UR  - http://arxiv.org/abs/2308.12833
Y2  - 2025/08/28/
N1  - <p>arXiv:2308.12833 [cs]</p>
N1  - <p>Date Added: 2025-08-28 13:15:02</p>
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - JOUR
TI  - Use of the Chatbot “Vivibot” to Deliver Positive Psychology Skills and Promote Well-Being Among Young People After Cancer Treatment: Randomized Controlled Feasibility Trial
AU  - Greer, Stephanie
AU  - Ramo, Danielle
AU  - Chang, Yin-Juei
AU  - Fu, Michael
AU  - Moskowitz, Judith
AU  - Haritatos, Jana
T2  - JMIR mHealth and uHealth
AB  - Background Positive psychology interventions show promise for reducing psychosocial distress associated with health adversity and have the potential to be widely disseminated to young adults through technology. Objective This pilot randomized controlled trial examined the feasibility of delivering positive psychology skills via the Vivibot chatbot and its effects on key psychosocial well-being outcomes in young adults treated for cancer. Methods Young adults (age 18-29 years) were recruited within 5 years of completing active cancer treatment by using the Vivibot chatbot on Facebook messenger. Participants were randomized to either immediate access to Vivibot content (experimental group) or access to only daily emotion ratings and access to full chatbot content after 4 weeks (control). Created using a human-centered design process with young adults treated for cancer, Vivibot content includes 4 weeks of positive psychology skills, daily emotion ratings, video, and other material produced by survivors, and periodic feedback check-ins. All participants were assessed for psychosocial well-being via online surveys at baseline and weeks 2, 4, and 8. Analyses examined chatbot engagement and open-ended feedback on likability and perceived helpfulness and compared experimental and control groups with regard to anxiety and depression symptoms and positive and negative emotion changes between baseline and 4 weeks. To verify the main effects, follow-up analyses compared changes in the main outcomes between 4 and 8 weeks in the control group once participants had access to all chatbot content. Results Data from 45 young adults (36 women; mean age: 25 [SD 2.9]; experimental group: n=25; control group: n=20) were analyzed. Participants in the experimental group spent an average of 74 minutes across an average of 12 active sessions chatting with Vivibot and rated their experience as helpful (mean 2.0/3, SD 0.72) and would recommend it to a friend (mean 6.9/10; SD 2.6). Open-ended feedback noted its nonjudgmental nature as a particular benefit of the chatbot. After 4 weeks, participants in the experimental group reported an average reduction in anxiety of 2.58 standardized t-score units, while the control group reported an increase in anxiety of 0.7 units. A mixed-effects models revealed a trend-level (P=.09) interaction between group and time, with an effect size of 0.41. Those in the experimental group also experienced greater reductions in anxiety when they engaged in more sessions (z=–1.9, P=.06). There were no significant (or trend level) effects by group on changes in depression, positive emotion, or negative emotion. Conclusions The chatbot format provides a useful and acceptable way of delivering positive psychology skills to young adults who have undergone cancer treatment and supports anxiety reduction. Further analysis with a larger sample size is required to confirm this pattern.
DA  - 2019///
PY  - 2019
DO  - 10.2196/15018
VL  - 7
IS  - 10
SP  - e15018
J2  - JMIR mHealth and uHealth
SN  - 2291-5222
ST  - Use of the Chatbot “Vivibot” to Deliver Positive Psychology Skills and Promote Well-Being Among Young People After Cancer Treatment
UR  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6913733/
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 12:31:23</p>
N1  - <p>PMID: 31674920 PMCID: PMC6913733</p>
ER  - 

TY  - JOUR
TI  - Using Machine Learning to Learn Machines: A Cross-Cultural Study of Users’ Responses to Machine-Generated Artworks
AU  - Xu, Kun
AU  - Liu, Fanjue
AU  - Mou, Yi
AU  - Wu, Yuheng
AU  - Zeng, Jing
AU  - Schäfer, Mike S.
T2  - Journal of Broadcasting & Electronic Media
AB  - Drawing from prior literature on machine-generated news, this study examines machine-generated artworks in a cross-cultural context. It combines machine learning approaches with online experiments and investigates how different genres of artworks and different authorship cues influence participants’ open-ended responses to machine-generated works. Results suggest that while genres and cultures affected participants’ discussion topics and word use, the differences between participants’ responses to machine-generated artworks and human-generated ones were not evident. This study tests the explanatory power of machine heuristic and demonstrates the feasibility of integrating multiple methods in future AI-based media research.
DA  - 2020///
PY  - 2020
DO  - 10.1080/08838151.2020.1835136
VL  - 64
IS  - 4
SP  - 566
EP  - 591
J2  - Journal of Broadcasting & Electronic Media
SN  - 0883-8151
ST  - Using Machine Learning to Learn Machines
UR  - https://doi.org/10.1080/08838151.2020.1835136
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:06</p>
N1  - <p>Publisher: Routledge _eprint: https://doi.org/10.1080/08838151.2020.1835136</p>
ER  - 

TY  - JOUR
TI  - Using Psychological Artificial Intelligence (Tess) to Relieve Symptoms of Depression and Anxiety: Randomized Controlled Trial
AU  - Fulmer, Russell
AU  - Joerin, Angela
AU  - Gentile, Breanna
AU  - Lakerink, Lysanne
AU  - Rauws, Michiel
T2  - JMIR Mental Health
AB  - Background: Students in need of mental health care face many barriers including cost, location, availability, and stigma. Studies show that computer-assisted therapy and 1 conversational chatbot delivering cognitive behavioral therapy (CBT) offer a less-intensive and more cost-effective alternative for treating depression and anxiety. Although CBT is one of the most effective treatment methods, applying an integrative approach has been linked to equally effective posttreatment improvement. Integrative psychological artificial intelligence (AI) offers a scalable solution as the demand for affordable, convenient, lasting, and secure support grows. Objective: This study aimed to assess the feasibility and efficacy of using an integrative psychological AI, Tess, to reduce self-identified symptoms of depression and anxiety in college students. Methods: In this randomized controlled trial, 75 participants were recruited from 15 universities across the United States. All participants completed Web-based surveys, including the Patient Health Questionnaire (PHQ-9), Generalized Anxiety Disorder Scale (GAD-7), and Positive and Negative Affect Scale (PANAS) at baseline and 2 to 4 weeks later (T2). The 2 test groups consisted of 50 participants in total and were randomized to receive unlimited access to Tess for either 2 weeks (n=24) or 4 weeks (n=26). The information-only control group participants (n=24) received an electronic link to the National Institute of Mental Health’s (NIMH) eBook on depression among college students and were only granted access to Tess after completion of the study. Results: A sample of 74 participants completed this study with 0% attrition from the test group and less than 1% attrition from the control group (1/24). The average age of participants was 22.9 years, with 70% of participants being female (52/74), mostly Asian (37/74, 51%), and white (32/74, 41%). Group 1 received unlimited access to Tess, with daily check-ins for 2 weeks. Group 2 received unlimited access to Tess with biweekly check-ins for 4 weeks. The information-only control group was provided with an electronic link to the NIMH’s eBook. Multivariate analysis of covariance was conducted. We used an alpha level of .05 for all statistical tests. Results revealed a statistically significant difference between the control group and group 1, such that group 1 reported a significant reduction in symptoms of depression as measured by the PHQ-9 (P=.03), whereas those in the control group did not. A statistically significant difference was found between the control group and both test groups 1 and 2 for symptoms of anxiety as measured by the GAD-7. Group 1 (P=.045) and group 2 (P=.02) reported a significant reduction in symptoms of anxiety, whereas the control group did not. A statistically significant difference was found on the PANAS between the control group and group 1 (P=.03) and suggests that Tess did impact scores. Conclusions: This study offers evidence that AI can serve as a cost-effective and accessible therapeutic agent. Although not designed to appropriate the role of a trained therapist, integrative psychological AI emerges as a feasible option for delivering support. Trial Registration: International Standard Randomized Controlled Trial Number: ISRCTN61214172; https://doi.org/10.1186/ISRCTN61214172.
DA  - 2018///
PY  - 2018
DO  - 10.2196/mental.9782
VL  - 5
IS  - 4
SP  - e9782
J2  - JMIR Mental Health
LA  - EN
ST  - Using Psychological Artificial Intelligence (Tess) to Relieve Symptoms of Depression and Anxiety
UR  - https://mental.jmir.org/2018/4/e64
Y2  - 2025/08/28/
N1  - <p>Company: JMIR Mental Health Distributor: JMIR Mental Health Institution: JMIR Mental Health Label: JMIR Mental Health Publisher: JMIR Publications Inc., Toronto, Canada</p>
N1  - <p>Date Added: 2025-08-28 13:15:08</p>
N1  - <p>Rights: Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in the Journal of Medical Internet Research...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.</p>
ER  - 

TY  - JOUR
TI  - Watch Me Improve—Algorithm Aversion and Demonstrating the Ability to Learn
AU  - Berger, Benedikt
AU  - Adam, Martin
AU  - Rühr, Alexander
AU  - Benlian, Alexander
T2  - Business & Information Systems Engineering
AB  - Owing to advancements in artificial intelligence (AI) and specifically in machine learning, information technology (IT) systems can support humans in an increasing number of tasks. Yet, previous research indicates that people often prefer human support to support by an IT system, even if the latter provides superior performance – a phenomenon called algorithm aversion. A possible cause of algorithm aversion put forward in literature is that users lose trust in IT systems they become familiar with and perceive to err, for example, making forecasts that turn out to deviate from the actual value. Therefore, this paper evaluates the effectiveness of demonstrating an AI-based system’s ability to learn as a potential countermeasure against algorithm aversion in an incentive-compatible online experiment. The experiment reveals how the nature of an erring advisor (i.e., human vs. algorithmic), its familiarity to the user (i.e., unfamiliar vs. familiar), and its ability to learn (i.e., non-learning vs. learning) influence a decision maker’s reliance on the advisor’s judgement for an objective and non-personal decision task. The results reveal no difference in the reliance on unfamiliar human and algorithmic advisors, but differences in the reliance on familiar human and algorithmic advisors that err. Demonstrating an advisor’s ability to learn, however, offsets the effect of familiarity. Therefore, this study contributes to an enhanced understanding of algorithm aversion and is one of the first to examine how users perceive whether an IT system is able to learn. The findings provide theoretical and practical implications for the employment and design of AI-based systems.
DA  - 2021///
PY  - 2021
DO  - 10.1007/s12599-020-00678-5
VL  - 63
IS  - 1
SP  - 55
EP  - 68
J2  - Business & Information Systems Engineering
LA  - en
SN  - 1867-0202
UR  - https://doi.org/10.1007/s12599-020-00678-5
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:11</p>
KW  - Advice taking
KW  - Algorithm aversion
KW  - Artificial intelligence
KW  - Decision support
KW  - Machine learning
ER  - 

TY  - JOUR
TI  - What to expect from opening up ‘black boxes’? Comparing perceptions of justice between human and automated agents
AU  - Schlicker, Nadine
AU  - Langer, Markus
AU  - Ötting, Sonja K.
AU  - Baum, Kevin
AU  - König, Cornelius J.
AU  - Wallach, Dieter
T2  - Computers in Human Behavior
AB  - Advances in artificial intelligence contribute to increasing automation of decisions. In a healthcare-scheduling context, this study compares effects of decision agents and explanations for decisions on decision-recipients’ perceptions of justice. In a 2 (decision agent: automated vs. human) × 3 (explanation: no explanation vs. equality-explanation vs. equity-explanation) between-subjects online study, 209 healthcare professionals were asked to put themselves in a situation where their vacation request was denied by either a human or an automated agent. Participants either received no explanation or an explanation based on equality or equity norms. Perceptions of interpersonal justice were stronger for the human agent. Additionally, participants perceived human agents as offering more voice and automated agents as being more consistent in decision-making. When given no explanation, perceptions of informational justice were impaired only for the human decision agent. In the study's second part, participants took the perspective of a decision-maker and were given the choice to delegate decision-making to an automated system. Participants who delegated an unpleasant decision to the system frequently externalized responsibility and showed different response patterns when confronted by a decision-recipient who asked for a rationale for the decision.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.chb.2021.106837
VL  - 122
SP  - 106837
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - What to expect from opening up ‘black boxes’?
UR  - https://www.sciencedirect.com/science/article/pii/S0747563221001606
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:13</p>
KW  - Automated and augmented decision-making
KW  - Explainability
KW  - Justice
KW  - Responsibility
ER  - 

TY  - JOUR
TI  - When algorithms meet journalism: The user perception to automated news in a cross-cultural context
AU  - Zheng, Yue
AU  - Zhong, Bu
AU  - Yang, Fan
T2  - Computers in Human Behavior
AB  - Automated journalism – the use of algorithms in writing news reports – underscores the new direction of media transformation in the 21st century as it may reshape how the news is produced and consumed. Such writing algorithms have been increasingly adopted in U.S. and Chinese newsroom, but how well they are accepted by news users deserves more research. A comparative study was thus conducted to examine how U.S. and Chinese news users perceive the quality of algorithm-generated news reports, how much they like and trust such reports. Results show that U.S. and Chinese users demonstrated more shared, rather than different, perceptions to automated news. The users did not perceive automated content in a linear way, but viewed them by considering the interaction of the authors (i.e., journalists or algorithms), the media outlets (i.e., traditional or online media) and cultural background (i.e., U.S. or Chinese users).
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.chb.2018.04.046
VL  - 86
SP  - 266
EP  - 275
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - When algorithms meet journalism
UR  - https://www.sciencedirect.com/science/article/pii/S0747563218302103
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:25</p>
KW  - Algorithm
KW  - Automated news
KW  - Computational journalism
KW  - Cultural difference
KW  - News user
KW  - Robot journalism
ER  - 

TY  - JOUR
TI  - When expert recommendation contradicts peer opinion: Relative social influence of valence, group identity and artificial intelligence
AU  - Wang, Jinping
AU  - Molina, Maria D.
AU  - Sundar, S. Shyam
T2  - Computers in Human Behavior
AB  - Whom do we trust more, the recommendation of an expert or public opinion from a crowd of other users of the site? Does it matter if the expert belongs to our in-group? And, what, if anything, would change if an Artificial Intelligence (AI) system was the recommender rather than a human expert? In order to answer these research questions, we conducted a between-subjects online experiment, informed by MAIN Model (Sundar, 2008), which posits that interface cues signaling different types of sources can influence perceived credibility of content by triggering distinct cognitive heuristics. Participants were assigned to a scenario wherein the expert review contrasted the peer rating about recommending photos for business profiles, with systematic variations in expert review valence (negative vs. positive), expert identity (ingroup vs. outgroup vs. no identity), and agent type (human vs. AI). Results show that positive ratings are more influential on user judgements. However, for negative ratings, human ingroup members generated greater effects than no-identity experts. Moreover, AI systems were as influential as human experts, suggesting the potential for AI to substitute human experts for online recommendations.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.chb.2020.106278
VL  - 107
SP  - 106278
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - When expert recommendation contradicts peer opinion
UR  - https://www.sciencedirect.com/science/article/pii/S0747563220300340
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:33</p>
KW  - Artificial intelligence
KW  - Expert vs. peer
KW  - Group identity
KW  - MAIN model
KW  - Online rating
KW  - Social influence
ER  - 

TY  - JOUR
TI  - When humans and computers induce social stress through negative feedback: Effects on performance and subjective state
AU  - Thuillard, S.
AU  - Adams, M.
AU  - Jelmini, G.
AU  - Schmutz, S.
AU  - Sonderegger, A.
AU  - Sauer, J.
T2  - Computers in Human Behavior
AB  - People increasingly work with autonomous systems, which progressively take over functions previously performed exclusively by humans. This may lead to situations in which automated agents give negative performance feedback, which represents an important work-related social stressor. Little is known about how negative feedback provided by computers (as opposed to humans) affects human performance and subjective state. A first experiment (N = 60) focused on the influence of human feedback on performance. After participants had performed a cognitive task, they received a manipulated performance feedback (either positive or negative) from a human (comparing to a control with no feedback) and subsequent performance on several cognitive tasks and the participants' subjective state was measured. The results showed that while negative feedback had a negative influence on several subjective state measures, performance remained unimpaired. In a second experiment (N = 89), participants received manipulated negative feedback by a human or by a computer (or no feedback at all) after having completed an ability test. Subsequent performance was measured on attention tasks and creativity tasks and participants' subjective state was assessed. Although participants felt stressed by both negative computer and human feedback, subsequent performance was again not impaired. However, computer feedback was rated as being less fair than human feedback. Overall, our findings show that there are costs of protecting one's performance against negative feedback and they call for caution regarding the use of negative feedback by both human and automated agents in work settings.
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.chb.2022.107270
VL  - 133
SP  - 107270
J2  - Computers in Human Behavior
SN  - 0747-5632
ST  - When humans and computers induce social stress through negative feedback
UR  - https://www.sciencedirect.com/science/article/pii/S0747563222000929
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:42</p>
KW  - 6 max): Social stress
KW  - Automation
KW  - Computer feedback
KW  - Interpersonal fairness
KW  - Negative feedback
KW  - Performance
ER  - 

TY  - JOUR
TI  - When people are defeated by artificial intelligence in a competition task requiring logical thinking, how do they make causal attribution?
AU  - Yokoi, Ryosuke
AU  - Nakayachi, Kazuya
T2  - Current Psychology
AB  - Given that artificial intelligence (AI) has been predicted to eventually take on human tasks demanding logical thinking, it makes sense that we should examine psychological responses of humans when their performance is inferior to AI. Research has demonstrated that after people fail a task, whether they reorient their behavior towards success depends on what they attribute the failure to. This study investigated the causal attributions people made in a competition task requiring such thinking. We also recorded whether they wanted to re-challenge the games after they were defeated by AI. Experiments 1 (N = 74) and 2 (N = 788) recruited Japanese participants, while Experiment 3 (N = 500) comprised American participants. There were two conditions: in the first, participants competed against an AI opponent and in the other, they believed they were competing against a human. The results of the three experiments showed that participants attributed the loss to their own and their opponent’s abilities more than any other factor, irrespective of the opponent type. The number of participants choosing to re-challenge the game did not differ significantly between the AI and human conditions in Experiments 1 and 3, although the number was lower in the AI condition than in the human condition in Experiment 2. Besides providing fresh insight on how people make causal attributions when competing against AI, our findings also predict how people will respond after their jobs are replaced by AI.
DA  - 2023///
PY  - 2023
DO  - 10.1007/s12144-021-02559-w
VL  - 42
IS  - 16
SP  - 13369
EP  - 13384
J2  - Current Psychology
LA  - en
SN  - 1936-4733
UR  - https://doi.org/10.1007/s12144-021-02559-w
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:53</p>
KW  - Artificial intelligence
KW  - Behavioral response
KW  - Causal attribution
KW  - Competition game
KW  - Self-effacing bias
ER  - 

TY  - JOUR
TI  - When rational decision-making becomes irrational: a critical assessment and re-conceptualization of intuition effectiveness
AU  - Julmi, Christian
T2  - Business Research
AB  - Intuition can lead to more effective decision-making than analysis under certain conditions. This assumption can be regarded as common sense. However, dominant research streams on intuition effectiveness in decision-making conceptualize intuition inadequately, because intuition is considered either detrimental or as a form of analysis. Current findings in general intuition research show that intuition is a holistic form of information processing that is distinct from analysis and can be superior in some cases. To reconcile this mismatch, this article first critically assesses dominant conceptions on intuition effectiveness and then offers a re-conceptualization that builds on current findings of general intuition research. Basically, the article suggests the structuredness of the decision problem as the main criterion for intuition effectiveness, and proposes organization information processing theory to establish this link conceptually. It is not the uncertainty but the equivocality of decision problems that call for an intuitive approach. The article conclusively derives implications for further research and discusses potential restrictions and constraints.
DA  - 2019///
PY  - 2019
DO  - 10.1007/s40685-019-0096-4
VL  - 12
IS  - 1
SP  - 291
EP  - 314
J2  - Business Research
LA  - en
SN  - 2198-2627
ST  - When rational decision-making becomes irrational
UR  - https://doi.org/10.1007/s40685-019-0096-4
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:55</p>
KW  - Analysis
KW  - Decision-making effectiveness
KW  - Equivocality
KW  - Intuition
KW  - Intuition effectiveness
KW  - Organization information processing theory
KW  - Rationality
ER  - 

TY  - CONF
TI  - Who Is Included in Human Perceptions of AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust
AU  - Lee, Min Kyung
AU  - Rich, Katherine
T2  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
T3  - CHI '21
AB  - Emerging research suggests that people trust algorithmic decisions less than human decisions. However, different populations, particularly in marginalized communities, may have different levels of trust in human decision-makers. Do people who mistrust human decision-makers perceive human decisions to be more trustworthy and fairer than algorithmic decisions? Or do they trust algorithmic decisions as much as or more than human decisions? We examine the role of mistrust in human systems in people’s perceptions of algorithmic decisions. We focus on healthcare Artificial Intelligence (AI), group-based medical mistrust, and Black people in the United States. We conducted a between-subjects online experiment to examine people’s perceptions of skin cancer screening decisions made by an AI versus a human physician depending on their medical mistrust, and we conducted interviews to understand how to cultivate trust in healthcare AI. Our findings highlight that research around human experiences of AI should consider critical differences in social groups.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3411764.3445570
SP  - 1
EP  - 14
PB  - Association for Computing Machinery
SN  - 978-1-4503-8096-6
ST  - Who Is Included in Human Perceptions of AI?
UR  - https://dl.acm.org/doi/10.1145/3411764.3445570
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:15:57</p>
ER  - 

TY  - JOUR
TI  - Who made the decisions: Human or robot umpires? The effects of anthropomorphism on perceptions toward robot umpires
AU  - Wonseok (Eric), Jang
AU  - Young Woo, Kang
AU  - Yeonheung, Kang
T2  - Telematics and Informatics
AB  - This study examined how the adoption of robot umpires in baseball alters people’s evaluation of ball/strike decisions, the trust in umpires, game enjoyment, and their behavioral consequences. The results indicated that people perceived umpire calls as fairer and more credible, and demonstrated greater trust in human umpires than in robot umpires; however, these negative effects were attenuated when robot umpires were humanized by giving them human-like characteristics. Furthermore, the effects of robot umpires were moderated by age and gender. Younger adults and males generally had more positive perceptions toward robot umpires, while older adults and females had more positive perceptions toward humanized robot umpires.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.tele.2021.101695
VL  - 64
SP  - 101695
J2  - Telematics and Informatics
SN  - 0736-5853
ST  - Who made the decisions
UR  - https://www.sciencedirect.com/science/article/pii/S0736585321001349
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:16:00</p>
KW  - Age
KW  - Anthropomorphism theory
KW  - Baseball
KW  - Gender
KW  - Robot umpire
KW  - Source bias framework
ER  - 

TY  - JOUR
TI  - Would an AI chatbot persuade you: an empirical answer from the elaboration likelihood model
AU  - Chen, Qian
AU  - Yin, Changqin
AU  - Gong, Yeming
T2  - Information Technology & People
AB  - This study investigates how artificial intelligence (AI) chatbots persuade customers to accept their recommendations in the online shopping context.Drawing on the elaboration likelihood model, this study establishes a research model to reveal the antecedents and internal mechanisms of customers' adoption of AI chatbot recommendations. The authors tested the model with survey data from 530 AI chatbot users.The results show that in the AI chatbot recommendation adoption process, central and peripheral cues significantly affected a customer's intention to adopt an AI chatbot's recommendation, and a customer's cognitive and emotional trust in the AI chatbot mediated the relationships. Moreover, a customer's mind perception of the AI chatbot, including perceived agency and perceived experience, moderated the central and peripheral paths, respectively.This study has theoretical and practical implications for AI chatbot designers and provides management insights for practitioners to enhance a customer's intention to adopt an AI chatbot's recommendation.The study investigates customers' adoption of AI chatbots' recommendation.The authors develop research model based on ELM theory to reveal central and peripheral cues and paths.The central and peripheral cues are generalized according to cooperative principle theory.Central cues include recommendation reliability and accuracy, and peripheral cues include human-like empathy and recommendation choice.Central and peripheral cues affect customers' adoption to recommendation through trust in AI.Customers' mind perception positively moderates the central and peripheral paths.
DA  - 2023///
PY  - 2023
DO  - 10.1108/ITP-10-2021-0764
VL  - 38
IS  - 2
SP  - 937
EP  - 962
J2  - Information Technology & People
SN  - 0959-3845
ST  - Would an AI chatbot persuade you
UR  - https://doi.org/10.1108/ITP-10-2021-0764
Y2  - 2025/08/28/
N1  - <p>Date Added: 2025-08-28 13:16:17</p>
ER  - 

