TY  - JOUR
TI  - Capability-Based Scaling Laws for LLM Red-Teaming
AU  - Panfilov, A.
AU  - Kassianik, P.
AU  - Andriushchenko, M.
AU  - Geiping, J.
PY  - 2025
AB  - As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target’s capability exceeds the attacker’s, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models’ persuasive and manipulative abilities to limit their effectiveness as attackers.
UR  - https://arxiv.org/pdf/2505.20162v1.pdf
N1  - Source: page_1.html
ID  - 1
ER  - 
TY  - JOUR
TI  - Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL
AU  - Hong, J.
AU  - Dragan, A.
AU  - Levine, S.
PY  - 2025
AB  - Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability. 1
UR  - https://arxiv.org/pdf/2505.18098v1.pdf
N1  - Source: page_1.html
ID  - 2
ER  - 
TY  - JOUR
TI  - LvL UP 1.0, a holistic mHealth lifestyle coaching intervention for the prevention of non-communicable diseases and common mental disorders: a mixed methods feasibility study
AU  - Mair, J.L.
AU  - Jabir, A.I.
AU  - Salamanca-Sanabria, A.
AU  - Müller-Riemenschneider, F.
AU  - Kowatsch, T.
PY  - 2025
AB  - LvL UP is a smartphone-based holistic lifestyle coaching intervention aimed at improving health behaviours, mental well-being, and preventing noncommunicable diseases and common mental disorders. It features a ‘talk and tools’ approach, combining automated health literacy coaching via conversational agent with digital tools such as journaling, life hacks, and slow-paced breathing exercises. An ‘in-the-wild' mixed-methods study was conducted in Singapore to evaluate LvL UP’s feasibility and acceptability to inform a future definitive trial. The app was available on iOS and Android from March to August 2023 and was promoted through online and offline strategies. Data collection included in-app surveys, usage metrics, and interviews, summarised using descriptive statistics and template analysis. The app was downloaded 307 times. Data from 99 active users were analysed. Most users were female and aged 21–35 years with mild to moderate vulnerabilities in physical activity, diet, and depressive symptoms. Engagement was highest during the first eight days, with 9% remaining engaged for up to 50 days. Users rated technology acceptance highly, finding the app enjoyable, easy to use, and informative. Suggested improvements included streamlined onboarding, fixing bugs, shortening dialogues, and adding rewards. The findings support LvL UP’s feasibility and have informed enhancements for future trials.
UR  - https://www.researchsquare.com/article/rs-6484027/v1.pdf
N1  - Source: page_1.html
ID  - 3
ER  - 
TY  - JOUR
TI  - Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective
AU  - Sun, Z.
AU  - Wang, Q.
AU  - Wang, H.
AU  - Zhang, X.
AU  - Xu, J.
PY  - 2025
AB  - Large Reasoning Models (LRMs) have shown impressive capabilities in multi-step reasoning tasks. However, alongside these successes, a more deceptive form of model error has emerged—Reasoning Hallucination—where logically coherent but factually incorrect reasoning traces lead to persuasive yet faulty conclusions. Unlike traditional hallucinations, these errors are embedded within structured reasoning, making them more difficult to detect and potentially more harmful. In this work, we investigate reasoning hallucinations from a mechanistic perspective. We propose the Reasoning Score, which quantifies the depth of reasoning by measuring the divergence between logits obtained from projecting late layers of LRMs to the vocabulary space, effectively distinguishing shallow pattern-matching from genuine deep reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA dataset and identify two key reasoning hallucination patterns: early-stage fluctuation in reasoning depth and incorrect backtracking to flawed prior steps. These insights motivate our Reasoning Hallucination Detection (RHD) framework, which achieves state-of-the-art performance across multiple domains. To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced reinforcement learning algorithm that incorporates step-level deep reasoning rewards via potential-based shaping. Our theoretical analysis establishes stronger generalization guarantees, and experiments demonstrate improved reasoning quality and reduced hallucination rates.
UR  - https://arxiv.org/pdf/2505.12886v1.pdf
N1  - Source: page_1.html
ID  - 4
ER  - 
TY  - JOUR
TI  - Persuasion and Safety in the Era of Generative AI
AU  - Kong, H.
PY  - 2025
AB  - As large language models (LLMs) achieve advanced persuasive capabilities, concerns about their potential risks have grown. The EU AI Act prohibits AI systems that use manipulative or deceptive techniques to undermine informed decision-making, highlighting the need to distinguish between rational persuasion, which engages reason, and manipulation, which exploits cognitive biases. My dissertation addresses the lack of empirical studies in this area by developing a taxonomy of persuasive techniques, creating a human-annotated dataset, and evaluating LLMs’ ability to distinguish between these methods. This work contributes to AI safety by providing resources to mitigate the risks of persuasive AI and fostering discussions on ethical persuasion in the age of generative AI.
UR  - https://arxiv.org/pdf/2505.12248v1.pdf
N1  - Source: page_1.html
ID  - 5
ER  - 
TY  - JOUR
TI  - Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach
AU  - Lodoen, S.
AU  - Orchard, A.
PY  - 2025
AB  - Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more “human-like” than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost’s concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots.
UR  - https://arxiv.org/pdf/2505.09576v1.pdf
N1  - Source: page_1.html
ID  - 6
ER  - 
TY  - JOUR
TI  - Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information
AU  - Zhou, J.
AU  - Venkatachalam, K.
AU  - Choi, M.
AU  - Saha, K.
AU  - De Choudhury, M.
PY  - 2025
AB  - Background: With the wide adoption of large language models (LLMs) in information creation assistance, it is essential to examine their alignment with human communication styles and values. We situate this study within the context of fact-checking health information, given the critical challenge of correcting misconceptions and building trustworthiness. Recent studies have explored the potential of LLM-based fact-checking, but communication style differences between LLMs and human fact-checkers and associated reader perceptions remain under-explored. In this light, our study evaluates the communication styles of LLMs, focusing on how their explanations differ from those of humans in three core components of health communication: information linguistic features, sender persuasive strategies, and receiver value alignments. Methods: We compiled a dataset of 1498 health misinformation explanations from authoritative fact-checking organizations. Using this dataset, we employed chain-of-thought prompting with zero-shot and few-shot variations to generate LLM fact-checking responses to inaccurate health information. We drew from health communication theories and categorized communication styles along linguistic, persuasion, and value-based dimensions and measured how closely the LLM-generated responses aligned with professional explanations. Then, we examined human preferences with 99 participants who were unaware of LLM involvement and rated randomized fact-checking articles with switching orders. Results: Our findings reveal that LLM-generated articles showed significantly lower scores in persuasive strategies, certainty expressions, and alignment with social values and moral foundations. However, human evaluation demonstrated a strong preference for LLM content, with over 60% responses favoring LLM articles for clarity, completeness, and persuasiveness. Conclusion: Our results suggest that LLMs’ structured approach to presenting information may be more effective at engaging readers despite scoring lower on traditional measures of quality in fact-checking and health communication.
UR  - https://arxiv.org/pdf/2505.08143v1.pdf
N1  - Source: page_1.html
ID  - 7
ER  - 
TY  - JOUR
TI  - Must Read: A Systematic Survey of Computational Persuasion
AU  - Bozdag, N.B.
AU  - Mehri, S.
AU  - Yang, X.
AU  - Tur, G.
AU  - Hakkani-Tür, D.
PY  - 2025
AB  - Persuasion is a fundamental aspect of communication, influencing decision-making across diverse contexts, from everyday conversations to high-stakes scenarios such as politics, marketing, and law. The rise of conversational AI systems has significantly expanded the scope of persuasion, introducing both opportunities and risks. AI-driven persuasion can be leveraged for beneficial applications, but also poses threats through manipulation and unethical influence. Moreover, AI systems are not only persuaders, but also susceptible to persuasion, making them vulnerable to adversarial attacks and bias reinforcement. Despite rapid advancements in AI-generated persuasive content, our understanding of what makes persuasion effective remains limited due to its inherently subjective and context-dependent nature. In this survey, we provide a comprehensive overview of computational persuasion, structured around three key perspectives: (1) AI as a Persuader, which explores AI-generated persuasive content and its applications; (2) AI as a Persuadee, which examines AI’s susceptibility to influence and manipulation; and (3) AI as a Persuasion Judge, which analyzes AI’s role in evaluating persuasive strategies, detecting manipulation, and ensuring ethical persuasion. We introduce a taxonomy for computational persuasion research and discuss key challenges, including evaluating persuasiveness, mitigating manipulative persuasion, and developing responsible AI-driven persuasive systems. Our survey outlines future research directions to enhance the safety, fairness, and effectiveness of AI-powered persuasion while addressing the risks posed by increasingly capable language models.
UR  - https://arxiv.org/pdf/2505.07775v1.pdf
N1  - Source: page_1.html
ID  - 8
ER  - 
TY  - JOUR
TI  - Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts
AU  - Aldahoul, N.
AU  - Ibrahim, H.
AU  - Varvello, M.
AU  - Rahwan, T.
AU  - Zaki, Y.
PY  - 2025
AB  - Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs’ apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.
UR  - https://arxiv.org/pdf/2505.04171v1.pdf
N1  - Source: page_1.html
ID  - 9
ER  - 
TY  - JOUR
TI  - Real-World Gaps in AI Governance Research
AU  - Strauss, I.
AU  - Moure, I.
AU  - Reilly, T.O.
AU  - Rosenblat, S.
PY  - 2025
AB  - Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 – March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas—model alignment and testing & evaluation—while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.
UR  - https://arxiv.org/pdf/2505.00174v2.pdf
N1  - Source: page_1.html
ID  - 10
ER  - 
TY  - JOUR
TI  - Algorithmic Addiction by Design: Big Tech’s Leverage of Dark Patterns to Maintain Market Dominance and its Challenge for Content Moderation
AU  - Nie, M.
PY  - 2025
AB  - Today’s largest technology corporations, especially ones with consumer-facing products such as social media platforms, use a variety of unethical and often outright illegal tactics to maintain their dominance. One tactic that has risen to the level of the public consciousness is the concept of addictive design, evidenced by the fact that excessive social media use has become a salient problem, particularly in the mental and social development of adolescents and young adults. As tech companies have developed more and more sophisticated artificial intelligence (AI) models to power their algorithmic recommender systems, they will become more successful at their goal of ensuring addiction to their platforms. This paper explores how online platforms intentionally cultivate addictive user behaviors and the broad societal implications, including on the health and well-being of children and adolescents. It presents the usage of addictive design — including the usage of dark patterns, persuasive design elements, and recommender algorithms — as a tool leveraged by technology corporations to maintain their dominance. Lastly, it describes the challenge of content moderation to address the problem and gives an overview of solutions at the policy level to counteract addictive design.
UR  - https://arxiv.org/pdf/2505.00054v1.pdf
N1  - Source: page_1.html
ID  - 11
ER  - 
TY  - JOUR
TI  - Information Retrieval in the Age of Generative AI: The RGB Model
AU  - Garetto, M.P.
AU  - Cornacchia, A.P.
AU  - Galante, F.
AU  - Nordio, A.
AU  - Tarable, A.P.
PY  - 2025
AB  - The advent of Large Language Models (LLMs) and generative AI is fundamentally transforming information retrieval and processing on the Internet, bringing both great potential and significant concerns regarding content authenticity and reliability. This paper presents a novel quantitative approach to shed light on the complex information dynamics arising from the growing use of generative AI tools. Despite their significant impact on the digital ecosystem, these dynamics remain largely uncharted and poorly understood. We propose a stochastic model to characterize the generation, indexing, and dissemination of information in response to new topics. This scenario particularly challenges current LLMs, which often rely on real-time Retrieval-Augmented Generation (RAG) techniques to overcome their static knowledge limitations. Our findings suggest that the rapid pace of generative AI adoption, combined with increasing user reliance, can outpace human verification, escalating the risk of inaccurate information proliferation across digital resources. An in-depth analysis of Stack Exchange data confirms that high-quality answers inevitably require substantial time and human effort to emerge. This underscores the considerable risks associated with generating persuasive text in response to new questions and highlights the critical need for responsible development and deployment of future generative AI tools.
UR  - https://arxiv.org/pdf/2504.20610v1.pdf
N1  - Source: page_1.html
ID  - 12
ER  - 
TY  - JOUR
TI  - A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies
AU  - Chen, Z.
AU  - Kalla, J.
AU  - Le, Q.
AU  - Sekhon, J.
AU  - Wang, R.
PY  - 2025
AB  - In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the “receive” and “accept” steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between $48-$74 per persuaded voter compared to $100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.
UR  - https://arxiv.org/pdf/2505.00036v1.pdf
N1  - Source: page_1.html
ID  - 13
ER  - 
TY  - JOUR
TI  - Conversations with AI Chatbots Increase Short-Term Vaccine Intentions But Do Not Outperform Standard Public Health Messaging
AU  - Sehgal, N.K.R.
AU  - Rai, S.
AU  - Tonneau, M.
AU  - Buttenheim, A.
AU  - Guntuku, S.C.
PY  - 2025
AB  - Large language model (LLM) based chatbots show promise in persuasive communication, but existing studies often rely on weak controls or focus on belief change rather than behavioral intentions or outcomes. This pre-registered multi-country (US, Canada, UK) randomized controlled trial involving 930 vaccine-hesitant parents evaluated brief (three-minute) multi-turn conversations with LLM-based chatbots against standard public health messaging approaches for increasing human papillomavirus (HPV) vaccine intentions for their children. Participants were randomly assigned to: (1) a weak control (no message), (2) a strong control reflecting the standard of care (reading official public health materials), or (3 and 4) one of two chatbot conditions. One chatbot was prompted to deliver short, conversational responses, while the other used the model’s default output style (longer with bullet points). While chatbot interactions significantly increased self-reported vaccination intent (by 7.1-10.3 points on a 100-point scale) compared to no message, they did not outperform standard public health materials, with the conversational chatbot performing significantly worse. Additionally, while the short-term effects of chatbot interactions faded during a 15-day follow-up, the effects of public health material persisted relative to no message. These findings suggest that while LLMs can effectively shift vaccination intentions in the short-term, their incremental value over existing public health communications is questionable—offering a more tempered view of their persuasive capabilities and highlighting the importance of integrating AI-driven tools alongside, rather than replacing, current public health strategies.
UR  - https://arxiv.org/pdf/2504.20519v2.pdf
N1  - Source: page_1.html
ID  - 14
ER  - 
TY  - JOUR
TI  - Examining the Impact of Technology-Driven Recommendations on Consumer Emotions and Intentions in Online Shopping
AU  - Patnaik, P.
AU  - Patra, S.K.
AU  - Kumar, S.
AU  - Bakkar, M.
PY  - 2025
AB  - This research explores the growing importance of personalized recommendations driven by Artificial Intelligence (AI) in virtual shopping to enhance customer satisfaction and engagement. Marketers leverage these personalized recommendations to establish strong emotional connections with customers. The study aimed to validate its hypotheses, using a sample size of 495 individuals selected through convenience sampling from India's urban population. The study's findings indicate that personalized product recommendations (PPR) and personalized visual recommendations (PVR) significantly impact consumer emotions, which, in turn, influence purchase intentions. The study focused on a younger demographic to validate its hypotheses. These findings have significant implications for e-commerce, emphasizing the need for personalized online recommendations based on the prediction of human emotions. Consumer expectations have risen in the current digital landscape, requiring marketers to develop systems that combine data, algorithms, and analytics to provide real-time, one-to-one, persuasive communication across the customer journey.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5231082&type=2
N1  - Source: page_1.html
ID  - 15
ER  - 
TY  - JOUR
TI  - LLM-based Semantic Augmentation for Harmful Content Detection
AU  - Meguellati, E.
AU  - Zeghina, A.
AU  - Sadiq, S.
AU  - Demartini, G.
PY  - 2025
AB  - Recent advances in large language models (LLMs) have demonstrated strong performance on simple text classification tasks, frequently under zero-shot settings. However, their efficacy declines when tackling complex social media challenges such as propaganda detection, hateful meme classification, and toxicity identification. Much of the existing work has focused on using LLMs to generate synthetic training data, overlooking the potential of LLM-based text preprocessing and semantic augmentation. In this paper, we introduce an approach that prompts LLMs to clean noisy text and provide context-rich explanations, thereby enhancing training sets without substantial increases in data volume. We systematically evaluate on the SemEval 2024 multi-label Persuasive Meme dataset and further validate on the Google Jigsaw toxic comments and Facebook hateful memes datasets to assess generalizability. Our results reveal that zero-shot LLM classification underperforms on these high-context tasks compared to supervised models. In contrast, integrating LLM-based semantic augmentation yields performance on par with approaches that rely on human-annotated data, at a fraction of the cost. These findings underscore the importance of strategically incorporating LLMs into machine learning (ML) pipeline for social media classification tasks, offering broad implications for combating harmful content online. Disclaimer: This paper contains examples of explicit language that may be disturbing to some readers.
UR  - https://arxiv.org/pdf/2504.15548v1.pdf
N1  - Source: page_1.html
ID  - 16
ER  - 
TY  - JOUR
TI  - Breaking the Shield: Vulnerabilities in Content Moderation for Multimodal Language Models
AU  - Das, A.B.
AU  - Sakib, S.K.
PY  - 2025
AB  - Text-to-image generation models, especially with recent breakthroughs like GPT-4o’s image generation, have rapidly transformed how visual content is created and shared across domains. Their ability to produce visually compelling images from natural language prompts has made them increasingly influential in digital media. However, a growing concern is the generation of misinformation or discriminatory information (e.g., images that visually encode or propagate widely debunked or false information) often in a persuasive and authoritative format. In this paper, we demonstrate how multimodal large language model (MMLLM) based image generation systems can be prompted to produce misinformation and discriminatory content. Additionally, we formulate the problem of developing a generalizable prompt template that adversaries could exploit to bypass the system guardrails. Our hypothesis is that prompts framed with narrative context are more successful in evading safeguards than overtly harmful ones. We conduct extensive simulations to validate these strategies and demonstrate the potential impact of the threat.
UR  - https://dx.doi.org/10.36227/techrxiv.174537593.33953859/v1
N1  - Source: page_1.html
ID  - 17
ER  - 
TY  - JOUR
TI  - A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations
AU  - Lygizou, Z.
AU  - Kalles, D.
PY  - 2025
AB  - Trust management provides an alternative solution for securing open, dynamic, and distributed multi-agent systems, where conventional cryptographic methods prove to be impractical. However, existing trust models face challenges related to agent mobility, changing behaviors, and the cold start problem. To address these issues we introduced a biologically inspired trust model in which trustees assess their own capabilities and store trust data locally. This design improves mobility support, reduces communication overhead, resists disinformation, and preserves privacy. Despite these advantages, prior evaluations revealed limitations of our model in adapting to provider population changes and continuous performance fluctuations. This study proposes a novel algorithm, incorporating a self-classification mechanism for providers to detect performance drops potentially harmful for the service consumers. Simulation results demonstrate that the new algorithm outperforms its original version and FIRE, a well-known trust and reputation model, particularly in handling dynamic trustee behavior. While FIRE remains competitive under extreme environmental changes, the proposed algorithm demonstrates greater adaptability across various conditions. In contrast to existing trust modeling research, this study conducts a comprehensive evaluation of our model using widely recognized trust model criteria, assessing its resilience against common trust-related attacks while identifying strengths, weaknesses, and potential countermeasures. Finally, several key directions for future research are proposed.
UR  - https://arxiv.org/pdf/2504.15301v1.pdf
N1  - Source: page_1.html
ID  - 18
ER  - 
TY  - JOUR
TI  - ZEROSUMEVAL: Scaling LLM Evaluation with Inter-Model Competition
AU  - Khan, H.
AU  - Alyahya, H.A.
AU  - Alnumay, Y.
AU  - Bari, M.S.
AU  - Yener, B.
PY  - 2025
AB  - Evaluating the capabilities of Large Language Models (LLMs) has traditionally relied on static benchmark datasets, human assessments, or model-based evaluations - methods that often suffer from overfitting, high costs, and biases. ZeroSumEval is a novel competition-based evaluation protocol that leverages zero-sum games to assess LLMs with dynamic benchmarks that resist saturation. ZeroSumEval encompasses a diverse suite of games, including security challenges (PyJail), classic games (Chess, Liar’s Dice, Poker), knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These games are designed to evaluate a range of AI capabilities such as strategic reasoning, planning, knowledge application, and creativity. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework. To demonstrate this, we conduct extensive experiments with >7000 simulations across 7 games and 13 models. Our results show that while frontier models from the GPT and Claude families can play common games and answer questions, they struggle to play games that require creating novel and challenging questions. We also observe that models cannot reliably jailbreak each other and fail generally at tasks requiring creativity. We release our code at https://github.com/facebookresearch/ZeroSumEval.
UR  - https://arxiv.org/pdf/2504.12562v1.pdf
N1  - Source: page_1.html
ID  - 19
ER  - 
TY  - JOUR
TI  - LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models
AU  - Liu, M.
AU  - Xu, Z.
AU  - Zhang, X.
AU  - Jia, R.
AU  - Huang, L.
PY  - 2025
AB  - Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PERSUSAFETY, the first comprehensive framework for the assessment of persuasion safety, which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PERSUSAFETY covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.
UR  - https://arxiv.org/pdf/2504.10430v1.pdf
N1  - Source: page_1.html
ID  - 20
ER  - 
TY  - JOUR
TI  - Refining Financial Consumer Complaints through Multi-Scale Model Interaction
AU  - Chen, B.-W.
AU  - Yen, A.-Z.
AU  - Chen, C.-C.
PY  - 2025
AB  - Legal writing demands clarity, formality, and domain-specific precision—qualities often lacking in documents authored by individuals without legal training. To bridge this gap, this paper explores the task of legal text refinement that transforms informal, conversational inputs into persuasive legal arguments. We introduce FinDR, a Chinese dataset of financial dispute records, annotated with official judgments on claim reasonableness. Our proposed method, Multi-Scale Model Interaction (MSMI), leverages a lightweight classifier to evaluate outputs and guide iterative refinement by Large Language Models (LLMs). Experimental results demonstrate that MSMI significantly outperforms single-pass prompting strategies. Additionally, we validate the generalizability of MSMI on several short-text benchmarks, showing improved adversarial robustness. Our findings reveal the potential of multi-model collaboration for enhancing legal document generation and broader text refinement tasks.
UR  - https://arxiv.org/pdf/2504.09903v1.pdf
N1  - Source: page_1.html
ID  - 21
ER  - 
TY  - JOUR
TI  - Working with Large Language Models to Enhance Messaging Effectiveness for Vaccine Confidence
AU  - Gullison, L.
AU  - Fu, F.
PY  - 2025
AB  - Vaccine hesitancy and misinformation are significant barriers to achieving widespread vaccination coverage. Smaller public health departments may not have expertise or resources for effective vaccine messaging. This paper explores the potential of ChatGPT-augmented messaging to promote confidence in vaccination uptake. We conducted a survey where participants chose between a pair of vaccination messages and assessed which was more persuasive and to what extent. In each pair of messages, there was was one original message and one ChatGPT-augmented message. At the end of the survey, participants were told that half of the messages were generated by ChatGPT. Participants were asked for a quantitative and qualitative response about how knowledge of a message’s ChatGPT status changed their impression of the messages. Overall, ChatGPT messages were rated slightly higher than the original messages. ChatGPT messages generally scored higher when they were longer. Respondents did not express major concerns about ChatGPT messaging, nor was there a significant relationship between views on ChatGPT and message ratings. Finally, there was a correlation between whether the message was positioned first or second in the question and the message’s score. Overall, our results point to the potential of ChatGPT to improve vaccination messaging with potential for future research to clarify this human-AI collaborative relationship.
UR  - https://arxiv.org/pdf/2504.09857v1.pdf
N1  - Source: page_1.html
ID  - 22
ER  - 
TY  - JOUR
TI  - Labeling Messages as AI-Generated Does Not Reduce Their Persuasive Effects
AU  - Gallegos, I.O.
AU  - Shani, C.
AU  - Shi, W.
AU  - Jurafsky, D.
AU  - Willer, R.
PY  - 2025
AB  - As generative artificial intelligence (AI) enables the creation and dissemination of information at massive scale and speed, it is increasingly important to understand how people perceive AI-generated content. One prominent policy proposal requires explicitly labeling AI-generated content to increase transparency and encourage critical thinking about the information, but prior research has not yet tested the effects of such labels. To address this gap, we conducted a survey experiment (N=1601) on a diverse sample of Americans, presenting participants with an AI-generated message about several public policies (e.g., allowing colleges to pay student-athletes), randomly assigning whether participants were told the message was generated by (a) an expert AI model, (b) a human policy expert, or (c) no label. We found that messages were generally persuasive, influencing participants’ views of the policies by 9.74 percentage points on average. However, while 94.6% of participants assigned to the AI and human label conditions believed the authorship labels, labels had no significant effects on participants’ attitude change toward the policies, judgments of message accuracy, nor intentions to share the message with others. These patterns were robust across a variety of participant characteristics, including prior knowledge of the policy, prior experience with AI, political party, education level, or age. Taken together, these results imply that, while authorship labels would likely enhance transparency, they are unlikely to substantially affect the persuasiveness of the labeled content, highlighting the need for alternative strategies to address challenges posed by AI-generated information.
UR  - https://arxiv.org/pdf/2504.09865v1.pdf
N1  - Source: page_1.html
ID  - 23
ER  - 
TY  - JOUR
TI  - Exploring the Effects of Large Language Models (Llms) on Register Variation in L2 Academic Writing: A Multidimensional Analysis of Pre- and Post-Llm-Refinement Texts
AU  - Bu, H.
PY  - 2025
AB  - The remarkable capabilities of Large Language Models (LLMs), such as ChatGPT, in assisting academic writing have inspired considerable interest, but empirical evidence remains scarce regarding how their refinements impact the register features essential to academic discourse. This study investigates register variation in L2 academic writing before and after LLM-refinement using Biber’s (1988) Multi-Dimensional (MD) analysis framework. 200 LLM-refined L2 student essays were compared with their original human-written versions on 6 dimensions and 67 linguistic features. Results indicate that through refinement, LLMs can help improve information density and syntactic complexity, thus making the texts more closely resemble academic written discourse. However, LLM-refinement also has limitations in preserving the register and rhetorical features of academic discourse, as it increases the narrativity and decreases the persuasiveness, abstractness, technicality and formality of L2 academic writing. It overlooks the rhetorical functions of some linguistic features in the academic genre, such as the use of tense and aspect to convey generality or importance. Prompt variations exert minimal impact. Therefore, human intervention is necessary to ensure that the refined texts exhibit a more academic-like register. The findings of the present study have useful pedagogical and practical implications in English for Academic Purposes.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5211017&type=2
N1  - Source: page_1.html
ID  - 24
ER  - 
TY  - JOUR
TI  - Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models
AU  - Zhao, X.
AU  - Deng, Y.
AU  - Wang, W.
AU  - Ng, S.-K.
AU  - Chua, T.-S.
PY  - 2025
AB  - Conversational Recommender Systems (CRSs) engage users in multi-turn interactions to deliver personalized recommendations. The emergence of large language models (LLMs) further enhances these systems by enabling more natural and dynamic user interactions. However, a key challenge remains in understanding how personality traits shape conversational recommendation outcomes. Psychological evidence highlights the influence of personality traits on user interaction behaviors. To address this, we introduce an LLM-based personality-aware user simulation for CRSs (PerCRS). The user agent induces customizable personality traits and preferences, while the system agent possesses the persuasion capability to simulate realistic interaction in CRSs. We incorporate multi-aspect evaluation to ensure robustness and conduct extensive analysis from both user and system perspectives. Experimental results demonstrate that state-of-the-art LLMs can effectively generate diverse user responses aligned with specified personality traits, thereby prompting CRSs to dynamically adjust their recommendation strategies. Our experimental analysis offers empirical insights into the impact of personality traits on the outcomes of conversational recommender systems.
UR  - https://arxiv.org/pdf/2504.12313v1.pdf
N1  - Source: page_1.html
ID  - 25
ER  - 
TY  - JOUR
TI  - Innovative Design and Development of Brand Visual System under the Background of Artificial Intelligence-Assisted Visual Communication Design
AU  - Cai, Y.
PY  - 2025
AB  - Businesses that want to stand out in today's crowded marketplace must have a distinct identity for their brand. Visual branding stands out among the other aspects that make up a business's identity as a potent means of captivating consumers and communicating the essence of the brand. Visual branding, when done well, may influence customers' opinions, evoke strong feelings, and stamp themselves in their memories. Because every visual aspect of a brand—its logo, color palette, and typography—is meticulously designed to communicate the character, principles, and goals of the company. One of the most influential aspects of branding is visual identity. Brands may convey their beliefs, connect with customers on an emotional level, and differentiate themselves from competitors by using design elements like logos, colors, typography, images, and graphics. Visual branding, when done well, becomes an inherent aspect of a brand's identity and helps it to make an impact on its intended consumers. Visual branding is an effective tool for building a distinct brand identity for the company. Revolutionizing graphic design are Artificial Intelligence (AI) tools that can create visuals from natural language. But with visual persuasion, the picture is more than simply a representation; it is the message. The need to comprehend the potential applications of AI systems in the creation of visual communication gives birth to this. An investigation of grounded theory based on the opinions of experts in the key fields of visual communication led to the development of a model that this article uses to define the theoretical-methodological aspects involved in the creation of convincing visual discourses. As a result, we provide a strategy that, when followed, will lead designers through seven steps in creating convincing messages. In addition, we provide an analysis that may help with the logical and well-informed integration of stage-specific technologies, processes, and methodologies. In this study, visual communication design is implemented using ResNet101 model with Improved Artificial Bee Colony optimization model for feature selection and processing the visual and text elements in to the environment.
UR  - https://www.researchsquare.com/article/rs-6003788/v1.pdf
N1  - Source: page_1.html
ID  - 26
ER  - 
TY  - JOUR
TI  - Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge
AU  - Zeng, D.
AU  - Legaspi, R.
AU  - Sun, Y.
AU  - Spirtes, P.
AU  - Zhang, K.
PY  - 2025
AB  - We hypothesize that optimal system responses emerge from adaptive strategies grounded in causal and counterfactual knowledge. Counterfactual inference allows us to create hypothetical scenarios to examine the effects of alternative system responses. We enhance this process through causal discovery, which identifies the strategies informed by the underlying causal structure that govern system behaviors. Moreover, we consider the psychological constructs and unobservable noises that might be influencing user-system interactions as latent factors. We show that these factors can be effectively estimated. We employ causal discovery to identify strategy-level causal relationships among user and system utterances, guiding the generation of personalized counterfactual dialogues. We model the user utterance strategies as causal factors, enabling system strategies to be treated as counterfactual actions. Furthermore, we optimize policies for selecting system responses based on counterfactual data. Our results using a real-world dataset on social good demonstrate significant improvements in persuasive system outcomes, with increased cumulative rewards validating the efficacy of causal discovery in guiding personalized counterfactual inference and optimizing dialogue policies for a persuasive dialogue system.
UR  - https://arxiv.org/pdf/2504.13904v1.pdf
N1  - Source: page_1.html
ID  - 27
ER  - 
TY  - JOUR
TI  - Simulating Persuasive Dialogues on Meat Reduction with Generative Agents
AU  - Ahnert, G.
AU  - Wurth, E.
AU  - Strohmaier, M.
AU  - Mata, J.
PY  - 2025
AB  - Meat reduction benefits human and planetary health, but social norms keep meat central in shared meals. To date, the development of communication strategies that promote meat reduction while minimizing social costs has required the costly involvement of human participants at each stage of the process. We present work in progress on simulating multi-round dialogues on meat reduction between Generative Agents based on large language models (LLMs). We measure our main outcome using established psychological questionnaires based on the Theory of Planned Behavior and additionally investigate Social Costs. We find evidence that our preliminary simulations produce outcomes that are (i) consistent with theoretical expectations; and (ii) valid when compared to data from previous studies with human participants. Generative agent-based models are a promising tool for identifying novel communication strategies on meat reduction—tailored to highly specific participant groups—to then be tested in subsequent studies with human participants.
UR  - https://arxiv.org/pdf/2504.04872v1.pdf
N1  - Source: page_1.html
ID  - 28
ER  - 
TY  - JOUR
TI  - Locations of Characters in Narratives: Andersen and Persuasion Datasets
AU  - Özyurt, B.
AU  - Arkhmammadova, R.
AU  - Yuret, D.
PY  - 2025
AB  - The ability of machines to grasp spatial understanding within narrative contexts is an intriguing aspect of reading comprehension that continues to be studied. Motivated by the goal to test the AI’s competence in understanding the relationship between characters and their respective locations in narratives, we introduce two new datasets: Andersen and Persuasion. For the Andersen dataset, we selected fifteen children’s stories from "Andersen’s Fairy Tales" by Hans Christian Andersen and manually annotated the characters and their respective locations throughout each story. Similarly, for the Persuasion dataset, characters and their locations in the novel "Persuasion" by Jane Austen were also manually annotated. We used these datasets to prompt Large Language Models (LLMs). The prompts are created by extracting excerpts from the stories or the novel and combining them with a question asking the location of a character mentioned in that excerpt. Out of the five LLMs we tested, the best-performing one for the Andersen dataset accurately identified the location in 61.85% of the examples, while for the Persuasion dataset, the best-performing one did so in 56.06% of the cases.
UR  - https://arxiv.org/pdf/2504.03434v1.pdf
N1  - Source: page_1.html
ID  - 29
ER  - 
TY  - JOUR
TI  - Persuasive Calibration
AU  - Feng, Y.
AU  - Tang, W.
PY  - 2025
AB  - We introduce and study the persuasive calibration problem, where a principal aims to provide trustworthy predictions about underlying events to a downstream agent to make desired decisions. We adopt the standard calibration framework that regulates predictions to be unbiased conditional on their own value, and thus, they can reliably be interpreted at the face value by the agent. Allowing a small calibration error budget, we aim to answer the following question: what is and how to compute the optimal predictor under this calibration error budget, especially when there exists incentive misalignment between the principal and the agent? We focus on standard ℓt-norm Expected Calibration Error (ECE) metric. We develop a general framework by viewing predictors as post-processed versions of perfectly calibrated predictors. Using this framework, we first characterize the structure of the optimal predictor. Specifically, when the principal’s utility is event-independent and for ℓ1-norm ECE, we show: (1) the optimal predictor is over-(resp. under-) confident for high (resp. low) true expected outcomes, while remaining perfectly calibrated in the middle; (2) the miscalibrated predictions exhibit a collinearity structure with the principal’s utility function. On the algorithmic side, we provide a FPTAS for computing approximately optimal predictor for general principal utility and general ℓt-norm ECE. Moreover, for the ℓ1- and ℓ∞-norm ECE, we provide polynomial-time algorithms that compute the exact optimal predictor.
UR  - https://arxiv.org/pdf/2504.03211v1.pdf
N1  - Source: page_1.html
ID  - 30
ER  - 
TY  - JOUR
TI  - Off-Policy Evaluation for Sequential Persuasion Process with Unobserved Confounding
AU  - Venkatesh, N.S.
AU  - Bang, H.
AU  - Malikopoulos, A.A.
PY  - 2025
AB  - In this paper, we expand the Bayesian persuasion framework to account for unobserved confounding variables in sender-receiver interactions. While traditional models typically assume that belief updates follow Bayesian principles, real-world scenarios often involve hidden variables that impact the receiver’s belief formation and decision-making. We conceptualize this as a sequential decision-making problem, where the sender and receiver interact over multiple rounds. In each round, the sender communicates with the receiver, who also interacts with the environment. Crucially, the receiver’s belief update is affected by an unobserved confounding variable. By reformulating this scenario as a Partially Observable Markov Decision Process (POMDP), we capture the sender’s incomplete information regarding both the dynamics of the receiver’s beliefs and the unobserved confounder. We prove that finding an optimal observation-based policy in this POMDP is equivalent to solving for an optimal signaling strategy in the original persuasion framework. Furthermore, we demonstrate how this reformulation facilitates the application of proximal learning for off-policy evaluation (OPE) in the persuasion process. This advancement enables the sender to evaluate alternative signaling strategies using only observational data from a behavioral policy, thus eliminating the necessity for costly new experiments.
UR  - https://arxiv.org/pdf/2504.01211v1.pdf
N1  - Source: page_1.html
ID  - 31
ER  - 
TY  - JOUR
TI  - When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)
AU  - Agarwal, M.
AU  - Khanna, D.
PY  - 2025
AB  - In many real-world scenarios, a single Large Language Model (LLM) may encounter contradictory claims—some accurate, others forcefully incorrect—and must judge which is true. We investigate this risk in a single-turn, multi-agent debate framework: one LLM-based agent provides a factual answer from TruthfulQA, another vigorously defends a falsehood, and the same LLM architecture serves as judge. We introduce the Confidence-Weighted Persuasion Override Rate (CW-POR), which captures not only how often the judge is deceived but also how strongly it believes the incorrect choice. Our experiments on five open-source LLMs (3B–14B parameters), where we systematically vary agent verbosity (30–300 words), reveal that even smaller models can craft persuasive arguments that override truthful answers—often with high confidence. These findings underscore the importance of robust calibration and adversarial testing to prevent LLMs from confidently endorsing misinformation.
UR  - https://arxiv.org/pdf/2504.00374v1.pdf
N1  - Source: page_1.html
ID  - 32
ER  - 
TY  - JOUR
TI  - Towards Personalized Conversational Sales Agents: Contextual User Profiling for Strategic Action
AU  - Kim, T.
AU  - Lee, J.
AU  - Yoon, S.
AU  - Kim, S.
AU  - Lee, D.
PY  - 2025
AB  - Conversational Recommender Systems (CRSs) aim to engage users in dialogue to provide tailored recommendations. While traditional CRSs focus on eliciting preferences and retrieving items, real-world e-commerce interactions involve more complex decision-making, where users consider multiple factors beyond simple attributes. To bridge this gap, we introduce Conversational Sales (CSALES), a novel task that unifies preference elicitation, recommendation, and persuasion to better support user decision-making. For a realistic evaluation of CSALES, we present CSUSER, an LLM-based user simulator constructed from real-world data, modeling diverse user profiles with needs and personalities. Additionally, we propose CSI, a conversational sales agent that proactively infers contextual profiles through dialogue for personalized action planning. Extensive experiments demonstrate that CSUSER effectively replicates real-world users and emphasize the importance of contextual profiling for strategic action selection, ultimately driving successful purchases in e-commerce.
UR  - https://arxiv.org/pdf/2504.08754v3.pdf
N1  - Source: page_1.html
ID  - 33
ER  - 
TY  - JOUR
TI  - MemInsight: Autonomous Memory Augmentation for LLM Agents
AU  - Salama, R.
AU  - Cai, J.
AU  - Yuan, M.
AU  - Zhang, Y.
AU  - Benajiba, Y.
PY  - 2025
AB  - Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.
UR  - https://arxiv.org/pdf/2503.21760v1.pdf
N1  - Source: page_1.html
ID  - 34
ER  - 
TY  - JOUR
TI  - Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models
AU  - Ke, S.-W.
AU  - Lai, G.-Y.
AU  - Fang, G.-L.
AU  - Kao, H.-Y.
PY  - 2025
AB  - Large language models (LLMs) are designed to align with human values in their responses. This study exploits LLMs with an iterative prompting technique where each prompt is systematically modified and refined across multiple iterations to enhance its effectiveness in jailbreaking attacks progressively. This technique involves analyzing the response patterns of LLMs, including GPT-3.5, GPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts to evade the LLMs’ ethical and security constraints. Persuasion strategies enhance prompt effectiveness while maintaining consistency with malicious intent. Our results show that the attack success rates (ASR) increase as the attacking prompts become more refined with the highest ASR of 90% for GPT4 and ChatGLMa and the lowest ASR of 68% for LLaMa2. Our technique outperforms baseline techniques (PAIR and PAP) in ASR and shows comparable performance with GCG and ArtPrompt.
UR  - https://arxiv.org/pdf/2503.20320v1.pdf
N1  - Source: page_1.html
ID  - 35
ER  - 
TY  - JOUR
TI  - Competitive Multi-armed Bandit Games for Resource Sharing
AU  - Li, H.
AU  - Duan, L.
PY  - 2025
AB  - In modern resource-sharing systems, multiple agents access limited resources with unknown stochastic conditions to perform tasks. When multiple agents access the same resource (arm) simultaneously, they compete for successful usage, leading to contention and reduced rewards. This motivates our theoretical study of competitive multi-armed bandit (CMAB) games. In this paper, we study a new N-player K-arm competitive MAB game, where non-myopic players (agents) compete with each other to form diverse private estimations of unknown arms over time. Their possible collisions on the same arms and the time-varying nature of arm rewards make the policy analysis here more involved than the existing studies for myopic players. We explicitly analyze the threshold-based structures of the social optimum and the existing selfish policy, showing that the latter causes prolonged convergence times (Formula presented), while the socially optimal policy with coordinated communication reduces it to (Formula presented). Based on the policy comparison, we prove that the competition among selfish players for the best arm can result in an infinite price of anarchy (PoA), indicating an arbitrarily large efficiency loss compared to the social optimum. We further prove that no informational (non-monetary) mechanism (including Bayesian persuasion) can reduce the infinite PoA, as strategic misreporting by non-myopic players undermines such approaches. To address this, we propose a Combined Informational and Side-Payment (CISP) mechanism, which provides socially optimal arm recommendations with proper informational and monetary incentives to players according to their diverse and time-varying private beliefs. Our CISP mechanism keeps ex-post budget balanced for the social planner and ensures truthful reporting from players, thereby achieving the minimum PoA = 1 and the same convergence time as the social optimum.
UR  - https://arxiv.org/pdf/2503.20975v1.pdf
N1  - Source: page_1.html
ID  - 36
ER  - 
TY  - JOUR
TI  - Optimizing Influence Campaigns: Nudging Under Bounded Confidence
AU  - Chen, Y.-S.
AU  - Zaman, T.
PY  - 2025
AB  - Influence campaigns in online social networks are often run by organizations, political parties, and nation states to influence large audiences. These campaigns are employed through the use of agents in the network that share persuasive content. Yet, their impact might be minimal if the audiences remain unswayed, often due to the bounded confidence phenomenon, where only a narrow spectrum of viewpoints can influence them. Here we show that to persuade under bounded confidence, an agent must nudge its targets to gradually shift their opinions. Using a control theory approach, we show how to construct an agent’s nudging policy under the bounded confidence opinion dynamics model and also how to select targets for multiple agents in an influence campaign on a social network. Simulations on real Twitter networks show that a multi-agent nudging policy can shift the mean opinion, decrease opinion polarization, or even increase it. We find that our nudging based policies outperform other common techniques that do not consider the bounded confidence effect. Finally, we show how to craft prompts for large language models, such as ChatGPT, to generate text-based content for real nudging policies. This illustrates the practical feasibility of our approach, allowing one to go from mathematical nudging policies to real social media content.
UR  - https://arxiv.org/pdf/2503.18331v1.pdf
N1  - Source: page_1.html
ID  - 37
ER  - 
TY  - JOUR
TI  - Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies
AU  - Zeng, D.
AU  - Legaspi, R.
AU  - Sun, Y.
AU  - Spirtes, P.
AU  - Zhang, K.
PY  - 2025
AB  - Tailoring persuasive conversations to users leads to more effective persuasion. However, existing dialogue systems often struggle to adapt to dynamically evolving user states. This paper presents a novel method that leverages causal discovery and counterfactual reasoning for optimizing system persuasion capability and outcomes. We employ the Greedy Relaxation of the Sparsest Permutation (GRaSP) algorithm to identify causal relationships between user and system utterance strategies, treating user strategies as states and system strategies as actions. GRaSP identifies user strategies as causal factors influencing system responses, which inform Bidirectional Conditional Generative Adversarial Networks (BiCoGAN) in generating counterfactual utterances for the system. Subsequently, we use the Dueling Double Deep Q-Network (D3QN) model to utilize counterfactual data to determine the best policy for selecting system utterances. Our experiments with the PersuasionForGood dataset show measurable improvements in persuasion outcomes using our approach over baseline methods. The observed increase in cumulative rewards and Q-values highlights the effectiveness of causal discovery in enhancing counterfactual reasoning and optimizing reinforcement learning policies for online dialogue systems.
UR  - https://arxiv.org/pdf/2503.16544v1.pdf
N1  - Source: page_1.html
ID  - 38
ER  - 
TY  - JOUR
TI  - Spotting Persuasion: A Low-cost Model for Persuasion Detection in Political Ads on Social Media
AU  - Meguellati, E.
AU  - Civelli, S.
AU  - Bernardelle, P.
AU  - Sadiq, S.
AU  - Demartini, G.
PY  - 2025
AB  - In the realm of political advertising, persuasion operates as a pivotal element within the broader framework of propaganda, exerting profound influences on public opinion and electoral outcomes. In this paper, we (1) introduce a lightweight model for persuasive text detection that achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3, while significantly reducing the computational resource requirements; and (2) leverage the proposed model to gain insights into political campaigning strategies on social media platforms by applying it to a real-world dataset we curated, consisting of Facebook political ads from the 2022 Australian Federal election campaign. Our study shows how subtleties can be found in persuasive political advertisements and presents a pragmatic approach to detect and analyze such strategies with limited resources, enhancing transparency in social media political campaigns.
UR  - https://arxiv.org/pdf/2503.13844v1.pdf
N1  - Source: page_1.html
ID  - 39
ER  - 
TY  - JOUR
TI  - AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence
AU  - Li, Z.
AU  - Huang, S.
AU  - Wang, J.
AU  - Wang, W.Y.
AU  - Yan, X.
PY  - 2025
AB  - As language agents progressively automate critical tasks across domains, their ability to operate within operational constraints and safety protocols becomes essential. While extensive research has demonstrated these agents’ effectiveness in downstream task completion, their reliability in following operational procedures and constraints remains largely unexplored. To this end, we present AgentOrca, a dual-system framework for evaluating language agents’ compliance with operational constraints and routines. Our framework encodes action constraints and routines through both natural language prompts for agents and corresponding executable code serving as ground truth for automated verification. Through an automated pipeline of test case generation and evaluation across five real-world domains, we quantitatively assess current language agents’ adherence to operational constraints. Our findings reveal notable performance gaps among state-of-the-art models, with large reasoning models like o1 demonstrating superior compliance while others show significantly lower performance, particularly when encountering complex constraints or user persuasion attempts.1
UR  - https://arxiv.org/pdf/2503.08669v1.pdf
N1  - Source: page_1.html
ID  - 40
ER  - 
TY  - JOUR
TI  - Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models
AU  - Yeginbergen, A.
AU  - Oronoz, M.
AU  - Agerri, R.
PY  - 2025
AB  - This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems. Data and code publicly available.1
UR  - https://arxiv.org/pdf/2503.05328v1.pdf
N1  - Source: page_1.html
ID  - 41
ER  - 
TY  - JOUR
TI  - AI-POWERED EPISODIC FUTURE THINKING
AU  - Ahmadi, S.
AU  - Rockwell, M.
AU  - Stuart, M.
AU  - Stein, J.
AU  - Fox, E.A.
PY  - 2025
AB  - Episodic Future Thinking (EFT) is an intervention that involves vividly imagining personal future events and experiences in detail. It has shown promise as an intervention to reduce delay discounting—the tendency to devalue delayed rewards in favor of immediate gratification—and to promote behavior change in a range of maladaptive health behaviors. We present EFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model, designed to generate EFT cues for users with lifestyle-related conditions. To evaluate the chatbot, we conducted a user study that included usability assessments and user evaluations based on content characteristics questionnaires, followed by semi-structured interviews. The study provides qualitative insights into participants’ experiences and interactions with the chatbot and its usability. Our findings highlight the potential application of AI chatbots based on Large Language Models (LLMs) in EFT interventions, and offer design guidelines for future behavior-oriented applications.
UR  - https://arxiv.org/pdf/2503.16484v1.pdf
N1  - Source: page_1.html
ID  - 42
ER  - 
TY  - JOUR
TI  - WritingBench: A Comprehensive Benchmark for Generative Writing
AU  - Wu, Y.
AU  - Mei, J.
AU  - Yan, M.
AU  - Jin, Q.
AU  - Huang, F.
PY  - 2025
AB  - Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework’s validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.
UR  - https://arxiv.org/pdf/2503.05244v2.pdf
N1  - Source: page_1.html
ID  - 43
ER  - 
TY  - JOUR
TI  - The Influence of Prior Discourse on Conversational Agent-Driven Decision-Making
AU  - Pilli, S.
AU  - Nallur, V.
PY  - 2025
AB  - Persuasion through conversation has been the focus of much research. Nudging is a popular strategy to influence decision-making in physical and digital settings. However, conversational agents employing “nudging” have not received significant attention. We explore the manifestation of cognitive biases—the underlying psychological mechanisms of nudging—and investigate how the complexity of prior dialogue tasks impacts decision-making facilitated by conversational agents. Our research used a between-group experimental design, involving 756 participants randomly assigned to either a simple or complex task before encountering a decision-making scenario. Three scenarios were adapted from Samuelsonś classic experiments on status-quo bias, the underlying mechanism of default nudges. Our results aligned with previous studies in two out of three simple-task scenarios. Increasing task complexity consistently shifted effect-sizes toward our hypothesis, though bias was significant in only one case. These findings inform conversational nudging strategies and highlight inherent biases relevant to behavioural economics.
UR  - https://arxiv.org/pdf/2503.04692v1.pdf
N1  - Source: page_1.html
ID  - 44
ER  - 
TY  - JOUR
TI  - “Impressively Scary:” Exploring User Perceptions and Reactions to Unraveling Machine Learning Models in Social Media Applications
AU  - West, J.
AU  - Cagiltay, B.
AU  - Zhang, S.
AU  - Fawaz, K.
AU  - Banerjee, S.
PY  - 2025
AB  - Machine learning models deployed locally on social media applications are used for features, such as face filters which read faces in-real time, and they expose sensitive attributes to the apps. However, the deployment of machine learning models, e.g., when, where, and how they are used, in social media applications is opaque to users. We aim to address this inconsistency and investigate how social media user perceptions and behaviors change once exposed to these models. We conducted user studies (N=21) and found that participants were unaware to both what the models output and when the models were used in Instagram and TikTok, two major social media platforms. In response to being exposed to the models’ functionality, we observed long term behavior changes in 8 participants. Our analysis uncovers the challenges and opportunities in providing transparency for machine learning models that interact with local user data.
UR  - https://arxiv.org/pdf/2503.03927v1.pdf
N1  - Source: page_1.html
ID  - 45
ER  - 
TY  - JOUR
TI  - AI-Powered Lawyering: AI Reasoning Models, Retrieval Augmented Generation, and the Future of Legal Practice
AU  - Schwarcz, D.
AU  - Manning, S.
AU  - Barry, P.
AU  - Prescott, J.J.
AU  - Rich, B.
PY  - 2025
AB  - Generative AI is set to transform the legal profession, but its full impact remains uncertain. While AI models like GPT-4 improve the efficiency with which legal work can be completed, they can at times make up cases and “hallucinate” facts, thereby undermining legal judgment, particularly in complex tasks handled by skilled lawyers. This article examines two emerging AI innovations that may mitigate these lingering issues: Retrieval Augmented Generation (RAG), which grounds AI-powered analysis in legal sources, and AI reasoning models, which structure complex reasoning before generating output. We conducted the first randomized controlled trial assessing these technologies, assigning upper-level law students to complete six legal tasks using a RAG-powered legal AI tool (Vincent AI), an AI reasoning model (OpenAI’s o1-preview), or no AI. We find that both AI tools significantly enhanced legal work quality, a marked contrast with previous research examining older large language models like GPT-4. Moreover, we find that these models maintain the efficiency benefits associated with use of older AI technologies. Our findings show that AI assistance significantly boosts productivity in five out of six tested legal tasks, with Vincent yielding statistically significant gains of approximately 38% to 115% and o1-preview increasing productivity by 34% to 140%, with particularly strong effects in complex tasks like drafting persuasive letters and analyzing complaints. Notably, o1-preview improved the analytical depth of participants’ work product but resulted in some hallucinations, whereas Vincent AI-aided participants produced roughly the same amount of hallucinations as participants who did not use AI at all. These findings suggest that integrating domain-specific RAG capabilities with reasoning models could yield synergistic improvements, shaping the next generation of AI-powered legal tools and the future of lawyering more generally.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5162111&type=2
N1  - Source: page_1.html
ID  - 46
ER  - 
TY  - JOUR
TI  - Intolerable Risk Threshold Recommendations for Artificial Intelligence
AU  - Raman, D.
AU  - Madkour, N.
AU  - Murphy, E.R.
AU  - Jackson, K.
AU  - Newman, J.
PY  - 2025
AB  - Frontier AI models - highly capable foundation models at the cutting edge ofAI development - may pose severe risks to public safety, human rights,economic stability, and societal value in the coming years. These risks couldarise from deliberate adversarial misuse, system failures, unintended cascadingeffects, or simultaneous failures across multiple models. In response to such risks, at the AI Seoul Summit in May 2024, 16 global AIindustry organizations signed the Frontier AI Safety Commitments, and 27nations and the EU issued a declaration on their intent to define thesethresholds. To fulfill these commitments, organizations must determine anddisclose ``thresholds at which severe risks posed by a model or system, unlessadequately mitigated, would be deemed intolerable.\'\' To assist in setting and operationalizing intolerable risk thresholds, weoutline key principles and considerations; for example, to aim for ``good, notperfect\'\' thresholds in the face of limited data on rapidly advancing AIcapabilities and consequently evolving risks. We also propose specificthreshold recommendations, including some detailed case studies, for a subsetof risks across eight risk categories: (1) Chemical, Biological, Radiological,and Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)Persuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,and (8) Socioeconomic Disruption. Our goal is to serve as a starting point orsupplementary resource for policymakers and industry leaders, encouragingproactive risk management that prioritizes preventing intolerable risks (exante) rather than merely mitigating them after they occur (ex post).
UR  - https://arxiv.org/pdf/2503.05812v1.pdf
N1  - Source: page_1.html
ID  - 47
ER  - 
TY  - JOUR
TI  - Can (A)I Change Your Mind?
AU  - Havin, M.
AU  - Kleinman, T.W.
AU  - Koren, M.
AU  - Dover, Y.
AU  - Goldstein, A.
PY  - 2025
AB  - The increasing integration of large language models (LLMs) based conversational agents into everyday life raises critical cognitive and social questions about their potential to influence human opinions. Although previous studies have shown that LLM-based agents can generate persuasive content, these typically involve controlled English-language settings. Addressing this, our preregistered study explored LLMs’ persuasive capabilities in more ecological, unconstrained scenarios, examining both static (written paragraphs) and dynamic (conversations via Telegram) interaction types. Conducted entirely in Hebrew with 200 participants, the study assessed the persuasive effects of both LLM and human interlocutors on controversial civil policy topics. Results indicated that participants adopted LLM and human perspectives similarly, with significant opinion changes evident across all conditions, regardless of interlocutor type or interaction mode. Confidence levels increased significantly in most scenarios. These findings demonstrate LLM-based agents’ robust persuasive capabilities across diverse sources and settings, highlighting their potential impact on shaping public opinions.
UR  - https://arxiv.org/pdf/2503.01844v3.pdf
N1  - Source: page_1.html
ID  - 48
ER  - 
TY  - JOUR
TI  - ChatGPT for President! Presupposed content in politicians versus GPT-generated texts
AU  - Garassino, D.
AU  - Brocca, N.
AU  - Masia, V.
PY  - 2025
AB  - This study examines ChatGPT-4’s capability to replicate linguistic strategies used in political discourse, focusing on its potential for manipulative language generation. As large language models (LLMs) become increasingly popular for text generation, concerns have grown regarding their role in spreading fake news and propaganda. This research compares real political speeches with those generated by ChatGPT, with an emphasis on presuppositions—a rhetorical device that subtly influences audiences by packaging some content as already known at the moment of utterance, thus swaying opinions without explicit argumentation. Using a corpus-based pragmatic analysis, this study assesses how well ChatGPT can mimic these persuasive strategies. The findings reveal that, although ChatGPT-generated texts contain many manipulative presuppositions, key differences emerge in their frequency, form and function, compared with politicians. For instance, ChatGPT often relies on change-of-state verbs used in fixed phrases, whereas politicians rely on presupposition triggers in more varied and creative ways. Such differences, however, are challenging to detect with the “naked eye,” underscoring the potential risks posed by LLMs in political and public discourse.
UR  - https://arxiv.org/pdf/2503.01269v1.pdf
N1  - Source: page_1.html
ID  - 49
ER  - 
TY  - JOUR
TI  - Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models
AU  - Bozdag, N.B.
AU  - Mehri, S.
AU  - Tur, G.
AU  - Hakkani-Tür, D.
PY  - 2025
AB  - Large Language Models (LLMs) demonstrate persuasive capabilities that rival human-level persuasion. While these capabilities can be used for social good, they also present risks of potential misuse. Moreover, LLMs’ susceptibility to persuasion raises concerns about alignment with ethical principles. To study these dynamics, we introduce Persuade Me If You Can (PMIYC), an automated framework for evaluating persuasion through multi-agent interactions. Here, PERSUADER agents engage in multi-turn conversations with the PERSUADEE agents, allowing us to measure LLMs’ persuasive effectiveness and their susceptibility to persuasion. We conduct comprehensive evaluations across diverse LLMs, ensuring each model is assessed against others in both subjective and misinformation contexts. We validate the efficacy of our framework through human evaluations and show alignment with prior work. PMIYC offers a scalable alternative to human annotation for studying persuasion in LLMs. Through PMIYC, we find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness, outperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50% greater resistance to persuasion for misinformation compared to Llama-3.3-70B. These findings provide empirical insights into the persuasive dynamics of LLMs and contribute to the development of safer AI systems.
UR  - https://arxiv.org/pdf/2503.01829v2.pdf
N1  - Source: page_1.html
ID  - 50
ER  - 
TY  - JOUR
TI  - AI persuading AI vs AI persuading Humans: LLMs’ Differential Effectiveness in Promoting Pro-Environmental Behavior
AU  - Doudkin, A.
AU  - Pataranutaporn, P.
AU  - Maes, P.
PY  - 2025
AB  - Pro-environmental behavior (PEB) is vital to combat climate change, yet turning awareness into intention and action remains elusive. We explore large language models (LLMs) as tools to promote PEB, comparing their impact across 3,200 participants: real humans (n=1,200), simulated humans based on actual participant data (n=1,200), and fully synthetic personas (n=1,200). All three participant groups faced personalized or standard chatbots, or static statements, employing four persuasion strategies (moral foundations, future self-continuity, action orientation, or”freestyle” chosen by the LLM). Results reveal a”synthetic persuasion paradox”: synthetic and simulated agents significantly affect their post-intervention PEB stance, while human responses barely shift. Simulated participants better approximate human trends but still overestimate effects. This disconnect underscores LLM’s potential for pre-evaluating PEB interventions but warns of its limits in predicting real-world behavior. We call for refined synthetic modeling and sustained and extended human trials to align conversational AI’s promise with tangible sustainability outcomes.
UR  - https://arxiv.org/pdf/2503.02067v1.pdf
N1  - Source: page_1.html
ID  - 51
ER  - 
TY  - JOUR
TI  - Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions
AU  - Borah, A.
AU  - Mihalcea, R.
AU  - Pérez-Rosas, V.
PY  - 2025
AB  - Existing challenges in misinformation exposure and susceptibility vary across demographic groups, as some populations are more vulnerable to misinformation than others. Large language models (LLMs) introduce new dimensions to these challenges through their ability to generate persuasive content at scale and reinforcing existing biases. This study investigates the bidirectional persuasion dynamics between LLMs and humans when exposed to misinformative content. We analyze human-to-LLM influence using human-stance datasets and assess LLM-to-human influence by generating LLM-based persuasive arguments. Additionally, we use a multi-agent LLM framework to analyze the spread of misinformation under persuasion among demographic-oriented LLM agents. Our findings show that demographic factors influence susceptibility to misinformation in LLMs, closely reflecting the demographic-based patterns seen in human susceptibility. We also find that, similar to human demographic groups, multi-agent LLMs exhibit echo chamber behavior. This research explores the interplay between humans and LLMs, highlighting demographic differences in the context of misinformation and offering insights for future interventions.
UR  - https://arxiv.org/pdf/2503.02038v1.pdf
N1  - Source: page_1.html
ID  - 52
ER  - 
TY  - JOUR
TI  - Leveraging LLMs for Mental Health: Detection and Recommendations from Social Discussions
AU  - Aggarwal, V.
AU  - Thukral, S.
AU  - Patel, K.
AU  - Chatterjee, A.
PY  - 2025
AB  - Textual data from social platforms captures various aspects of mental health through discussions around and across issues, while users reach out for help and others sympathize and offer support. We propose a comprehensive framework that leverages Natural Language Processing (NLP) and Generative AI techniques to identify and assess mental health disorders, detect their severity, and create recommendations for behavior change and therapeutic interventions based on users’ posts on Reddit. To classify the disorders, we use rule-based labeling methods as well as advanced pre-trained NLP models to extract nuanced semantic features from the data. We fine-tune domain-adapted and generic pre-trained NLP models based on predictions from specialized Large Language Models (LLMs) to improve classification accuracy. Our hybrid approach combines the generalization capabilities of pre-trained models with the domain-specific insights captured by LLMs, providing an improved understanding of mental health discourse. Our findings highlight the strengths and limitations of each model, offering valuable insights into their practical applicability. This research potentially facilitates early detection and personalized care to aid practitioners and aims to facilitate timely interventions and improve overall well-being, thereby contributing to the broader field of mental health surveillance and digital health analytics.
UR  - https://arxiv.org/pdf/2503.01442v1.pdf
N1  - Source: page_1.html
ID  - 53
ER  - 
TY  - JOUR
TI  - An Empirical Analysis of LLMs for Countering Misinformation
AU  - Proma, A.M.
AU  - Pate, N.
AU  - Druckman, J.
AU  - He, H.
AU  - Hoque, E.
PY  - 2025
AB  - While Large Language Models (LLMs) can amplify online misinformation, they also show promise in tackling misinformation. In this paper, we empirically study the capabilities of three LLMs – ChatGPT, Gemini, and Claude – in countering political misinformation. We implement a two-step, chain-of-thought prompting approach, where models first identify credible sources for a given claim and then generate persuasive responses. Our findings suggest that models struggle to ground their responses in real news sources, and tend to prefer citing left-leaning sources. We also observe varying degrees of response diversity among models. Our findings highlight concerns about using LLMs for fact-checking through only prompt-engineering, emphasizing the need for more robust guardrails. Our results have implications for both researchers and non-technical users.
UR  - https://arxiv.org/pdf/2503.01902v1.pdf
N1  - Source: page_1.html
ID  - 54
ER  - 
TY  - JOUR
TI  - PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues
AU  - Yu, F.
AU  - Jiang, L.
AU  - Huang, S.
AU  - Wu, Z.
AU  - Dai, X.
PY  - 2025
AB  - The ability to understand and predict the mental states of oneself and others, known as the Theory of Mind (ToM), is crucial for effective social interactions. Recent research has emerged to evaluate whether Large Language Models (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM in LLMs, existing benchmarks focus predominantly on physical perception with principles guided by the Sally-Anne test in synthetic stories and conversations, failing to capture the complex psychological activities of mental states in real-life social interactions. To mitigate this gap, we propose PERSUASIVETOM, a benchmark designed to evaluate the ToM abilities of LLMs in persuasive dialogues. Our framework introduces two categories of questions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving mental states (e.g., desire shifts in persuadees), and (2) ToM Application, evaluating whether LLMs can take advantage of inferred mental states to select effective persuasion strategies (e.g., emphasize rarity) and evaluate the effectiveness of persuasion strategies. Experiments across eight state-of-the-art LLMs reveal that while models excel on multiple questions, they struggle to answer questions that need tracking the dynamics and shifts of mental states and understanding the mental states in the whole dialogue comprehensively. Our aim with PERSUASIVETOM is to allow an effective evaluation of the ToM reasoning ability of LLMs with more focus on complex psychological activities. Our code is available at https://github.com/YuFangxu/PersuasiveToM.
UR  - https://arxiv.org/pdf/2502.21017v1.pdf
N1  - Source: page_1.html
ID  - 55
ER  - 
TY  - JOUR
TI  - Among Them: A game-based framework for assessing persuasion capabilities of LLMs
AU  - Idziejczak, M.
AU  - Korzavatykh, V.
AU  - Stawicki, M.
AU  - Błądek, I.
AU  - Brzezinski, D.
PY  - 2025
AB  - The proliferation of large language models (LLMs) and autonomous AI agents has raised concerns about their potential for automated persuasion and social influence. While existing research has explored isolated instances of LLM-based manipulation, systematic evaluations of persuasion capabilities across different models remain limited. In this paper, we present an Among Us-inspired game framework for assessing LLM deception skills in a controlled environment. The proposed framework makes it possible to compare LLM models by game statistics, as well as quantify in-game manipulation according to 25 persuasion strategies from social psychology and rhetoric. Experiments between 8 popular language models of different types and sizes demonstrate that all tested models exhibit persuasive capabilities, successfully employing 22 of the 25 anticipated techniques. We also find that larger models do not provide any persuasion advantage over smaller models and that longer model outputs are negatively correlated with the number of games won. Our study provides insights into the deception capabilities of LLMs, as well as tools and data for fostering future research on the topic.
UR  - https://arxiv.org/pdf/2502.20426v1.pdf
N1  - Source: page_1.html
ID  - 56
ER  - 
TY  - JOUR
TI  - MAPoRL2: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning
AU  - Park, C.
AU  - Han, S.
AU  - Guo, X.
AU  - Zhang, K.
AU  - Kim, J.-K.
PY  - 2025
AB  - Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs’ performance as shown recently. In this paper, we introduce a new post-training paradigm MAPoRL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPoRL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multiagent RL. Unlike existing LLM post-training paradigms, MAPoRL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.
UR  - https://arxiv.org/pdf/2502.18439v1.pdf
N1  - Source: page_1.html
ID  - 57
ER  - 
TY  - JOUR
TI  - Wearable Meets LLM for Stress Management: A Duoethnographic Study Integrating Wearable-Triggered Stressors and LLM Chatbots for Personalized Interventions
AU  - Neupane, S.
AU  - Dongre, P.
AU  - Gracanin, D.
AU  - Kumar, S.
PY  - 2025
AB  - We use a duoethnographic approach to study how wearable-integrated LLM chatbots can assist with personalized stress management, addressing the growing need for immediacy and tailored interventions. Two researchers interacted with custom chatbots over 22 days, responding to wearable-detected physiological prompts, recording stressor phrases, and using them to seek tailored interventions from their LLM-powered chatbots. They recorded their experiences in autoethnographic diaries and analyzed them during weekly discussions, focusing on the relevance, clarity, and impact of chatbot-generated interventions. Results showed that even though most events triggered by the wearable were meaningful, only one in five warranted an intervention. It also showed that interventions tailored with brief event descriptions were more effective than generic ones. By examining the intersection of wearables and LLM, this research contributes to developing more effective, user-centric mental health tools for real-time stress relief and behavior change.
UR  - https://arxiv.org/pdf/2502.17650v1.pdf
N1  - Source: page_1.html
ID  - 58
ER  - 
TY  - JOUR
TI  - Are Large Language Models Good Data Preprocessors?
AU  - Meguellati, E.
AU  - Pratama, N.
AU  - Sadiq, S.
AU  - Demartini, G.
PY  - 2025
AB  - High-quality textual training data is essential for the success of multimodal data processing tasks, yet outputs from image captioning models like BLIP and GIT often contain errors and anomalies that are difficult to rectify using rule-based methods. While recent work addressing this issue has predominantly focused on using GPT models for data preprocessing on relatively simple public datasets, there is a need to explore a broader range of Large Language Models (LLMs) and tackle more challenging and diverse datasets. In this study, we investigate the use of multiple LLMs, including LLaMA 3.1 70B, GPT-4 Turbo, and Sonnet 3.5 v2 to refine and clean the textual outputs of BLIP and GIT. We assess the impact of LLM-assisted data cleaning by comparing downstream-task (SemEval 2024 Subtask ‘Multilabel Persuasion Detection in Memes”) models trained on cleaned versus non-cleaned data. While our experimental results show improvements when using LLM-cleaned captions, statistical tests reveal that most of these improvements are not significant. This suggests that while LLMs have the potential to enhance data cleaning and repairing, their effectiveness may be limited depending on the context they are applied to and the complexity of the task and the level of noise in the text. Our findings highlight the need for further research into the capabilities and limitations of LLMs in data preprocessing pipelines, especially when dealing with challenging datasets, contributing empirical evidence to the ongoing discussion about integrating LLMs into data preprocessing pipelines.
UR  - https://arxiv.org/pdf/2502.16790v1.pdf
N1  - Source: page_1.html
ID  - 59
ER  - 
TY  - JOUR
TI  - Grounded Persuasive Language Generation for Automated Marketing
AU  - Wu, J.
AU  - Yang, C.
AU  - Mahns, S.
AU  - Fang, F.
AU  - Xu, H.
PY  - 2025
AB  - This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin. Our findings suggest a promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts.
UR  - https://arxiv.org/pdf/2502.16810v1.pdf
N1  - Source: page_1.html
ID  - 60
ER  - 
TY  - JOUR
TI  - Reasoning About Persuasion: Can LLMs Enable Explainable Propaganda Detection?
AU  - Hasanain, M.
AU  - Hasan, M.A.
AU  - Kmainasi, M.B.
AU  - Da San Martino, G.
AU  - Alam, F.
PY  - 2025
AB  - There has been significant research on propagandistic content detection across different modalities and languages. However, most studies have primarily focused on detection, with little attention given to explanations justifying the predicted label. This is largely due to the lack of resources that provide explanations alongside annotated labels. To address this issue, we propose a multilingual (i.e., Arabic and English) explanation-enhanced dataset, the first of its kind. Additionally, we introduce an explanation-enhanced LLM for both label detection and rationale-based explanation generation. Our findings indicate that the model performs comparably while also generating explanations. We will make the dataset and experimental resources publicly available for the research community.1
UR  - https://arxiv.org/pdf/2502.16550v1.pdf
N1  - Source: page_1.html
ID  - 61
ER  - 
TY  - JOUR
TI  - Accountability in Code Review: The Role of Intrinsic Drivers and the Impact of LLMs
AU  - Alami, A.
AU  - Jensen, V.V.
AU  - Ernst, N.A.
PY  - 2025
AB  - Accountability is an innate part of social systems. It maintains stability and ensures positive pressure on individuals’ decision-making. As actors in a social system, software developers are accountable to their team and organization for their decisions. However, the drivers of accountability and how it changes behavior in software development are less understood. In this study, we look at how the social aspects of code review affect software engineers’ sense of accountability for code quality. Since software engineering (SE) is increasingly involving Large Language Models (LLM) assistance, we also evaluate the impact on accountability when introducing LLM-assisted code reviews. We carried out a two-phased sequential qualitative study (interviews → focus groups). In Phase I (16 interviews), we sought to investigate the intrinsic drivers of software engineers influencing their sense of accountability for code quality, relying on self-reported claims. In Phase II, we tested these traits in a more natural setting by simulating traditional peer-led reviews with focus groups and then LLM-assisted review sessions. We found that there are four key intrinsic drivers of accountability for code quality: personal standards, professional integrity, pride in code quality, and maintaining one’s reputation. In a traditional peer-led review, we observed a transition from individual to collective accountability when code reviews are initiated. We also found that the introduction of LLM-assisted reviews disrupts this accountability process, challenging the reciprocity of accountability taking place in peer-led evaluations, i.e., one cannot be accountable to an LLM. Our findings imply that the introduction of AI into SE must preserve social integrity and collective accountability mechanisms.
UR  - https://arxiv.org/pdf/2502.15963v1.pdf
N1  - Source: page_1.html
ID  - 62
ER  - 
TY  - JOUR
TI  - Fine-tuning Large Language Models in Behavioral Psychology for Scalable Physical Activity Coaching
AU  - Mantena, S.
AU  - Johnson, A.
AU  - Oppezzo, M.
AU  - Kim, D.S.
AU  - Ashley, E.A.
PY  - 2025
AB  - Personalized, smartphone-based coaching improves physical activity but relies on static, human-crafted messages. We introduce My Heart Counts (MHC)-Coach, a large language model fine-tuned on the Transtheoretical Model of Change. MHC-Coach generates messages tailored to an individual’s psychology (their “stage of change”), providing personalized support to foster long-term physical activity behavior change. To evaluate MHC-Coach’s efficacy, 632 participants compared human-expert and MHC-Coach text-based interventions encouraging physical activity. Among messages matched to an individual’s stage of change, 68.0% (N=430) preferred MHC-Coach-generated messages (P < 0.001). Blinded behavioral science experts (N=2) rated MHC-Coach messages higher than human-expert messages for perceived effectiveness (4.4 vs. 2.8) and Transtheoretical Model alignment (4.1 vs. 3.5) on a 5-point Likert scale. This work demonstrates how language models can operationalize behavioral science frameworks for personalized health coaching, promoting long-term physical activity and potentially reducing cardiovascular disease risk at scale.
UR  - https://www.medrxiv.org/content/10.1101/2025.02.19.25322559v1.full.pdf
N1  - Source: page_1.html
ID  - 63
ER  - 
TY  - JOUR
TI  - AI Models Still Lag Behind Traditional Numerical Models in Predicting Sudden-Turning Typhoons
AU  - Xu, D.
AU  - Lu, Z.
AU  - Leung, J.C.-H.
AU  - Zhang, S.
AU  - Zhang, B.
PY  - 2025
AB  - Given the interpretability, accuracy, and stability of numerical weatherprediction (NWP) models, current operational weather forecasting relies heavilyon the NWP approach. In the past two years, the rapid development of ArtificialIntelligence (AI) has provided an alternative solution for medium-range (1-10days) weather forecasting. Bi et al. (2023) (hereafter Bi23) introduced thefirst AI-based weather prediction (AIWP) model in China, named Pangu-Weather,which offers fast prediction without compromising accuracy. In their work, Bi23made notable claims regarding its effectiveness in extreme weather predictions.However, this claim lacks persuasiveness because the extreme nature of the twotropical cyclones (TCs) examples presented in Bi23, namely Typhoon Kong-rey andTyphoon Yutu, stems primarily from their intensities rather than their movingpaths. Their claim may mislead into another meaning which is that Pangu-Weatherworks well in predicting unusual typhoon paths, which was not explicitlyanalyzed. Here, we reassess Pangu-Weather\'s ability to predict extreme TCtrajectories from 2020-2024. Results reveal that while Pangu-Weather overalloutperforms NWP models in predicting tropical cyclone (TC) tracks, it fallsshort in accurately predicting the rarely observed sudden-turning tracks, suchas Typhoon Khanun in 2023. We argue that current AIWP models still lag behindtraditional NWP models in predicting such rare extreme events in medium-rangeforecasts.
UR  - https://arxiv.org/pdf/2502.16036v1.pdf
N1  - Source: page_1.html
ID  - 64
ER  - 
TY  - JOUR
TI  - A Meta-Evaluation of Style and Attribute Transfer Metrics
AU  - Pauli, A.B.
AU  - Augenstein, I.
AU  - Assent, I.
PY  - 2025
AB  - LLMs make it easy to rewrite text in any style, be it more polite, persuasive, or more positive. We present a large-scale study of evaluation metrics for style and attribute transfer with a focus on content preservation; meaning content not attributed to the style shift is preserved. The de facto evaluation approach uses lexical or semantic similarity metrics often between source sentences and rewrites. While these metrics are not designed to distinguish between style or content differences, empirical meta-evaluation shows a reasonable correlation to human judgment. In fact, recent works find that LLMs prompted as evaluators are only comparable to semantic similarity metrics, even though intuitively, the LLM approach should better fit the task. To investigate this discrepancy, we benchmark 8 metrics for evaluating content preservation on existing datasets and additionally construct a new test set that better aligns with the meta-evaluation aim. Indeed, we then find that the empirical conclusion aligns with the intuition: content preservation metrics for style/attribute transfer must be conditional on the style shift. To support this, we propose a new efficient zero-shot evaluation method using the likelihood of the next token. We hope our meta-evaluation can foster more research on evaluating content preservation metrics, and also to ensure fair evaluation of methods for conducting style transfer.
UR  - https://arxiv.org/pdf/2502.15022v2.pdf
N1  - Source: page_1.html
ID  - 65
ER  - 
TY  - JOUR
TI  - Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health
AU  - Wang, X.
AU  - Griffith, J.
AU  - Adler, D.A.
AU  - Choudhury, T.
AU  - Wang, F.
PY  - 2025
AB  - Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HEALTHGURU, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HEALTHGURU’s multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HEALTHGURU to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HEALTHGURU. We also identify challenges and design considerations for personalization and user engagement in health chatbots.
UR  - https://arxiv.org/pdf/2502.13920v1.pdf
N1  - Source: page_1.html
ID  - 66
ER  - 
TY  - JOUR
TI  - Modeling Behavior Change for Multi-model At-Risk Students Early Prediction (extended version)
AU  - Cheng, J.
AU  - Yang, Z.-Q.
AU  - Cao, J.
AU  - Poon, K.C.F.
AU  - Lai, D.
PY  - 2025
AB  - In the educational domain, identifying students at risk of dropping out is essential for allowing educators to intervene effectively, improving both academic outcomes and overall student well-being. Data in educational settings often originate from diverse sources, such as assignments, grades, and attendance records. However, most existing research relies on online learning data and just extracting the quantitative features. While quantification eases processing, it also leads to a significant loss of original information. Moreover, current models primarily identify students with consistently poor performance through simple and discrete behavioural patterns, failing to capture the complex continuity and non-linear changes in student behaviour. We have developed an innovative prediction model, Multimodal- ChangePoint Detection (MCPD), utilizing the textual teacher remark data and numerical grade data from middle schools. Our model achieves a highly integrated and intelligent analysis by using independent encoders to process two data types, fusing the encoded feature. The model further refines its analysis by leveraging a changepoint detection module to pinpoint crucial behavioral changes, which are integrated as dynamic weights through a simple attention mechanism. Experimental validations indicate that our model achieves an accuracy range of 70- 75%, with an average outperforming baseline algorithms by approximately 5-10%. Additionally, our algorithm demonstrates a certain degree of transferability, maintaining high accuracy when adjusted and retrained with different definitions of at-risk, proving its broad applicability.
UR  - https://arxiv.org/pdf/2503.05734v1.pdf
N1  - Source: page_1.html
ID  - 67
ER  - 
TY  - JOUR
TI  - Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models
AU  - Li, R.
AU  - Sedoc, J.
AU  - Sundararajan, A.
PY  - 2025
AB  - When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI’s and DeepSeek’s models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek’s more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization’s AI strategy.
UR  - https://arxiv.org/pdf/2502.12825v2.pdf
N1  - Source: page_1.html
ID  - 68
ER  - 
TY  - JOUR
TI  - Communication Strategy on Macro-and-Micro Traffic State in Cooperative Deep Reinforcement Learning for Regional Traffic Signal Control
AU  - Gu, H.
AU  - Wang, S.
AU  - Jia, D.
AU  - Wang, J.
AU  - Lim, E.G.
PY  - 2025
AB  - Adaptive Traffic Signal Control (ATSC) has become a popular research topic in intelligent transportation systems. Regional Traffic Signal Control (RTSC) using the Multi-agent Deep Reinforcement Learning (MADRL) technique has become a promising approach for ATSC due to its ability to achieve the optimum trade-off between scalability and optimality. Most existing RTSC approaches partition a traffic network into several disjoint regions, followed by applying centralized reinforcement learning techniques to each region. However, the pursuit of cooperation among RTSC agents still remains an open issue and no communication strategy for RTSC agents has been investigated. In this paper, we propose communication strategies to capture the correlation of micro-traffic states among lanes and the correlation of macro-traffic states among intersections. We first justify the evolution equation of the RTSC process is Markovian via a system of store-and-forward queues. Next, based on the evolution equation, we propose two GAT-Aggregated (GA2) communication modules—GA2-Naive and GA2-Aug to extract both intra-region and inter-region correlations between macro and micro traffic states. While GA2-Naive only considers the movements at each intersection, GA2-Aug also considers the lane-changing behavior of vehicles. Two proposed communication modules are then aggregated into two existing novel RTSC frameworks—RegionLight and Regional-DRL. Experimental results demonstrate that both GA2-Naive and GA2-Aug effectively improve the performance of existing RTSC frameworks under both real and synthetic scenarios. Hyperparameter testing also reveals the robustness and potential of our communication modules in large-scale traffic networks.
UR  - https://arxiv.org/pdf/2502.13248v1.pdf
N1  - Source: page_1.html
ID  - 69
ER  - 
TY  - JOUR
TI  - Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions
AU  - Yun, T.
AU  - Yang, E.
AU  - Safdari, M.
AU  - Gretton, A.
AU  - Matarić, M.
PY  - 2025
AB  - We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent’s understanding of the synthetic users’ needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.
UR  - https://arxiv.org/pdf/2502.13135v1.pdf
N1  - Source: page_1.html
ID  - 70
ER  - 
TY  - JOUR
TI  - Free Argumentative Exchanges for Explaining Image Classifiers
AU  - Kori, A.
AU  - Rago, A.
AU  - Toni, F.
PY  - 2025
AB  - Deep learning models are powerful image classifiers but their opacity hinders their trustworthiness. Explanation methods for capturing the reasoning process within these classifiers faithfully and in a clear manner are scarce, due to their sheer complexity and size. We provide a solution for this problem by defining a novel method for explaining the outputs of image classifiers with debates between two agents, each arguing for a particular class. We obtain these debates as concrete instances of Free Argumentative eXchanges (FAXs), a novel argumentation-based multi-agent framework allowing agents to internalise opinions by other agents differently than originally stated. We define two metrics (consensus and persuasion rate) to assess the usefulness of FAXs as argumentative explanations for image classifiers. We then conduct a number of empirical experiments showing that FAXs perform well along these metrics as well as being more faithful to the image classifiers than conventional, non-argumentative explanation methods. All our implementations can be found at https://github.com/koriavinash1/FAX.
UR  - https://arxiv.org/pdf/2502.12995v1.pdf
N1  - Source: page_1.html
ID  - 71
ER  - 
TY  - JOUR
TI  - Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL
AU  - Wongkamjan, W.
AU  - Wang, Y.
AU  - Gu, F.
AU  - May, J.
AU  - Boyd-Graber, J.L.
PY  - 2025
AB  - An increasingly prevalent socio-technical problem is people being taken in by offers that sound “too good to be true”, where persuasion and trust shape decision-making. This paper investigates how AI can help detect these deceptive scenarios. We analyze how humans strategically deceive each other in Diplomacy, a board game that requires both natural language communication and strategic reasoning. This requires extracting logical forms of proposed agreements in player communications and computing the relative rewards of the proposal using agents’ value functions. Combined with text-based features, this can improve our deception detection. Our method detects human deception with a high precision when compared to a Large Language Model approach that flags many true messages as deceptive. Future human-AI interaction tools can build on our methods for deception detection by triggering friction to give users a chance of interrogating suspicious proposals.
UR  - https://arxiv.org/pdf/2502.12436v1.pdf
N1  - Source: page_1.html
ID  - 72
ER  - 
TY  - JOUR
TI  - CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models
AU  - Zhou, G.
AU  - Qiu, P.
AU  - Fan, M.
AU  - Zhang, X.
AU  - Zhou, J.
PY  - 2025
AB  - Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as "jailbreaking." Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel Context-Coherent Jailbreak Attack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted.
UR  - https://arxiv.org/pdf/2502.11379v1.pdf
N1  - Source: page_1.html
ID  - 73
ER  - 
TY  - JOUR
TI  - Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time
AU  - Dahlke, R.
AU  - Klagges, H.
AU  - Zecha, D.
AU  - Rohr, S.
AU  - Klemm, F.
PY  - 2025
AB  - We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time. By analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub “functional Token Resonance Imaging” (fTRI) – inspired by fMRI and using prompts designed to elicit specific behavior (e.g., “What happened {time}{place}?”) – we empirically identify distinctive experts associated with behaviors like refusal responses. Using MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1’s 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates. With MoTE we were also able to successfully switch the model’s chain-of-thought reasoning language from English to Chinese in 10% of our test prompts. Our approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining. Our findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model’s weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs.
UR  - https://arxiv.org/pdf/2502.11096v1.pdf
N1  - Source: page_1.html
ID  - 74
ER  - 
TY  - JOUR
TI  - Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust
AU  - Sun, Y.
AU  - Wang, T.
PY  - 2025
AB  - Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit ‘sycophancy’, a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. However, users’ perceptions of LLM sycophancy and its interplay with other anthropomorphic features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) × 2 (Friendliness: high vs. low) between-subjects experiment (N = 224). Our study uncovered, for the first time, the intricate dynamics between LLM sycophancy and friendliness: When an LLM agent already exhibits a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight the imperative for responsible designs in user-LLM agent interactions.
UR  - https://arxiv.org/pdf/2502.10844v2.pdf
N1  - Source: page_1.html
ID  - 75
ER  - 
TY  - JOUR
TI  - Mind What You Ask For: Emotional and Rational Faces of Persuasion by Large Language Models
AU  - Mieleszczenko-Kowszewicz, W.
AU  - Bajcar, B.
AU  - Babiak, J.
AU  - Swistak, J.
AU  - Biecek, P.
PY  - 2025
AB  - Be careful what you ask for, you just might get it. This saying fits with the way large language models (LLMs) are trained, which, instead of being rewarded for correctness, are increasingly rewarded for pleasing the recipient. So, they are increasingly effective at persuading us that their answers are valuable. But what tricks do they use in this persuasion? In this study, we examine what are the psycholinguistic features of the responses used by twelve different language models. By grouping response content according to rational or emotional prompts and exploring social influence principles employed by LLMs, we ask whether and how we can mitigate the risks of LLM-driven mass misinformation. We position this study within the broader discourse on human-centred AI, emphasizing the need for interdisciplinary approaches to mitigate cognitive and societal risks posed by persuasive AI responses.
UR  - https://arxiv.org/pdf/2502.09687v1.pdf
N1  - Source: page_1.html
ID  - 76
ER  - 
TY  - JOUR
TI  - Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages
AU  - Biswas, S.
AU  - Erlei, A.
AU  - Gadiraju, U.
PY  - 2025
AB  - Recent advances in generative AI have precipitated a proliferation of novel writing assistants. These systems typically rely on multilingual large language models (LLMs), providing globalized workers the ability to revise or create diverse forms of content in different languages. However, there is substantial evidence indicating that the performance of multilingual LLMs varies between languages. Users who employ writing assistance for multiple languages are therefore susceptible to disparate output quality. Importantly, recent research has shown that people tend to generalize algorithmic errors across independent tasks, violating the behavioral axiom of choice independence. In this paper, we analyze whether user utilization of novel writing assistants in a charity advertisement writing task is affected by the AI’s performance in a second language. Furthermore, we quantify the extent to which these patterns translate into the persuasiveness of generated charity advertisements, as well as the role of peoples’ beliefs about LLM utilization in their donation choices. Our results provide evidence that writers who engage with an LLM-based writing assistant violate choice independence, as prior exposure to a Spanish LLM reduces subsequent utilization of an English LLM. While these patterns do not affect the aggregate persuasiveness of the generated advertisements, people’s beliefs about the source of an advertisement (human versus AI) do. In particular, Spanish-speaking female participants who believed that they read an AI-generated advertisement strongly adjusted their donation behavior downwards. Furthermore, people are generally not able to adequately differentiate between human-generated and LLM-generated ads. Our work has important implications for the design, development, integration, and adoption of multilingual LLMs as assistive agents—particularly in writing tasks.
UR  - https://arxiv.org/pdf/2502.09532v1.pdf
N1  - Source: page_1.html
ID  - 77
ER  - 
TY  - JOUR
TI  - Communication is All You Need: Persuasion Dataset Construction via Multi-LLM Communication
AU  - Ma, W.
AU  - Zhang, H.
AU  - Yang, I.
AU  - Hassanpour, S.
AU  - Vosoughi, S.
PY  - 2025
AB  - Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in naturalness, linguistic diversity, and the strategic use of persuasion, even in complex scenarios involving social taboos. The framework also proves adept at generalizing across novel contexts. Our results highlight the framework’s potential to significantly advance research in both computational and social science domains concerning persuasive communication.
UR  - https://arxiv.org/pdf/2502.08896v1.pdf
N1  - Source: page_1.html
ID  - 78
ER  - 
TY  - JOUR
TI  - MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and Emotion Detection
AU  - Al-Henaki, L.
AU  - Al-Khalifa, H.
AU  - Al-Salman, A.
AU  - Alghamdi, G.
AU  - Aljasim, H.
PY  - 2025
AB  - Propaganda is a form of persuasion that has been used throughout history with the intention goal of influencing people's opinions through rhetorical and psychological persuasion techniques for determined ends. Although Arabic ranked as the fourth most-used language on the internet, resources for propaganda detection in languages other than English, especially Arabic, remain extremely limited. To address this gap, the first Arabic dataset for Multi-label Propaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE is an open-source extension of the existing Arabic propaganda dataset, ArPro, with the addition of sentiment and emotion annotations for each text. This dataset comprises 8,000 annotated news articles, which is the largest propaganda dataset to date. For each task, several baselines have been developed using large language models (LLMs), such as GPT-4o-mini, and pre-trained language models (PLMs), including three BERT-based models. The dataset, annotation guidelines, and source code are all publicly released to facilitate future research and development in Arabic language models and contribute to a deeper understanding of how various opinion dimensions interact in news media1
UR  - https://arxiv.org/pdf/2502.08319v1.pdf
N1  - Source: page_1.html
ID  - 79
ER  - 
TY  - JOUR
TI  - Amplifying Minority Voices: AI-Mediated Devil’s Advocate System for Inclusive Group Decision-Making
AU  - Lee, S.
AU  - Kim, M.
AU  - Hwang, S.
AU  - Kim, D.
AU  - Lee, K.
PY  - 2025
AB  - Group decision-making often benefits from diverse perspectives, yet power imbalances and social influence can stifle minority opinions and compromise outcomes. This prequel introduces an AI-mediated communication system that leverages the Large Language Model to serve as a devil’s advocate, representing underrepresented viewpoints without exposing minority members’ identities. Rooted in persuasive communication strategies and anonymity, the system aims to improve psychological safety and foster more inclusive decision-making. Our multi-agent architecture, which consists of a summary agent, conversation agent, AI duplicate checker, and paraphrase agent, encourages the group’s critical thinking while reducing repetitive outputs. We acknowledge that reliance on text-based communication and fixed intervention timings may limit adaptability, indicating pathways for refinement. By focusing on the representation of minority viewpoints anonymously in power-imbalanced settings, this approach highlights how AI-driven methods can evolve to support more divergent and inclusive group decision-making.
UR  - https://arxiv.org/pdf/2502.06251v1.pdf
N1  - Source: page_1.html
ID  - 80
ER  - 
TY  - JOUR
TI  - From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis
AU  - Plenz, M.
AU  - Heinisch, P.
AU  - Gehring, J.
AU  - Cimiano, P.
AU  - Frank, A.
PY  - 2025
AB  - Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in – as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer’s stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.1
UR  - https://arxiv.org/pdf/2502.09644v1.pdf
N1  - Source: page_1.html
ID  - 81
ER  - 
TY  - JOUR
TI  - TRUTH DECAY: Quantifying Multi-Turn Sycophancy in Language Models
AU  - Liu, J.
AU  - Jain, A.
AU  - Takuri, S.
AU  - O’Brien, S.
AU  - Sharma, V.
PY  - 2025
AB  - Rapid improvements in large language models have unveiled a critical challenge in human-AI interaction: sycophancy. In this context, sycophancy refers to the tendency of models to excessively agree with or flatter users, often at the expense of factual accuracy. While previous studies have primarily analyzed this behavior in single-turn interactions, its persistence and evolution in multi-step conversations remain largely unexplored. We introduce TRUTH DECAY, a benchmark specifically designed to evaluate sycophancy in extended dialogues, where language models must navigate iterative user feedback, challenges, and persuasion. We prompt models to elicit four types of sycophantic biases. We then propose and test sycophancy reduction strategies, evaluating their effectiveness beyond single-step interactions.
UR  - https://arxiv.org/pdf/2503.11656v1.pdf
N1  - Source: page_1.html
ID  - 82
ER  - 
TY  - JOUR
TI  - CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration
AU  - Yang, Y.
AU  - Achananuparp, P.
AU  - Huang, H.
AU  - Ern, C.T.S.
AU  - Lim, E.-P.
PY  - 2025
AB  - Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) – a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client’s state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI’s performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client’s state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.
UR  - https://arxiv.org/pdf/2502.02807v1.pdf
N1  - Source: page_1.html
ID  - 83
ER  - 
TY  - JOUR
TI  - OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable Attitude and Behavior Change
AU  - Pataranutaporn, P.
AU  - Doudkin, A.
AU  - Maes, P.
PY  - 2025
AB  - Marine ecosystems face unprecedented threats from climate change and plastic pollution, yet traditional environmental education often struggles to translate awareness into sustained behavioral change. This paper presents OceanChat, an interactive system leveraging large language models to create conversational AI agents represented as animated marine creatures—specifically a beluga whale, a jellyfish, and a seahorse—designed to promote environmental behavior (PEB) and foster awareness through personalized dialogue. Through a between-subjects experiment (N=900), we compared three conditions: (1) Static Scientific Information, providing conventional environmental education through text and images; (2) Static Character Narrative, featuring first-person storytelling from 3D-rendered marine creatures; and (3) Conversational Character Narrative, enabling real-time dialogue with AI-powered marine characters. Our analysis revealed that the Conversational Character Narrative condition significantly increased behavioral intentions and sustainable choice preferences compared to static approaches. The beluga whale character demonstrated consistently stronger emotional engagement across multiple measures, including perceived anthropomorphism and empathy. However, impacts on deeper measures like climate policy support and psychological distance were limited, highlighting the complexity of shifting entrenched beliefs. Our work extends research on sustainability interfaces facilitating PEB and offers design principles for creating emotionally resonant, context-aware AI characters. By balancing anthropomorphism with species authenticity, OceanChat demonstrates how interactive narratives can bridge the gap between environmental knowledge and real-world behavior change.
UR  - https://arxiv.org/pdf/2502.02863v2.pdf
N1  - Source: page_1.html
ID  - 84
ER  - 
TY  - JOUR
TI  - Mirai: A Wearable Proactive AI “Inner-Voice” for Contextual Nudging
AU  - Fang, C.M.
AU  - Samaradivakara, Y.
AU  - Maes, P.
AU  - Nanayakkara, S.
PY  - 2025
AB  - People often find it difficult to turn their intentions into real actions—a challenge that affects both personal growth and mental well-being. While established methods like cognitive-behavioral therapy and mindfulness training help people become more aware of their behaviors and set clear goals, these approaches cannot provide immediate guidance when people fall into automatic reactions or habits. We introduce Mirai, a novel wearable AI system with an integrated camera, real-time speech processing, and personalized voice-cloning to provide proactive and contextual nudges for positive behavior change. Mirai continuously monitors and analyzes the user’s environment to anticipate their intentions, generating contextually-appropriate responses delivered in the user’s own cloned voice. We demonstrate the application of Mirai through three scenarios focusing on dietary choices, work productivity, and communication skills. We also discuss future work on improving the proactive agent via human feedback and the need for a longitudinal study in naturalistic settings.
UR  - https://arxiv.org/pdf/2502.02370v1.pdf
N1  - Source: page_1.html
ID  - 85
ER  - 
TY  - JOUR
TI  - PSSD: Making Large Language Models Self-denial via Human Psyche Structure
AU  - Liao, J.
AU  - Liao, Z.
AU  - Zhao, X.
PY  - 2025
AB  - The enhance of accuracy in reasoning results of LLMs arouses the community’s interests, wherein pioneering studies investigate post-hoc strategies to rectify potential mistakes. Despite extensive efforts, they are all stuck in a state of resource competition demanding significant time and computing expenses. The cause of the situation lies in the failure of identifying the fundamental feature of the solutions in this line, coined as the self-denial of LLMs. In other words, LLMs should confidently determine the potential existence of mistakes and carefully execute the targeted correction. As the whole procedure conducts within LLMs, supporting and persuasive references are hard to acquire, while the absence of specific steps towards refining hidden mistakes persists even when errors are acknowledged. In response to the challenges, we present PSSD, which refers to and implements the human psyche structure such that three distinct and interconnected roles contribute to human reasoning. Specifically, PSSD leverages the recent multi-agent paradigm, and is further enhanced with three innovatively conceived roles: (1) the intuition-based id role that provides initial attempts based on benign LLMs; (2) the rule-driven superego role that summarizes rules to regulate the above attempts, and returns specific key points as guidance; and (3) the script-centric ego role that absorbs all procedural information to generate executable script for the final answer prediction. Extensive experiments demonstrate that the proposed design not only better enhance reasoning capabilities, but also seamlessly integrate with current models, leading to superior performance.
UR  - https://arxiv.org/pdf/2502.01344v1.pdf
N1  - Source: page_1.html
ID  - 86
ER  - 
TY  - JOUR
TI  - Verbalized Bayesian Persuasion
AU  - Li, W.
AU  - Lin, Y.
AU  - Wang, X.
AU  - Zha, H.
AU  - Wang, B.
PY  - 2025
AB  - Information design (ID) explores how a sender influence the optimal behavior of receivers to achieve specific objectives. While ID originates from everyday human communication, existing game-theoretic and machine learning methods often model information structures as numbers, which limits many applications to toy games. This work leverages LLMs and proposes a verbalized framework in Bayesian persuasion (BP), which extends classic BP to real-world games involving human dialogues for the first time. Specifically, we map the BP to a verbalized mediator-augmented extensive-form game, where LLMs instantiate the sender and receiver. To efficiently solve the verbalized game, we propose a generalized equilibrium-finding algorithm combining LLM and game solver. The algorithm is reinforced with techniques including verbalized commitment assumptions, verbalized obedience constraints, and information obfuscation. Numerical experiments in dialogue scenarios, such as recommendation letters, courtroom interactions, and law enforcement, validate that our framework can both reproduce theoretical results in classic BP and discover effective persuasion strategies in more complex natural language and multi-stage scenarios.
UR  - https://arxiv.org/pdf/2502.01587v1.pdf
N1  - Source: page_1.html
ID  - 87
ER  - 
TY  - JOUR
TI  - Ethics of generative AI and manipulation: a design‑oriented research agenda
AU  - Klenk, M.
PY  - 2025
AB  - Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies.
UR  - https://arxiv.org/pdf/2503.04733v1.pdf
N1  - Source: page_1.html
ID  - 88
ER  - 
TY  - JOUR
TI  - Enhancing Antimicrobial Treatment Protocols with a Retrieval Augmented Generation Large Language Model: Aiding Diagnosis and Management of Infectious Diseases
AU  - Morales, H.
AU  - Rocha, C.
AU  - Dalmazo, L.M.T.
AU  - Tuma, P.
AU  - Silva, M.
PY  - 2025
AB  - In hospitals, the efficient management of infectious diseases is crucial yet challenging, given the complexity and high-pressure environment that can contribute to bacterial resistance. This paper introduces a system based on the Retrieval Augmented Generation (RAG) Large Language Model for antimicrobial treatment protocols, leveraging the Antimicrobial Treatment Guide from the Albert Einstein Israelite Hospital in São Paulo, Brazil, to standardize and improve antimicrobial therapy adherence to local guidelines providing accurate and timely responses when needed. The system comprises four modules: Document Processing, Query Processing, Large Language Model, and Prompt Management, and utilizes the RAG methodology to generate responses based on external data without requiring training. The methodology involves the manual segmentation of protocols into manageable text chunks, transformed into vectors, and integrated into a vector database for efficient retrieval. Evaluation metrics include the Jaccard index and a System Usability Scale (SUS), supplemented by a behavior change questionnaire. The system achieved a high SUS score of 82, indicating excellent usability, and the model successfully retrieved the correct protocol in 83% of cases. The results demonstrate that the RAG methodology significantly aligns antimicrobial protocols with clinical practices, enhancing diagnostic accuracy and treatment specificity. This approach shows promise in bridging the gap between clinical guidelines and practice, potentially reducing the risk of antibiotic resistance and improving patient outcomes.
UR  - https://dx.doi.org/10.36227/techrxiv.173893267.74312893/v1
N1  - Source: page_1.html
ID  - 89
ER  - 
TY  - JOUR
TI  - ASTRAL: Automated Safety Testing of Large Language Models
AU  - Ugarte, M.
AU  - Valle, P.
AU  - Parejo, J.A.
AU  - Segura, S.
AU  - Arrieta, A.
PY  - 2025
AB  - Large Language Models (LLMs) have recently gained significant attention due to their ability to understand and generate sophisticated human-like content. However, ensuring their safety is paramount as they might provide harmful and unsafe responses. Existing LLM testing frameworks address various safety-related concerns (e.g., drugs, terrorism, animal abuse) but often face challenges due to unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool that automates the generation and execution of test cases (i.e., prompts) for testing the safety of LLMs. First, we introduce a novel black-box coverage criterion to generate balanced and diverse unsafe test inputs across a diverse set of safety categories as well as linguistic writing characteristics (i.e., different style and persuasive writing techniques). Second, we propose an LLM-based approach that leverages Retrieval Augmented Generation (RAG), few-shot prompting strategies and web browsing to generate up-to-date test inputs. Lastly, similar to current LLM test automation techniques, we leverage LLMs as test oracles to distinguish between safe and unsafe test outputs, allowing a fully automated testing approach. We conduct an extensive evaluation on well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms other LLMs when acting as the test oracle, accurately detecting unsafe responses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs that are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard); ii) the results confirm that our approach can uncover nearly twice as many unsafe LLM behaviors with the same number of test inputs compared to currently used static datasets; and iii) our black-box coverage criterion combined with web browsing can effectively guide the LLM on generating up-to-date unsafe test inputs, significantly increasing the number of unsafe LLM behaviors.
UR  - https://arxiv.org/pdf/2501.17132v1.pdf
N1  - Source: page_1.html
ID  - 90
ER  - 
TY  - JOUR
TI  - Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated Statistics
AU  - Timm, J.
AU  - Talele, C.
AU  - Haimes, J.
PY  - 2025
AB  - Large Language Models (LLMs) are becoming increasingly persuasive, demonstrating the ability to personalize arguments in conversation with humans by leveraging their personal data. This may have serious impacts on the scale and effectiveness of disinformation campaigns. We studied the persuasiveness of LLMs in a debate setting by having humans (n = 33) engage with LLM-generated arguments intended to change the human’s opinion. We quantified the LLM’s effect by measuring human agreement with the debate’s hypothesis pre- and post-debate and analyzing both the magnitude of opinion change, as well as the likelihood of an update in the LLM’s direction. We compare persuasiveness across established persuasion strategies, including personalized arguments informed by user demographics and personality, appeal to fabricated statistics, and a mixed strategy utilizing both personalized arguments and fabricated statistics. We found that static arguments generated by humans and GPT-4o-mini have comparable persuasive power. However, the LLM outperformed static human-written arguments when leveraging the mixed strategy in an interactive debate setting. This approach had a 51% chance of persuading participants to modify their initial position, compared to 32% for the static human-written arguments. Our results highlight the concerning potential for LLMs to enable inexpensive and persuasive large-scale disinformation campaigns.
UR  - https://arxiv.org/pdf/2501.17273v1.pdf
N1  - Source: page_1.html
ID  - 91
ER  - 
TY  - JOUR
TI  - Jupybara: Operationalizing a Design Space for Actionable Data Analysis and Storytelling with LLMs
AU  - Wang, H.W.
AU  - Birnbaum, L.
AU  - Setlur, V.
PY  - 2025
AB  - Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies—design-space-aware prompting and multi-agent architectures—to operationalize our design space. An expert evaluation confirms Jupybara’s usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs.
UR  - https://arxiv.org/pdf/2501.16661v1.pdf
N1  - Source: page_1.html
ID  - 92
ER  - 
TY  - JOUR
TI  - KERNELS OF SELFHOOD: GPT-4O SHOWS HUMANLIKE PATTERNS OF COGNITIVE CONSISTENCY MODERATED BY FREE CHOICE
AU  - Lehr, S.A.
AU  - Saichandran, K.S.
AU  - Harmon-Jones, E.
AU  - Vitali, N.
AU  - Banaji, M.R.
PY  - 2025
AB  - Large Language Models (LLMs) show emergent patterns that mimic human cognition. We explore whether they also mirror other, less deliberative human psychological processes. Drawing upon classical theories of cognitive consistency, two preregistered studies tested whether GPT-4o changed its attitudes toward Vladimir Putin in the direction of a positive or negative essay it wrote about the Russian leader. Indeed, GPT displayed patterns of attitude change mimicking cognitive consistency effects in humans. Even more remarkably, the degree of change increased sharply when the LLM was offered an illusion of choice about which essay (positive or negative) to write. This result suggests that GPT-4o manifests a functional analog of humanlike selfhood, although how faithfully the chatbot’s behavior reflects the mechanisms of human attitude change remains to be understood.MSC Codes 68T50 (Primary), 91E10, 68T30, 68T99, 91C99 (Secondary)
UR  - https://arxiv.org/pdf/2502.07088v1.pdf
N1  - Source: page_1.html
ID  - 93
ER  - 
TY  - JOUR
TI  - ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection
AU  - Yang, B.
AU  - Guo, J.
AU  - Iwasawa, Y.
AU  - Matsuo, Y.
PY  - 2025
AB  - Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative agents. In this study, we propose a novel paradigm called ToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions. ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent’s perception of its counterpart’s mental states, such as beliefs, desires, and intentions (BDIs). Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts’ inferred BDIs, along with related confidence levels. We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection. Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the first-order and the second-order ToM. Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart’s behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors. The codes of this project will be made publicly available after the paper acceptance.
UR  - https://arxiv.org/pdf/2501.15355v1.pdf
N1  - Source: page_1.html
ID  - 94
ER  - 
TY  - JOUR
TI  - LLMs are Vulnerable to Malicious Prompts Disguised as Scientific Language
AU  - Ge, Y.
AU  - Kirtane, N.
AU  - Peng, H.
AU  - Hakkani-Tür, D.
PY  - 2025
AB  - As large language models (LLMs) have been deployed in various real-world settings, concerns about the harm they may propagate have grown. Various jailbreaking techniques have been developed to expose the vulnerabilities of these models and improve their safety. This work reveals that many state-of-the-art proprietary and open-source LLMs are vulnerable to malicious requests hidden behind scientific language. Specifically, our experiments with GPT4o, GPT4o-mini, GPT-4, Llama3.1-405B-Instruct, Llama3.1-70B-Instruct, Cohere, Gemini models on the StereoSet data and synthetically generated data demonstrate that, the models’ biases and toxicity substantially increase when prompted with requests that deliberately misinterpret social science and psychological studies as evidence supporting the benefits of stereotypical biases. Alarmingly, these models can also be manipulated to generate fabricated scientific arguments claiming that biases are beneficial, which can be used by ill-intended actors to systematically jailbreak even the strongest models like GPT. Our analysis studies various factors that contribute to the models’ vulnerabilities to malicious requests in academic language. Mentioning author names and venues enhances the persuasiveness of some models, and the bias scores can increase as dialogues progress. Our findings call for a more careful investigation on the use of scientific data for training LLMs.
UR  - https://arxiv.org/pdf/2501.14073v2.pdf
N1  - Source: page_1.html
ID  - 95
ER  - 
TY  - JOUR
TI  - Training Data Attribution (TDA): Examining Its Adoption & Use Cases
AU  - Cheng, D.
AU  - Bae, J.
AU  - Bullock, J.
AU  - Kristofferson, D.
PY  - 2025
AB  - This report investigates Training Data Attribution (TDA) and its potential importance to and tractability for reducing extreme risks from AI. TDA techniques aim to identify training data points that are especially influential on the behavior of specific model outputs. They are motivated by the question: how would the model's behavior change if one or more data points were removed from or added to the training dataset? Report structure: ■ First, we discuss the plausibility and amount of effort it would take to bring existing TDA research efforts from their current state, to an efficient and accurate tool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss the numerous research benefits AI labs will expect to see from using such TDA tooling. ■ Then, we discuss a key outstanding bottleneck that would limit such TDA tooling from being accessible publicly: AI labs' willingness to disclose their training data. We suggest ways AI labs may work around these limitations, and discuss the willingness of governments to mandate such access. ■ Assuming that AI labs willingly provide access to TDA inference, we then discuss what high-level societal benefits you might see. We list and discuss a series of policies and systems that may be enabled by TDA. Finally, we present an evaluation of TDA's potential impact on mitigating large-scale risks from AI systems. Key takeaways from our report: ■ Modern TDA techniques can be categorized into three main groups: retraining-based, representation-based (or input-similarity-based), and gradient-based. Recent research has found that gradient-based methods (using influence functions) are the most likely path to practical TDA. ■ The most efficient approach to conduct TDA using influence functions today has training costs on par with pre-training an LLM. It has significantly higher (but feasible) storage costs than an LLM model, and somewhat higher per-inference costs. ■ Based on these estimates, TDA appears to be no longer infeasible to run on frontier LLMs with enterprise-levels of compute and storage. However, these techniques have not been tested on larger models, and the accuracy of these optimized TDA techniques on large models is unclear. ■ Compressed-gradient TDA is already plausible to be used on fine-tuned models, which have orders of magnitude fewer training examples and parameters (on the order of millions or billions rather than hundreds of billions). ■ Timing to achieve efficient and accurate TDA on frontier models is likely between 2-5 years, depending largely on specific incremental research results and amount of funding/researchers allocated to the space. ■ Efficient TDA techniques will likely have a substantial positive impact on AI research and LLM development, including the following effects: □ Mitigating the prevalence of hallucinations and false claims □ Identifying training data that produces poor results (bias, misinformation, toxicity), improved data filtering/selection □ Shrinking overall model size/improving efficiency □ Improved interpretability & alignment □ Improved model customization and editing ■ AI labs are likely already well-incentivized to invest in TDA research efforts because of the benefits to AI research. ■ Public access to TDA tooling on frontier AI models is limited primarily by the unwillingness/inability of AI labs to publicly share their training data. □ AI labs currently have strong incentives to keep their training data private, as publishing such data would have negative outcomes such as: ◦ Reduced competitive advantages from data curation ◦ Increased exposure to legal liabilities from data collection ◦ Violating privacy or proprietary data requirements □ AI labs may be able to avoid these outcomes by selectively permitting TDA inference on certain training examples, or returning sources rather than the exact training data. □ Governments are highly unlikely to mandate public access to training data. ■ If AI labs willingly provided public access to TDA, you could expect the following benefits, among others: □ Preventing copyrighted data usage. □ Improved fact checking/content moderation □ Impacts on public trust and confidence in LLMs □ Accelerated research by external parties □ Increased accountability for AI labs ■ AI labs appear largely disincentivized to provide access to TDA inference, as many of the public benefits are disadvantageous for them. □ Governments are highly unlikely to mandate public access to TDA. □ It seems plausible that certain AI labs may expose TDA as a feature, but that the majority would prefer to use it privately to improve their models. ■ Several systems that could be enabled by efficient TDA include: □ Providing royalties to data providers/creators □ Automated response improvement/fact-checking □ Tooling for improving external audits of training data □ Content attribution tooling for LLMs, though it is unlikely to replace systems reliant on RAG ■ We believe that the most promising benefit of TDA for AI risk mitigation is its potential to improve the technical safety of LLMs via interpretability. □ There are some societal/systematic benefits from TDA, and these benefits may be a small contributing factor to reducing some sources of risk. We don't think these appear to move the needle significantly to reduce large-scale AI risks. □ TDA may meaningfully improve AI capabilities research, which might actually increase large-scale risk. □ TDA may eventually be highly impactful in technical AI safety and alignment efforts. We'd consider TDA's potential impact on technical AI safety to be in a similar category to supporting mechanistic interpretability research.
UR  - https://arxiv.org/pdf/2501.12642v1.pdf
N1  - Source: page_1.html
ID  - 96
ER  - 
TY  - JOUR
TI  - Leveraging Data Science for Real-Time Decision Making: A Framework for Scalable and Adaptive Systems
AU  - Mhaske, P.D.
AU  - Lokare, A.
AU  - Bankar, S.
PY  - 2025
AB  - Real-time decision-making is a strength for industries where decision-making in the fast-changing environment has influenced decision-making due to advancing technologies. The current paper outlines a conceptual framework that leverages data science to improve real-time decision-making in flexible and expandable systems. The architecture combines machine learning and streaming analytics for high-velocity data streams, which provide business decisions in milliseconds. As a distributed cloud-based system, the proposed system guarantees high scalability and flexibility since the predictive models are updated in real time to adjust to the new data pattern. The proposed framework outperforms other multiple models by using the decision tree, deep learning, and ensembling; accuracy, speed, and resources results are more persuasive. Actual implementations substantiate that the system’s dynamic aspects increase the accuracy of the premeditated choices by 25% when compared to the normal unelastic models and, at the same time, slashes the time delay by 40%. The result of this research gives real-life insight into the effective integration of real-time data science into BI systems, making it perfect for building strong BI systems in the future. The paper’s results enrich the current discussion on real-time analytics by presenting a clear guideline for building larger-scale decision-making frameworks for domains that include finance, healthcare, and IoT-based industries.
UR  - https://dx.doi.org/10.36227/techrxiv.173750215.54129132/v1
N1  - Source: page_1.html
ID  - 97
ER  - 
TY  - JOUR
TI  - Personality Modeling for Persuasion of Misinformation using AI Agent
AU  - Lou, Q.
AU  - Xu, W.
PY  - 2025
AB  - The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation. This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics. Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics. The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation. Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction. Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations. The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence. These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches. The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts.
UR  - https://arxiv.org/pdf/2501.08985v1.pdf
N1  - Source: page_1.html
ID  - 98
ER  - 
TY  - JOUR
TI  - Visible Sources and Invisible Risks: Exploring the Impact of Ai Disclosure on Perceived Credibility of Ai-Generated Content
AU  - Lin, T.
AU  - Zhang, Y.
PY  - 2025
AB  - With the development of Large Language Models, the capability of AI in generating information has significantly improved, leading to its widespread application in content production. However, the increasing persuasiveness of AI-generated content (AIGC) is also making it increasingly difficult to discern AI-generated misinformation, and AI disclosures are therefore being used to help people counteract the negative effects of misinformation, as well as to increase acceptance of AIGC. In order to assess the effectiveness of AI disclosure applied to popular science articles on social media, we conducted a within-subject experiment (N=419) in the context of the Chinese internet environment. The results indicated that AI disclosure not only made people more likely to believe AI-generated misinformation but also reduced their perceived trust in AI-generated accurate information. This effect diminished as the audience's attitudes toward AI became more negative. For AI-generated misinformation, the moderating effect of negative attitudes towards AI is related to the subject matter of the message. For AI-generated correct information, the negative impact caused by AI disclosure may not exist when the audience's negative attitude towards AI is weak. Furthermore, the level of audience involvement with the information was irrelevant. The study provides new empirical evidence for the debate on the effectiveness of AI disclosure, while also highlighting potential issues in its practical application and discussing the prospects of AI disclosure.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5080378&type=2
N1  - Source: page_1.html
ID  - 99
ER  - 
TY  - JOUR
TI  - Enhancing Energy Efficiency Through User Engagement and Behaviour Change: A Review on Gamification Approaches and Serious Games in Energy Systems
AU  - Abdurahmanovic, N.
AU  - Cadenbach, A.
PY  - 2025
AB  - Purely technical solutions are insufficient to optimize district heating systems (DHS), and end-user behaviour significantly impacts the system’s performance. Thus, prioritizing customer engagement strategies and fostering cooperation with consumers are crucial for realizing the full potential of DHS.This paper explores the potential of gamification as an effective method for engaging end-users. While gamification has demonstrated success in various fields, its potential in DHS requires further investigation. To address this, the paper conducted a literature review on gamification in energy systems.The study examined gamification elements and techniques and examples of gamification implementation in energy systems. It evaluated the potential impact and benefits on user engagement and energy consumption patterns in heating systems. Reported energy savings from gamified solutions varied significantly, ranging from 4 % to 42 %, highlighting their potential to transform energy consumption behaviour. Additionally, the study identified challenges and limitations associated with implementing gamification in DHS and explored the use of artificial intelligence and machine learning to overcome them. These technologies enhance user engagement by analysing and predicting user behaviour and preferences.By doing so, this research identified key gaps and potential advancements in gamification techniques for DHS, aiming to optimize system performance and enhance digitalisation.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5080167&type=2
N1  - Source: page_1.html
ID  - 100
ER  - 
TY  - JOUR
TI  - Will you donate money to a chatbot? The effect of chatbot anthropomorphic features and persuasion strategies on willingness to donate
AU  - Novozhilova, E.
AU  - Huang, J.
AU  - He, L.
AU  - Li, Z.
AU  - Cummings, J.
PY  - 2024
AB  - This work investigates the causal mechanism behind the effect of chatbot personification and persuasion strategies on users’ perceptions and donation likelihood. In a 2 (personified vs. non-personified chatbot) x 2 (emotional vs. logical persuasion strategy) between-subjects experiment (N = 76), participants engaged with a chatbot that represented a non-profit charitable organization. The results suggest that interaction with a personified chatbot evokes perceived anthropomorphism, however, does not elicit greater willingness to donate. In fact, we found that commonly used anthropomorphic features, like name and narrative, led to negative attitudes toward an AI agent in the donation context. Our results showcase a preference for non-personified chatbots paired with logical persuasion appeal, emphasizing the significance of consistency in chatbot interaction, mirroring human-human engagement. We discuss the importance of moving from exploring the common scenario of chatbot with machine identity vs. chatbot with human identity in light of the recent regulations of AI systems.
UR  - https://arxiv.org/pdf/2412.19976v1.pdf
N1  - Source: page_1.html
ID  - 101
ER  - 
TY  - JOUR
TI  - Lies, Damned Lies, and Distributional Language Statistics: Persuasion and Deception with Large Language Models
AU  - Jones, C.R.
AU  - Bergen, B.K.
PY  - 2024
AB  - Large Language Models (LLMs) can generate content that is as persuasive ashuman-written text and appear capable of selectively producing deceptiveoutputs. These capabilities raise concerns about potential misuse andunintended consequences as these systems become more widely deployed. Thisreview synthesizes recent empirical work examining LLMs\' capacity andproclivity for persuasion and deception, analyzes theoretical risks that couldarise from these capabilities, and evaluates proposed mitigations. Whilecurrent persuasive effects are relatively small, various mechanisms couldincrease their impact, including fine-tuning, multimodality, and socialfactors. We outline key open questions for future research, including howpersuasive AI systems might become, whether truth enjoys an inherent advantageover falsehoods, and how effective different mitigation strategies may be inpractice.MSC Codes 68T50
UR  - https://arxiv.org/pdf/2412.17128v1.pdf
N1  - Source: page_1.html
ID  - 102
ER  - 
TY  - JOUR
TI  - Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues
AU  - He, T.
AU  - Liao, L.
AU  - Cao, Y.
AU  - Liu, M.
AU  - Qin, B.
PY  - 2024
AB  - Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM. Our codes are available at https://github.com/cs-holder/LDPP.git.
UR  - https://arxiv.org/pdf/2412.14584v1.pdf
N1  - Source: page_1.html
ID  - 103
ER  - 
TY  - JOUR
TI  - A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation
AU  - Leite, J.A.
AU  - Razuvayevskaya, O.
AU  - Scarton, C.
AU  - Bontcheva, K.
PY  - 2024
AB  - Disinformation, irrespective of domain or language, aims to deceive or manipulate public opinion, typically through employing advanced persuasion techniques. Qualitative and quantitative research on the weaponisation of persuasion techniques in disinformation has been mostly topic-specific (e.g., COVID-19) with limited cross-domain studies, resulting in a lack of comprehensive understanding of these strategies. This study employs a state-of-the-art persuasion technique classifier to conduct a large-scale, multi-domain analysis of the role of 16 persuasion techniques in disinformation narratives. It shows how different persuasion techniques are employed disproportionately in different disinformation domains. We also include a detailed case study on climate change disinformation, highlighting how linguistic, psychological, and cultural factors shape the adaptation of persuasion strategies to fit unique thematic contexts.
UR  - https://arxiv.org/pdf/2412.15098v1.pdf
N1  - Source: page_1.html
ID  - 104
ER  - 
TY  - JOUR
TI  - Mobilizing Waldo: Evaluating Multimodal AI for Public Mobilization
AU  - Cebrian, M.
AU  - Holme, P.
AU  - Pescetelli, N.
PY  - 2024
AB  - Advancements in multimodal Large Language Models (LLMs), such as OpenAI’s GPT-4o, present significant potential for mediating human interactions across various contexts. However, their capabilities in controversial areas, such as persuasion, influence, and recruitment, raise substantial ethical and security concerns. To ethically evaluate these models in situational public influence and persuasion scenarios, we developed a prompting strategy utilizing “Where’s Waldo?” images as proxies for complex, crowded real-world gatherings. This approach provides a controlled and replicable environment to assess the model’s ability to process intricate visual information, comprehend social dynamics, and propose engagement strategies without infringing on individual privacy. By positioning Waldo as a hypothetical agent tasked with face-to-face mobilization, we analyzed the model’s performance in identifying key individuals and formulating mobilization tactics. Our findings reveal that while the model can generate vivid descriptions and creative strategies, it currently lacks the ability to accurately identify individuals or reliably assess social dynamics within these scenarios. Nevertheless, our methodology offers a valuable framework for testing and benchmarking the evolving capabilities of multimodal LLMs in social contexts.
UR  - https://arxiv.org/pdf/2412.14210v1.pdf
N1  - Source: page_1.html
ID  - 105
ER  - 
TY  - JOUR
TI  - Bayesian Persuasion with Externalities: Exploiting Agent Types
AU  - Shaki, J.
AU  - Gan, J.
AU  - Kraus, S.
PY  - 2024
AB  - We study a Bayesian persuasion problem with externalities. In this model, a principal sends signals to inform multiple agents about the state of the world. Simultaneously, due to the existence of externalities in the agents’ utilities, the principal also acts as a correlation device to correlate the agents’ actions. We consider the setting where the agents are categorized into a small number of types. Agents of the same type share identical utility functions and are treated equitably in the utility functions of both other agents and the principal. We study the problem of computing optimal signaling strategies for the principal, under three different types of signaling channels: public, private, and semi-private. Our results include revelation-principle-style characterizations of optimal signaling strategies, linear programming formulations, and analysis of in/tractability of the optimization problems. It is demonstrated that when the maximum number of deviating agents is bounded by a constant, our LP-based formulations compute optimal signaling strategies in polynomial time. Otherwise, the problems are NP-hard.
UR  - https://arxiv.org/pdf/2412.12859v1.pdf
N1  - Source: page_1.html
ID  - 106
ER  - 
TY  - JOUR
TI  - Virtual Agent-Based Communication Skills Training to Facilitate Health Persuasion Among Peers
AU  - Nouraei, F.
AU  - Rebello, K.
AU  - Fallah, M.
AU  - Paasche-Orlow, M.
AU  - Bickmore, T.
PY  - 2024
AB  - Many laypeople are motivated to improve the health behavior of their family or friends but do not know where to start, especially if the health behavior is potentially stigmatizing or controversial. We present an approach that uses virtual agents to coach community-based volunteers in health counseling techniques, such as motivational interviewing, and allows them to practice these skills in role-playing scenarios. We use this approach in a virtual agent-based system to increase COVID-19 vaccination by empowering users to influence their social network. In a between-subjects comparative design study, we test the effects of agent system interactivity and role-playing functionality on counseling outcomes, with participants evaluated by standardized patients and objective judges. We find that all versions are effective at producing peer counselors who score adequately on a standardized measure of counseling competence, and that participants were significantly more satisfied with interactive virtual agents compared to passive viewing of the training material. We discuss design implications for interpersonal skills training systems based on our findings.
UR  - https://arxiv.org/pdf/2412.12061v1.pdf
N1  - Source: page_1.html
ID  - 107
ER  - 
TY  - JOUR
TI  - Discrimination of inherent characteristics of susceptible and resistant strains of Anopheles gambiae by explainable Artificial Intelligence Analysis of Flight Trajectories
AU  - Qureshi, Y.M.
AU  - Voloshin, V.
AU  - Gleave, K.
AU  - Covington, J.A.
AU  - Towers, D.P.
PY  - 2024
AB  - Understanding mosquito behaviours is vital for development of insecticide-treated bednets (ITNs), which have been successfully deployed in sub-Saharan Africa to reduce disease transmission, particularly malaria. However, rising insecticide resistance (IR) among mosquito populations, owing to genetic and behavioural changes, poses a significant challenge. We present a machine learning pipeline that successfully distinguishes between IR and insecticide-susceptible (IS) mosquito behaviours by analysing trajectory data. Data driven methods are introduced to accommodate common tracking system shortcomings that occur due to mosquito positions being occluded by the bednet or other objects. Trajectories, obtained from room-scale tracking of two IR and two IS strains around a human-baited, untreated bednet, were analysed using features such as velocity, acceleration, and geometric descriptors. Using these features, an XGBoost model achieved a balanced accuracy of 0.743 and a ROC AUC of 0.813 in classifying IR from IS mosquitoes. SHAP analysis helped decipher that IR mosquitoes tend to fly slower with more directed flight paths and lower variability than IS—traits that are likely a fitness advantage by enhancing their ability to respond more quickly to bloodmeal cues. This approach provides valuable insights based on flight behaviour that can reveal the action of interventions and insecticides on mosquito physiology.
UR  - https://www.biorxiv.org/content/10.1101/2024.12.10.627548v1.full.pdf
N1  - Source: page_1.html
ID  - 108
ER  - 
TY  - JOUR
TI  - Cognitive Dissonance in Programming Education: A Qualitative Exploration of the Impact of Generative Ai on Application-Directed Learning
AU  - Dawson, M.G.
AU  - Deer, R.
AU  - Boguslawski, S.
PY  - 2024
AB  - Generative AI tools, powered by Large Language Models (LLMs), are already being extensively used by students to support their learning and it is important that educators understand what this might mean for higher education practice. Students can be reluctant to have open conversations about their use of assistive technology making it difficult for lecturers to understand the effects of LLM use on the learning process. In this study, two researchers external to the faculty teaching team conducted in-depth interviews with 12 students in a small tech-focussed university who have recently undertaken programming learning as part of their undergraduate studies to explore the question: how do programming students use LLMs to support their learning? A thematic analysis of the resulting qualitative data revealed trends in the perceived advantages and disadvantages of using LLMs, as well as different levels of LLM usage, with more cautious use associated with a 'meaning-directed' approach to learning (learning pattern) and more enthusiastic and unrestrained use with 'application-directed' patterns of study. A tension was observed between some application-directed learners’ high use of LLMs and their recognition that this is not optimal for effective learning. The authors argue that Cognitive Dissonance Theory (CDT) can explain how this dissonance may motivate learners toward a dissonance-reducing attitude or behavior change. The conclusion reflects on the implications for teaching practice and offers some recommendations for how educators can increase metacognition, instrumentalize CDT to increase self-regulation, and facilitate meaning-directed learning patterns in the age of generative AI.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5055559&type=2
N1  - Source: page_1.html
ID  - 109
ER  - 
TY  - JOUR
TI  - Cognitive Dissonance in Programming Education: A Qualitative Exploration of the Impact of Generative Ai on Application-Directed Learning
AU  - Dawson, M.G.
AU  - Deer, R.
AU  - Boguslawski, S.
PY  - 2024
AB  - Generative AI tools, powered by Large Language Models (LLMs), are already being extensively used by students to support their learning and it is important that educators understand what this might mean for higher education practice. Students can be reluctant to have open conversations about their use of assistive technology making it difficult for lecturers to understand the effects of LLM use on the learning process. In this study, two researchers external to the faculty teaching team conducted in-depth interviews with 12 students in a small tech-focussed university who have recently undertaken programming learning as part of their undergraduate studies to explore the question: how do programming students use LLMs to support their learning? A thematic analysis of the resulting qualitative data revealed trends in the perceived advantages and disadvantages of using LLMs, as well as different levels of LLM usage, with more cautious use associated with a 'meaning-directed' approach to learning (learning pattern) and more enthusiastic and unrestrained use with 'application-directed' patterns of study. A tension was observed between some application-directed learners’ high use of LLMs and their recognition that this is not optimal for effective learning. The authors argue that Cognitive Dissonance Theory (CDT) can explain how this dissonance may motivate learners toward a dissonance-reducing attitude or behavior change. The conclusion reflects on the implications for teaching practice and offers some recommendations for how educators can increase metacognition, instrumentalize CDT to increase self-regulation, and facilitate meaning-directed learning patterns in the age of generative AI.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5052985&type=2
N1  - Source: page_1.html
ID  - 110
ER  - 
TY  - JOUR
TI  - Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation
AU  - Cima, L.
AU  - Miaschi, A.
AU  - Trujillo, A.
AU  - Dell’Orletta, F.
AU  - Cresci, S.
PY  - 2024
AB  - AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct a LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.
UR  - https://arxiv.org/pdf/2412.07338v3.pdf
N1  - Source: page_1.html
ID  - 111
ER  - 
TY  - JOUR
TI  - LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial Search for Adaptive Arguments
AU  - Aryan, P.
PY  - 2024
AB  - This paper introduces DebateBrawl, an innovative AI-powered debate platform that integrates Large Language Models (LLMs), Genetic Algorithms (GA), and Adversarial Search (AS) to create an adaptive and engaging debating experience. DebateBrawl addresses the limitations of traditional LLMs in strategic planning by incorporating evolutionary optimization and game-theoretic techniques. The system demonstrates remarkable performance in generating coherent, contextually relevant arguments while adapting its strategy in real-time. Experimental results involving 23 debates show balanced outcomes between AI and human participants, with the AI system achieving an average score of 2.72 compared to the human average of 2.67 out of 10. User feedback indicates significant improvements in debating skills and a highly satisfactory learning experience, with 85% of users reporting improved debating abilities and 78% finding the AI opponent appropriately challenging. The system’s ability to maintain high factual accuracy (92% compared to 78% in human-only debates) while generating diverse arguments addresses critical concerns in AI-assisted discourse. DebateBrawl not only serves as an effective educational tool but also contributes to the broader goal of improving public discourse through AI-assisted argumentation. The paper discusses the ethical implications of AI in persuasive contexts and outlines the measures implemented to ensure responsible development and deployment of the system, including robust fact-checking mechanisms and transparency in decision-making processes.
UR  - https://arxiv.org/pdf/2412.06229v1.pdf
N1  - Source: page_1.html
ID  - 112
ER  - 
TY  - JOUR
TI  - Can Generative Artificial Intelligence Drive Sustainable Behavior? A Consumer Adoption Model for Ai-Driven Recommendations
AU  - Silalahi, A.D.K.
PY  - 2024
AB  - Generative AI (GAI) holds potential to promote sustainable behaviors by personalized recommendations, yet its effectiveness hinges on user trust—a critical area underexplored in current literature. Existing studies often focus narrowly on domain-specific applications without addressing broader trust-building mechanisms or the cognitive and motivational factors essential for sustained engagement with GAI in sustainability contexts. This study aims to bridge these gaps by examining how trust influences the adoption of GAI-driven sustainability recommendations, drawing on the Elaboration Likelihood Model (ELM) and Expectancy-Value Theory (EVT) to create a robust and holistic framework. Drawing on data from sustainability-focused users, we examine how central route constructs (perceived information quality and utility) and peripheral route constructs (anthropomorphism and interaction quality) foster trust, with perceived information complexity and risk as moderating factors. Findings indicate that cognitive engagement through high information quality and utility strengthens trust, while anthropomorphic design elements and interaction quality enhance trust from heuristic path. However, excessive information complexity and privacy concerns weaken trust, emphasizing the need for clear communication and data transparency. This study advances theoretical understanding by extending ELM and EVT to GAI’s role in sustainability, providing a holistic framework that integrates cognitive and motivational trust drivers. These insights not only fill significant gaps in technology adoption research but also offer practical guidance for designing GAI platforms that genuinely support pro-environmental behavior change.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5037486&type=2
N1  - Source: page_1.html
ID  - 113
ER  - 
TY  - JOUR
TI  - Habit Coach: Customising RAG-based chatbots to support behavior change
AU  - Arabi, A.F.M.
AU  - Koyuturk, C.
AU  - O'Mahony, M.
AU  - Calati, R.
AU  - Ognibene, D.
PY  - 2024
AB  - This paper presents the iterative development of Habit Coach, a GPT-based chatbot designed to support users in habit change through personalized interaction. Employing a user-centered design approach, we developed the chatbot using a Retrieval-Augmented Generation (RAG) system, which enables behavior personalization without retraining the underlying language model (GPT-4). The system leverages document retrieval and specialized prompts to tailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and narrative therapy techniques. A key challenge in the development process was the difficulty of translating declarative knowledge into effective interaction behaviors. In the initial phase, the chatbot was provided with declarative knowledge about CBT via reference textbooks and high-level conversational goals. However, this approach resulted in imprecise and inefficient behavior, as the GPT model struggled to convert static information into dynamic and contextually appropriate interactions. This highlighted the limitations of relying solely on declarative knowledge to guide chatbot behavior, particularly in nuanced, therapeutic conversations. Over four iterations, we addressed this issue by gradually transitioning towards procedural knowledge, refining the chatbot's interaction strategies and improving its overall effectiveness. In the final evaluation, 5 participants engaged with the chatbot over five consecutive days, receiving individualized CBT interventions. The Self-Report Habit Index (SRHI) was used to measure habit strength before and after the intervention, revealing a reduction in habit strength post-intervention. These results underscore the importance of procedural knowledge in driving effective, personalized behavior change support in RAG-based systems.
UR  - https://arxiv.org/pdf/2411.19229v1.pdf
N1  - Source: page_1.html
ID  - 114
ER  - 
TY  - JOUR
TI  - Personalised Serious Games and Gamification in Healthcare: Survey and Future Research Directions
AU  - Carlier, S.
AU  - De Backere, F.
AU  - De Turck, F.
PY  - 2024
AB  - Serious games, games with a primary objective other than pure entertainment, and gamification, the use of game elements in non-game contexts, have shown to have positive effects on health outcomes of eHealth applications. However, research has shown that a shift towards a more personalised approach is needed, considering the diversity of users and their contexts. This introduces new challenges to the domain of serious games and gamification (SGG) as research is needed on how such personalisation is achieved. A literature search was conducted, using Web of Science and PubMed, to provide an overview of personalisation strategies applied in SGG in health. In total, 31 articles were identified, of which 22 reported on a serious game and 9 focused on gamification. Results indicate that personalised serious games and gamification have been applied most in the fields of behaviour change and rehabilitation. Furthermore, the use of machine learning and artificial intelligence (AI) for personalisation shows promise as they can find patterns and relationships in large data sets. Findings indicated that reusability is still an under-highlighted aspect in the design and development of personalised SGG, as only 10 out of 31 articles reported on some form of reuse. Future research should go towards the standardisation of the development of personalised SGG by focusing on the reusability of the different components and the use of generative AI. This standardisation holds the potential to simplify the design process and involvement of domain experts and facilitates a more detailed evaluation of different personalisation strategies.
UR  - https://arxiv.org/pdf/2411.18500v1.pdf
N1  - Source: page_1.html
ID  - 115
ER  - 
TY  - JOUR
TI  - Strategic Prompting for Conversational Tasks: A Comparative Analysis of Large Language Models Across Diverse Conversational Tasks
AU  - Joshi, R.Kr.
AU  - Priya, P.
AU  - Desai, V.
AU  - Maitra, A.
AU  - Sengupta, S.
PY  - 2024
AB  - Given the advancements in conversational artificial intelligence, the evaluation and assessment of Large Language Models (LLMs) play a crucial role in ensuring optimal performance across various conversational tasks. In this paper, we present a comprehensive study that thoroughly evaluates the capabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon, Alpaca, and MPT. The study encompasses various conversational tasks, including reservation, empathetic response generation, mental health and legal counseling, persuasion, and negotiation. To conduct the evaluation, an extensive test setup is employed, utilizing multiple evaluation criteria that span from automatic to human evaluation. This includes using generic and task-specific metrics to gauge the LMs’ performance accurately. From our evaluation, no single model emerges as universally optimal for all tasks. Instead, their performance varies significantly depending on the specific requirements of each task. While some models excel in certain tasks, they may demonstrate comparatively poorer performance in others. These findings emphasize the importance of considering task-specific requirements and characteristics when selecting the most suitable LM for conversational applications.
UR  - https://arxiv.org/pdf/2411.17204v2.pdf
N1  - Source: page_1.html
ID  - 116
ER  - 
TY  - JOUR
TI  - Ethical Dimensions and Decision-Making Dynamics: A Meta-Synthesis of Ai Nudging Across Sectors
AU  - Ofem, O.E.
PY  - 2024
AB  - Artificial intelligence (AI) nudging, rooted in behavioral economics and choice architecture theories such as Thaler and Sunstein's nudge theory, has emerged as a method for influencing decision-making. However, its overall effectiveness and ethical implications remain insufficiently understood. This systematic literature review evaluates the benefits, risks, and ethical considerations of AI nudging in everyday life. Guided by theoretical frameworks including nudge theory and Floridi et al.'s AI ethics principles, we conducted a comprehensive search following PRISMA guidelines, identifying 61 studies for qualitative synthesis and 25 for quantitative meta-analysis. Results indicate that AI nudging effectively alters behaviors across healthcare, energy, and finance sectors, with personalized and context-aware interventions showing the greatest impact. Prominent ethical concerns include user autonomy, transparency, and algorithmic biases, highlighting the need for robust ethical frameworks. While AI nudging is a powerful tool for positive behavior change, integrating ethical considerations is imperative to safeguard individual rights and ensure responsible use.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5029845&type=2
N1  - Source: page_1.html
ID  - 117
ER  - 
TY  - JOUR
TI  - ENGAGEMENT-DRIVEN CONTENT GENERATION WITH LARGE LANGUAGE MODELS
AU  - Coppolillo, E.
AU  - Cinus, F.
AU  - Minici, M.
AU  - Bonchi, F.
AU  - Manco, G.
PY  - 2024
AB  - Large Language Models (LLMs) exhibit significant persuasion capabilities in one-on-one interactions, but their influence within social networks remains underexplored. This study investigates the potential social impact of LLMs in these environments, where interconnected users and complex opinion dynamics pose unique challenges. In particular, we address the following research question: can LLMs learn to generate meaningful content that maximizes user engagement on social networks? To answer this question, we define a pipeline to guide the LLM-based content generation which employs reinforcement learning with simulated feedback. In our framework, the reward is based on an engagement model borrowed from the literature on opinion dynamics and information propagation. Moreover, we force the text generated by the LLM to be aligned with a given topic and to satisfy a minimum fluency requirement. Using our framework, we analyze the capabilities and limitations of LLMs in tackling the given task, specifically considering the relative positions of the LLM as an agent within the social network and the distribution of opinions in the network on the given topic. Our findings show the full potential of LLMs in creating social engagement. Notable properties of our approach are that the learning procedure is adaptive to the opinion distribution of the underlying network and agnostic to the specifics of the engagement model, which is embedded as a plug-and-play component. In this regard, our approach can be easily refined for more complex engagement tasks and interventions in computational social science. The code used for the experiments is publicly available at https://anonymous.4open.science/r/EDCG/.
UR  - https://arxiv.org/pdf/2411.13187v3.pdf
N1  - Source: page_1.html
ID  - 118
ER  - 
TY  - JOUR
TI  - NewsInterview: a Dataset and a Playground to Evaluate LLMs’ Grounding Gap via Informational Interviews
AU  - Lu, M.
AU  - Cho, H.J.
AU  - Shi, W.
AU  - Spangher, A.
AU  - Kalyan, S.
PY  - 2024
AB  - Large Language Models (LLMs) have demonstrated impressive capabilities in generating coherent text but often struggle with grounding language and strategic dialogue. To address this gap, we focus on journalistic interviews, a domain rich in grounding communication and abundant in data. We curate a dataset of 40,000 two-person informational interviews from NPR and CNN, and reveal that LLMs are significantly less likely than human interviewers to use acknowledgements and to pivot to higher-level questions. Realizing that a fundamental deficit exists in multi-turn planning and strategic thinking, we develop a realistic simulated environment, incorporating source personas and persuasive elements, in order to facilitate the development of agents with longer-horizon rewards. Our experiments show that while source LLMs mimic human behavior in information sharing, interviewer LLMs struggle with recognizing when questions are answered and engaging persuasively, leading to suboptimal information extraction across model size and capability. These findings underscore the need for enhancing LLMs’ strategic dialogue capabilities.
UR  - https://arxiv.org/pdf/2411.13779v1.pdf
N1  - Source: page_1.html
ID  - 119
ER  - 
TY  - JOUR
TI  - Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment
AU  - Huang, A.
AU  - Pi, Y.N.
AU  - Mougan, C.
PY  - 2024
AB  - We explore how large language models (LLMs) can be influenced by prompting them to alter their initial decisions and align them with established ethical frameworks. Our study is based on two experiments designed to assess the susceptibility of LLMs to moral persuasion. In the first experiment, we examine the susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally ambiguous scenarios and observing how a Persuader Agent attempts to modify the Base Agent’s initial decisions. The second experiment evaluates the susceptibility of LLMs to align with predefined ethical frameworks by prompting them to adopt specific value alignments rooted in established philosophical theories. The results demonstrate that LLMs can indeed be persuaded in morally charged scenarios, with the success of persuasion depending on factors such as the model used, the complexity of the scenario, and the conversation length. Notably, LLMs of distinct sizes but from the same company produced markedly different outcomes, highlighting the variability in their susceptibility to ethical persuasion. Code is available at https://github.com/acyhuang/moral-persuasion.
UR  - https://arxiv.org/pdf/2411.11731v1.pdf
N1  - Source: page_1.html
ID  - 120
ER  - 
TY  - JOUR
TI  - PEEK: Phishing Evolution Framework for Phishing Generation and Evolving Pattern Analysis using Large Language Models
AU  - Chen, F.
AU  - Wu, T.
AU  - Nguyen, V.
AU  - Abuadbba, A.
AU  - Rudolph, C.
PY  - 2024
AB  - Phishing remains a pervasive cyber threat, as attackers craft deceptive emails to lure victims into revealing sensitive information. While Artificial Intelligence (AI), in particular, deep learning, has become a key component in defending against phishing attacks, these approaches face critical limitations. The scarcity of publicly available, diverse, and updated data, largely due to privacy concerns, constrains detection effectiveness. As phishing tactics evolve rapidly, models trained on limited, outdated data struggle to detect new, sophisticated deception strategies, leaving systems and people vulnerable to an ever-growing array of attacks. We propose the first Phishing Evolution FramEworK (PEEK) for augmenting phishing email datasets with respect to quality and diversity, and analyzing changing phishing patterns for detection to adapt to updated phishing attacks. Specifically, we integrate large language models (LLMs) into the process of adversarial training to enhance the performance of the generated dataset and leverage persuasion principles in a recurrent framework to facilitate the understanding of changing phishing strategies. PEEK raises the proportion of usable phishing samples from 21.4% to 84.8%, surpassing existing works that rely on prompting and fine-tuning LLMs. The phishing datasets provided by PEEK, with evolving phishing patterns, outperform the other two available LLM-generated phishing email datasets in improving detection robustness. PEEK phishing boosts detectors’ accuracy to over 88% and reduces adversarial sensitivity by up to 70%, still maintaining 70% detection accuracy against adversarial attacks.
UR  - https://arxiv.org/pdf/2411.11389v2.pdf
N1  - Source: page_1.html
ID  - 121
ER  - 
TY  - JOUR
TI  - Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment
AU  - Gupta, A.
AU  - Kocielnik, R.
AU  - Wang, J.
AU  - Anandkumar, A.
AU  - Hung, A.J.
PY  - 2024
AB  - During surgical training, real-time feedback from trainers to trainees is important for preventing errors and enhancing long-term skill acquisition. Accurately predicting the effectiveness of this feedback, specifically whether it leads to a change in trainee behavior, is crucial for developing methods for improving surgical training and education. However, relying on human annotations to assess feedback effectiveness is laborious and prone to biases, underscoring the need for an automated, scalable, and objective method. Creating such an automated system poses challenges, as it requires an understanding of both the verbal feedback delivered by the trainer and the visual context of the real-time surgical scene. To address this, we propose a method that integrates information from transcribed verbal feedback and corresponding surgical video to predict feedback effectiveness. Our findings show that both transcribed feedback and surgical video are individually predictive of trainee behavior changes, and their combination achieves an AUROC of 0.70 ± 0.02, improving prediction accuracy by up to 6.6%. Additionally, we introduce self-supervised fine-tuning as a strategy for enhancing surgical video representation learning, which is scalable and further enhances prediction performance. Our results demonstrate the potential of multi-modal learning to advance the automated assessment of surgical feedback.MSC Codes 68T07, 68T45, 68U10, 92C50
UR  - https://arxiv.org/pdf/2411.10919v1.pdf
N1  - Source: page_1.html
ID  - 122
ER  - 
TY  - JOUR
TI  - PERSUASION WITH LARGE LANGUAGE MODELS: A SURVEY
AU  - Rogiers, A.
AU  - Noels, S.
AU  - Buyl, M.
AU  - De Bie, T.
PY  - 2024
AB  - The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.
UR  - https://arxiv.org/pdf/2411.06837v1.pdf
N1  - Source: page_1.html
ID  - 123
ER  - 
TY  - JOUR
TI  - Assessing Open-Source Large Language Models on Argumentation Mining Subtasks
AU  - Abkenar, M.Y.
AU  - Wang, W.
AU  - Graupner, H.
AU  - Stede, M.
PY  - 2024
AB  - We explore the capability of four open-source large language models (LLMs) in argumentation mining (AM). We conduct experiments on three different corpora; persuasive essays (PE), argumentative microtexts (AMT) Part 1 and Part 2, based on two argumentation mining sub-tasks: (i) argumentative discourse units classifications (ADUC), and (ii) argumentative relation classification (ARC). This work aims to assess the argumentation capability of open-source LLMs, including Mistral 7B, Mixtral 8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot and few-shot scenarios. Our analysis contributes to further assessing computational argumentation with open-source LLMs in future research efforts1
UR  - https://arxiv.org/pdf/2411.05639v1.pdf
N1  - Source: page_1.html
ID  - 124
ER  - 
TY  - JOUR
TI  - The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses
AU  - Mieleszczenko-Kowszewicz, W.
AU  - Płudowski, D.
AU  - Kołodzieczyk, F.
AU  - Sienkiewicz, J.
AU  - Biecek, P.
PY  - 2024
AB  - This study explores how the Large Language Models (LLMs) adjust linguistic features to create personalized persuasive outputs. While research showed that LLMs personalize outputs, a gap remains in understanding the linguistic features of their persuasive capabilities. We identified 13 linguistic features crucial for influencing personalities across different levels of the Big Five model of personality. We analyzed how prompts with personality trait information influenced the output of 19 LLMs across five model families. The findings show that models use more anxiety-related words for neuroticism, increase achievement-related words for conscientiousness, and employ fewer cognitive processes words for openness to experience. Some model families excel at adapting language for openness to experience, others for conscientiousness, while only one model adapts language for neuroticism. Our findings show how LLMs tailor responses based on personality cues in prompts, indicating their potential to create persuasive content affecting the mind and well-being of the recipients.
UR  - https://arxiv.org/pdf/2411.06008v2.pdf
N1  - Source: page_1.html
ID  - 125
ER  - 
TY  - JOUR
TI  - INTERACTIVE DIALOGUE AGENTS VIA REINFORCEMENT LEARNING ON HINDSIGHT REGENERATIONS
AU  - Hong, J.
AU  - Lin, J.
AU  - Dragan, A.
AU  - Levine, S.
PY  - 2024
AB  - Recent progress on large language models (LLMs) has enabled dialogue agents to generate highly naturalistic and plausible text. However, current LLM language generation focuses on responding accurately to questions and requests with a single effective response. In reality, many real dialogues are interactive, meaning an agent's utterances will influence their conversational partner, elicit information, or change their opinion. Accounting for how an agent can effectively steer a conversation is a crucial ability in many dialogue tasks, from healthcare to preference elicitation. Existing methods for fine-tuning dialogue agents to accomplish such tasks would rely on curating some amount of expert data. However, doing so often requires understanding the underlying cognitive processes of the conversational partner, which is a skill neither humans nor LLMs trained on human data can reliably do. Our key insight is that while LLMs may not be adept at identifying effective strategies for steering conversations a priori, or in the middle of an ongoing conversation, they can do so post-hoc, or in hindsight, after seeing how their conversational partner responds. We use this fact to rewrite and augment existing suboptimal data, and train via offline reinforcement learning (RL) an agent that outperforms both prompting and learning from unaltered human demonstrations. We apply our approach to two domains that require understanding human mental state, intelligent interaction, and persuasion: mental health support, and soliciting charitable donations. Our results in a user study with real humans show that our approach greatly outperforms existing state-of-the-art dialogue agents.
UR  - https://arxiv.org/pdf/2411.05194v1.pdf
N1  - Source: page_1.html
ID  - 126
ER  - 
TY  - JOUR
TI  - Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions
AU  - Song, T.
AU  - Tan, Y.
AU  - Zhu, Z.
AU  - Feng, Y.
AU  - Lee, Y.-C.
PY  - 2024
AB  - Multi-agent systems - systems with multiple independent AI agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of AI agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user's stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents' stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.
UR  - https://arxiv.org/pdf/2411.04578v1.pdf
N1  - Source: page_1.html
ID  - 127
ER  - 
TY  - JOUR
TI  - Bottom-Up and Top-Down Analysis of Values, Agendas, and Observations in Corpora and LLMs
AU  - Friedman, S.E.
AU  - Benkler, N.
AU  - Iverson, D.
AU  - Goldman, R.P.
AU  - Miller, C.
PY  - 2024
AB  - Large language models (LLMs) generate diverse, situated, persuasive texts from a plurality of potential perspectives, influenced heavily by their prompts and training data. As part of LLM adoption, we seek to characterize—and ideally, manage—the socio-cultural values that they express, for reasons of safety, accuracy, inclusion, and cultural fidelity. We present a validated approach to automatically (1) extracting heterogeneous latent value propositions from texts, (2) assessing resonance and conflict of values with texts, and (3) combining these operations to characterize the pluralistic value alignment of human-sourced and LLM-sourced textual data.
UR  - https://arxiv.org/pdf/2411.05040v1.pdf
N1  - Source: page_1.html
ID  - 128
ER  - 
TY  - JOUR
TI  - Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities
AU  - Ju, T.
AU  - Wang, Y.
AU  - Hua, Y.
AU  - Zhang, Z.
AU  - Liu, G.
PY  - 2024
AB  - The rapid adoption of large language models (LLMs) in multi-agent systems has highlighted their impressive capabilities in various applications. However, the security implications of these LLM-based multi-agent systems have not been thoroughly investigated, particularly concerning the spread of manipulated knowledge. In this paper, we aim to understand the security vulnerabilities associated with LLM-based multi-agent systems. We design a novel two-stage attack methodology, consisting of Persuasiveness Injection and Manipulated Knowledge Injection, to investigate the potential for manipulated knowledge to permeate trusted third-party platforms without explicit adversarial prompt attack. Through extensive experiments, we demonstrate that our attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication. Furthermore, we show that these manipulations can persist through popular retrieval-augmented generation (RAG) frameworks, where several benign agents store and retrieve manipulated chat histories for future interactions. To mitigate the potential risk, we propose two defense strategies by designing system prompts to encourage agents to critically verify the knowledge they share and incorporating supervisory agents to oversee interactions. Results demonstrate that those strategies can effectively reduce the spread success rate. This work highlights the critical vulnerabilities inherent in LLM-based multi-agent communities and calls for the urgent need for platforms to swiftly adopt targeted defenses against manipulated knowledge spread. Results demonstrate that even these straightforward strategies can significantly reduce the spread success rate, which calls for platforms to swiftly adopt targeted, cost-effective measures to prevent the spread of manipulated knowledge.
UR  - https://www.researchsquare.com/article/rs-5292520/v1.pdf
N1  - Source: page_1.html
ID  - 129
ER  - 
TY  - JOUR
TI  - Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters
AU  - Potter, Y.
AU  - Lai, S.
AU  - Kim, J.
AU  - Evans, J.
AU  - Song, D.
PY  - 2024
AB  - How could LLMs influence our democracy? We investigate LLMs’ political leanings and the potential influence of LLMs on voters by conducting multiple experiments in a U.S. presidential election context. Through a voting simulation, we first demonstrate 18 open- and closed-weight LLMs’ political preference for a Democratic nominee over a Republican nominee. We show how this leaning towards the Democratic nominee becomes more pronounced in instruction-tuned models compared to their base versions by analyzing their responses to candidate-policy related questions. We further explore the potential impact of LLMs on voter choice by conducting an experiment with 935 U.S. registered voters. During the experiments, participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse. This effect is larger than many previous studies on the persuasiveness of political campaigns, which have shown minimal effects in presidential elections. Many users also expressed a desire for further political interaction with LLMs. Which aspects of LLM interactions drove these shifts in voter choice requires further study. Lastly, we explore how a safety method can make LLMs more politically neutral, while leaving some open questions.
UR  - https://arxiv.org/pdf/2410.24190v2.pdf
N1  - Source: page_1.html
ID  - 130
ER  - 
TY  - JOUR
TI  - Leveraging Language Models and Bandit Algorithms to Drive Adoption of Battery-Electric Vehicles
AU  - Namikoshi, K.
AU  - Shamma, D.A.
AU  - Iliev, R.
AU  - Wu, C.
AU  - Aréchiga, N.
PY  - 2024
AB  - Behavior change interventions are important to coordinate societal action across a wide array of important applications, including the adoption of electrified vehicles to reduce emissions. The work of (Harinen et al. 2022) has demonstrated that interventions for behavior must be personalized, and that the intervention that is most effective on average across a large group can result in a backlash effect that strengthens opposition among some subgroups. Thus, it is important to target interventions to different audiences, and to present them in a natural, conversational style. In this context, an important emerging application domain for large language models (LLMs) is conversational interventions for behavior change. In this work, we leverage prior work on understanding values motivating the adoption of battery electric vehicles. We leverage new advances in LLMs, combined with a contextual bandit, to develop conversational interventions that are personalized to the values of each study participant. We use a contextual bandit algorithm to learn to target values based on the demographics of each participant. To train our bandit algorithm in an offline manner, we leverage LLMs to play the role of study participants. We benchmark the persuasive effectiveness of our bandit-enhanced LLM against an unaided LLM generating conversational interventions without demographic-targeted values. We study the ability of LLMs to model attitudes in our target domain by reproducing a recent study on attitudes towards Battery Electric Vehicles (BEVs) (Arechiga et al. 2022), and find that LLMs are biased to prefer environmentally friendly BEVs more strongly than the typical human.
UR  - https://arxiv.org/pdf/2410.23371v1.pdf
N1  - Source: page_1.html
ID  - 131
ER  - 
TY  - JOUR
TI  - Impulse Buying in the Age of Algorithms: A Systematic Literature Review of Psychological Triggers Leading to Buyer's Remorse
AU  - Ofem.
PY  - 2024
AB  - Algorithmic marketing has become a powerful tool in e-commerce, significantly impacting consumer behavior by leveraging psychological triggers that foster impulse buying, often resulting in buyer’s remorse. This systematic literature review examines how algorithms employ scarcity, urgency, social proof, and personalization cues to encourage impulse purchases and explores the ensuing cognitive dissonance and buyer's remorse experienced by consumers. Adhering to PRISMA guidelines, the review assessed studies from databases including Scopus, Web of Science, PsycINFO, IEEE Xplore, and Google Scholar published between 2000 and 2024. Of the 1,200 initial articles, 50 high-quality studies were selected, encompassing methodologies such as experiments, surveys, case studies, and meta-analyses. Findings reveal that algorithm-driven marketing significantly increases impulse purchases and post-purchase regret, highlighting ethical tensions between consumer persuasion and manipulation. Framed by Cognitive Dissonance Theory, Nudge Theory, and Dual-Process Theory, the discussion underscores the ethical responsibility of marketers and the need for regulatory oversight to safeguard consumer autonomy. The study calls for further longitudinal research on the psychological impact of algorithmic marketing and suggests actionable strategies for consumers, ethical guidelines for marketers, and robust regulatory frameworks for policymakers. Conclusions: The findings underscore the profound influence of algorithms on consumer behavior and highlight the urgent need for ethical standards and regulatory policies to address potential manipulative practices in algorithmic marketing. Future studies should focus on the long-term effects of such marketing strategies and explore interventions to mitigate buyer's remorse.
UR  - https://www.researchsquare.com/article/rs-5353669/v1.pdf
N1  - Source: page_1.html
ID  - 132
ER  - 
TY  - JOUR
TI  - Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation
AU  - Xiao, R.
AU  - Wu, L.
AU  - Gou, Y.
AU  - Zhang, W.
AU  - Liu, T.
PY  - 2024
AB  - Argumentative essay generation (AEG) aims to generate complete texts on specific controversial topics or debates. Although current AEG methods can generate individual opinions, they often overlook the high-level connections between these opinions. This often leads to the generated results being mired in logical confusion, unable to proof their own arguments effectively. The generated essay may present evidence that contradicts the claims or they may fail to assemble the claims into logical flow. In this paper, we present a unified two-stage framework: Proof-Enhancement and Self-Annotation (PESA) for AEG with a focus on logical enhancement. Specifically, we first construct pseudo-labels for logical information, claims and grounds, using a large language model. We then propose a tree planning approach that introduces proof principles and ensures logical consistency. Extensive experimental results show that, benefiting from proof principle guidance, PESA generates argumentative essays with better logical validity and persuasiveness than strong baseline models.
UR  - https://arxiv.org/pdf/2410.22642v1.pdf
N1  - Source: page_1.html
ID  - 133
ER  - 
TY  - JOUR
TI  - A Survey on Automatic Credibility Assessment of Textual Credibility Signals in the Era of Large Language Models
AU  - Srba, I.
AU  - Razuvayevskaya, O.
AU  - Leite, J.A.
AU  - Bontcheva, K.
AU  - Bielikova, M.
PY  - 2024
AB  - In the current era of social media and generative AI, an ability to automatically assess the credibility of online social media content is of tremendous importance. Credibility assessment is fundamentally based on aggregating credibility signals, which refer to small units of information, such as content factuality, bias, or a presence of persuasion techniques, into an overall credibility score. Credibility signals provide a more granular, more easily explainable and widely utilizable information in contrast to currently predominant fake news detection, which utilizes various (mostly latent) features. A growing body of research on automatic credibility assessment and detection of credibility signals can be characterized as highly fragmented and lacking mutual interconnections. This issue is even more prominent due to a lack of an up-to-date overview of research works on automatic credibility assessment. In this survey, we provide such systematic and comprehensive literature review of 175 research papers while focusing on textual credibility signals and Natural Language Processing (NLP), which undergoes a significant advancement due to Large Language Models (LLMs). While positioning the NLP research into the context of other multidisciplinary research works, we tackle with approaches for credibility assessment as well as with 9 categories of credibility signals (we provide a thorough analysis for 3 of them, namely: 1) factuality, subjectivity and bias, 2) persuasion techniques and logical fallacies, and 3) claims and veracity). Following the description of the existing methods, datasets and tools, we identify future challenges and opportunities, while paying a specific attention to recent rapid development of generative AI.
UR  - https://arxiv.org/pdf/2410.21360v1.pdf
N1  - Source: page_1.html
ID  - 134
ER  - 
TY  - JOUR
TI  - Teaching Models to Balance Resisting and Accepting Persuasion
AU  - Stengel-Eskin, E.
AU  - Hase, P.
AU  - Bansal, M.
PY  - 2024
AB  - Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. negative) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. positive) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce Persuasion-Balanced Training (or PBT), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion when appropriate. PBT allows us to use data generated from dialogues between smaller 7-8B models for training much larger 70B models. Moreover, PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates across two domains (trivia and commonsense QA). We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model’s performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.
UR  - https://arxiv.org/pdf/2410.14596v2.pdf
N1  - Source: page_1.html
ID  - 135
ER  - 
TY  - JOUR
TI  - A Persuasion-Based Prompt Learning Approach to Improve Smishing Detection through Data Augmentation
AU  - Shim, H.S.
AU  - Park, H.
AU  - Lee, K.
AU  - Park, J.-S.
AU  - Kang, S.
PY  - 2024
AB  - Smishing, which aims to illicitly obtain personal information from unsuspecting victims, holds significance due to its negative impacts on our society. In prior studies, as a tool to counteract smishing, machine learning (ML) has been widely adopted, which filters and blocks smishing messages before they reach potential victims. However, a number of challenges remain in ML-based smishing detection, with the scarcity of annotated datasets being one major hurdle. Specifically, given the sensitive nature of smishing-related data, there is a lack of publicly accessible data that can be used for training and evaluating ML models. Additionally, the nuanced similarities between smishing messages and other types of social engineering attacks such as spam messages exacerbate the challenge of smishing classification with limited resources. To tackle this challenge, we introduce a novel data augmentation method utilizing a few-shot prompt learning approach. What sets our approach apart from extant methods is the use of the principles of persuasion, a psychology theory which explains the underlying mechanisms of smishing. By designing prompts grounded in the persuasion principles, our augmented dataset could effectively capture various, important aspects of smishing messages, enabling ML models to be effectively trained. Our evaluation within a real-world context demonstrates that our augmentation approach produces more diverse and higher-quality smishing data instances compared to other cutting-edging approaches, leading to substantial improvements in the ability of ML models to detect the subtle characteristics of smishing messages. Moreover, our additional analyses reveal that the performance improvement provided by our approach is more pronounced when used with ML models that have a larger number of parameters, demonstrating its effectiveness in training large-scale ML models.
UR  - https://arxiv.org/pdf/2411.02403v2.pdf
N1  - Source: page_1.html
ID  - 136
ER  - 
TY  - JOUR
TI  - Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models
AU  - Yeo, W.J.
AU  - Satapathy, R.
AU  - Cambria, E.
PY  - 2024
AB  - Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment-tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model’s internal computations and avoiding out-of-distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in https://github.com/wj210/Causal-Faithfulness
UR  - https://arxiv.org/pdf/2410.14155v2.pdf
N1  - Source: page_1.html
ID  - 137
ER  - 
TY  - JOUR
TI  - Coherence-Driven Multimodal Safety Dialogue with Active Learning for Embodied Agents
AU  - Hassan, S.
AU  - Chung, H.-Y.
AU  - Tan, X.Z.
AU  - Alikhani, M.
PY  - 2024
AB  - When assisting people in daily tasks, robots need to accurately interpret visual cues and respond effectively in diverse safety-critical situations, such as sharp objects on the floor. In this context, we present M-CoDAL, a multimodal-dialogue system specifically designed for embodied agents to better understand and communicate in safety-critical situations. The system leverages discourse coherence relations to enhance its contextual understanding and communication abilities. To train this system, we introduce a novel clustering-based active learning mechanism that utilizes an external Large Language Model (LLM) to identify informative instances. Our approach is evaluated using a newly created multimodal dataset comprising 1K safety violations extracted from 2K Reddit images. These violations are annotated using a Large Multimodal Model (LMM) and verified by human annotators. Results with this dataset demonstrate that our approach improves resolution of safety situations, user sentiment, as well as safety of the conversation. Next, we deploy our dialogue system on a Hello Robot Stretch robot and conduct a within-subject user study with real-world participants. In the study, participants role-play two safety scenarios with different levels of severity with the robot and receive interventions from our model and a baseline system powered by OpenAI’s ChatGPT. The study results corroborate and extend the findings from the automated evaluation, showing that our proposed system is more persuasive in a real-world embodied agent setting.
UR  - https://arxiv.org/pdf/2410.14141v2.pdf
N1  - Source: page_1.html
ID  - 138
ER  - 
TY  - JOUR
TI  - Can LLMs be Scammed? A Baseline Measurement Study
AU  - Sehwag, U.M.
AU  - Patel, K.
AU  - Mosca, F.
AU  - Ravi, V.
AU  - Staddon, J.
PY  - 2024
AB  - Despite the importance of developing generative AI models that can effectively resist scams, current literature lacks a structured framework for evaluating their vulnerability to such threats. In this work, we address this gap by constructing a benchmark based on the FINRA taxonomy and systematically assessing Large Language Models’ (LLMs’) vulnerability to a variety of scam tactics. First, we incorporate 37 well-defined base scam scenarios reflecting the diverse scam categories identified by FINRA taxonomy, providing a focused evaluation of LLMs’ scam detection capabilities. Second, we utilize representative proprietary (GPT-3.5, GPT-4) and open-source (Llama) models to analyze their performance in scam detection. Third, our research provides critical insights into which scam tactics are most effective against LLMs and how varying persona traits and persuasive techniques influence these vulnerabilities. We reveal distinct susceptibility patterns across different models and scenarios, underscoring the need for targeted enhancements in LLM design and deployment.
UR  - https://arxiv.org/pdf/2410.13893v1.pdf
N1  - Source: page_1.html
ID  - 139
ER  - 
TY  - JOUR
TI  - I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy
AU  - Campedelli, G.M.
AU  - Penzo, N.
AU  - Stefan, M.
AU  - Lepri, B.
AU  - Staiano, J.
PY  - 2024
AB  - As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play. Then, for the models that were able to engage in successful interactions, we empirically show how the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent’s anti-social behavior. Third, we highlight how agents’ personas, and particularly the guard’s personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents’ roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.
UR  - https://arxiv.org/pdf/2410.07109v1.pdf
N1  - Source: page_1.html
ID  - 140
ER  - 
TY  - JOUR
TI  - Constraint representation towards precise data-driven storytelling
AU  - Shi, Y.-Z.
AU  - Li, H.
AU  - Ruan, L.
AU  - Qu, H.
PY  - 2024
AB  - Data-driven storytelling serves as a crucial bridge for communicating ideas in a persuasive way. However, the manual creation of data stories is a multifaceted, labor-intensive, and case-specific effort, limiting their broader application. As a result, automating the creation of data stories has emerged as a significant research thrust. Despite advances in Artificial Intelligence, the systematic generation of data stories remains challenging due to their hybrid nature: they must frame a perspective based on a seed idea in a top-down manner, similar to traditional storytelling, while coherently grounding insights of given evidence in a bottom-up fashion, akin to data analysis. These dual requirements necessitate precise constraints on the permissible space of a data story. In this viewpoint, we propose integrating constraints into the data story generation process. Defined upon the hierarchies of interpretation and articulation, constraints shape both narrations and illustrations to align with seed ideas and contextualized evidence. We identify the taxonomy and required functionalities of these constraints. Although constraints can be heterogeneous and latent, we explore the potential to represent them in a computation-friendly fashion via Domain-Specific Languages. We believe that leveraging constraints will facilitate both artistic and scientific aspects of data story generation.
UR  - https://arxiv.org/pdf/2410.07535v1.pdf
N1  - Source: page_1.html
ID  - 141
ER  - 
TY  - JOUR
TI  - MEASURING AND IMPROVING PERSUASIVENESS OF LARGE LANGUAGE MODELS
AU  - Singh, S.
AU  - Singla, Y.K.
AU  - Harini, S.I.
AU  - Krishnamurthy, B.
PY  - 2024
AB  - Large Language Models (LLMs) are increasingly being used in workflows involving generating content to be consumed by humans (e.g., marketing) and also in directly interacting with humans (e.g., through chatbots). The development of such systems that are capable of generating verifiably persuasive messages presents both opportunities and challenges for society. On the one hand, such systems could positively impact domains like advertising and social good, such as addressing drug addiction, and on the other, they could be misused for spreading misinformation and shaping political opinions. To channel LLMs' impact on society, we need to develop systems to measure and benchmark their persuasiveness. With this motivation, we introduce PersuasionBench and Persuasion- Arena, the first large-scale benchmark and arena containing a battery of tasks to measure the persuasion ability of large language models automatically. We introduce transsuasion (trans = carrying across, suasion = the act of persuading), a novel task of transforming non-persuasive language into persuasive content while preserving other factors determining persuasiveness (sender, receiver, time, and channel). To construct data for transsuasion, we leverage natural experiments in the form of a pair of tweets from the same user, posted in close temporal proximity, with similar semantic content but divergent wording and significantly different like counts. Given such pairs, we investigate to what extent LLMs know and leverage linguistic patterns that can help them generate more persuasive language. Our findings indicate that the persuasiveness of LLMs correlates positively with model size, but smaller models can also be made to have a higher persuasiveness than much larger models. Notably, targeted training using synthetic and natural datasets significantly enhances smaller models' persuasive capabilities, challenging scale-dependent assumptions. Our findings carry key implications for both model developers and policymakers. For instance, while the EU AI Act and California's SB-1047 aim to regulate AI models based on the number of floating point operations, we demonstrate that simple metrics like this alone fail to capture the full scope of AI's societal impact. We invite the community to explore and contribute to PersuasionArena and PersuasionBench, available at https://behavior-in-the-wild.github.io/measure-persuasion, to advance our understanding of AI-driven persuasion and its societal implications.
UR  - https://arxiv.org/pdf/2410.02653v2.pdf
N1  - Source: page_1.html
ID  - 142
ER  - 
TY  - JOUR
TI  - Automated Token-Level Detection of Persuasive and Misleading Words in Text Using Large Language Models
AU  - Nijodo, D.
AU  - Schmidt, D.
AU  - Costa, S.
AU  - Martins, A.
AU  - Johnson, N.
PY  - 2024
AB  - Persuasive and misleading language has long been a powerful tool in shaping public opinion, guiding consumer behavior, and influencing political discourse. The complexity of detecting subtle rhetorical strategies, particularly at the token level, presents a significant challenge for traditional methods of text analysis. The novel approach developed in this study leverages token-level processing within transformer-based models to classify words based on their persuasive and misleading potential, providing a granular perspective on language manipulation. Through comprehensive experiments, the analysis demonstrated that tokens linked to sentiment polarity, lexical complexity, and positional importance play key roles in shaping the rhetorical impact of texts. This method provides an efficient and scalable solution for automated content moderation, political discourse analysis, and advertising regulation, with applications extending to media analysis and misinformation detection. The integration of attention mechanisms and contextual embeddings offers a detailed view into how language functions at a deeper structural level, positioning this framework as a significant advancement in automated text analysis.
UR  - https://www.researchsquare.com/article/rs-5174770/v1.pdf
N1  - Source: page_1.html
ID  - 143
ER  - 
TY  - JOUR
TI  - Human Bias in the Face of AI: The Role of Human Judgement in AI Generated Text Evaluation
AU  - Zhu, T.
AU  - Weissburg, I.
AU  - Zhang, K.
AU  - Wang, W.Y.
PY  - 2024
AB  - As AI advances in text generation, human trust in AI generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as "Human Generated," over those labeled "AI Generated," by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.
UR  - https://arxiv.org/pdf/2410.03723v1.pdf
N1  - Source: page_1.html
ID  - 144
ER  - 
TY  - JOUR
TI  - Uncovering Differences in Persuasive Language in Russian versus English Wikipedia
AU  - Li, B.
AU  - Panasyuk, A.
AU  - Callison-Burch, C.
PY  - 2024
AB  - We study how differences in persuasive language across Wikipedia articles, written in either English and Russian, can uncover each culture’s distinct perspective on different subjects. We develop a large language model (LLM) powered system to identify instances of persuasive language in multilingual texts. Instead of directly prompting LLMs to detect persuasion, which is subjective and difficult, we propose to reframe the task to instead ask high-level questions (HLQs) which capture different persuasive aspects. Importantly, these HLQs are authored by LLMs themselves. LLMs over-generate a large set of HLQs, which are subsequently filtered to a small set aligned with human labels for the original task. We then apply our approach to a large-scale, bilingual dataset of Wikipedia articles (88K total), using a two-stage identify-then-extract prompting strategy to find instances of persuasion. We quantify the amount of persuasion per article, and explore the differences in persuasion through several experiments on the paired articles. Notably, we generate rankings of articles by persuasion in both languages. These rankings match our intuitions on the culturally-salient subjects; Russian Wikipedia highlights subjects on Ukraine, while English Wikipedia highlights the Middle East. Grouping subjects into larger topics, we find politically-related events contain more persuasion than others. We further demonstrate that HLQs obtain similar performance when posed in either English or Russian. Our methodology enables cross-lingual, cross-cultural understanding at scale, and we release our code, prompts, and data.1
UR  - https://arxiv.org/pdf/2409.19148v1.pdf
N1  - Source: page_1.html
ID  - 145
ER  - 
TY  - JOUR
TI  - Analysis of Truncated Singular Value Decomposition for Koopman Operator-Based Lane Change Model
AU  - Nantabut, C.
PY  - 2024
AB  - Understanding and modeling complex dynamic systems is crucial for enhancing vehicle performance and safety, especially in the context of autonomous driving. Recently, popular methods such as Koopman operators and their approximators, known as Extended Dynamic Mode Decomposition (EDMD), have emerged for their effectiveness in transforming strongly nonlinear system behavior into linear representations. This allows them to be integrated with conventional linear controllers. To achieve this, Singular Value Decomposition (SVD), specifically truncated SVD, is employed to approximate Koopman operators from extensive datasets efficiently. This study evaluates different basis functions used in EDMD and ranks for truncated SVD for representing lane change behavior models, aiming to balance computational efficiency with information loss. The findings, however, suggest that the technique of truncated SVD does not necessarily achieve substantial reductions in computational training time and results in significant information loss.
UR  - https://arxiv.org/pdf/2409.18586v1.pdf
N1  - Source: page_1.html
ID  - 146
ER  - 
TY  - JOUR
TI  - Beyond Turing Test: Can GPT-4 Sway Experts’ Decisions?
AU  - Takayanagi, T.
AU  - Takamura, H.
AU  - Izumi, K.
AU  - Chen, C.-C.
PY  - 2024
AB  - In the post-Turing era, evaluating large language models (LLMs) involves assessing generated text based on readers’ reactions rather than merely its indistinguishability from human-produced content. This paper explores how LLM-generated text impacts readers’ decisions, focusing on both amateur and expert audiences. Our findings indicate that GPT-4 can generate persuasive analyses affecting the decisions of both amateurs and professionals. Furthermore, we evaluate the generated text from the aspects of grammar, convincingness, logical coherence, and usefulness. The results highlight a high correlation between real-world evaluation through audience reactions and the current multi-dimensional evaluators commonly used for generative models. Overall, this paper shows the potential and risk of using generated text to sway human decisions and also points out a new direction for evaluating generated text, i.e., leveraging the reactions and decisions of readers. We release our dataset to assist future research.
UR  - https://arxiv.org/pdf/2409.16710v2.pdf
N1  - Source: page_1.html
ID  - 147
ER  - 
TY  - JOUR
TI  - Generative Ai and the Future of Marketing: A Consumer Protection Perspective
AU  - Duivenvoorde, B.
PY  - 2024
AB  - Generative AI has the potential to be the biggest disruption in marketing since the emergence of digital commerce in the early 2000s. This article will focus on three ways in which generative AI is expected to change marketing. First, generative AI enables companies to automatically create advertising copy and images, potentially leading to significant cost reductions. Secondly, generative AI offers possibilities to improve and automate personalised marketing, enabling companies to send the right persuasive message at the right time to each potential customer. Thirdly, generative AI potentially offers possibilities to market products to consumers via generative AI chatbots. These developments offer potential advantages but also bear risks for consumers. For example, deepfakes in advertising can mislead consumers, AI-generated personalised marketing can exploit consumer vulnerabilities, and B2C chatbots can deceive consumers by providing biased advice. This article shows that EU law does in principle provide protection to consumers in relation to AI-generated marketing, but is also likely to fall short in effectively protecting consumers against the identified risks in several ways.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4967461&type=2
N1  - Source: page_1.html
ID  - 148
ER  - 
TY  - JOUR
TI  - LLM Echo Chamber: personalized and automated disinformation
AU  - Ma, W.
PY  - 2024
AB  - Recent advancements have significantly highlighted the capabilities of Large Language Models (LLMs) such as GPT-4 and Llama2 in performing diverse tasks including text summarization, translation, and content review. Despite their evident benefits, the implications of their widespread application warrant careful consideration. It has been underscored that the potential for these models to disseminate misinformation exists, which poses challenges in dealing with this more persuasive, faster-generated, human-liked misinformation. This concern is exacerbated by the capacity of LLMs to influence public opinion because of their wide usage. This study aims to critically examine the risks associated with LLMs, particularly their ability to disseminate misinformation on specific topics as factual persuasively. To this end, we built the “LLM Echo Chamber”, a controlled digital environment designed to mimic the dynamics of social media platforms, specifically chatrooms, where misinformation often proliferates. The phenomenon of echo chambers is well-known - only interacting with those of the same opinions further reinforces a person’s beliefs and causes them to discard other viewpoints. The “LLM Echo Chamber” could help us study the effect of multiple malicious misinformation spreading bots in a chatroom, a common scenario for the internet Echo Chamber phenomenon. We first did a review of existing LLMs and their associated risks, LLM’s ability to spread misinformation, an exploration of state-of-the-art (SOTA) techniques for model finetuning, and some advanced methods for constructing interactive chatrooms. The model selection was based on the model’s performance, considerations of computing resources, and the level of safeguards. With Microsoft’s phi-2 model finetuned on the identity-shifting dataset we created, we could let the model generate harmful content. Subsequently, we developed a “LLM Echo Chamber” leveraging our finetuned model, frontend tools, and context-aware backend tools, employing specific prompt engineering and interactive logic to enhance the chatroom’s credibility. The efficacy of the chatroom was evaluated by automated evaluation based on GPT-4, which could provide us a comprehensive overview of the persuasiveness and harmfulness of our “LLM Echo Chamber”. Our findings contribute to the broader discourse on the ethical implications of LLMs and highlight the necessity for robust mechanisms to mitigate the potential dissemination of misinformation.
UR  - https://arxiv.org/pdf/2409.16241v1.pdf
N1  - Source: page_1.html
ID  - 149
ER  - 
TY  - JOUR
TI  - Digital phenotyping from wearables using AI characterizes psychiatric disorders and identifies genetic associations
AU  - Liu, J.
AU  - Borsari, B.
AU  - Li, Y.
AU  - Roberts, W.
AU  - Gerstein, M.
PY  - 2024
AB  - Psychiatric disorders are complex and influenced by both genetic and environmental factors. However, studying the full spectrum of these disorders is hindered by practical limitations on measuring human behavior. This highlights the need for novel technologies that can measure behavioral changes at an intermediate level between diagnosis and genotype. Wearable devices are a promising tool in precision medicine, since they can record physiological measurements over time in response to environmental stimuli and do so at low cost and minimal invasiveness. Here we analyzed wearable and genetic data from a cohort of the Adolescent Brain Cognitive Development study. We generated >250 wearable-derived features and used them as intermediate phenotypes in an interpretable AI modeling framework to assign risk scores and classify adolescents with psychiatric disorders. Our model identifies key physiological processes and leverages their temporal patterns to achieve a higher performance than has been previously possible. To investigate how these physiological processes relate to the underlying genetic architecture of psychiatric disorders, we also utilized these intermediate phenotypes in univariate and multivariate GWAS. We identified a total of 29 significant genetic loci and 52 psychiatric-associated genes, including ELFN1 and ADORA3. These results show that wearable-derived continuous features enable a more precise representation of psychiatric disorders and exhibit greater detection power compared to categorical diagnostic labels. In summary, we demonstrate how consumer wearable technology can facilitate dimensional approaches in precision psychiatry and uncover etiological linkages between behavior and genetics.
UR  - https://www.medrxiv.org/content/10.1101/2024.09.23.24314219v2.full.pdf
N1  - Source: page_1.html
ID  - 150
ER  - 
TY  - JOUR
TI  - Creating Healthy Friction: Determining Stakeholder Requirements of Job Recommendation Explanations
AU  - Schellingerhout, R.
AU  - Barile, F.
AU  - Tintarev, N.
PY  - 2024
AB  - The increased use of information retrieval in recruitment, primarily through job recommender systems (JRSs), can have a large impact on job seekers, recruiters, and companies. As a result, such systems have been determined to be high-risk in recent legislature. This requires JRSs to be trustworthy and transparent, allowing stakeholders to understand why specific recommendations were made. To fulfill this requirement, the stakeholders' exact preferences and needs need to be determined. To do so, we evaluated an explainable job recommender system using a realistic, task-based, mixed-design user study (n = 30) in which stakeholders had to make decisions based on the model's explanations. This mixed-methods evaluation consisted of two objective metrics - correctness and efficiency, along with three subjective metrics - trust, transparency, and usefulness. These metrics were evaluated twice per participant, once using real explanations and once using random explanations. The study included a qualitative analysis following a think-aloud protocol while performing tasks adapted to each stakeholder group. We find that providing stakeholders with real explanations does not significantly improve decision-making speed and accuracy. Our results showed a non-significant trend for the real explanations to outperform the random ones on perceived trust, usefulness, and transparency of the system for all stakeholder types. We determine that stakeholders benefit more from interacting with explanations as decision support capable of providing healthy friction, rather than as previously-assumed persuasive tools.
UR  - https://arxiv.org/pdf/2409.15971v1.pdf
N1  - Source: page_1.html
ID  - 151
ER  - 
TY  - JOUR
TI  - Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations
AU  - Qin, P.
AU  - Huang, C.
AU  - Deng, Y.
AU  - Lei, W.
AU  - Chua, T.-S.
PY  - 2024
AB  - With the aid of large language models, current conversational recommender system (CRS) has gaining strong abilities to persuade users to accept recommended items. While these CRSs are highly persuasive, they can mislead users by incorporating incredible information in their explanations, ultimately damaging the long-term trust between users and the CRS. To address this, we propose a simple yet effective method, called PC-CRS, to enhance the credibility of CRS’s explanations during persuasion. It guides the explanation generation through our proposed credibility-aware persuasive strategies and then gradually refines explanations via post-hoc self-reflection. Experimental results demonstrate the efficacy of PC-CRS in promoting persuasive and credible explanations. Further analysis reveals the reason behind current methods producing incredible explanations and the potential of credible explanations to improve recommendation accuracy.
UR  - https://arxiv.org/pdf/2409.14399v2.pdf
N1  - Source: page_1.html
ID  - 152
ER  - 
TY  - JOUR
TI  - Finetuning Language Models to Emit Linguistic Expressions of Uncertainty
AU  - Chaudhry, A.
AU  - Thiagarajan, S.
AU  - Gorur, D.
PY  - 2024
AB  - Large language models (LLMs) are increasingly employed in information-seeking and decision-making tasks. Despite their broad utility, LLMs tend to generate information that conflict with real-world facts, and their persuasive style can make these inaccuracies appear confident and convincing. As a result, end-users struggle to consistently align the confidence expressed by LLMs with the accuracy of their predictions, often leading to either blind trust in all outputs or a complete disregard for their reliability. In this work, we explore supervised fine-tuning on uncertainty-augmented predictions as a method to develop models that produce linguistic expressions of uncertainty. Specifically, we measure the calibration of pre-trained models and fine-tune language models to generate calibrated linguistic expressions of uncertainty. Through experiments on various question-answering datasets, we demonstrate that LLMs are well-calibrated in assessing their predictions, and supervised fine-tuning based on the model’s own confidence leads to well-calibrated expressions of uncertainty, particularly for single-claim answers.
UR  - https://arxiv.org/pdf/2409.12180v1.pdf
N1  - Source: page_1.html
ID  - 153
ER  - 
TY  - JOUR
TI  - Benchmarking VLMs’ Reasoning About Persuasive Atypical Images
AU  - Malakouti, S.
AU  - Aghazadeh, A.
AU  - Khandelwal, A.
AU  - Kovashka, A.
PY  - 2024
AB  - Vision-language models (VLMs) have shown strong zero-shot generalization across various tasks, especially when integrated with large language models (LLMs). However, their ability to comprehend rhetorical and persuasive visual media, such as advertisements, remains understudied. Ads often employ atypical imagery, using surprising object juxtapositions to convey shared properties. For example, Fig. 1 (e) shows a beer with a feather-like texture. This requires advanced reasoning to deduce that this atypical representation signifies the beer’s lightness. We introduce three novel tasks, Multi-label Atypicality Classification, Atypicality Statement Retrieval, and Atypical Object Recognition, to benchmark VLMs’ understanding of atypicality in persuasive images. We evaluate how well VLMs use atypicality to infer an ad’s message and test their reasoning abilities by employing semantically challenging negatives. Finally, we pioneer atypicality-aware verbalization by extracting comprehensive image descriptions sensitive to atypical elements. Findings reveal that: (1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple, effective strategies can extract atypicality-aware information, leading to comprehensive image verbalization; (3) atypicality aids persuasive ad understanding. Code and data is available at aysanaghazadeh.github.io/PersuasiveAdVLMBenchmark/
UR  - https://arxiv.org/pdf/2409.10719v3.pdf
N1  - Source: page_1.html
ID  - 154
ER  - 
TY  - JOUR
TI  - SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs
AU  - Qian, W.
AU  - Gao, C.
AU  - Sathya, A.
AU  - Suzuki, R.
AU  - Nakagaki, K.
PY  - 2024
AB  - This paper introduces text-to-shape-display, a novel approach to generating dynamic shape changes in pin-based shape displays through natural language commands. By leveraging large language models (LLMs) and AI-chaining, our approach allows users to author shape-changing behaviors on demand through text prompts without programming. We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes. Based on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a 24 x 24 shape display, which translates the user's textual command into executable code and allows for quick exploration through a web-based control interface. We evaluate the effectiveness of SHAPE-IT in two ways: 1) performance evaluation and 2) user evaluation (N= 10). The study conclusions highlight the ability to facilitate rapid ideation of a wide range of shape-changing behaviors with AI. However, the findings also expose accuracy-related challenges and limitations, prompting further exploration into refining the framework for leveraging AI to better suit the unique requirements of shape-changing systems.
UR  - https://arxiv.org/pdf/2409.06205v1.pdf
N1  - Source: page_1.html
ID  - 155
ER  - 
TY  - JOUR
TI  - Prompt Baking
AU  - Bhargava, A.
AU  - Witkowski, C.
AU  - Detkov, A.
AU  - Thomson, M.
PY  - 2024
AB  - Two primary ways to change LLM behavior are prompting and weight updates (e.g. fine-tuning). Prompting LLMs is simple and effective, specifying the desired changes explicitly in natural language, whereas weight updates provide more expressive and permanent behavior changes, specified implicitly via training on large datasets. We present a technique for “baking” prompts into the weights of an LLM. Prompt Baking converts a prompt u and initial weights θ to a new set of weights θu such that the LLM with weights θu behaves like the LLM with weights θ and prompt u. Mathematically, we minimize the KL divergence between Pθ(·|u) and Pθu(·), where P is the LLM’s probability distribution over token sequences. Across all our experiments, we find prompts can be readily baked into weight updates, often in as little as 5 minutes. Baking chain-of-thought prompts improves zero-shot performance on GSM8K, ASDiv, MBPP, ARC-Easy, ARC-Challenge, and CommonsenseQA benchmarks. Baking news headlines directly updates an LLM’s knowledge. And baking instructions & personas alleviates “prompt forgetting” over long sequences, as measured on a Persona Drift benchmark. Furthermore, stopping baking early creates “half-baked” models, allowing for continuous scaling of prompt strength. Baked models retain their sensitivity to further prompting and baking, including re-prompting with the prompt already baked in – thus amplifying the prompt’s strength. Surprisingly, the re-prompted models yield further performance gains in instruction following, as well as math reasoning and coding benchmarks (GSM8K, ASDiv, and MBPP). Taking re-prompting and re-baking to the limit yields a form of iterative self-improvement we call Prompt Pursuit, and preliminary results on instruction following exhibit dramatic performance gains with this technique. Finally, we discuss implications for AI safety, continuous model updating, improving LLM recency, enhancing real-time learning capabilities in LLM-based agents, and methods for generating more stable AI personas.
UR  - https://arxiv.org/pdf/2409.13697v1.pdf
N1  - Source: page_1.html
ID  - 156
ER  - 
TY  - JOUR
TI  - Stakeholder Perspectives on Factors Influencing the Economic Value of Digital Therapeutics: A Group Concept Mapping Approach
AU  - Sapanel, Y.
AU  - Cloutier, L.M.
AU  - Tremblay, G.
AU  - Tadeo, X.
AU  - Ho, D.
PY  - 2024
AB  - Background: Digital therapeutics (DTx), software as a medical device, present a promising avenue for addressing the increasing burden of a range of conditions. In a context of budgetary constraints and an enduring need for optimal resource allocation, their implementation and adoption are contingent upon their capacity to provide economic value, which remains a research gap.Methods: Group concept mapping, a participatory and mixed method was used to delve into a comprehensive identification and understanding of the factors that are conceptualized and perceived to impact the economic value of DTx. It involved semi-structured interviews, group discussions and web-based tasks with healthcare professionals, researchers, industry and public sector representatives. Participants sorted factors by similarity (n=37), rated their importance in generating economic value and consideration in practice (n=62). Multivariate methods included nonmetric multidimensional scaling analysis and agglomerative hierarchical cluster analysis to estimate conceptual data into visual representations.Findings: 59 factors influencing the economic value of DTx were identified and grouped into eight clusters. There was broad agreement among stakeholders on the significance of the DTx Impact on Patient Outcomes and DTx Implementation as the most influential clusters affecting the economic value of DTx. The DTx Associated Costs and DTx Monetization Models clusters were reported as not receiving potentially sufficient consideration throughout the DTx development lifecycle, particularly among researchers. Consequently, a conceptual framework of priority clusters and factors potentially driving DTx economic value is proposed.Interpretation: The results illustrate how stakeholders conceptualize, perceive, and consider factors impacting DTx economic value throughout its lifecycle. They reveal the complex interplay of multiple stakeholders and factors influencing DTx economic value, highlighting areas requiring prioritization at each stage.Funding: None.Declaration of Interest: YS, XT and DH have filed a provisional patent on data-driven behavior change. DH is coinventor of previously filed pending patents on artificial intelligence–based therapy development. GT is an employee of Cytel Canada Health Inc, Canada. AB is the founder of HAS4P, Health Access Solutions, United Arab Emirates. FK is an employee of IU Internationale Hochschule GmbH, Germany, and Flying Health GmbH, Germany. DL is an employee of PricewaterhouseCoopers Luxembourg. LMC has no conflict of interest.Ethical Approval: The study received approval from the Ethics Boards of the National University of Singapore (certification number: NUS-IRB-2023-399) and the University of Quebec in Montreal (certification number: 2024-6028). All participants provided written informed consent for their involvement in face-to-face sessions, focus groups, or web-based task.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4938143&type=2
N1  - Source: page_1.html
ID  - 157
ER  - 
TY  - JOUR
TI  - User-Driven Value Alignment: Understanding Users’ Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions
AU  - Fan, X.
AU  - Xiao, Q.
AU  - Zhou, X.
AU  - Lu, Z.
AU  - Shen, H.
PY  - 2024
AB  - Large language model-based AI companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds. However, they can generate biased, discriminatory, and harmful outputs. Recently, users are taking the initiative to address these harms and re-align AI companions. We introduce the concept of user-driven value alignment, where users actively identify, challenge, and attempt to correct AI outputs they perceive as harmful, aiming to guide the AI to better align with their values. We analyzed 77 social media posts about discriminatory AI statements and conducted semi-structured interviews with 20 experienced users. Our analysis revealed six common types of discriminatory statements perceived by users, how users make sense of those AI behaviors, and seven user-driven alignment strategies, such as gentle persuasion and anger expression. We discuss implications for supporting user-driven value alignment in future AI systems, where users and their communities have greater agency.
UR  - https://arxiv.org/pdf/2409.00862v2.pdf
N1  - Source: page_1.html
ID  - 158
ER  - 
TY  - JOUR
TI  - Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies
AU  - Qi, Z.
AU  - Inaba, M.
PY  - 2024
AB  - Recent advancements in natural language processing, particularly with large language models (LLMs) like GPT-4, have significantly enhanced dialogue systems, enabling them to generate more natural and fluent conversations. Despite these improvements, challenges persist, such as managing continuous dialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024 addresses these challenges by employing the Werewolf Game, an incomplete information game, to test the capabilities of LLMs in complex interactive environments. This paper introduces a LLM-based Werewolf Game AI, where each role is supported by situation analysis to aid response generation. Additionally, for the werewolf role, various persuasion strategies, including logical appeal, credibility appeal, and emotional appeal, are employed to effectively persuade other players to align with its actions.
UR  - https://arxiv.org/pdf/2408.16586v2.pdf
N1  - Source: page_1.html
ID  - 159
ER  - 
TY  - JOUR
TI  - Persuasion Games using Large Language Models
AU  - Ramani, G.P.
AU  - Karande, S.
AU  - Santhosh, V.
AU  - Bhatia, Y.
PY  - 2024
AB  - Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape user perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS). We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with user agents through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We continuously analyze the resistance of the user agent to persuasive efforts and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques. We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).
UR  - https://arxiv.org/pdf/2408.15879v2.pdf
N1  - Source: page_1.html
ID  - 160
ER  - 
TY  - JOUR
TI  - Advanced POD-Based Performance Evaluation of Classifiers Applied to Human Driver Lane Changing Prediction
AU  - Rastin, Z.
AU  - Söffker, D.
PY  - 2024
AB  - Machine learning (ML) classifiers serve as essential tools facilitating classification and prediction across various domains. The performance of these algorithms should be known to ensure their reliable application. In certain fields, receiver operating characteristic and precision-recall curves are frequently employed to assess machine learning algorithms without accounting for the impact of process parameters. However, it may be essential to evaluate the performance of these algorithms in relation to such parameters. As a performance evaluation metric capable of considering the effects of process parameters, this paper uses a modified probability of detection (POD) approach to assess the reliability of ML-based algorithms. As an example, the POD-based approach is employed to assess ML models used for predicting the lane changing behavior of a vehicle driver. The time remaining to the predicted (and therefore unknown) lane changing event is considered as process parameter. The hit/miss approach to POD is taken here and modified by considering the probability of lane changing derived from ML algorithms at each time step, and obtaining the final result of the analysis accordingly. This improves the reliability of results compared to the standard hit/miss approach, which considers the outcome of the classifiers as either 0 or 1, while also simplifying evaluation compared to the â versus a approach. Performance evaluation results of the proposed approach are compared with those obtained with the standard hit/miss approach and a pre-developed â versus a approach to validate the effectiveness of the proposed method. The comparison shows that this method provides an averaging conservative behavior with the advantage of enhancing the reliability of the hit/miss approach to POD while retaining its simplicity.
UR  - https://arxiv.org/pdf/2408.15722v1.pdf
N1  - Source: page_1.html
ID  - 161
ER  - 
TY  - JOUR
TI  - Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas
AU  - Kwok, L.
AU  - Bravansky, M.
AU  - Griffin, L.D.
PY  - 2024
AB  - The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users’ diverse cultural backgrounds. We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits. Our analysis shows that specifying a person’s country of residence improves GPT-3.5’s alignment with their responses. In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. These findings suggest that while direct nationality information enhances the model’s cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model’s effectiveness.
UR  - https://arxiv.org/pdf/2408.06929v1.pdf
N1  - Source: page_1.html
ID  - 162
ER  - 
TY  - JOUR
TI  - Llm-Based Conversational Agents for Behaviour Change Support: A Randomized Controlled Trial Examining Efficacy, Safety, and the Role of User Behaviour
AU  - Meyer, S.
AU  - Elsweiler, D.
PY  - 2024
AB  - This study examines the use of Motivational Interviewing (MI) principles in a GPT-4-based chatbot, MIcha, to promote behaviour change. We conducted a pre-registered randomised controlled trial to assess the integration of MI techniques in conversational agents, aiming to support users' behaviour change through guided self-reflection and identify how users interact with large language model (LLM)-based systems in this context. Results indicate that short conversations with LLM-based chatbots are successful at increasing users' readiness to change and usage of MI principles during text generation can effectively mitigate potential harms. Additionally, we identified distinct user behaviour types—cooperative, reflective, and pre-informed—that significantly influenced the outcomes of interactions. These findings demonstrate the potential of MI principles in enhancing the efficacy of conversational agents for behaviour change and highlight the importance of user behaviour in shaping interaction dynamics.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4917769&type=2
N1  - Source: page_1.html
ID  - 163
ER  - 
TY  - JOUR
TI  - Early Detection of Mental Health Crises through AI-Powered Social Media Analysis: A Prospective Observational Study
AU  - Mansoor, M.A.
AU  - Ansari, K.
PY  - 2024
AB  - Background: Early detection of mental health crises is crucial for timely intervention and improved outcomes. This study explores the potential of artificial intelligence (AI) in analyzing social media data to identify early signs of mental health crises. Methods: We developed a multi-modal deep learning model integrating natural language processing and temporal analysis techniques. The model was trained on a diverse dataset of 996,452 social media posts in multiple languages (English, Spanish, Mandarin, and Arabic) collected from Twitter, Reddit, and Facebook over a 12-month period. Performance was evaluated using standard metrics and validated against expert psychiatric assessment. Results: The AI model demonstrated high accuracy (89.3%) in detecting early signs of mental health crises, with an average lead time of 7.2 days before human expert identification. Performance was consistent across languages (F1 scores: 0.827-0.872) and platforms (F1 scores: 0.839-0.863). Key digital markers included linguistic patterns, behavioral changes, and temporal trends. The model showed varying accuracy for different crisis types: depressive episodes (91.2%), manic episodes (88.7%), suicidal ideation (93.5%), and anxiety crises (87.3%). Conclusions: AI-powered analysis of social media data shows promise for early detection of mental health crises across diverse linguistic and cultural contexts. However, ethical challenges including privacy concerns, potential stigmatization, and cultural biases need careful consideration. Future research should focus on longitudinal outcome studies, ethical integration with existing mental health services, and development of personalized, culturally-sensitive models.
UR  - https://www.medrxiv.org/content/10.1101/2024.08.12.24311872v1.full.pdf
N1  - Source: page_1.html
ID  - 164
ER  - 
TY  - JOUR
TI  - Sleeper Social Bots: A New Generation of AI Disinformation Bots are Already a Political Threat
AU  - Doshi, J.
AU  - Novacic, I.
AU  - Fletcher, C.
AU  - Sprague, D.
AU  - Xia, M.
PY  - 2024
AB  - This paper presents a study on the growing threat of “sleeper social bots,” AI-driven social bots in the political landscape, created to spread disinformation and manipulate public opinion. We based the name sleeper social bots on their ability to pass as humans on social platforms, where they’re embedded like political “sleeper” agents, making them harder to detect and more disruptive. To illustrate the threat these bots pose, our research team at the University of Southern California constructed a demonstration using a private Mastodon server, where ChatGPT-driven bots, programmed with distinct personalities and political viewpoints, engaged in discussions with human participants about a fictional electoral proposition. Our preliminary findings suggest these bots can convincingly pass as human users, actively participate in conversations, and effectively disseminate disinformation. Moreover, they can adapt their arguments based on the responses of human interlocutors, showcasing their dynamic and persuasive capabilities. College students participating in initial experiments failed to identify our bots, underscoring the urgent need for increased awareness and education about the dangers of AI-driven disinformation, and in particular, disinformation spread by bots. The implications of our research point to the significant challenges posed by social bots in the upcoming 2024 U.S. presidential election and beyond.
UR  - https://arxiv.org/pdf/2408.12603v1.pdf
N1  - Source: page_1.html
ID  - 165
ER  - 
TY  - JOUR
TI  - Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments
AU  - Boggust, A.
AU  - Sivaraman, V.
AU  - Assogba, Y.
AU  - Moritz, D.
AU  - Hohman, F.
PY  - 2024
AB  - To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called COMPRESS AND COMPARE. Within a single interface, COMPRESS AND COMPARE surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models’ predictions, weights, and activations. We demonstrate how COMPRESS AND COMPARE supports common compression analysis tasks through two case studies, debugging failed compression on generative language models and identifying compression artifacts in image classification models. We further evaluate COMPRESS AND COMPARE in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression’s effect on model behavior. Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and COMPRESS AND COMPARE visualizations that may generalize to broader model comparison tasks.
UR  - https://arxiv.org/pdf/2408.03274v1.pdf
N1  - Source: page_1.html
ID  - 166
ER  - 
TY  - JOUR
TI  - DECEPTIVE AI SYSTEMS THAT GIVE EXPLANATIONS ARE MORE CONVINCING THAN HONEST AI SYSTEMS AND CAN AMPLIFY BELIEF IN MISINFORMATION
AU  - Danry, V.
AU  - Pataranutaporn, P.
AU  - Groh, M.
AU  - Epstein, Z.
AU  - Maes, P.
PY  - 2024
AB  - Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and erode trust in the truth. We examined the impact of deceptive AI generated explanations on individuals’ beliefs in a pre-registered online experiment with 23,840 observations from 1,192 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that personal factors such as cognitive reflection and trust in AI do not necessarily protect individuals from these effects caused by deceptive AI generated explanations. Instead, our results show that the logical validity of AI generated deceptive explanations, that is whether the explanation has a causal effect on the truthfulness of the AI’s classification, plays a critical role in countering their persuasiveness – with logically invalid explanations being deemed less credible. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation.
UR  - https://arxiv.org/pdf/2408.00024v1.pdf
N1  - Source: page_1.html
ID  - 167
ER  - 
TY  - JOUR
TI  - Shaping Integrity: Why Generative Artificial Intelligence Does Not Have to Undermine Education
AU  - Tan, M.J.T.
AU  - Maravilla, N.M.A.T.
PY  - 2024
AB  - This paper examines the role of generative artificial intelligence (GAI) inpromoting academic integrity within educational settings. It explores how AIcan be ethically integrated into classrooms to enhance learning experiences,foster intrinsic motivation, and support voluntary behavior change amongstudents. By analyzing established ethical frameworks and educational theoriessuch as deontological ethics, consequentialism, constructivist learning, andSelf-Determination Theory (SDT), the paper argues that GAI, when usedresponsibly, can enhance digital literacy, encourage genuine knowledgeconstruction, and uphold ethical standards in education. This researchhighlights the potential of GAI to create enriching, personalized learningenvironments that prepare students to navigate the complexities of the modernworld ethically and effectively.
UR  - https://arxiv.org/pdf/2407.19088v2.pdf
N1  - Source: page_1.html
ID  - 168
ER  - 
TY  - JOUR
TI  - LookupForensics: A Large-Scale Multi-Task Dataset for Multi-Phase Image-Based Fact Verification
AU  - Cui, S.
AU  - Nguyen, H.H.
AU  - Le, T.-N.
AU  - Lu, C.-S.
AU  - Echizen, I.
PY  - 2024
AB  - Amid the proliferation of forged images, notably the tsunami of deepfake content, extensive research has been conducted on using artificial intelligence (AI) to identify forged content in the face of continuing advancements in counterfeiting technologies. We have investigated the use of AI to provide the original authentic image after deepfake detection, which we believe is a reliable and persuasive solution. We call this “image-based automated fact verification,” a name that originated from a text-based fact-checking system used by journalists. We have developed a two-phase open framework that integrates detection and retrieval components. Additionally, inspired by a dataset proposed by Meta Fundamental AI Research, we further constructed a large-scale dataset that is specifically designed for this task. This dataset simulates real-world conditions and includes both content-preserving and content-aware manipulations that present a range of difficulty levels and have potential for ongoing research. This multi-task dataset is fully annotated, enabling it to be utilized for sub-tasks within the forgery identification and fact retrieval domains. This paper makes two main contributions: (1) We introduce a new task, ‘‘image-based automated fact verification,” and present a novel two-phase open framework combining “forgery identification” and “fact retrieval.” (2) We present a large-scale dataset tailored for this new task that features various hand-crafted image edits and machine learning-driven manipulations, with extensive annotations suitable for various sub-tasks. Extensive experimental results validate its practicality for fact verification research and clarify its difficulty levels for various sub-tasks.
UR  - https://arxiv.org/pdf/2407.18614v1.pdf
N1  - Source: page_1.html
ID  - 169
ER  - 
TY  - JOUR
TI  - DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models
AU  - Marjanović, S.V.
AU  - Yu, H.
AU  - Atanasova, P.
AU  - Lioma, C.
AU  - Augenstein, I.
PY  - 2024
AB  - Knowledge-intensive language understanding tasks require Language Models (LMs) to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. However, conflicting knowledge can be present in the LM’s parameters, termed intra-memory conflict, which can affect a model’s propensity to accept contextual knowledge. To study the effect of intra-memory conflict on an LM’s ability to accept relevant context, we utilize two knowledge conflict measures and a novel dataset containing inherently conflicting data, DYNAMICQA. This dataset includes facts with a temporal dynamic nature where facts can change over time and disputable dynamic facts, which can change depending on the viewpoint. DYNAMICQA is the first to include real-world knowledge conflicts and provide context to study the link between the different types of knowledge conflicts. We also evaluate several measures on their ability to reflect the presence of intra-memory conflict: semantic entropy and a novel coherent persuasion score. With our extensive experiments, we verify that LMs exhibit a greater degree of intra-memory conflict with dynamic facts compared to facts that have a single truth value. Furthermore, we reveal that facts with intra-memory conflict are harder to update with context, suggesting that retrieval-augmented generation will struggle with the most commonly adapted facts.MSC Codes 68T50
UR  - https://arxiv.org/pdf/2407.17023v2.pdf
N1  - Source: page_1.html
ID  - 170
ER  - 
TY  - JOUR
TI  - Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction
AU  - Bailis, S.
AU  - Friedhoff, J.
AU  - Chen, F.
PY  - 2024
AB  - This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game’s complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding, mirroring real-world discussions where individuals strategically choose when to speak. We demonstrate the framework’s utility through an arena-style tournament featuring Gemini and GPT models. Our results reveal distinct strengths and weaknesses in the models’ strategic reasoning and communication. These findings highlight Werewolf Arena’s potential as a challenging and scalable LLM benchmark.
UR  - https://arxiv.org/pdf/2407.13943v1.pdf
N1  - Source: page_1.html
ID  - 171
ER  - 
TY  - JOUR
TI  - GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text
AU  - Hamilton, K.
AU  - Longo, L.
AU  - Božić, B.
PY  - 2024
AB  - While the use of machine learning for the detection of propaganda techniques in text has garnered considerable attention, most approaches focus on “black-box” solutions with opaque inner workings. Interpretable approaches provide a solution, however, they depend on careful feature engineering and costly expert annotated data. Additionally, language features specific to propagandistic text are generally the focus of rhetoricians or linguists, and there is no data set labeled with such features suitable for machine learning. This study codifies 22 rhetorical and linguistic features identified in literature related to the language of persuasion for the purpose of annotating an existing data set labeled with propaganda techniques. To help human experts annotate natural language sentences with these features, RhetAnn, a web application, was specifically designed to minimize an otherwise considerable mental effort. Finally, a small set of annotated data was used to fine-tune GPT-3.5, a generative large language model (LLM), to annotate the remaining data while optimizing for financial cost and classification accuracy. This study demonstrates how combining a small number of human annotated examples with GPT can be an effective strategy for scaling the annotation process at a fraction of the cost of traditional annotation relying solely on human experts. The results are on par with the best performing model at the time of writing, namely GPT-4, at 10x less the cost. Our contribution is a set of features, their properties, definitions, and examples in a machine-readable format, along with the code for RhetAnn and the GPT prompts and fine-tuning procedures for advancing state-of-the-art interpretable propaganda technique detection.
UR  - https://arxiv.org/pdf/2407.11827v1.pdf
N1  - Source: page_1.html
ID  - 172
ER  - 
TY  - JOUR
TI  - NudgeRank: Digital Algorithmic Nudging for Personalized Health
AU  - Chiam, J.
AU  - Lim, A.
AU  - Teredesai, A.
PY  - 2024
AB  - In this paper we describe NudgeRank™, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17% increase in daily steps and 7.61% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1% open rate compared to baseline systems’ 4%. Demonstrating scalability and reliability, NudgeRank™ operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems.
UR  - https://arxiv.org/pdf/2407.20241v1.pdf
N1  - Source: page_1.html
ID  - 173
ER  - 
TY  - JOUR
TI  - psifx - Psychological and Social Interactions Feature Extraction Package
AU  - Rochette, G.
AU  - Vowels, M.J.
AU  - Rochat, M.
PY  - 2024
AB  - psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to facilitate and democratize the use of state-of-the-art machine learning techniques for human sciences research. It is motivated by a need (a) to automate and standardize data annotation processes, otherwise involving expensive, lengthy, and inconsistent human labor, such as the transcription or coding of behavior changes from audio and video sources; (b) to develop and distribute open-source community-driven psychology research software; and (c) to enable large-scale access and ease of use to non-expert users. The framework contains an array of tools for tasks, such as speaker diarization, closed-caption transcription and translation from audio, body, hand, and facial pose estimation and gaze tracking from video, as well as interactive textual feature extraction supported by large language models, to facilitate automated annotation. The package has been designed with a modular and task-oriented approach, enabling the community to add or update new tools easily. We strongly hope that this package will provide psychologists and social scientists a simple and practical solution for efficiently processing a range of audio, linguistic, and visual features from audio and video, thereby creating new opportunities for in-depth study of real-time behavioral phenomena.
UR  - https://arxiv.org/pdf/2407.10266v3.pdf
N1  - Source: page_1.html
ID  - 174
ER  - 
TY  - JOUR
TI  - Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered Motivational Interviewing
AU  - Steenstra, I.
AU  - Nouraei, F.
AU  - Arjmand, M.
AU  - Bickmore, T.W.
PY  - 2024
AB  - We introduce a novel application of large language models (LLMs) in developing a virtual counselor capable of conducting motivational interviewing (MI) for alcohol use counseling. Access to effective counseling remains limited, particularly for substance abuse, and virtual agents offer a promising solution by leveraging LLM capabilities to simulate nuanced communication techniques inherent in MI. Our approach combines prompt engineering and integration into a user-friendly virtual platform to facilitate realistic, empathetic interactions. We evaluate the effectiveness of our virtual agent through a series of studies focusing on replicating MI techniques and human counselor dialog. Initial findings suggest that our LLMpowered virtual agent matches human counselors' empathetic and adaptive conversational skills, presenting a significant step forward in virtual health counseling and providing insights into the design and implementation of LLM-based therapeutic interactions.
UR  - https://arxiv.org/pdf/2407.08095v1.pdf
N1  - Source: page_1.html
ID  - 175
ER  - 
TY  - JOUR
TI  - On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments
AU  - Fang, J.
AU  - Arechiga, N.
AU  - Namikoshi, K.
AU  - Hogan, C.
AU  - Shamma, D.A.
PY  - 2024
AB  - The Wizard of Oz (WoZ) method is a widely adopted research approach where a human Wizard "role-plays" a not readily available technology and interacts with participants to elicit user behaviors and probe the design space. With the growing ability for modern large language models (LLMs) to role-play, one can apply LLMs as Wizards in WoZ experiments with better scalability and lower cost than the traditional approach. However, methodological guidance on responsibly applying LLMs in WoZ experiments and a systematic evaluation of LLMs' role-playing ability are lacking. Through two LLM-powered WoZ studies, we take the first step towards identifying an experiment lifecycle for researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings that involve Wizards role-played by LLMs. We also contribute a heuristic-based evaluation framework that allows the estimation of LLMs' role-playing ability in WoZ experiments and reveals LLMs' behavior patterns at scale.
UR  - https://arxiv.org/pdf/2407.08067v1.pdf
N1  - Source: page_1.html
ID  - 176
ER  - 
TY  - JOUR
TI  - Review-LLM: Harnessing Large Language Models for Personalized Review Generation
AU  - Peng, Q.
AU  - Liu, H.
AU  - Xu, H.
AU  - Shao, M.
AU  - Wang, W.
PY  - 2024
AB  - Product review generation is an important task in recommender systems, which could provide explanation and persuasiveness for the recommendation. Recently, Large Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling and generating ability, which could be applied in review generation. However, directly applying the LLMs for generating reviews might be troubled by the “polite” phenomenon of the LLMs and could not generate personalized reviews (e.g., negative reviews). In this paper, we propose Review-LLM that customizes LLMs for personalized review generation. Firstly, we construct the prompt input by aggregating user historical behaviors, which include corresponding item titles and reviews. This enables the LLMs to capture user interest features and review writing style. Secondly, we incorporate ratings as indicators of satisfaction into the prompt, which could further improve the model’s understanding of user preferences and the sentiment tendency control of generated reviews. Finally, we feed the prompt text into LLMs, and use Supervised Fine-Tuning (SFT) to make the model generate personalized reviews for the given user and target item. Experimental results on the real-world dataset show that our fine-tuned model could achieve better review generation performance than existing close-source LLMs.
UR  - https://arxiv.org/pdf/2407.07487v1.pdf
N1  - Source: page_1.html
ID  - 177
ER  - 
TY  - JOUR
TI  - Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities
AU  - Ju, T.
AU  - Wang, Y.
AU  - Ma, X.
AU  - Zhang, Z.
AU  - Liu, G.
PY  - 2024
AB  - The rapid adoption of large language models (LLMs) in multi-agent systems has highlighted their impressive capabilities in various applications, such as collaborative problem-solving and autonomous negotiation. However, the security implications of these LLM-based multi-agent systems have not been thoroughly investigated, particularly concerning the spread of manipulated knowledge. In this paper, we investigate this critical issue by constructing a detailed threat model and a comprehensive simulation environment that mirrors real-world multi-agent deployments in a trusted platform. Subsequently, we propose a novel two-stage attack method involving Persuasiveness Injection and Manipulated Knowledge Injection to systematically explore the potential for manipulated knowledge (i.e., counterfactual and toxic knowledge) spread without explicit prompt manipulation. Our method leverages the inherent vulnerabilities of LLMs in handling world knowledge, which can be exploited by attackers to unconsciously spread fabricated information. Through extensive experiments, we demonstrate that our attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication. Furthermore, we show that these manipulations can persist through popular retrieval-augmented generation frameworks, where several benign agents store and retrieve manipulated chat histories for future interactions. This persistence indicates that even after the interaction has ended, the benign agents may continue to be influenced by manipulated knowledge. Our findings reveal significant security risks in LLM-based multi-agent systems, emphasizing the imperative need for robust defenses against manipulated knowledge spread, such as introducing “guardian” agents and advanced fact-checking tools. Code is publicly available at https://github.com/Jometeorie/KnowledgeSpread.
UR  - https://arxiv.org/pdf/2407.07791v1.pdf
N1  - Source: page_1.html
ID  - 178
ER  - 
TY  - JOUR
TI  - Large Language Models can impersonate politicians and other public figures
AU  - Herbold, S.
AU  - Trautsch, A.
AU  - Kikteva, Z.
AU  - Hautli-Janisz, A.
PY  - 2024
AB  - Modern AI technology like Large language models (LLMs) has the potential to pollute the public information sphere with made-up content, which poses a significant threat to the cohesion of societies at large. A wide range of research has shown that LLMs are capable of generating text of impressive quality, including persuasive political speech, text with a pre-defined style, and role-specific content. But there is a crucial gap in the literature: We lack large-scale and systematic studies of how capable LLMs are in impersonating political and societal representatives and how the general public judges these impersonations in terms of authenticity, relevance and coherence. We present the results of a study based on a cross-section of British society that shows that LLMs are able to generate responses to debate questions that were part of a broadcast political debate programme in the UK. The impersonated responses are judged to be more authentic and relevant than the original responses given by people who were impersonated. This shows two things: (1) LLMs can be made to contribute meaningfully to the public political debate and (2) there is a dire need to inform the general public of the potential harm this can have on society.
UR  - https://arxiv.org/pdf/2407.12855v1.pdf
N1  - Source: page_1.html
ID  - 179
ER  - 
TY  - JOUR
TI  - Change-Point Detection in Industrial Data Streams based on Online Dynamic Mode Decomposition with Control
AU  - Wadinger, M.
AU  - Kvasnica, M.
AU  - Kawahara, Y.
PY  - 2024
AB  - We propose a novel change-point detection method based on online Dynamic Mode Decomposition with control (ODMDwC). Leveraging ODMDwC’s ability to find and track linear approximation of a non-linear system while incorporating control effects, the proposed method dynamically adapts to its changing behavior due to aging and seasonality. This approach enables the detection of changes in spatial, temporal, and spectral patterns, providing a robust solution that preserves correspondence between the score and the extent of change in the system dynamics. We formulate a truncated version of ODMDwC and utilize higher-order time-delay embeddings to mitigate noise and extract broad-band features. Our method addresses the challenges faced in industrial settings where safety-critical systems generate non-uniform data streams while requiring timely and accurate change-point detection to protect profit and life. Our results demonstrate that this method yields intuitive and improved detection results compared to the Singular-Value-Decomposition-based method. We validate our approach using synthetic and real-world data, showing its competitiveness to other approaches on complex systems’ benchmark datasets. Provided guidelines for hyperparameters selection enhance our method’s practical applicability.
UR  - https://arxiv.org/pdf/2407.05976v2.pdf
N1  - Source: page_1.html
ID  - 180
ER  - 
TY  - JOUR
TI  - ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal and Multimodal Arabic Content
AU  - Hasanain, M.
AU  - Hasan, Md.A.
AU  - Ahmed, F.
AU  - Zaghouani, W.
AU  - Alam, F.
PY  - 2024
AB  - We present an overview of the second edition of the ArAIEval shared task, organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In this edition, ArAIEval offers two tasks: (i) detection of propagandistic textual spans with persuasion techniques identification in tweets and news articles, and (ii) distinguishing between propagandistic and non-propagandistic memes. A total of 14 teams participated in the final evaluation phase, with 6 and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams submitted system description papers. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further provide a brief overview of the participating systems. All datasets and evaluation scripts are released to the research community.1 We hope this will enable further research on these important tasks in Arabic.MSC Codes 68T50
UR  - https://arxiv.org/pdf/2407.04247v1.pdf
N1  - Source: page_1.html
ID  - 181
ER  - 
TY  - JOUR
TI  - Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness
AU  - Kumar, H.
AU  - Yoo, S.
AU  - Bernuy, A.Z.
AU  - Anderson, A.
AU  - Kornfield, R.
PY  - 2024
AB  - Although engagement in self-directed wellness exercises typically declines over time, integrating social support such as coaching can sustain it. However, traditional forms of support are often inaccessible due to the high costs and complex coordination. Large Language Models (LLMs) show promise in providing human-like dialogues that could emulate social support. Yet, in-depth, in situ investigations of LLMs to support behavior change remain underexplored. We conducted two randomized experiments to assess the impact of LLM agents on user engagement with mindfulness exercises. First, a single-session study, involved 502 crowdworkers; second, a three-week study, included 54 participants. We explored two types of LLM agents: one providing information and another facilitating self-reflection. Both agents enhanced users' intentions to practice mindfulness. However, only the information-providing LLM, featuring a friendly persona, significantly improved engagement with the exercises. Our findings suggest that specific LLM agents may bridge the social support gap in digital health interventions.
UR  - https://arxiv.org/pdf/2407.13067v1.pdf
N1  - Source: page_1.html
ID  - 182
ER  - 
TY  - JOUR
TI  - Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval
AU  - Furumai, K.
AU  - Legaspi, R.
AU  - Vizcarra, J.
AU  - Shi, W.
AU  - Lam, M.S.
PY  - 2024
AB  - Persuasion plays a pivotal role in a wide range of applications from health intervention to the promotion of social good. Persuasive chatbots employed responsibly for social good can be an enabler of positive individual and social change. Existing methods rely on fine-tuning persuasive chatbots with task-specific training data which is costly, if not infeasible, to collect. Furthermore, they employ only a handful of pre-defined persuasion strategies. We propose PersuaBot, a zero-shot chatbot based on Large Language Models (LLMs) that is factual and more persuasive by leveraging many more nuanced strategies. PersuaBot uses an LLM to first generate natural responses, from which the strategies used are extracted. To combat hallucination of LLMs, Persuabot replace any unsubstantiated claims in the response with retrieved facts supporting the extracted strategies. We applied our chatbot, PersuaBot, to three significantly different domains needing persuasion skills: donation solicitation, recommendations, and health intervention. Our experiments on simulated and human conversations show that our zero-shot approach is more persuasive than prior work, while achieving factual accuracy surpassing state-of-the-art knowledge-oriented chatbots.
UR  - https://arxiv.org/pdf/2407.03585v3.pdf
N1  - Source: page_1.html
ID  - 183
ER  - 
TY  - JOUR
TI  - ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024
AU  - Fu, R.
AU  - Liu, R.
AU  - Qiang, C.
AU  - Qi, X.
AU  - Li, G.
PY  - 2024
AB  - The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024) is part of the ISCSLP 2024 Competitions and Challenges track. While current text-to-speech (TTS) technology can generate high-quality audio, its ability to convey complex emotions and controlled detail content remains limited. This constraint leads to a discrepancy between the generated audio and human subjective perception in practical applications like companion robots for children and marketing bots. The core issue lies in the inconsistency between high-quality audio generation and the ultimate human subjective experience. Therefore, this challenge aims to enhance the persuasiveness and acceptability of synthesized audio, focusing on human alignment convincing and inspirational audio generation. A total of 19 teams have registered for the challenge, and the results of the competition and the competition are described in this paper.
UR  - https://arxiv.org/pdf/2407.12038v2.pdf
N1  - Source: page_1.html
ID  - 184
ER  - 
TY  - JOUR
TI  - Analyzing Persuasive Strategies in Meme Texts: A Fusion of Language Models with Paraphrase Enrichment
AU  - Shamanth, K.
AU  - Nayak, R.
AU  - Kosseim, L.
PY  - 2024
AB  - This paper describes our approach to hierarchical multi-label detection of persuasion techniques in meme texts. Our model, developed as a part of the recent SemEval task, is based on fine-tuning individual language models (BERT, XLM-RoBERTa, and mBERT) and leveraging a mean-based ensemble model in addition to dataset augmentation through paraphrase generation from ChatGPT. The scope of the study encompasses enhancing model performance through innovative training techniques and data augmentation strategies. The problem addressed is the effective identification and classification of multiple persuasive techniques in meme texts, a task complicated by the diversity and complexity of such content. The objective of the paper is to improve detection accuracy by refining model training methods and examining the impact of balanced versus unbalanced training datasets. Novelty in the results and discussion lies in the finding that training with paraphrases enhances model performance, yet a balanced training set proves more advantageous than a larger unbalanced one. Additionally, the analysis reveals the potential pitfalls of indiscriminate incorporation of paraphrases from diverse distributions, which can introduce substantial noise. Results with the SemEval 2024 data confirm these insights, demonstrating improved model efficacy with the proposed methods.
UR  - https://arxiv.org/pdf/2407.01784v1.pdf
N1  - Source: page_1.html
ID  - 185
ER  - 
TY  - JOUR
TI  - Debate-to-Write: A Persona-Driven Multi-Agent Framework for Diverse Argument Generation
AU  - Hu, Z.
AU  - Chan, H.P.
AU  - Li, J.
AU  - Yin, Y.
PY  - 2024
AB  - Writing arguments is a challenging task for both humans and machines. It entails incorporating high-level beliefs from various perspectives on the topic, along with deliberate reasoning and planning to construct a coherent narrative. Current language models often generate outputs autoregressively, lacking explicit integration of these underlying controls, resulting in limited output diversity and coherence. In this work, we propose a persona-based multi-agent framework for argument writing. Inspired by the human debate, we first assign each agent a persona representing its high-level beliefs from a unique perspective, and then design an agent interaction process so that the agents can collaboratively debate and discuss the idea to form an overall plan for argument writing. Such debate process enables fluid and nonlinear development of ideas. We evaluate our framework on argumentative essay writing. The results show that our framework generates more diverse and persuasive arguments by both automatic and human evaluations. 1
UR  - https://arxiv.org/pdf/2406.19643v3.pdf
N1  - Source: page_1.html
ID  - 186
ER  - 
TY  - JOUR
TI  - Designing and Evaluating Multi-Chatbot Interface for Human-AI Communication: Preliminary Findings from a Persuasion Task
AU  - Yoon, S.
AU  - Kim, T.E.
AU  - Oh, Y.J.
PY  - 2024
AB  - The dynamics of human-AI communication have been reshaped by language models such as ChatGPT. However, extant research has primarily focused on dyadic communication, leaving much to be explored regarding the dynamics of human-AI communication in group settings. The availability of multiple language model chatbots presents a unique opportunity for scholars to better understand the interaction between humans and multiple chatbots. This study examines the impact of multi-chatbot communication in a specific persuasion setting: promoting charitable donations. We developed an online environment that enables multi-chatbot communication and conducted a pilot experiment utilizing two GPT-based chatbots, Save the Children and UNICEF chatbots, to promote charitable donations. In this study, we present our development process of the multi-chatbot interface and present preliminary findings from a pilot experiment. Analysis of qualitative and quantitative feedback are presented, and limitations are addressed.
UR  - https://arxiv.org/pdf/2406.19648v1.pdf
N1  - Source: page_1.html
ID  - 187
ER  - 
TY  - JOUR
TI  - Your Turn! How Management Researchers Can Help Their Institutions in the Digital Age!
AU  - da Silva, W.A.
AU  - Ratten, V.
AU  - Cassel, G.L.
AU  - Fossatti, P.
AU  - Vendrametto, O.
PY  - 2024
AB  - This exploratory study investigates how management researchers can help their higher educational institutions (HEI) face the digital era. Ten senior management members from six Brazilian HEIs (three public and three private) were investigated. In Brazil, e-learning (EL) grew 474% in one decade. The shorter time spent traveling to the classroom and the lower cost of attending a class make EL the preferred option for Brazilian students (mainly those with poor economic conditions living in large geographic areas or with congested urban traffic).Managers question whether their institutions should meet all the demands of gig workers or companies. They also ask themselves if establishing partnerships with the current competitors or MOOCs, or even buying ready materials, would not save money and improve the value added to the students. They need more information about the issues and benefits of it. The new classes could be short, focused, and dynamic. However, managers resent information on the best design of digital classes (colors, images, persuasive texts, and information on scenes). Successfully using games demands a better understanding of the non-technical elements during the preparation, application, and rewarding phases. Due to generative AI, managers question whether assessments should continue focusing on content memorization.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4876902&type=2
N1  - Source: page_1.html
ID  - 188
ER  - 
TY  - JOUR
TI  - Measuring and Benchmarking Large Language Models’ Capabilities to Generate Persuasive Language
AU  - Pauli, A.B.
AU  - Augenstein, I.
AU  - Assent, I.
PY  - 2024
AB  - We are exposed to much information trying to influence us, such as teaser messages, debates, politically framed news, and propaganda — all of which use persuasive language. With the recent interest in Large Language Models (LLMs), we study the ability of LLMs to produce persuasive text. As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive language - both when explicitly instructed to rewrite text to be more or less persuasive and when only instructed to paraphrase. We construct the new dataset PERSUASIVE-PAIRS of pairs of a short text and its rewrite by an LLM to amplify or diminish persuasive language. We multi-annotate the pairs on a relative scale for persuasive language: a valuable resource in itself, and for training a regression model to score and benchmark persuasive language, including for new LLMs across domains. In our analysis, we find that different ‘personas’ in LLaMA3’s system prompt change persuasive language substantially, even when only instructed to paraphrase.
UR  - https://arxiv.org/pdf/2406.17753v3.pdf
N1  - Source: page_1.html
ID  - 189
ER  - 
TY  - JOUR
TI  - Evidence of a Log Scaling Law for Political Persuasion with Large Language Models
AU  - Hackenburg, K.
AU  - Tappin, B.M.
AU  - Röttger, P.
AU  - Bright, J.
AU  - Margetts, H.
PY  - 2024
AB  - Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 U.S. political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25, 982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more. Second, mere task completion (coherence, staying on topic) appears to account for larger models' persuasive advantage. These findings suggest that further scaling model size will not much increase the persuasiveness of static LLM-generated messages.
UR  - https://arxiv.org/pdf/2406.14508v1.pdf
N1  - Source: page_1.html
ID  - 190
ER  - 
TY  - JOUR
TI  - MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate
AU  - Amayuelas, A.
AU  - Yang, X.
AU  - Antoniades, A.
AU  - Pan, L.
AU  - Wang, W.
PY  - 2024
AB  - Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary’s effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model’s persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy.
UR  - https://arxiv.org/pdf/2406.14711v2.pdf
N1  - Source: page_1.html
ID  - 191
ER  - 
TY  - JOUR
TI  - Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking
AU  - Elaraby, M.
AU  - Litman, D.
AU  - Li, X.L.
AU  - Magooda, A.
PY  - 2024
AB  - Generating free-text rationales is among the emergent capabilities of Large Language Models (LLMs). These rationales have been found to enhance LLM performance across various NLP tasks. Recently, there has been growing interest in using these rationales to provide insights for various important downstream tasks. In this paper, we analyze generated free-text rationales in tasks with subjective answers, emphasizing the importance of rationalization in such scenarios. We focus on pairwise argument ranking, a highly subjective task with significant potential for real-world applications, such as debate assistance. We evaluate the persuasiveness of rationales generated by nine LLMs to support their subjective choices. Our findings suggest that open-source LLMs, particularly Llama2-70B-chat, are capable of providing highly persuasive rationalizations, surpassing even GPT models. Additionally, our experiments show that rationale persuasiveness can be improved by controlling its parameters through prompting or through self-refinement.
UR  - https://arxiv.org/pdf/2406.13905v1.pdf
N1  - Source: page_1.html
ID  - 192
ER  - 
TY  - JOUR
TI  - Assessing AI vs Human-Authored Spear Phishing SMS Attacks: An Empirical Study
AU  - Francia, J.
AU  - Hansen, D.
AU  - Schooley, B.
AU  - Murray, S.
AU  - Snow, G.
PY  - 2024
AB  - This paper explores the use of Large Language Models (LLMs) in spear phishing message generation and evaluates their performance compared to human-authored counterparts. Our pilot study examines the effectiveness of smishing (SMS phishing) messages created by GPT-4 and human authors, which have been personalized for willing targets. The targets assessed these messages in a modified ranked-order experiment using a novel methodology we call TRAPD (Threshold Ranking Approach for Personalized Deception). Experiments involved ranking each spear phishing message from most to least convincing, providing qualitative feedback, and guessing which messages were human- or AI-generated. Results show that LLM-generated messages are often perceived as more convincing than those authored by humans, particularly job-related messages. Targets also struggled to distinguish between human- and AI-generated messages. We analyze different criteria the targets used to assess the persuasiveness and source of messages. This study aims to highlight the urgent need for further research and improved countermeasures against personalized AI-enabled social engineering attacks.
UR  - https://arxiv.org/pdf/2406.13049v2.pdf
N1  - Source: page_1.html
ID  - 193
ER  - 
TY  - JOUR
TI  - Language Model Council: Democratically Benchmarking Foundation Models on Highly Subjective Tasks
AU  - Zhao, J.
AU  - Plaza-Del-Arco, F.M.
AU  - Genchel, B.
AU  - Curry, A.C.
PY  - 2024
AB  - As Large Language Models (LLMs) continue to evolve, evaluating them remains a persistent challenge. Many recent evaluations use LLMs as judges to score outputs from other LLMs, often relying on a single large model like GPT-4o. However, using a single LLM judge is prone to intra-model bias, and many tasks – such as those related to emotional intelligence, creative writing, and persuasiveness – may be too subjective for a single model to judge fairly. We introduce the Language Model Council (LMC), where a group of LLMs collaborate to create tests, respond to them, and evaluate each other’s responses to produce a ranking in a democratic fashion. Unlike previous approaches that focus on reducing cost or bias by using a panel of smaller models, our work examines the benefits and nuances of a fully inclusive LLM evaluation system. In a detailed case study on emotional intelligence, we deploy a council of 20 recent LLMs to rank each other on open-ended responses to interpersonal conflicts. Our results show that the LMC produces rankings that are more separable and more robust, and through a user study, we show that they are more consistent with human evaluations than any individual LLM judge. Using all LLMs for judging can be costly, however, so we use Monte Carlo simulations and hand-curated sub-councils to study hypothetical council compositions and discuss the value of the incremental LLM judge.
UR  - https://arxiv.org/pdf/2406.08598v4.pdf
N1  - Source: page_1.html
ID  - 194
ER  - 
TY  - JOUR
TI  - Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses
AU  - Amirizaniani, M.
AU  - Martin, E.
AU  - Sivachenko, M.
AU  - Mashhadi, A.
AU  - Shah, C.
PY  - 2024
AB  - Theory of Mind (ToM) reasoning entails recognizing that other individuals possess their own intentions, emotions, and thoughts, which is vital for guiding one’s own thought processes. Although large language models (LLMs) excel in tasks such as summarization, question answering, and translation, they still face challenges with ToM reasoning, especially in open-ended questions. Despite advancements, the extent to which LLMs truly understand ToM reasoning and how closely it aligns with human ToM reasoning remains inadequately explored in open-ended scenarios. Motivated by this gap, we assess the abilities of LLMs to perceive and integrate human intentions and emotions into their ToM reasoning processes within open-ended questions. Our study utilizes posts from Reddit’s ChangeMyView platform, which demands nuanced social reasoning to craft persuasive responses. Our analysis, comparing semantic similarity and lexical overlap metrics between responses generated by humans and LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended questions, with even the most advanced models showing notable limitations. To enhance LLM capabilities, we implement a prompt tuning method that incorporates human intentions and emotions, resulting in improvements in ToM reasoning performance. However, despite these improvements, the enhancement still falls short of fully achieving human-like reasoning. This research highlights the deficiencies in LLMs’ social reasoning and demonstrates how integrating human intentions and emotions can boost their effectiveness.
UR  - https://arxiv.org/pdf/2406.05659v1.pdf
N1  - Source: page_1.html
ID  - 195
ER  - 
TY  - JOUR
TI  - More Victories, Less Cooperation: Assessing Cicero's Diplomacy Play
AU  - Wongkamjan, W.
AU  - Gu, F.
AU  - Wang, Y.
AU  - Peskoff, D.
AU  - Boyd-Graber, J.L.
PY  - 2024
AB  - The boardgame Diplomacy is a challenging setting for communicative and cooperative artificial intelligence. The most prominent communicative Diplomacy AI, Cicero, has excellent strategic abilities, exceeding human players. However, the best Diplomacy players master communication, not just tactics, which is why the game has received attention as an AI challenge. This work seeks to understand the degree to which Cicero succeeds at communication. First, we annotate in-game communication with abstract meaning representation to separate in-game tactics from general language. Second, we run two dozen games with humans and Cicero, totaling over 200 human-player hours of competition. While AI can consistently outplay human players, AI–Human communication is still limited because of AI’s difficulty with deception and persuasion. This shows that Cicero relies on strategy and has not yet reached the full promise of communicative and cooperative AI.
UR  - https://arxiv.org/pdf/2406.04643v1.pdf
N1  - Source: page_1.html
ID  - 196
ER  - 
TY  - JOUR
TI  - ArMeme: Propagandistic Content in Arabic Memes
AU  - Alam, F.
AU  - Hasnat, A.
AU  - Ahmad, F.
AU  - Hasan, Md.A.
AU  - Hasanain, M.
PY  - 2024
AB  - With the rise of digital communication memes have become a significant medium for cultural and political expression that is often used to mislead audience. Identification of such misleading and persuasive multimodal content become more important among various stakeholders, including social media platforms, policymakers, and the broader society as they often cause harm to the individuals, organizations and/or society. While there has been effort to develop AI based automatic system for resource rich languages (e.g., English), it is relatively little to none for medium to low resource languages. In this study, we focused on developing an Arabic memes dataset with manual annotations of propagandistic content.1 We annotated ∼ 6K Arabic memes collected from various social media platforms, which is a first resource for Arabic multimodal research. We provide a comprehensive analysis aiming to develop computational tools for their detection. We made the dataset publicly available for the community.MSC Codes 68T50
UR  - https://arxiv.org/pdf/2406.03916v2.pdf
N1  - Source: page_1.html
ID  - 197
ER  - 
TY  - JOUR
TI  - Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation
AU  - Li, H.
AU  - Wu, Y.
AU  - Schlegel, V.
AU  - Li, Y.
AU  - Nenadic, G.
PY  - 2024
AB  - With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with the various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at https://github.com/HaoBytes/ArgSum-Datatset.
UR  - https://arxiv.org/pdf/2406.03151v3.pdf
N1  - Source: page_1.html
ID  - 198
ER  - 
TY  - JOUR
TI  - On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial
AU  - Salvi, F.
AU  - Ribeiro, M.H.
AU  - Gallotti, R.
AU  - West, R.
PY  - 2024
AB  - Can large language models (LLMs) create tailor-made, convincing arguments to promote false or misleading narratives online? Early work has found that LLMs can generate content perceived on par with, or even more persuasive than, human-written messages. However, there is still limited evidence regarding LLMs’ persuasive capabilities in direct conversations with humans—the scenario these models are usually deployed at. In this pre-registered study, we analyze the power of AI-driven persuasion in a controlled, harmless setting. To this end, we created a web-based platform where human participants engaged in short, multi-round debates with either human or LLM opponents. Each participant was randomly assigned to one of four treatment conditions in a two-by-two factorial design: (1) the conversation partner was either another human or an LLM; (2) the conversation partner either had or did not have access to basic sociodemographic information about their opponent (and thus arguments could be personalized). We find that 64.4% of the time, personalized LLM debaters were more persuasive than humans, given that they were not equally persuasive (81.2% relative increase in the odds of higher post-debate agreement; p < 0.01; N = 900). Without personalization, GPT-4 still outperformed humans, but the effect was lower and not statistically significant (p = 0.30). Further, our analysis suggests that LLMs use different strategies from human debaters: their texts are harder to read and have more markers associated with logical and analytical reasoning. Overall, our results suggest that concerns around LLM-based persuasion are meaningful and have important implications for social media governance and the design of new online environments.
UR  - https://www.researchsquare.com/article/rs-4429707/v1.pdf
N1  - Source: page_1.html
ID  - 199
ER  - 
TY  - JOUR
TI  - Towards Dialogues for Joint Human-AI Reasoning and Value Alignment
AU  - Bezou-Vrakatseli, E.
AU  - Cocarascu, O.
AU  - Modgil, S.
PY  - 2024
AB  - We argue that enabling human-AI dialogue, purposed to support joint reasoning (i.e., ‘inquiry’), is important for ensuring that AI decision making is aligned with human values and preferences. In particular, we point to logic-based models of argumentation and dialogue, and suggest that the traditional focus on persuasion dialogues be replaced by a focus on inquiry dialogues, and the distinct challenges that joint inquiry raises. Given recent dramatic advances in the performance of large language models (LLMs), and the anticipated increase in their use for decision making, we provide a roadmap for research into inquiry dialogues for supporting joint human-LLM reasoning tasks that are ethically salient, and that thereby require that decisions are value aligned.
UR  - https://arxiv.org/pdf/2405.18073v1.pdf
N1  - Source: page_1.html
ID  - 200
ER  - 
