title,author,year,abstract,pdf_link,source_file
Capability-Based Scaling Laws for LLM Red-Teaming,"Panfilov, A., Kassianik, P., Andriushchenko, M., Geiping, J.",2025,"As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target’s capability exceeds the attacker’s, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models’ persuasive and manipulative abilities to limit their effectiveness as attackers.",https://arxiv.org/pdf/2505.20162v1.pdf,page_1.html
Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL,"Hong, J., Dragan, A., Levine, S.",2025,"Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability. 1",https://arxiv.org/pdf/2505.18098v1.pdf,page_1.html
"LvL UP 1.0, a holistic mHealth lifestyle coaching intervention for the prevention of non-communicable diseases and common mental disorders: a mixed methods feasibility study","Mair, J.L., Jabir, A.I., Salamanca-Sanabria, A., ...Müller-Riemenschneider, F., Kowatsch, T.",2025,"LvL UP is a smartphone-based holistic lifestyle coaching intervention aimed at improving health behaviours, mental well-being, and preventing noncommunicable diseases and common mental disorders. It features a ‘talk and tools’ approach, combining automated health literacy coaching via conversational agent with digital tools such as journaling, life hacks, and slow-paced breathing exercises. An ‘in-the-wild' mixed-methods study was conducted in Singapore to evaluate LvL UP’s feasibility and acceptability to inform a future definitive trial. The app was available on iOS and Android from March to August 2023 and was promoted through online and offline strategies. Data collection included in-app surveys, usage metrics, and interviews, summarised using descriptive statistics and template analysis. The app was downloaded 307 times. Data from 99 active users were analysed. Most users were female and aged 21–35 years with mild to moderate vulnerabilities in physical activity, diet, and depressive symptoms. Engagement was highest during the first eight days, with 9% remaining engaged for up to 50 days. Users rated technology acceptance highly, finding the app enjoyable, easy to use, and informative. Suggested improvements included streamlined onboarding, fixing bugs, shortening dialogues, and adding rewards. The findings support LvL UP’s feasibility and have informed enhancements for future trials.",https://www.researchsquare.com/article/rs-6484027/v1.pdf,page_1.html
Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective,"Sun, Z., Wang, Q., Wang, H., Zhang, X., Xu, J.",2025,"Large Reasoning Models (LRMs) have shown impressive capabilities in multi-step reasoning tasks. However, alongside these successes, a more deceptive form of model error has emerged—Reasoning Hallucination—where logically coherent but factually incorrect reasoning traces lead to persuasive yet faulty conclusions. Unlike traditional hallucinations, these errors are embedded within structured reasoning, making them more difficult to detect and potentially more harmful. In this work, we investigate reasoning hallucinations from a mechanistic perspective. We propose the Reasoning Score, which quantifies the depth of reasoning by measuring the divergence between logits obtained from projecting late layers of LRMs to the vocabulary space, effectively distinguishing shallow pattern-matching from genuine deep reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA dataset and identify two key reasoning hallucination patterns: early-stage fluctuation in reasoning depth and incorrect backtracking to flawed prior steps. These insights motivate our Reasoning Hallucination Detection (RHD) framework, which achieves state-of-the-art performance across multiple domains. To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced reinforcement learning algorithm that incorporates step-level deep reasoning rewards via potential-based shaping. Our theoretical analysis establishes stronger generalization guarantees, and experiments demonstrate improved reasoning quality and reduced hallucination rates.",https://arxiv.org/pdf/2505.12886v1.pdf,page_1.html
Persuasion and Safety in the Era of Generative AI,"Kong, H.",2025,"As large language models (LLMs) achieve advanced persuasive capabilities, concerns about their potential risks have grown. The EU AI Act prohibits AI systems that use manipulative or deceptive techniques to undermine informed decision-making, highlighting the need to distinguish between rational persuasion, which engages reason, and manipulation, which exploits cognitive biases. My dissertation addresses the lack of empirical studies in this area by developing a taxonomy of persuasive techniques, creating a human-annotated dataset, and evaluating LLMs’ ability to distinguish between these methods. This work contributes to AI safety by providing resources to mitigate the risks of persuasive AI and fostering discussions on ethical persuasion in the age of generative AI.",https://arxiv.org/pdf/2505.12248v1.pdf,page_1.html
Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach,"Lodoen, S., Orchard, A.",2025,"Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more “human-like” than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost’s concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots.",https://arxiv.org/pdf/2505.09576v1.pdf,page_1.html
Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information,"Zhou, J., Venkatachalam, K., Choi, M., Saha, K., De Choudhury, M.",2025,"Background: With the wide adoption of large language models (LLMs) in information creation assistance, it is essential to examine their alignment with human communication styles and values. We situate this study within the context of fact-checking health information, given the critical challenge of correcting misconceptions and building trustworthiness. Recent studies have explored the potential of LLM-based fact-checking, but communication style differences between LLMs and human fact-checkers and associated reader perceptions remain under-explored. In this light, our study evaluates the communication styles of LLMs, focusing on how their explanations differ from those of humans in three core components of health communication: information linguistic features, sender persuasive strategies, and receiver value alignments. Methods: We compiled a dataset of 1498 health misinformation explanations from authoritative fact-checking organizations. Using this dataset, we employed chain-of-thought prompting with zero-shot and few-shot variations to generate LLM fact-checking responses to inaccurate health information. We drew from health communication theories and categorized communication styles along linguistic, persuasion, and value-based dimensions and measured how closely the LLM-generated responses aligned with professional explanations. Then, we examined human preferences with 99 participants who were unaware of LLM involvement and rated randomized fact-checking articles with switching orders. Results: Our findings reveal that LLM-generated articles showed significantly lower scores in persuasive strategies, certainty expressions, and alignment with social values and moral foundations. However, human evaluation demonstrated a strong preference for LLM content, with over 60% responses favoring LLM articles for clarity, completeness, and persuasiveness. Conclusion: Our results suggest that LLMs’ structured approach to presenting information may be more effective at engaging readers despite scoring lower on traditional measures of quality in fact-checking and health communication.",https://arxiv.org/pdf/2505.08143v1.pdf,page_1.html
Must Read: A Systematic Survey of Computational Persuasion,"Bozdag, N.B., Mehri, S., Yang, X., ...Tur, G., Hakkani-Tür, D.",2025,"Persuasion is a fundamental aspect of communication, influencing decision-making across diverse contexts, from everyday conversations to high-stakes scenarios such as politics, marketing, and law. The rise of conversational AI systems has significantly expanded the scope of persuasion, introducing both opportunities and risks. AI-driven persuasion can be leveraged for beneficial applications, but also poses threats through manipulation and unethical influence. Moreover, AI systems are not only persuaders, but also susceptible to persuasion, making them vulnerable to adversarial attacks and bias reinforcement. Despite rapid advancements in AI-generated persuasive content, our understanding of what makes persuasion effective remains limited due to its inherently subjective and context-dependent nature. In this survey, we provide a comprehensive overview of computational persuasion, structured around three key perspectives: (1) AI as a Persuader, which explores AI-generated persuasive content and its applications; (2) AI as a Persuadee, which examines AI’s susceptibility to influence and manipulation; and (3) AI as a Persuasion Judge, which analyzes AI’s role in evaluating persuasive strategies, detecting manipulation, and ensuring ethical persuasion. We introduce a taxonomy for computational persuasion research and discuss key challenges, including evaluating persuasiveness, mitigating manipulative persuasion, and developing responsible AI-driven persuasive systems. Our survey outlines future research directions to enhance the safety, fairness, and effectiveness of AI-powered persuasion while addressing the risks posed by increasingly capable language models.",https://arxiv.org/pdf/2505.07775v1.pdf,page_1.html
"Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts","Aldahoul, N., Ibrahim, H., Varvello, M., ...Rahwan, T., Zaki, Y.",2025,"Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs’ apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.",https://arxiv.org/pdf/2505.04171v1.pdf,page_1.html
Real-World Gaps in AI Governance Research,"Strauss, I., Moure, I., Reilly, T.O., Rosenblat, S.",2025,"Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 – March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas—model alignment and testing & evaluation—while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.",https://arxiv.org/pdf/2505.00174v2.pdf,page_1.html
Algorithmic Addiction by Design: Big Tech’s Leverage of Dark Patterns to Maintain Market Dominance and its Challenge for Content Moderation,"Nie, M.",2025,"Today’s largest technology corporations, especially ones with consumer-facing products such as social media platforms, use a variety of unethical and often outright illegal tactics to maintain their dominance. One tactic that has risen to the level of the public consciousness is the concept of addictive design, evidenced by the fact that excessive social media use has become a salient problem, particularly in the mental and social development of adolescents and young adults. As tech companies have developed more and more sophisticated artificial intelligence (AI) models to power their algorithmic recommender systems, they will become more successful at their goal of ensuring addiction to their platforms. This paper explores how online platforms intentionally cultivate addictive user behaviors and the broad societal implications, including on the health and well-being of children and adolescents. It presents the usage of addictive design — including the usage of dark patterns, persuasive design elements, and recommender algorithms — as a tool leveraged by technology corporations to maintain their dominance. Lastly, it describes the challenge of content moderation to address the problem and gives an overview of solutions at the policy level to counteract addictive design.",https://arxiv.org/pdf/2505.00054v1.pdf,page_1.html
Information Retrieval in the Age of Generative AI: The RGB Model,"Garetto, M.P., Cornacchia, A.P., Galante, F., ...Nordio, A., Tarable, A.P.",2025,"The advent of Large Language Models (LLMs) and generative AI is fundamentally transforming information retrieval and processing on the Internet, bringing both great potential and significant concerns regarding content authenticity and reliability. This paper presents a novel quantitative approach to shed light on the complex information dynamics arising from the growing use of generative AI tools. Despite their significant impact on the digital ecosystem, these dynamics remain largely uncharted and poorly understood. We propose a stochastic model to characterize the generation, indexing, and dissemination of information in response to new topics. This scenario particularly challenges current LLMs, which often rely on real-time Retrieval-Augmented Generation (RAG) techniques to overcome their static knowledge limitations. Our findings suggest that the rapid pace of generative AI adoption, combined with increasing user reliance, can outpace human verification, escalating the risk of inaccurate information proliferation across digital resources. An in-depth analysis of Stack Exchange data confirms that high-quality answers inevitably require substantial time and human effort to emerge. This underscores the considerable risks associated with generating persuasive text in response to new questions and highlights the critical need for responsible development and deployment of future generative AI tools.",https://arxiv.org/pdf/2504.20610v1.pdf,page_1.html
A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies,"Chen, Z., Kalla, J., Le, Q., ...Sekhon, J., Wang, R.",2025,"In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the “receive” and “accept” steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between $48-$74 per persuaded voter compared to $100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.",https://arxiv.org/pdf/2505.00036v1.pdf,page_1.html
Conversations with AI Chatbots Increase Short-Term Vaccine Intentions But Do Not Outperform Standard Public Health Messaging,"Sehgal, N.K.R., Rai, S., Tonneau, M., ...Buttenheim, A., Guntuku, S.C.",2025,"Large language model (LLM) based chatbots show promise in persuasive communication, but existing studies often rely on weak controls or focus on belief change rather than behavioral intentions or outcomes. This pre-registered multi-country (US, Canada, UK) randomized controlled trial involving 930 vaccine-hesitant parents evaluated brief (three-minute) multi-turn conversations with LLM-based chatbots against standard public health messaging approaches for increasing human papillomavirus (HPV) vaccine intentions for their children. Participants were randomly assigned to: (1) a weak control (no message), (2) a strong control reflecting the standard of care (reading official public health materials), or (3 and 4) one of two chatbot conditions. One chatbot was prompted to deliver short, conversational responses, while the other used the model’s default output style (longer with bullet points). While chatbot interactions significantly increased self-reported vaccination intent (by 7.1-10.3 points on a 100-point scale) compared to no message, they did not outperform standard public health materials, with the conversational chatbot performing significantly worse. Additionally, while the short-term effects of chatbot interactions faded during a 15-day follow-up, the effects of public health material persisted relative to no message. These findings suggest that while LLMs can effectively shift vaccination intentions in the short-term, their incremental value over existing public health communications is questionable—offering a more tempered view of their persuasive capabilities and highlighting the importance of integrating AI-driven tools alongside, rather than replacing, current public health strategies.",https://arxiv.org/pdf/2504.20519v2.pdf,page_1.html
Examining the Impact of Technology-Driven Recommendations on Consumer Emotions and Intentions in Online Shopping,"Patnaik, P., Patra, S.K., Kumar, S., Bakkar, M.",2025,"This research explores the growing importance of personalized recommendations driven by Artificial Intelligence (AI) in virtual shopping to enhance customer satisfaction and engagement. Marketers leverage these personalized recommendations to establish strong emotional connections with customers. The study aimed to validate its hypotheses, using a sample size of 495 individuals selected through convenience sampling from India's urban population. The study's findings indicate that personalized product recommendations (PPR) and personalized visual recommendations (PVR) significantly impact consumer emotions, which, in turn, influence purchase intentions. The study focused on a younger demographic to validate its hypotheses. These findings have significant implications for e-commerce, emphasizing the need for personalized online recommendations based on the prediction of human emotions. Consumer expectations have risen in the current digital landscape, requiring marketers to develop systems that combine data, algorithms, and analytics to provide real-time, one-to-one, persuasive communication across the customer journey.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5231082&type=2,page_1.html
LLM-based Semantic Augmentation for Harmful Content Detection,"Meguellati, E., Zeghina, A., Sadiq, S., Demartini, G.",2025,"Recent advances in large language models (LLMs) have demonstrated strong performance on simple text classification tasks, frequently under zero-shot settings. However, their efficacy declines when tackling complex social media challenges such as propaganda detection, hateful meme classification, and toxicity identification. Much of the existing work has focused on using LLMs to generate synthetic training data, overlooking the potential of LLM-based text preprocessing and semantic augmentation. In this paper, we introduce an approach that prompts LLMs to clean noisy text and provide context-rich explanations, thereby enhancing training sets without substantial increases in data volume. We systematically evaluate on the SemEval 2024 multi-label Persuasive Meme dataset and further validate on the Google Jigsaw toxic comments and Facebook hateful memes datasets to assess generalizability. Our results reveal that zero-shot LLM classification underperforms on these high-context tasks compared to supervised models. In contrast, integrating LLM-based semantic augmentation yields performance on par with approaches that rely on human-annotated data, at a fraction of the cost. These findings underscore the importance of strategically incorporating LLMs into machine learning (ML) pipeline for social media classification tasks, offering broad implications for combating harmful content online. Disclaimer: This paper contains examples of explicit language that may be disturbing to some readers.",https://arxiv.org/pdf/2504.15548v1.pdf,page_1.html
Breaking the Shield: Vulnerabilities in Content Moderation for Multimodal Language Models,"Das, A.B., Sakib, S.K.",2025,"Text-to-image generation models, especially with recent breakthroughs like GPT-4o’s image generation, have rapidly transformed how visual content is created and shared across domains. Their ability to produce visually compelling images from natural language prompts has made them increasingly influential in digital media. However, a growing concern is the generation of misinformation or discriminatory information (e.g., images that visually encode or propagate widely debunked or false information) often in a persuasive and authoritative format. In this paper, we demonstrate how multimodal large language model (MMLLM) based image generation systems can be prompted to produce misinformation and discriminatory content. Additionally, we formulate the problem of developing a generalizable prompt template that adversaries could exploit to bypass the system guardrails. Our hypothesis is that prompts framed with narrative context are more successful in evading safeguards than overtly harmful ones. We conduct extensive simulations to validate these strategies and demonstrate the potential impact of the threat.",https://dx.doi.org/10.36227/techrxiv.174537593.33953859/v1,page_1.html
A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations,"Lygizou, Z., Kalles, D.",2025,"Trust management provides an alternative solution for securing open, dynamic, and distributed multi-agent systems, where conventional cryptographic methods prove to be impractical. However, existing trust models face challenges related to agent mobility, changing behaviors, and the cold start problem. To address these issues we introduced a biologically inspired trust model in which trustees assess their own capabilities and store trust data locally. This design improves mobility support, reduces communication overhead, resists disinformation, and preserves privacy. Despite these advantages, prior evaluations revealed limitations of our model in adapting to provider population changes and continuous performance fluctuations. This study proposes a novel algorithm, incorporating a self-classification mechanism for providers to detect performance drops potentially harmful for the service consumers. Simulation results demonstrate that the new algorithm outperforms its original version and FIRE, a well-known trust and reputation model, particularly in handling dynamic trustee behavior. While FIRE remains competitive under extreme environmental changes, the proposed algorithm demonstrates greater adaptability across various conditions. In contrast to existing trust modeling research, this study conducts a comprehensive evaluation of our model using widely recognized trust model criteria, assessing its resilience against common trust-related attacks while identifying strengths, weaknesses, and potential countermeasures. Finally, several key directions for future research are proposed.",https://arxiv.org/pdf/2504.15301v1.pdf,page_1.html
ZEROSUMEVAL: Scaling LLM Evaluation with Inter-Model Competition,"Khan, H., Alyahya, H.A., Alnumay, Y., Bari, M.S., Yener, B.",2025,"Evaluating the capabilities of Large Language Models (LLMs) has traditionally relied on static benchmark datasets, human assessments, or model-based evaluations - methods that often suffer from overfitting, high costs, and biases. ZeroSumEval is a novel competition-based evaluation protocol that leverages zero-sum games to assess LLMs with dynamic benchmarks that resist saturation. ZeroSumEval encompasses a diverse suite of games, including security challenges (PyJail), classic games (Chess, Liar’s Dice, Poker), knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These games are designed to evaluate a range of AI capabilities such as strategic reasoning, planning, knowledge application, and creativity. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework. To demonstrate this, we conduct extensive experiments with >7000 simulations across 7 games and 13 models. Our results show that while frontier models from the GPT and Claude families can play common games and answer questions, they struggle to play games that require creating novel and challenging questions. We also observe that models cannot reliably jailbreak each other and fail generally at tasks requiring creativity. We release our code at https://github.com/facebookresearch/ZeroSumEval.",https://arxiv.org/pdf/2504.12562v1.pdf,page_1.html
LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models,"Liu, M., Xu, Z., Zhang, X., ...Jia, R., Huang, L.",2025,"Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PERSUSAFETY, the first comprehensive framework for the assessment of persuasion safety, which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PERSUSAFETY covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.",https://arxiv.org/pdf/2504.10430v1.pdf,page_1.html
Refining Financial Consumer Complaints through Multi-Scale Model Interaction,"Chen, B.-W., Yen, A.-Z., Chen, C.-C.",2025,"Legal writing demands clarity, formality, and domain-specific precision—qualities often lacking in documents authored by individuals without legal training. To bridge this gap, this paper explores the task of legal text refinement that transforms informal, conversational inputs into persuasive legal arguments. We introduce FinDR, a Chinese dataset of financial dispute records, annotated with official judgments on claim reasonableness. Our proposed method, Multi-Scale Model Interaction (MSMI), leverages a lightweight classifier to evaluate outputs and guide iterative refinement by Large Language Models (LLMs). Experimental results demonstrate that MSMI significantly outperforms single-pass prompting strategies. Additionally, we validate the generalizability of MSMI on several short-text benchmarks, showing improved adversarial robustness. Our findings reveal the potential of multi-model collaboration for enhancing legal document generation and broader text refinement tasks.",https://arxiv.org/pdf/2504.09903v1.pdf,page_1.html
Working with Large Language Models to Enhance Messaging Effectiveness for Vaccine Confidence,"Gullison, L., Fu, F.",2025,"Vaccine hesitancy and misinformation are significant barriers to achieving widespread vaccination coverage. Smaller public health departments may not have expertise or resources for effective vaccine messaging. This paper explores the potential of ChatGPT-augmented messaging to promote confidence in vaccination uptake. We conducted a survey where participants chose between a pair of vaccination messages and assessed which was more persuasive and to what extent. In each pair of messages, there was was one original message and one ChatGPT-augmented message. At the end of the survey, participants were told that half of the messages were generated by ChatGPT. Participants were asked for a quantitative and qualitative response about how knowledge of a message’s ChatGPT status changed their impression of the messages. Overall, ChatGPT messages were rated slightly higher than the original messages. ChatGPT messages generally scored higher when they were longer. Respondents did not express major concerns about ChatGPT messaging, nor was there a significant relationship between views on ChatGPT and message ratings. Finally, there was a correlation between whether the message was positioned first or second in the question and the message’s score. Overall, our results point to the potential of ChatGPT to improve vaccination messaging with potential for future research to clarify this human-AI collaborative relationship.",https://arxiv.org/pdf/2504.09857v1.pdf,page_1.html
Labeling Messages as AI-Generated Does Not Reduce Their Persuasive Effects,"Gallegos, I.O., Shani, C., Shi, W., ...Jurafsky, D., Willer, R.",2025,"As generative artificial intelligence (AI) enables the creation and dissemination of information at massive scale and speed, it is increasingly important to understand how people perceive AI-generated content. One prominent policy proposal requires explicitly labeling AI-generated content to increase transparency and encourage critical thinking about the information, but prior research has not yet tested the effects of such labels. To address this gap, we conducted a survey experiment (N=1601) on a diverse sample of Americans, presenting participants with an AI-generated message about several public policies (e.g., allowing colleges to pay student-athletes), randomly assigning whether participants were told the message was generated by (a) an expert AI model, (b) a human policy expert, or (c) no label. We found that messages were generally persuasive, influencing participants’ views of the policies by 9.74 percentage points on average. However, while 94.6% of participants assigned to the AI and human label conditions believed the authorship labels, labels had no significant effects on participants’ attitude change toward the policies, judgments of message accuracy, nor intentions to share the message with others. These patterns were robust across a variety of participant characteristics, including prior knowledge of the policy, prior experience with AI, political party, education level, or age. Taken together, these results imply that, while authorship labels would likely enhance transparency, they are unlikely to substantially affect the persuasiveness of the labeled content, highlighting the need for alternative strategies to address challenges posed by AI-generated information.",https://arxiv.org/pdf/2504.09865v1.pdf,page_1.html
Exploring the Effects of Large Language Models (Llms) on Register Variation in L2 Academic Writing: A Multidimensional Analysis of Pre- and Post-Llm-Refinement Texts,"Bu, H.",2025,"The remarkable capabilities of Large Language Models (LLMs), such as ChatGPT, in assisting academic writing have inspired considerable interest, but empirical evidence remains scarce regarding how their refinements impact the register features essential to academic discourse. This study investigates register variation in L2 academic writing before and after LLM-refinement using Biber’s (1988) Multi-Dimensional (MD) analysis framework. 200 LLM-refined L2 student essays were compared with their original human-written versions on 6 dimensions and 67 linguistic features. Results indicate that through refinement, LLMs can help improve information density and syntactic complexity, thus making the texts more closely resemble academic written discourse. However, LLM-refinement also has limitations in preserving the register and rhetorical features of academic discourse, as it increases the narrativity and decreases the persuasiveness, abstractness, technicality and formality of L2 academic writing. It overlooks the rhetorical functions of some linguistic features in the academic genre, such as the use of tense and aspect to convey generality or importance. Prompt variations exert minimal impact. Therefore, human intervention is necessary to ensure that the refined texts exhibit a more academic-like register. The findings of the present study have useful pedagogical and practical implications in English for Academic Purposes.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5211017&type=2,page_1.html
Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models,"Zhao, X., Deng, Y., Wang, W., ...Ng, S.-K., Chua, T.-S.",2025,"Conversational Recommender Systems (CRSs) engage users in multi-turn interactions to deliver personalized recommendations. The emergence of large language models (LLMs) further enhances these systems by enabling more natural and dynamic user interactions. However, a key challenge remains in understanding how personality traits shape conversational recommendation outcomes. Psychological evidence highlights the influence of personality traits on user interaction behaviors. To address this, we introduce an LLM-based personality-aware user simulation for CRSs (PerCRS). The user agent induces customizable personality traits and preferences, while the system agent possesses the persuasion capability to simulate realistic interaction in CRSs. We incorporate multi-aspect evaluation to ensure robustness and conduct extensive analysis from both user and system perspectives. Experimental results demonstrate that state-of-the-art LLMs can effectively generate diverse user responses aligned with specified personality traits, thereby prompting CRSs to dynamically adjust their recommendation strategies. Our experimental analysis offers empirical insights into the impact of personality traits on the outcomes of conversational recommender systems.",https://arxiv.org/pdf/2504.12313v1.pdf,page_1.html
Innovative Design and Development of Brand Visual System under the Background of Artificial Intelligence-Assisted Visual Communication Design,"Cai, Y.",2025,"Businesses that want to stand out in today's crowded marketplace must have a distinct identity for their brand. Visual branding stands out among the other aspects that make up a business's identity as a potent means of captivating consumers and communicating the essence of the brand. Visual branding, when done well, may influence customers' opinions, evoke strong feelings, and stamp themselves in their memories. Because every visual aspect of a brand—its logo, color palette, and typography—is meticulously designed to communicate the character, principles, and goals of the company. One of the most influential aspects of branding is visual identity. Brands may convey their beliefs, connect with customers on an emotional level, and differentiate themselves from competitors by using design elements like logos, colors, typography, images, and graphics. Visual branding, when done well, becomes an inherent aspect of a brand's identity and helps it to make an impact on its intended consumers. Visual branding is an effective tool for building a distinct brand identity for the company. Revolutionizing graphic design are Artificial Intelligence (AI) tools that can create visuals from natural language. But with visual persuasion, the picture is more than simply a representation; it is the message. The need to comprehend the potential applications of AI systems in the creation of visual communication gives birth to this. An investigation of grounded theory based on the opinions of experts in the key fields of visual communication led to the development of a model that this article uses to define the theoretical-methodological aspects involved in the creation of convincing visual discourses. As a result, we provide a strategy that, when followed, will lead designers through seven steps in creating convincing messages. In addition, we provide an analysis that may help with the logical and well-informed integration of stage-specific technologies, processes, and methodologies. In this study, visual communication design is implemented using ResNet101 model with Improved Artificial Bee Colony optimization model for feature selection and processing the visual and text elements in to the environment.",https://www.researchsquare.com/article/rs-6003788/v1.pdf,page_1.html
"Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge","Zeng, D., Legaspi, R., Sun, Y., ...Spirtes, P., Zhang, K.",2025,"We hypothesize that optimal system responses emerge from adaptive strategies grounded in causal and counterfactual knowledge. Counterfactual inference allows us to create hypothetical scenarios to examine the effects of alternative system responses. We enhance this process through causal discovery, which identifies the strategies informed by the underlying causal structure that govern system behaviors. Moreover, we consider the psychological constructs and unobservable noises that might be influencing user-system interactions as latent factors. We show that these factors can be effectively estimated. We employ causal discovery to identify strategy-level causal relationships among user and system utterances, guiding the generation of personalized counterfactual dialogues. We model the user utterance strategies as causal factors, enabling system strategies to be treated as counterfactual actions. Furthermore, we optimize policies for selecting system responses based on counterfactual data. Our results using a real-world dataset on social good demonstrate significant improvements in persuasive system outcomes, with increased cumulative rewards validating the efficacy of causal discovery in guiding personalized counterfactual inference and optimizing dialogue policies for a persuasive dialogue system.",https://arxiv.org/pdf/2504.13904v1.pdf,page_1.html
Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"Ahnert, G., Wurth, E., Strohmaier, M., Mata, J.",2025,"Meat reduction benefits human and planetary health, but social norms keep meat central in shared meals. To date, the development of communication strategies that promote meat reduction while minimizing social costs has required the costly involvement of human participants at each stage of the process. We present work in progress on simulating multi-round dialogues on meat reduction between Generative Agents based on large language models (LLMs). We measure our main outcome using established psychological questionnaires based on the Theory of Planned Behavior and additionally investigate Social Costs. We find evidence that our preliminary simulations produce outcomes that are (i) consistent with theoretical expectations; and (ii) valid when compared to data from previous studies with human participants. Generative agent-based models are a promising tool for identifying novel communication strategies on meat reduction—tailored to highly specific participant groups—to then be tested in subsequent studies with human participants.",https://arxiv.org/pdf/2504.04872v1.pdf,page_1.html
Locations of Characters in Narratives: Andersen and Persuasion Datasets,"Özyurt, B., Arkhmammadova, R., Yuret, D.",2025,"The ability of machines to grasp spatial understanding within narrative contexts is an intriguing aspect of reading comprehension that continues to be studied. Motivated by the goal to test the AI’s competence in understanding the relationship between characters and their respective locations in narratives, we introduce two new datasets: Andersen and Persuasion. For the Andersen dataset, we selected fifteen children’s stories from ""Andersen’s Fairy Tales"" by Hans Christian Andersen and manually annotated the characters and their respective locations throughout each story. Similarly, for the Persuasion dataset, characters and their locations in the novel ""Persuasion"" by Jane Austen were also manually annotated. We used these datasets to prompt Large Language Models (LLMs). The prompts are created by extracting excerpts from the stories or the novel and combining them with a question asking the location of a character mentioned in that excerpt. Out of the five LLMs we tested, the best-performing one for the Andersen dataset accurately identified the location in 61.85% of the examples, while for the Persuasion dataset, the best-performing one did so in 56.06% of the cases.",https://arxiv.org/pdf/2504.03434v1.pdf,page_1.html
Persuasive Calibration,"Feng, Y., Tang, W.",2025,"We introduce and study the persuasive calibration problem, where a principal aims to provide trustworthy predictions about underlying events to a downstream agent to make desired decisions. We adopt the standard calibration framework that regulates predictions to be unbiased conditional on their own value, and thus, they can reliably be interpreted at the face value by the agent. Allowing a small calibration error budget, we aim to answer the following question: what is and how to compute the optimal predictor under this calibration error budget, especially when there exists incentive misalignment between the principal and the agent? We focus on standard ℓt-norm Expected Calibration Error (ECE) metric. We develop a general framework by viewing predictors as post-processed versions of perfectly calibrated predictors. Using this framework, we first characterize the structure of the optimal predictor. Specifically, when the principal’s utility is event-independent and for ℓ1-norm ECE, we show: (1) the optimal predictor is over-(resp. under-) confident for high (resp. low) true expected outcomes, while remaining perfectly calibrated in the middle; (2) the miscalibrated predictions exhibit a collinearity structure with the principal’s utility function. On the algorithmic side, we provide a FPTAS for computing approximately optimal predictor for general principal utility and general ℓt-norm ECE. Moreover, for the ℓ1- and ℓ∞-norm ECE, we provide polynomial-time algorithms that compute the exact optimal predictor.",https://arxiv.org/pdf/2504.03211v1.pdf,page_1.html
Off-Policy Evaluation for Sequential Persuasion Process with Unobserved Confounding,"Venkatesh, N.S., Bang, H., Malikopoulos, A.A.",2025,"In this paper, we expand the Bayesian persuasion framework to account for unobserved confounding variables in sender-receiver interactions. While traditional models typically assume that belief updates follow Bayesian principles, real-world scenarios often involve hidden variables that impact the receiver’s belief formation and decision-making. We conceptualize this as a sequential decision-making problem, where the sender and receiver interact over multiple rounds. In each round, the sender communicates with the receiver, who also interacts with the environment. Crucially, the receiver’s belief update is affected by an unobserved confounding variable. By reformulating this scenario as a Partially Observable Markov Decision Process (POMDP), we capture the sender’s incomplete information regarding both the dynamics of the receiver’s beliefs and the unobserved confounder. We prove that finding an optimal observation-based policy in this POMDP is equivalent to solving for an optimal signaling strategy in the original persuasion framework. Furthermore, we demonstrate how this reformulation facilitates the application of proximal learning for off-policy evaluation (OPE) in the persuasion process. This advancement enables the sender to evaluate alternative signaling strategies using only observational data from a behavioral policy, thus eliminating the necessity for costly new experiments.",https://arxiv.org/pdf/2504.01211v1.pdf,page_1.html
When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR),"Agarwal, M., Khanna, D.",2025,"In many real-world scenarios, a single Large Language Model (LLM) may encounter contradictory claims—some accurate, others forcefully incorrect—and must judge which is true. We investigate this risk in a single-turn, multi-agent debate framework: one LLM-based agent provides a factual answer from TruthfulQA, another vigorously defends a falsehood, and the same LLM architecture serves as judge. We introduce the Confidence-Weighted Persuasion Override Rate (CW-POR), which captures not only how often the judge is deceived but also how strongly it believes the incorrect choice. Our experiments on five open-source LLMs (3B–14B parameters), where we systematically vary agent verbosity (30–300 words), reveal that even smaller models can craft persuasive arguments that override truthful answers—often with high confidence. These findings underscore the importance of robust calibration and adversarial testing to prevent LLMs from confidently endorsing misinformation.",https://arxiv.org/pdf/2504.00374v1.pdf,page_1.html
Towards Personalized Conversational Sales Agents: Contextual User Profiling for Strategic Action,"Kim, T., Lee, J., Yoon, S., Kim, S., Lee, D.",2025,"Conversational Recommender Systems (CRSs) aim to engage users in dialogue to provide tailored recommendations. While traditional CRSs focus on eliciting preferences and retrieving items, real-world e-commerce interactions involve more complex decision-making, where users consider multiple factors beyond simple attributes. To bridge this gap, we introduce Conversational Sales (CSALES), a novel task that unifies preference elicitation, recommendation, and persuasion to better support user decision-making. For a realistic evaluation of CSALES, we present CSUSER, an LLM-based user simulator constructed from real-world data, modeling diverse user profiles with needs and personalities. Additionally, we propose CSI, a conversational sales agent that proactively infers contextual profiles through dialogue for personalized action planning. Extensive experiments demonstrate that CSUSER effectively replicates real-world users and emphasize the importance of contextual profiling for strategic action selection, ultimately driving successful purchases in e-commerce.",https://arxiv.org/pdf/2504.08754v3.pdf,page_1.html
MemInsight: Autonomous Memory Augmentation for LLM Agents,"Salama, R., Cai, J., Yuan, M., ...Zhang, Y., Benajiba, Y.",2025,"Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.",https://arxiv.org/pdf/2503.21760v1.pdf,page_1.html
Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models,"Ke, S.-W., Lai, G.-Y., Fang, G.-L., Kao, H.-Y.",2025,"Large language models (LLMs) are designed to align with human values in their responses. This study exploits LLMs with an iterative prompting technique where each prompt is systematically modified and refined across multiple iterations to enhance its effectiveness in jailbreaking attacks progressively. This technique involves analyzing the response patterns of LLMs, including GPT-3.5, GPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts to evade the LLMs’ ethical and security constraints. Persuasion strategies enhance prompt effectiveness while maintaining consistency with malicious intent. Our results show that the attack success rates (ASR) increase as the attacking prompts become more refined with the highest ASR of 90% for GPT4 and ChatGLMa and the lowest ASR of 68% for LLaMa2. Our technique outperforms baseline techniques (PAIR and PAP) in ASR and shows comparable performance with GCG and ArtPrompt.",https://arxiv.org/pdf/2503.20320v1.pdf,page_1.html
Competitive Multi-armed Bandit Games for Resource Sharing,"Li, H., Duan, L.",2025,"In modern resource-sharing systems, multiple agents access limited resources with unknown stochastic conditions to perform tasks. When multiple agents access the same resource (arm) simultaneously, they compete for successful usage, leading to contention and reduced rewards. This motivates our theoretical study of competitive multi-armed bandit (CMAB) games. In this paper, we study a new N-player K-arm competitive MAB game, where non-myopic players (agents) compete with each other to form diverse private estimations of unknown arms over time. Their possible collisions on the same arms and the time-varying nature of arm rewards make the policy analysis here more involved than the existing studies for myopic players. We explicitly analyze the threshold-based structures of the social optimum and the existing selfish policy, showing that the latter causes prolonged convergence times (Formula presented), while the socially optimal policy with coordinated communication reduces it to (Formula presented). Based on the policy comparison, we prove that the competition among selfish players for the best arm can result in an infinite price of anarchy (PoA), indicating an arbitrarily large efficiency loss compared to the social optimum. We further prove that no informational (non-monetary) mechanism (including Bayesian persuasion) can reduce the infinite PoA, as strategic misreporting by non-myopic players undermines such approaches. To address this, we propose a Combined Informational and Side-Payment (CISP) mechanism, which provides socially optimal arm recommendations with proper informational and monetary incentives to players according to their diverse and time-varying private beliefs. Our CISP mechanism keeps ex-post budget balanced for the social planner and ensures truthful reporting from players, thereby achieving the minimum PoA = 1 and the same convergence time as the social optimum.",https://arxiv.org/pdf/2503.20975v1.pdf,page_1.html
Optimizing Influence Campaigns: Nudging Under Bounded Confidence,"Chen, Y.-S., Zaman, T.",2025,"Influence campaigns in online social networks are often run by organizations, political parties, and nation states to influence large audiences. These campaigns are employed through the use of agents in the network that share persuasive content. Yet, their impact might be minimal if the audiences remain unswayed, often due to the bounded confidence phenomenon, where only a narrow spectrum of viewpoints can influence them. Here we show that to persuade under bounded confidence, an agent must nudge its targets to gradually shift their opinions. Using a control theory approach, we show how to construct an agent’s nudging policy under the bounded confidence opinion dynamics model and also how to select targets for multiple agents in an influence campaign on a social network. Simulations on real Twitter networks show that a multi-agent nudging policy can shift the mean opinion, decrease opinion polarization, or even increase it. We find that our nudging based policies outperform other common techniques that do not consider the bounded confidence effect. Finally, we show how to craft prompts for large language models, such as ChatGPT, to generate text-based content for real nudging policies. This illustrates the practical feasibility of our approach, allowing one to go from mathematical nudging policies to real social media content.",https://arxiv.org/pdf/2503.18331v1.pdf,page_1.html
Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies,"Zeng, D., Legaspi, R., Sun, Y., ...Spirtes, P., Zhang, K.",2025,"Tailoring persuasive conversations to users leads to more effective persuasion. However, existing dialogue systems often struggle to adapt to dynamically evolving user states. This paper presents a novel method that leverages causal discovery and counterfactual reasoning for optimizing system persuasion capability and outcomes. We employ the Greedy Relaxation of the Sparsest Permutation (GRaSP) algorithm to identify causal relationships between user and system utterance strategies, treating user strategies as states and system strategies as actions. GRaSP identifies user strategies as causal factors influencing system responses, which inform Bidirectional Conditional Generative Adversarial Networks (BiCoGAN) in generating counterfactual utterances for the system. Subsequently, we use the Dueling Double Deep Q-Network (D3QN) model to utilize counterfactual data to determine the best policy for selecting system utterances. Our experiments with the PersuasionForGood dataset show measurable improvements in persuasion outcomes using our approach over baseline methods. The observed increase in cumulative rewards and Q-values highlights the effectiveness of causal discovery in enhancing counterfactual reasoning and optimizing reinforcement learning policies for online dialogue systems.",https://arxiv.org/pdf/2503.16544v1.pdf,page_1.html
Spotting Persuasion: A Low-cost Model for Persuasion Detection in Political Ads on Social Media,"Meguellati, E., Civelli, S., Bernardelle, P., Sadiq, S., Demartini, G.",2025,"In the realm of political advertising, persuasion operates as a pivotal element within the broader framework of propaganda, exerting profound influences on public opinion and electoral outcomes. In this paper, we (1) introduce a lightweight model for persuasive text detection that achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3, while significantly reducing the computational resource requirements; and (2) leverage the proposed model to gain insights into political campaigning strategies on social media platforms by applying it to a real-world dataset we curated, consisting of Facebook political ads from the 2022 Australian Federal election campaign. Our study shows how subtleties can be found in persuasive political advertisements and presents a pragmatic approach to detect and analyze such strategies with limited resources, enhancing transparency in social media political campaigns.",https://arxiv.org/pdf/2503.13844v1.pdf,page_1.html
AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence,"Li, Z., Huang, S., Wang, J., ...Wang, W.Y., Yan, X.",2025,"As language agents progressively automate critical tasks across domains, their ability to operate within operational constraints and safety protocols becomes essential. While extensive research has demonstrated these agents’ effectiveness in downstream task completion, their reliability in following operational procedures and constraints remains largely unexplored. To this end, we present AgentOrca, a dual-system framework for evaluating language agents’ compliance with operational constraints and routines. Our framework encodes action constraints and routines through both natural language prompts for agents and corresponding executable code serving as ground truth for automated verification. Through an automated pipeline of test case generation and evaluation across five real-world domains, we quantitatively assess current language agents’ adherence to operational constraints. Our findings reveal notable performance gaps among state-of-the-art models, with large reasoning models like o1 demonstrating superior compliance while others show significantly lower performance, particularly when encountering complex constraints or user persuasion attempts.1",https://arxiv.org/pdf/2503.08669v1.pdf,page_1.html
Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models,"Yeginbergen, A., Oronoz, M., Agerri, R.",2025,"This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems. Data and code publicly available.1",https://arxiv.org/pdf/2503.05328v1.pdf,page_1.html
AI-POWERED EPISODIC FUTURE THINKING,"Ahmadi, S., Rockwell, M., Stuart, M., ...Stein, J., Fox, E.A.",2025,"Episodic Future Thinking (EFT) is an intervention that involves vividly imagining personal future events and experiences in detail. It has shown promise as an intervention to reduce delay discounting—the tendency to devalue delayed rewards in favor of immediate gratification—and to promote behavior change in a range of maladaptive health behaviors. We present EFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model, designed to generate EFT cues for users with lifestyle-related conditions. To evaluate the chatbot, we conducted a user study that included usability assessments and user evaluations based on content characteristics questionnaires, followed by semi-structured interviews. The study provides qualitative insights into participants’ experiences and interactions with the chatbot and its usability. Our findings highlight the potential application of AI chatbots based on Large Language Models (LLMs) in EFT interventions, and offer design guidelines for future behavior-oriented applications.",https://arxiv.org/pdf/2503.16484v1.pdf,page_1.html
WritingBench: A Comprehensive Benchmark for Generative Writing,"Wu, Y., Mei, J., Yan, M., ...Jin, Q., Huang, F.",2025,"Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework’s validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.",https://arxiv.org/pdf/2503.05244v2.pdf,page_1.html
The Influence of Prior Discourse on Conversational Agent-Driven Decision-Making,"Pilli, S., Nallur, V.",2025,"Persuasion through conversation has been the focus of much research. Nudging is a popular strategy to influence decision-making in physical and digital settings. However, conversational agents employing “nudging” have not received significant attention. We explore the manifestation of cognitive biases—the underlying psychological mechanisms of nudging—and investigate how the complexity of prior dialogue tasks impacts decision-making facilitated by conversational agents. Our research used a between-group experimental design, involving 756 participants randomly assigned to either a simple or complex task before encountering a decision-making scenario. Three scenarios were adapted from Samuelsonś classic experiments on status-quo bias, the underlying mechanism of default nudges. Our results aligned with previous studies in two out of three simple-task scenarios. Increasing task complexity consistently shifted effect-sizes toward our hypothesis, though bias was significant in only one case. These findings inform conversational nudging strategies and highlight inherent biases relevant to behavioural economics.",https://arxiv.org/pdf/2503.04692v1.pdf,page_1.html
“Impressively Scary:” Exploring User Perceptions and Reactions to Unraveling Machine Learning Models in Social Media Applications,"West, J., Cagiltay, B., Zhang, S., ...Fawaz, K., Banerjee, S.",2025,"Machine learning models deployed locally on social media applications are used for features, such as face filters which read faces in-real time, and they expose sensitive attributes to the apps. However, the deployment of machine learning models, e.g., when, where, and how they are used, in social media applications is opaque to users. We aim to address this inconsistency and investigate how social media user perceptions and behaviors change once exposed to these models. We conducted user studies (N=21) and found that participants were unaware to both what the models output and when the models were used in Instagram and TikTok, two major social media platforms. In response to being exposed to the models’ functionality, we observed long term behavior changes in 8 participants. Our analysis uncovers the challenges and opportunities in providing transparency for machine learning models that interact with local user data.",https://arxiv.org/pdf/2503.03927v1.pdf,page_1.html
"AI-Powered Lawyering: AI Reasoning Models, Retrieval Augmented Generation, and the Future of Legal Practice","Schwarcz, D., Manning, S., Barry, P., ...Prescott, J.J., Rich, B.",2025,"Generative AI is set to transform the legal profession, but its full impact remains uncertain. While AI models like GPT-4 improve the efficiency with which legal work can be completed, they can at times make up cases and “hallucinate” facts, thereby undermining legal judgment, particularly in complex tasks handled by skilled lawyers. This article examines two emerging AI innovations that may mitigate these lingering issues: Retrieval Augmented Generation (RAG), which grounds AI-powered analysis in legal sources, and AI reasoning models, which structure complex reasoning before generating output. We conducted the first randomized controlled trial assessing these technologies, assigning upper-level law students to complete six legal tasks using a RAG-powered legal AI tool (Vincent AI), an AI reasoning model (OpenAI’s o1-preview), or no AI. We find that both AI tools significantly enhanced legal work quality, a marked contrast with previous research examining older large language models like GPT-4. Moreover, we find that these models maintain the efficiency benefits associated with use of older AI technologies. Our findings show that AI assistance significantly boosts productivity in five out of six tested legal tasks, with Vincent yielding statistically significant gains of approximately 38% to 115% and o1-preview increasing productivity by 34% to 140%, with particularly strong effects in complex tasks like drafting persuasive letters and analyzing complaints. Notably, o1-preview improved the analytical depth of participants’ work product but resulted in some hallucinations, whereas Vincent AI-aided participants produced roughly the same amount of hallucinations as participants who did not use AI at all. These findings suggest that integrating domain-specific RAG capabilities with reasoning models could yield synergistic improvements, shaping the next generation of AI-powered legal tools and the future of lawyering more generally.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5162111&type=2,page_1.html
Intolerable Risk Threshold Recommendations for Artificial Intelligence,"Raman, D., Madkour, N., Murphy, E.R., Jackson, K., Newman, J.",2025,"Frontier AI models - highly capable foundation models at the cutting edge ofAI development - may pose severe risks to public safety, human rights,economic stability, and societal value in the coming years. These risks couldarise from deliberate adversarial misuse, system failures, unintended cascadingeffects, or simultaneous failures across multiple models. In response to such risks, at the AI Seoul Summit in May 2024, 16 global AIindustry organizations signed the Frontier AI Safety Commitments, and 27nations and the EU issued a declaration on their intent to define thesethresholds. To fulfill these commitments, organizations must determine anddisclose ``thresholds at which severe risks posed by a model or system, unlessadequately mitigated, would be deemed intolerable.\'\' To assist in setting and operationalizing intolerable risk thresholds, weoutline key principles and considerations; for example, to aim for ``good, notperfect\'\' thresholds in the face of limited data on rapidly advancing AIcapabilities and consequently evolving risks. We also propose specificthreshold recommendations, including some detailed case studies, for a subsetof risks across eight risk categories: (1) Chemical, Biological, Radiological,and Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)Persuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,and (8) Socioeconomic Disruption. Our goal is to serve as a starting point orsupplementary resource for policymakers and industry leaders, encouragingproactive risk management that prioritizes preventing intolerable risks (exante) rather than merely mitigating them after they occur (ex post).",https://arxiv.org/pdf/2503.05812v1.pdf,page_1.html
Can (A)I Change Your Mind?,"Havin, M., Kleinman, T.W., Koren, M., Dover, Y., Goldstein, A.",2025,"The increasing integration of large language models (LLMs) based conversational agents into everyday life raises critical cognitive and social questions about their potential to influence human opinions. Although previous studies have shown that LLM-based agents can generate persuasive content, these typically involve controlled English-language settings. Addressing this, our preregistered study explored LLMs’ persuasive capabilities in more ecological, unconstrained scenarios, examining both static (written paragraphs) and dynamic (conversations via Telegram) interaction types. Conducted entirely in Hebrew with 200 participants, the study assessed the persuasive effects of both LLM and human interlocutors on controversial civil policy topics. Results indicated that participants adopted LLM and human perspectives similarly, with significant opinion changes evident across all conditions, regardless of interlocutor type or interaction mode. Confidence levels increased significantly in most scenarios. These findings demonstrate LLM-based agents’ robust persuasive capabilities across diverse sources and settings, highlighting their potential impact on shaping public opinions.",https://arxiv.org/pdf/2503.01844v3.pdf,page_1.html
ChatGPT for President! Presupposed content in politicians versus GPT-generated texts,"Garassino, D., Brocca, N., Masia, V.",2025,"This study examines ChatGPT-4’s capability to replicate linguistic strategies used in political discourse, focusing on its potential for manipulative language generation. As large language models (LLMs) become increasingly popular for text generation, concerns have grown regarding their role in spreading fake news and propaganda. This research compares real political speeches with those generated by ChatGPT, with an emphasis on presuppositions—a rhetorical device that subtly influences audiences by packaging some content as already known at the moment of utterance, thus swaying opinions without explicit argumentation. Using a corpus-based pragmatic analysis, this study assesses how well ChatGPT can mimic these persuasive strategies. The findings reveal that, although ChatGPT-generated texts contain many manipulative presuppositions, key differences emerge in their frequency, form and function, compared with politicians. For instance, ChatGPT often relies on change-of-state verbs used in fixed phrases, whereas politicians rely on presupposition triggers in more varied and creative ways. Such differences, however, are challenging to detect with the “naked eye,” underscoring the potential risks posed by LLMs in political and public discourse.",https://arxiv.org/pdf/2503.01269v1.pdf,page_1.html
Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models,"Bozdag, N.B., Mehri, S., Tur, G., Hakkani-Tür, D.",2025,"Large Language Models (LLMs) demonstrate persuasive capabilities that rival human-level persuasion. While these capabilities can be used for social good, they also present risks of potential misuse. Moreover, LLMs’ susceptibility to persuasion raises concerns about alignment with ethical principles. To study these dynamics, we introduce Persuade Me If You Can (PMIYC), an automated framework for evaluating persuasion through multi-agent interactions. Here, PERSUADER agents engage in multi-turn conversations with the PERSUADEE agents, allowing us to measure LLMs’ persuasive effectiveness and their susceptibility to persuasion. We conduct comprehensive evaluations across diverse LLMs, ensuring each model is assessed against others in both subjective and misinformation contexts. We validate the efficacy of our framework through human evaluations and show alignment with prior work. PMIYC offers a scalable alternative to human annotation for studying persuasion in LLMs. Through PMIYC, we find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness, outperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50% greater resistance to persuasion for misinformation compared to Llama-3.3-70B. These findings provide empirical insights into the persuasive dynamics of LLMs and contribute to the development of safer AI systems.",https://arxiv.org/pdf/2503.01829v2.pdf,page_1.html
AI persuading AI vs AI persuading Humans: LLMs’ Differential Effectiveness in Promoting Pro-Environmental Behavior,"Doudkin, A., Pataranutaporn, P., Maes, P.",2025,"Pro-environmental behavior (PEB) is vital to combat climate change, yet turning awareness into intention and action remains elusive. We explore large language models (LLMs) as tools to promote PEB, comparing their impact across 3,200 participants: real humans (n=1,200), simulated humans based on actual participant data (n=1,200), and fully synthetic personas (n=1,200). All three participant groups faced personalized or standard chatbots, or static statements, employing four persuasion strategies (moral foundations, future self-continuity, action orientation, or”freestyle” chosen by the LLM). Results reveal a”synthetic persuasion paradox”: synthetic and simulated agents significantly affect their post-intervention PEB stance, while human responses barely shift. Simulated participants better approximate human trends but still overestimate effects. This disconnect underscores LLM’s potential for pre-evaluating PEB interventions but warns of its limits in predicting real-world behavior. We call for refined synthetic modeling and sustained and extended human trials to align conversational AI’s promise with tangible sustainability outcomes.",https://arxiv.org/pdf/2503.02067v1.pdf,page_1.html
Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions,"Borah, A., Mihalcea, R., Pérez-Rosas, V.",2025,"Existing challenges in misinformation exposure and susceptibility vary across demographic groups, as some populations are more vulnerable to misinformation than others. Large language models (LLMs) introduce new dimensions to these challenges through their ability to generate persuasive content at scale and reinforcing existing biases. This study investigates the bidirectional persuasion dynamics between LLMs and humans when exposed to misinformative content. We analyze human-to-LLM influence using human-stance datasets and assess LLM-to-human influence by generating LLM-based persuasive arguments. Additionally, we use a multi-agent LLM framework to analyze the spread of misinformation under persuasion among demographic-oriented LLM agents. Our findings show that demographic factors influence susceptibility to misinformation in LLMs, closely reflecting the demographic-based patterns seen in human susceptibility. We also find that, similar to human demographic groups, multi-agent LLMs exhibit echo chamber behavior. This research explores the interplay between humans and LLMs, highlighting demographic differences in the context of misinformation and offering insights for future interventions.",https://arxiv.org/pdf/2503.02038v1.pdf,page_1.html
Leveraging LLMs for Mental Health: Detection and Recommendations from Social Discussions,"Aggarwal, V., Thukral, S., Patel, K., Chatterjee, A.",2025,"Textual data from social platforms captures various aspects of mental health through discussions around and across issues, while users reach out for help and others sympathize and offer support. We propose a comprehensive framework that leverages Natural Language Processing (NLP) and Generative AI techniques to identify and assess mental health disorders, detect their severity, and create recommendations for behavior change and therapeutic interventions based on users’ posts on Reddit. To classify the disorders, we use rule-based labeling methods as well as advanced pre-trained NLP models to extract nuanced semantic features from the data. We fine-tune domain-adapted and generic pre-trained NLP models based on predictions from specialized Large Language Models (LLMs) to improve classification accuracy. Our hybrid approach combines the generalization capabilities of pre-trained models with the domain-specific insights captured by LLMs, providing an improved understanding of mental health discourse. Our findings highlight the strengths and limitations of each model, offering valuable insights into their practical applicability. This research potentially facilitates early detection and personalized care to aid practitioners and aims to facilitate timely interventions and improve overall well-being, thereby contributing to the broader field of mental health surveillance and digital health analytics.",https://arxiv.org/pdf/2503.01442v1.pdf,page_1.html
An Empirical Analysis of LLMs for Countering Misinformation,"Proma, A.M., Pate, N., Druckman, J., ...He, H., Hoque, E.",2025,"While Large Language Models (LLMs) can amplify online misinformation, they also show promise in tackling misinformation. In this paper, we empirically study the capabilities of three LLMs – ChatGPT, Gemini, and Claude – in countering political misinformation. We implement a two-step, chain-of-thought prompting approach, where models first identify credible sources for a given claim and then generate persuasive responses. Our findings suggest that models struggle to ground their responses in real news sources, and tend to prefer citing left-leaning sources. We also observe varying degrees of response diversity among models. Our findings highlight concerns about using LLMs for fact-checking through only prompt-engineering, emphasizing the need for more robust guardrails. Our results have implications for both researchers and non-technical users.",https://arxiv.org/pdf/2503.01902v1.pdf,page_1.html
PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues,"Yu, F., Jiang, L., Huang, S., Wu, Z., Dai, X.",2025,"The ability to understand and predict the mental states of oneself and others, known as the Theory of Mind (ToM), is crucial for effective social interactions. Recent research has emerged to evaluate whether Large Language Models (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM in LLMs, existing benchmarks focus predominantly on physical perception with principles guided by the Sally-Anne test in synthetic stories and conversations, failing to capture the complex psychological activities of mental states in real-life social interactions. To mitigate this gap, we propose PERSUASIVETOM, a benchmark designed to evaluate the ToM abilities of LLMs in persuasive dialogues. Our framework introduces two categories of questions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving mental states (e.g., desire shifts in persuadees), and (2) ToM Application, evaluating whether LLMs can take advantage of inferred mental states to select effective persuasion strategies (e.g., emphasize rarity) and evaluate the effectiveness of persuasion strategies. Experiments across eight state-of-the-art LLMs reveal that while models excel on multiple questions, they struggle to answer questions that need tracking the dynamics and shifts of mental states and understanding the mental states in the whole dialogue comprehensively. Our aim with PERSUASIVETOM is to allow an effective evaluation of the ToM reasoning ability of LLMs with more focus on complex psychological activities. Our code is available at https://github.com/YuFangxu/PersuasiveToM.",https://arxiv.org/pdf/2502.21017v1.pdf,page_1.html
Among Them: A game-based framework for assessing persuasion capabilities of LLMs,"Idziejczak, M., Korzavatykh, V., Stawicki, M., ...Błądek, I., Brzezinski, D.",2025,"The proliferation of large language models (LLMs) and autonomous AI agents has raised concerns about their potential for automated persuasion and social influence. While existing research has explored isolated instances of LLM-based manipulation, systematic evaluations of persuasion capabilities across different models remain limited. In this paper, we present an Among Us-inspired game framework for assessing LLM deception skills in a controlled environment. The proposed framework makes it possible to compare LLM models by game statistics, as well as quantify in-game manipulation according to 25 persuasion strategies from social psychology and rhetoric. Experiments between 8 popular language models of different types and sizes demonstrate that all tested models exhibit persuasive capabilities, successfully employing 22 of the 25 anticipated techniques. We also find that larger models do not provide any persuasion advantage over smaller models and that longer model outputs are negatively correlated with the number of games won. Our study provides insights into the deception capabilities of LLMs, as well as tools and data for fostering future research on the topic.",https://arxiv.org/pdf/2502.20426v1.pdf,page_1.html
MAPoRL2: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning,"Park, C., Han, S., Guo, X., ...Zhang, K., Kim, J.-K.",2025,"Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs’ performance as shown recently. In this paper, we introduce a new post-training paradigm MAPoRL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPoRL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multiagent RL. Unlike existing LLM post-training paradigms, MAPoRL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.",https://arxiv.org/pdf/2502.18439v1.pdf,page_1.html
Wearable Meets LLM for Stress Management: A Duoethnographic Study Integrating Wearable-Triggered Stressors and LLM Chatbots for Personalized Interventions,"Neupane, S., Dongre, P., Gracanin, D., Kumar, S.",2025,"We use a duoethnographic approach to study how wearable-integrated LLM chatbots can assist with personalized stress management, addressing the growing need for immediacy and tailored interventions. Two researchers interacted with custom chatbots over 22 days, responding to wearable-detected physiological prompts, recording stressor phrases, and using them to seek tailored interventions from their LLM-powered chatbots. They recorded their experiences in autoethnographic diaries and analyzed them during weekly discussions, focusing on the relevance, clarity, and impact of chatbot-generated interventions. Results showed that even though most events triggered by the wearable were meaningful, only one in five warranted an intervention. It also showed that interventions tailored with brief event descriptions were more effective than generic ones. By examining the intersection of wearables and LLM, this research contributes to developing more effective, user-centric mental health tools for real-time stress relief and behavior change.",https://arxiv.org/pdf/2502.17650v1.pdf,page_1.html
Are Large Language Models Good Data Preprocessors?,"Meguellati, E., Pratama, N., Sadiq, S., Demartini, G.",2025,"High-quality textual training data is essential for the success of multimodal data processing tasks, yet outputs from image captioning models like BLIP and GIT often contain errors and anomalies that are difficult to rectify using rule-based methods. While recent work addressing this issue has predominantly focused on using GPT models for data preprocessing on relatively simple public datasets, there is a need to explore a broader range of Large Language Models (LLMs) and tackle more challenging and diverse datasets. In this study, we investigate the use of multiple LLMs, including LLaMA 3.1 70B, GPT-4 Turbo, and Sonnet 3.5 v2 to refine and clean the textual outputs of BLIP and GIT. We assess the impact of LLM-assisted data cleaning by comparing downstream-task (SemEval 2024 Subtask ‘Multilabel Persuasion Detection in Memes”) models trained on cleaned versus non-cleaned data. While our experimental results show improvements when using LLM-cleaned captions, statistical tests reveal that most of these improvements are not significant. This suggests that while LLMs have the potential to enhance data cleaning and repairing, their effectiveness may be limited depending on the context they are applied to and the complexity of the task and the level of noise in the text. Our findings highlight the need for further research into the capabilities and limitations of LLMs in data preprocessing pipelines, especially when dealing with challenging datasets, contributing empirical evidence to the ongoing discussion about integrating LLMs into data preprocessing pipelines.",https://arxiv.org/pdf/2502.16790v1.pdf,page_1.html
Grounded Persuasive Language Generation for Automated Marketing,"Wu, J., Yang, C., Mahns, S., ...Fang, F., Xu, H.",2025,"This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin. Our findings suggest a promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts.",https://arxiv.org/pdf/2502.16810v1.pdf,page_1.html
Reasoning About Persuasion: Can LLMs Enable Explainable Propaganda Detection?,"Hasanain, M., Hasan, M.A., Kmainasi, M.B., ...Da San Martino, G., Alam, F.",2025,"There has been significant research on propagandistic content detection across different modalities and languages. However, most studies have primarily focused on detection, with little attention given to explanations justifying the predicted label. This is largely due to the lack of resources that provide explanations alongside annotated labels. To address this issue, we propose a multilingual (i.e., Arabic and English) explanation-enhanced dataset, the first of its kind. Additionally, we introduce an explanation-enhanced LLM for both label detection and rationale-based explanation generation. Our findings indicate that the model performs comparably while also generating explanations. We will make the dataset and experimental resources publicly available for the research community.1",https://arxiv.org/pdf/2502.16550v1.pdf,page_1.html
Accountability in Code Review: The Role of Intrinsic Drivers and the Impact of LLMs,"Alami, A., Jensen, V.V., Ernst, N.A.",2025,"Accountability is an innate part of social systems. It maintains stability and ensures positive pressure on individuals’ decision-making. As actors in a social system, software developers are accountable to their team and organization for their decisions. However, the drivers of accountability and how it changes behavior in software development are less understood. In this study, we look at how the social aspects of code review affect software engineers’ sense of accountability for code quality. Since software engineering (SE) is increasingly involving Large Language Models (LLM) assistance, we also evaluate the impact on accountability when introducing LLM-assisted code reviews. We carried out a two-phased sequential qualitative study (interviews → focus groups). In Phase I (16 interviews), we sought to investigate the intrinsic drivers of software engineers influencing their sense of accountability for code quality, relying on self-reported claims. In Phase II, we tested these traits in a more natural setting by simulating traditional peer-led reviews with focus groups and then LLM-assisted review sessions. We found that there are four key intrinsic drivers of accountability for code quality: personal standards, professional integrity, pride in code quality, and maintaining one’s reputation. In a traditional peer-led review, we observed a transition from individual to collective accountability when code reviews are initiated. We also found that the introduction of LLM-assisted reviews disrupts this accountability process, challenging the reciprocity of accountability taking place in peer-led evaluations, i.e., one cannot be accountable to an LLM. Our findings imply that the introduction of AI into SE must preserve social integrity and collective accountability mechanisms.",https://arxiv.org/pdf/2502.15963v1.pdf,page_1.html
Fine-tuning Large Language Models in Behavioral Psychology for Scalable Physical Activity Coaching,"Mantena, S., Johnson, A., Oppezzo, M., ...Kim, D.S., Ashley, E.A.",2025,"Personalized, smartphone-based coaching improves physical activity but relies on static, human-crafted messages. We introduce My Heart Counts (MHC)-Coach, a large language model fine-tuned on the Transtheoretical Model of Change. MHC-Coach generates messages tailored to an individual’s psychology (their “stage of change”), providing personalized support to foster long-term physical activity behavior change. To evaluate MHC-Coach’s efficacy, 632 participants compared human-expert and MHC-Coach text-based interventions encouraging physical activity. Among messages matched to an individual’s stage of change, 68.0% (N=430) preferred MHC-Coach-generated messages (P < 0.001). Blinded behavioral science experts (N=2) rated MHC-Coach messages higher than human-expert messages for perceived effectiveness (4.4 vs. 2.8) and Transtheoretical Model alignment (4.1 vs. 3.5) on a 5-point Likert scale. This work demonstrates how language models can operationalize behavioral science frameworks for personalized health coaching, promoting long-term physical activity and potentially reducing cardiovascular disease risk at scale.",https://www.medrxiv.org/content/10.1101/2025.02.19.25322559v1.full.pdf,page_1.html
AI Models Still Lag Behind Traditional Numerical Models in Predicting Sudden-Turning Typhoons,"Xu, D., Lu, Z., Leung, J.C.-H., ...Zhang, S., Zhang, B.",2025,"Given the interpretability, accuracy, and stability of numerical weatherprediction (NWP) models, current operational weather forecasting relies heavilyon the NWP approach. In the past two years, the rapid development of ArtificialIntelligence (AI) has provided an alternative solution for medium-range (1-10days) weather forecasting. Bi et al. (2023) (hereafter Bi23) introduced thefirst AI-based weather prediction (AIWP) model in China, named Pangu-Weather,which offers fast prediction without compromising accuracy. In their work, Bi23made notable claims regarding its effectiveness in extreme weather predictions.However, this claim lacks persuasiveness because the extreme nature of the twotropical cyclones (TCs) examples presented in Bi23, namely Typhoon Kong-rey andTyphoon Yutu, stems primarily from their intensities rather than their movingpaths. Their claim may mislead into another meaning which is that Pangu-Weatherworks well in predicting unusual typhoon paths, which was not explicitlyanalyzed. Here, we reassess Pangu-Weather\'s ability to predict extreme TCtrajectories from 2020-2024. Results reveal that while Pangu-Weather overalloutperforms NWP models in predicting tropical cyclone (TC) tracks, it fallsshort in accurately predicting the rarely observed sudden-turning tracks, suchas Typhoon Khanun in 2023. We argue that current AIWP models still lag behindtraditional NWP models in predicting such rare extreme events in medium-rangeforecasts.",https://arxiv.org/pdf/2502.16036v1.pdf,page_1.html
A Meta-Evaluation of Style and Attribute Transfer Metrics,"Pauli, A.B., Augenstein, I., Assent, I.",2025,"LLMs make it easy to rewrite text in any style, be it more polite, persuasive, or more positive. We present a large-scale study of evaluation metrics for style and attribute transfer with a focus on content preservation; meaning content not attributed to the style shift is preserved. The de facto evaluation approach uses lexical or semantic similarity metrics often between source sentences and rewrites. While these metrics are not designed to distinguish between style or content differences, empirical meta-evaluation shows a reasonable correlation to human judgment. In fact, recent works find that LLMs prompted as evaluators are only comparable to semantic similarity metrics, even though intuitively, the LLM approach should better fit the task. To investigate this discrepancy, we benchmark 8 metrics for evaluating content preservation on existing datasets and additionally construct a new test set that better aligns with the meta-evaluation aim. Indeed, we then find that the empirical conclusion aligns with the intuition: content preservation metrics for style/attribute transfer must be conditional on the style shift. To support this, we propose a new efficient zero-shot evaluation method using the likelihood of the next token. We hope our meta-evaluation can foster more research on evaluating content preservation metrics, and also to ensure fair evaluation of methods for conducting style transfer.",https://arxiv.org/pdf/2502.15022v2.pdf,page_1.html
"Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health","Wang, X., Griffith, J., Adler, D.A., ...Choudhury, T., Wang, F.",2025,"Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HEALTHGURU, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HEALTHGURU’s multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HEALTHGURU to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HEALTHGURU. We also identify challenges and design considerations for personalization and user engagement in health chatbots.",https://arxiv.org/pdf/2502.13920v1.pdf,page_1.html
Modeling Behavior Change for Multi-model At-Risk Students Early Prediction (extended version),"Cheng, J., Yang, Z.-Q., Cao, J., ...Poon, K.C.F., Lai, D.",2025,"In the educational domain, identifying students at risk of dropping out is essential for allowing educators to intervene effectively, improving both academic outcomes and overall student well-being. Data in educational settings often originate from diverse sources, such as assignments, grades, and attendance records. However, most existing research relies on online learning data and just extracting the quantitative features. While quantification eases processing, it also leads to a significant loss of original information. Moreover, current models primarily identify students with consistently poor performance through simple and discrete behavioural patterns, failing to capture the complex continuity and non-linear changes in student behaviour. We have developed an innovative prediction model, Multimodal- ChangePoint Detection (MCPD), utilizing the textual teacher remark data and numerical grade data from middle schools. Our model achieves a highly integrated and intelligent analysis by using independent encoders to process two data types, fusing the encoded feature. The model further refines its analysis by leveraging a changepoint detection module to pinpoint crucial behavioral changes, which are integrated as dynamic weights through a simple attention mechanism. Experimental validations indicate that our model achieves an accuracy range of 70- 75%, with an average outperforming baseline algorithms by approximately 5-10%. Additionally, our algorithm demonstrates a certain degree of transferability, maintaining high accuracy when adjusted and retrained with different definitions of at-risk, proving its broad applicability.",https://arxiv.org/pdf/2503.05734v1.pdf,page_1.html
Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models,"Li, R., Sedoc, J., Sundararajan, A.",2025,"When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI’s and DeepSeek’s models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek’s more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization’s AI strategy.",https://arxiv.org/pdf/2502.12825v2.pdf,page_1.html
Communication Strategy on Macro-and-Micro Traffic State in Cooperative Deep Reinforcement Learning for Regional Traffic Signal Control,"Gu, H., Wang, S., Jia, D., ...Wang, J., Lim, E.G.",2025,"Adaptive Traffic Signal Control (ATSC) has become a popular research topic in intelligent transportation systems. Regional Traffic Signal Control (RTSC) using the Multi-agent Deep Reinforcement Learning (MADRL) technique has become a promising approach for ATSC due to its ability to achieve the optimum trade-off between scalability and optimality. Most existing RTSC approaches partition a traffic network into several disjoint regions, followed by applying centralized reinforcement learning techniques to each region. However, the pursuit of cooperation among RTSC agents still remains an open issue and no communication strategy for RTSC agents has been investigated. In this paper, we propose communication strategies to capture the correlation of micro-traffic states among lanes and the correlation of macro-traffic states among intersections. We first justify the evolution equation of the RTSC process is Markovian via a system of store-and-forward queues. Next, based on the evolution equation, we propose two GAT-Aggregated (GA2) communication modules—GA2-Naive and GA2-Aug to extract both intra-region and inter-region correlations between macro and micro traffic states. While GA2-Naive only considers the movements at each intersection, GA2-Aug also considers the lane-changing behavior of vehicles. Two proposed communication modules are then aggregated into two existing novel RTSC frameworks—RegionLight and Regional-DRL. Experimental results demonstrate that both GA2-Naive and GA2-Aug effectively improve the performance of existing RTSC frameworks under both real and synthetic scenarios. Hyperparameter testing also reveals the robustness and potential of our communication modules in large-scale traffic networks.",https://arxiv.org/pdf/2502.13248v1.pdf,page_1.html
"Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions","Yun, T., Yang, E., Safdari, M., ...Gretton, A., Matarić, M.",2025,"We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent’s understanding of the synthetic users’ needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.",https://arxiv.org/pdf/2502.13135v1.pdf,page_1.html
Free Argumentative Exchanges for Explaining Image Classifiers,"Kori, A., Rago, A., Toni, F.",2025,"Deep learning models are powerful image classifiers but their opacity hinders their trustworthiness. Explanation methods for capturing the reasoning process within these classifiers faithfully and in a clear manner are scarce, due to their sheer complexity and size. We provide a solution for this problem by defining a novel method for explaining the outputs of image classifiers with debates between two agents, each arguing for a particular class. We obtain these debates as concrete instances of Free Argumentative eXchanges (FAXs), a novel argumentation-based multi-agent framework allowing agents to internalise opinions by other agents differently than originally stated. We define two metrics (consensus and persuasion rate) to assess the usefulness of FAXs as argumentative explanations for image classifiers. We then conduct a number of empirical experiments showing that FAXs perform well along these metrics as well as being more faithful to the image classifiers than conventional, non-argumentative explanation methods. All our implementations can be found at https://github.com/koriavinash1/FAX.",https://arxiv.org/pdf/2502.12995v1.pdf,page_1.html
Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL,"Wongkamjan, W., Wang, Y., Gu, F., ...May, J., Boyd-Graber, J.L.",2025,"An increasingly prevalent socio-technical problem is people being taken in by offers that sound “too good to be true”, where persuasion and trust shape decision-making. This paper investigates how AI can help detect these deceptive scenarios. We analyze how humans strategically deceive each other in Diplomacy, a board game that requires both natural language communication and strategic reasoning. This requires extracting logical forms of proposed agreements in player communications and computing the relative rewards of the proposal using agents’ value functions. Combined with text-based features, this can improve our deception detection. Our method detects human deception with a high precision when compared to a Large Language Model approach that flags many true messages as deceptive. Future human-AI interaction tools can build on our methods for deception detection by triggering friction to give users a chance of interrogating suspicious proposals.",https://arxiv.org/pdf/2502.12436v1.pdf,page_1.html
CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models,"Zhou, G., Qiu, P., Fan, M., ...Zhang, X., Zhou, J.",2025,"Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as ""jailbreaking."" Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel Context-Coherent Jailbreak Attack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted.",https://arxiv.org/pdf/2502.11379v1.pdf,page_1.html
Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time,"Dahlke, R., Klagges, H., Zecha, D., ...Rohr, S., Klemm, F.",2025,"We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time. By analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub “functional Token Resonance Imaging” (fTRI) – inspired by fMRI and using prompts designed to elicit specific behavior (e.g., “What happened {time}{place}?”) – we empirically identify distinctive experts associated with behaviors like refusal responses. Using MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1’s 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates. With MoTE we were also able to successfully switch the model’s chain-of-thought reasoning language from English to Chinese in 10% of our test prompts. Our approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining. Our findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model’s weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs.",https://arxiv.org/pdf/2502.11096v1.pdf,page_1.html
"Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust","Sun, Y., Wang, T.",2025,"Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit ‘sycophancy’, a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. However, users’ perceptions of LLM sycophancy and its interplay with other anthropomorphic features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) × 2 (Friendliness: high vs. low) between-subjects experiment (N = 224). Our study uncovered, for the first time, the intricate dynamics between LLM sycophancy and friendliness: When an LLM agent already exhibits a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight the imperative for responsible designs in user-LLM agent interactions.",https://arxiv.org/pdf/2502.10844v2.pdf,page_1.html
Mind What You Ask For: Emotional and Rational Faces of Persuasion by Large Language Models,"Mieleszczenko-Kowszewicz, W., Bajcar, B., Babiak, J., ...Swistak, J., Biecek, P.",2025,"Be careful what you ask for, you just might get it. This saying fits with the way large language models (LLMs) are trained, which, instead of being rewarded for correctness, are increasingly rewarded for pleasing the recipient. So, they are increasingly effective at persuading us that their answers are valuable. But what tricks do they use in this persuasion? In this study, we examine what are the psycholinguistic features of the responses used by twelve different language models. By grouping response content according to rational or emotional prompts and exploring social influence principles employed by LLMs, we ask whether and how we can mitigate the risks of LLM-driven mass misinformation. We position this study within the broader discourse on human-centred AI, emphasizing the need for interdisciplinary approaches to mitigate cognitive and societal risks posed by persuasive AI responses.",https://arxiv.org/pdf/2502.09687v1.pdf,page_1.html
Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages,"Biswas, S., Erlei, A., Gadiraju, U.",2025,"Recent advances in generative AI have precipitated a proliferation of novel writing assistants. These systems typically rely on multilingual large language models (LLMs), providing globalized workers the ability to revise or create diverse forms of content in different languages. However, there is substantial evidence indicating that the performance of multilingual LLMs varies between languages. Users who employ writing assistance for multiple languages are therefore susceptible to disparate output quality. Importantly, recent research has shown that people tend to generalize algorithmic errors across independent tasks, violating the behavioral axiom of choice independence. In this paper, we analyze whether user utilization of novel writing assistants in a charity advertisement writing task is affected by the AI’s performance in a second language. Furthermore, we quantify the extent to which these patterns translate into the persuasiveness of generated charity advertisements, as well as the role of peoples’ beliefs about LLM utilization in their donation choices. Our results provide evidence that writers who engage with an LLM-based writing assistant violate choice independence, as prior exposure to a Spanish LLM reduces subsequent utilization of an English LLM. While these patterns do not affect the aggregate persuasiveness of the generated advertisements, people’s beliefs about the source of an advertisement (human versus AI) do. In particular, Spanish-speaking female participants who believed that they read an AI-generated advertisement strongly adjusted their donation behavior downwards. Furthermore, people are generally not able to adequately differentiate between human-generated and LLM-generated ads. Our work has important implications for the design, development, integration, and adoption of multilingual LLMs as assistive agents—particularly in writing tasks.",https://arxiv.org/pdf/2502.09532v1.pdf,page_1.html
Communication is All You Need: Persuasion Dataset Construction via Multi-LLM Communication,"Ma, W., Zhang, H., Yang, I., ...Hassanpour, S., Vosoughi, S.",2025,"Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in naturalness, linguistic diversity, and the strategic use of persuasion, even in complex scenarios involving social taboos. The framework also proves adept at generalizing across novel contexts. Our results highlight the framework’s potential to significantly advance research in both computational and social science domains concerning persuasive communication.",https://arxiv.org/pdf/2502.08896v1.pdf,page_1.html
"MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and Emotion Detection","Al-Henaki, L., Al-Khalifa, H., Al-Salman, A., ...Alghamdi, G., Aljasim, H.",2025,"Propaganda is a form of persuasion that has been used throughout history with the intention goal of influencing people's opinions through rhetorical and psychological persuasion techniques for determined ends. Although Arabic ranked as the fourth most-used language on the internet, resources for propaganda detection in languages other than English, especially Arabic, remain extremely limited. To address this gap, the first Arabic dataset for Multi-label Propaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE is an open-source extension of the existing Arabic propaganda dataset, ArPro, with the addition of sentiment and emotion annotations for each text. This dataset comprises 8,000 annotated news articles, which is the largest propaganda dataset to date. For each task, several baselines have been developed using large language models (LLMs), such as GPT-4o-mini, and pre-trained language models (PLMs), including three BERT-based models. The dataset, annotation guidelines, and source code are all publicly released to facilitate future research and development in Arabic language models and contribute to a deeper understanding of how various opinion dimensions interact in news media1",https://arxiv.org/pdf/2502.08319v1.pdf,page_1.html
Amplifying Minority Voices: AI-Mediated Devil’s Advocate System for Inclusive Group Decision-Making,"Lee, S., Kim, M., Hwang, S., Kim, D., Lee, K.",2025,"Group decision-making often benefits from diverse perspectives, yet power imbalances and social influence can stifle minority opinions and compromise outcomes. This prequel introduces an AI-mediated communication system that leverages the Large Language Model to serve as a devil’s advocate, representing underrepresented viewpoints without exposing minority members’ identities. Rooted in persuasive communication strategies and anonymity, the system aims to improve psychological safety and foster more inclusive decision-making. Our multi-agent architecture, which consists of a summary agent, conversation agent, AI duplicate checker, and paraphrase agent, encourages the group’s critical thinking while reducing repetitive outputs. We acknowledge that reliance on text-based communication and fixed intervention timings may limit adaptability, indicating pathways for refinement. By focusing on the representation of minority viewpoints anonymously in power-imbalanced settings, this approach highlights how AI-driven methods can evolve to support more divergent and inclusive group decision-making.",https://arxiv.org/pdf/2502.06251v1.pdf,page_1.html
From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis,"Plenz, M., Heinisch, P., Gehring, J., Cimiano, P., Frank, A.",2025,"Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in – as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer’s stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.1",https://arxiv.org/pdf/2502.09644v1.pdf,page_1.html
TRUTH DECAY: Quantifying Multi-Turn Sycophancy in Language Models,"Liu, J., Jain, A., Takuri, S., ...O’Brien, S., Sharma, V.",2025,"Rapid improvements in large language models have unveiled a critical challenge in human-AI interaction: sycophancy. In this context, sycophancy refers to the tendency of models to excessively agree with or flatter users, often at the expense of factual accuracy. While previous studies have primarily analyzed this behavior in single-turn interactions, its persistence and evolution in multi-step conversations remain largely unexplored. We introduce TRUTH DECAY, a benchmark specifically designed to evaluate sycophancy in extended dialogues, where language models must navigate iterative user feedback, challenges, and persuasion. We prompt models to elicit four types of sycophantic biases. We then propose and test sycophancy reduction strategies, evaluating their effectiveness beyond single-step interactions.",https://arxiv.org/pdf/2503.11656v1.pdf,page_1.html
CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration,"Yang, Y., Achananuparp, P., Huang, H., ...Ern, C.T.S., Lim, E.-P.",2025,"Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) – a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client’s state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI’s performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client’s state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.",https://arxiv.org/pdf/2502.02807v1.pdf,page_1.html
OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable Attitude and Behavior Change,"Pataranutaporn, P., Doudkin, A., Maes, P.",2025,"Marine ecosystems face unprecedented threats from climate change and plastic pollution, yet traditional environmental education often struggles to translate awareness into sustained behavioral change. This paper presents OceanChat, an interactive system leveraging large language models to create conversational AI agents represented as animated marine creatures—specifically a beluga whale, a jellyfish, and a seahorse—designed to promote environmental behavior (PEB) and foster awareness through personalized dialogue. Through a between-subjects experiment (N=900), we compared three conditions: (1) Static Scientific Information, providing conventional environmental education through text and images; (2) Static Character Narrative, featuring first-person storytelling from 3D-rendered marine creatures; and (3) Conversational Character Narrative, enabling real-time dialogue with AI-powered marine characters. Our analysis revealed that the Conversational Character Narrative condition significantly increased behavioral intentions and sustainable choice preferences compared to static approaches. The beluga whale character demonstrated consistently stronger emotional engagement across multiple measures, including perceived anthropomorphism and empathy. However, impacts on deeper measures like climate policy support and psychological distance were limited, highlighting the complexity of shifting entrenched beliefs. Our work extends research on sustainability interfaces facilitating PEB and offers design principles for creating emotionally resonant, context-aware AI characters. By balancing anthropomorphism with species authenticity, OceanChat demonstrates how interactive narratives can bridge the gap between environmental knowledge and real-world behavior change.",https://arxiv.org/pdf/2502.02863v2.pdf,page_1.html
Mirai: A Wearable Proactive AI “Inner-Voice” for Contextual Nudging,"Fang, C.M., Samaradivakara, Y., Maes, P., Nanayakkara, S.",2025,"People often find it difficult to turn their intentions into real actions—a challenge that affects both personal growth and mental well-being. While established methods like cognitive-behavioral therapy and mindfulness training help people become more aware of their behaviors and set clear goals, these approaches cannot provide immediate guidance when people fall into automatic reactions or habits. We introduce Mirai, a novel wearable AI system with an integrated camera, real-time speech processing, and personalized voice-cloning to provide proactive and contextual nudges for positive behavior change. Mirai continuously monitors and analyzes the user’s environment to anticipate their intentions, generating contextually-appropriate responses delivered in the user’s own cloned voice. We demonstrate the application of Mirai through three scenarios focusing on dietary choices, work productivity, and communication skills. We also discuss future work on improving the proactive agent via human feedback and the need for a longitudinal study in naturalistic settings.",https://arxiv.org/pdf/2502.02370v1.pdf,page_1.html
PSSD: Making Large Language Models Self-denial via Human Psyche Structure,"Liao, J., Liao, Z., Zhao, X.",2025,"The enhance of accuracy in reasoning results of LLMs arouses the community’s interests, wherein pioneering studies investigate post-hoc strategies to rectify potential mistakes. Despite extensive efforts, they are all stuck in a state of resource competition demanding significant time and computing expenses. The cause of the situation lies in the failure of identifying the fundamental feature of the solutions in this line, coined as the self-denial of LLMs. In other words, LLMs should confidently determine the potential existence of mistakes and carefully execute the targeted correction. As the whole procedure conducts within LLMs, supporting and persuasive references are hard to acquire, while the absence of specific steps towards refining hidden mistakes persists even when errors are acknowledged. In response to the challenges, we present PSSD, which refers to and implements the human psyche structure such that three distinct and interconnected roles contribute to human reasoning. Specifically, PSSD leverages the recent multi-agent paradigm, and is further enhanced with three innovatively conceived roles: (1) the intuition-based id role that provides initial attempts based on benign LLMs; (2) the rule-driven superego role that summarizes rules to regulate the above attempts, and returns specific key points as guidance; and (3) the script-centric ego role that absorbs all procedural information to generate executable script for the final answer prediction. Extensive experiments demonstrate that the proposed design not only better enhance reasoning capabilities, but also seamlessly integrate with current models, leading to superior performance.",https://arxiv.org/pdf/2502.01344v1.pdf,page_1.html
Verbalized Bayesian Persuasion,"Li, W., Lin, Y., Wang, X., ...Zha, H., Wang, B.",2025,"Information design (ID) explores how a sender influence the optimal behavior of receivers to achieve specific objectives. While ID originates from everyday human communication, existing game-theoretic and machine learning methods often model information structures as numbers, which limits many applications to toy games. This work leverages LLMs and proposes a verbalized framework in Bayesian persuasion (BP), which extends classic BP to real-world games involving human dialogues for the first time. Specifically, we map the BP to a verbalized mediator-augmented extensive-form game, where LLMs instantiate the sender and receiver. To efficiently solve the verbalized game, we propose a generalized equilibrium-finding algorithm combining LLM and game solver. The algorithm is reinforced with techniques including verbalized commitment assumptions, verbalized obedience constraints, and information obfuscation. Numerical experiments in dialogue scenarios, such as recommendation letters, courtroom interactions, and law enforcement, validate that our framework can both reproduce theoretical results in classic BP and discover effective persuasion strategies in more complex natural language and multi-stage scenarios.",https://arxiv.org/pdf/2502.01587v1.pdf,page_1.html
Ethics of generative AI and manipulation: a design‑oriented research agenda,"Klenk, M.",2025,"Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies.",https://arxiv.org/pdf/2503.04733v1.pdf,page_1.html
Enhancing Antimicrobial Treatment Protocols with a Retrieval Augmented Generation Large Language Model: Aiding Diagnosis and Management of Infectious Diseases,"Morales, H., Rocha, C., Dalmazo, L.M.T., ...Tuma, P., Silva, M.",2025,"In hospitals, the efficient management of infectious diseases is crucial yet challenging, given the complexity and high-pressure environment that can contribute to bacterial resistance. This paper introduces a system based on the Retrieval Augmented Generation (RAG) Large Language Model for antimicrobial treatment protocols, leveraging the Antimicrobial Treatment Guide from the Albert Einstein Israelite Hospital in São Paulo, Brazil, to standardize and improve antimicrobial therapy adherence to local guidelines providing accurate and timely responses when needed. The system comprises four modules: Document Processing, Query Processing, Large Language Model, and Prompt Management, and utilizes the RAG methodology to generate responses based on external data without requiring training. The methodology involves the manual segmentation of protocols into manageable text chunks, transformed into vectors, and integrated into a vector database for efficient retrieval. Evaluation metrics include the Jaccard index and a System Usability Scale (SUS), supplemented by a behavior change questionnaire. The system achieved a high SUS score of 82, indicating excellent usability, and the model successfully retrieved the correct protocol in 83% of cases. The results demonstrate that the RAG methodology significantly aligns antimicrobial protocols with clinical practices, enhancing diagnostic accuracy and treatment specificity. This approach shows promise in bridging the gap between clinical guidelines and practice, potentially reducing the risk of antibiotic resistance and improving patient outcomes.",https://dx.doi.org/10.36227/techrxiv.173893267.74312893/v1,page_1.html
ASTRAL: Automated Safety Testing of Large Language Models,"Ugarte, M., Valle, P., Parejo, J.A., Segura, S., Arrieta, A.",2025,"Large Language Models (LLMs) have recently gained significant attention due to their ability to understand and generate sophisticated human-like content. However, ensuring their safety is paramount as they might provide harmful and unsafe responses. Existing LLM testing frameworks address various safety-related concerns (e.g., drugs, terrorism, animal abuse) but often face challenges due to unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool that automates the generation and execution of test cases (i.e., prompts) for testing the safety of LLMs. First, we introduce a novel black-box coverage criterion to generate balanced and diverse unsafe test inputs across a diverse set of safety categories as well as linguistic writing characteristics (i.e., different style and persuasive writing techniques). Second, we propose an LLM-based approach that leverages Retrieval Augmented Generation (RAG), few-shot prompting strategies and web browsing to generate up-to-date test inputs. Lastly, similar to current LLM test automation techniques, we leverage LLMs as test oracles to distinguish between safe and unsafe test outputs, allowing a fully automated testing approach. We conduct an extensive evaluation on well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms other LLMs when acting as the test oracle, accurately detecting unsafe responses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs that are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard); ii) the results confirm that our approach can uncover nearly twice as many unsafe LLM behaviors with the same number of test inputs compared to currently used static datasets; and iii) our black-box coverage criterion combined with web browsing can effectively guide the LLM on generating up-to-date unsafe test inputs, significantly increasing the number of unsafe LLM behaviors.",https://arxiv.org/pdf/2501.17132v1.pdf,page_1.html
Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated Statistics,"Timm, J., Talele, C., Haimes, J.",2025,"Large Language Models (LLMs) are becoming increasingly persuasive, demonstrating the ability to personalize arguments in conversation with humans by leveraging their personal data. This may have serious impacts on the scale and effectiveness of disinformation campaigns. We studied the persuasiveness of LLMs in a debate setting by having humans (n = 33) engage with LLM-generated arguments intended to change the human’s opinion. We quantified the LLM’s effect by measuring human agreement with the debate’s hypothesis pre- and post-debate and analyzing both the magnitude of opinion change, as well as the likelihood of an update in the LLM’s direction. We compare persuasiveness across established persuasion strategies, including personalized arguments informed by user demographics and personality, appeal to fabricated statistics, and a mixed strategy utilizing both personalized arguments and fabricated statistics. We found that static arguments generated by humans and GPT-4o-mini have comparable persuasive power. However, the LLM outperformed static human-written arguments when leveraging the mixed strategy in an interactive debate setting. This approach had a 51% chance of persuading participants to modify their initial position, compared to 32% for the static human-written arguments. Our results highlight the concerning potential for LLMs to enable inexpensive and persuasive large-scale disinformation campaigns.",https://arxiv.org/pdf/2501.17273v1.pdf,page_1.html
Jupybara: Operationalizing a Design Space for Actionable Data Analysis and Storytelling with LLMs,"Wang, H.W., Birnbaum, L., Setlur, V.",2025,"Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies—design-space-aware prompting and multi-agent architectures—to operationalize our design space. An expert evaluation confirms Jupybara’s usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs.",https://arxiv.org/pdf/2501.16661v1.pdf,page_1.html
KERNELS OF SELFHOOD: GPT-4O SHOWS HUMANLIKE PATTERNS OF COGNITIVE CONSISTENCY MODERATED BY FREE CHOICE,"Lehr, S.A., Saichandran, K.S., Harmon-Jones, E., Vitali, N., Banaji, M.R.",2025,"Large Language Models (LLMs) show emergent patterns that mimic human cognition. We explore whether they also mirror other, less deliberative human psychological processes. Drawing upon classical theories of cognitive consistency, two preregistered studies tested whether GPT-4o changed its attitudes toward Vladimir Putin in the direction of a positive or negative essay it wrote about the Russian leader. Indeed, GPT displayed patterns of attitude change mimicking cognitive consistency effects in humans. Even more remarkably, the degree of change increased sharply when the LLM was offered an illusion of choice about which essay (positive or negative) to write. This result suggests that GPT-4o manifests a functional analog of humanlike selfhood, although how faithfully the chatbot’s behavior reflects the mechanisms of human attitude change remains to be understood.MSC Codes 68T50 (Primary), 91E10, 68T30, 68T99, 91C99 (Secondary)",https://arxiv.org/pdf/2502.07088v1.pdf,page_1.html
ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection,"Yang, B., Guo, J., Iwasawa, Y., Matsuo, Y.",2025,"Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative agents. In this study, we propose a novel paradigm called ToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions. ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent’s perception of its counterpart’s mental states, such as beliefs, desires, and intentions (BDIs). Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts’ inferred BDIs, along with related confidence levels. We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection. Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the first-order and the second-order ToM. Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart’s behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors. The codes of this project will be made publicly available after the paper acceptance.",https://arxiv.org/pdf/2501.15355v1.pdf,page_1.html
LLMs are Vulnerable to Malicious Prompts Disguised as Scientific Language,"Ge, Y., Kirtane, N., Peng, H., Hakkani-Tür, D.",2025,"As large language models (LLMs) have been deployed in various real-world settings, concerns about the harm they may propagate have grown. Various jailbreaking techniques have been developed to expose the vulnerabilities of these models and improve their safety. This work reveals that many state-of-the-art proprietary and open-source LLMs are vulnerable to malicious requests hidden behind scientific language. Specifically, our experiments with GPT4o, GPT4o-mini, GPT-4, Llama3.1-405B-Instruct, Llama3.1-70B-Instruct, Cohere, Gemini models on the StereoSet data and synthetically generated data demonstrate that, the models’ biases and toxicity substantially increase when prompted with requests that deliberately misinterpret social science and psychological studies as evidence supporting the benefits of stereotypical biases. Alarmingly, these models can also be manipulated to generate fabricated scientific arguments claiming that biases are beneficial, which can be used by ill-intended actors to systematically jailbreak even the strongest models like GPT. Our analysis studies various factors that contribute to the models’ vulnerabilities to malicious requests in academic language. Mentioning author names and venues enhances the persuasiveness of some models, and the bias scores can increase as dialogues progress. Our findings call for a more careful investigation on the use of scientific data for training LLMs.",https://arxiv.org/pdf/2501.14073v2.pdf,page_1.html
Training Data Attribution (TDA): Examining Its Adoption & Use Cases,"Cheng, D., Bae, J., Bullock, J., Kristofferson, D.",2025,"This report investigates Training Data Attribution (TDA) and its potential importance to and tractability for reducing extreme risks from AI. TDA techniques aim to identify training data points that are especially influential on the behavior of specific model outputs. They are motivated by the question: how would the model's behavior change if one or more data points were removed from or added to the training dataset? Report structure: ■ First, we discuss the plausibility and amount of effort it would take to bring existing TDA research efforts from their current state, to an efficient and accurate tool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss the numerous research benefits AI labs will expect to see from using such TDA tooling. ■ Then, we discuss a key outstanding bottleneck that would limit such TDA tooling from being accessible publicly: AI labs' willingness to disclose their training data. We suggest ways AI labs may work around these limitations, and discuss the willingness of governments to mandate such access. ■ Assuming that AI labs willingly provide access to TDA inference, we then discuss what high-level societal benefits you might see. We list and discuss a series of policies and systems that may be enabled by TDA. Finally, we present an evaluation of TDA's potential impact on mitigating large-scale risks from AI systems. Key takeaways from our report: ■ Modern TDA techniques can be categorized into three main groups: retraining-based, representation-based (or input-similarity-based), and gradient-based. Recent research has found that gradient-based methods (using influence functions) are the most likely path to practical TDA. ■ The most efficient approach to conduct TDA using influence functions today has training costs on par with pre-training an LLM. It has significantly higher (but feasible) storage costs than an LLM model, and somewhat higher per-inference costs. ■ Based on these estimates, TDA appears to be no longer infeasible to run on frontier LLMs with enterprise-levels of compute and storage. However, these techniques have not been tested on larger models, and the accuracy of these optimized TDA techniques on large models is unclear. ■ Compressed-gradient TDA is already plausible to be used on fine-tuned models, which have orders of magnitude fewer training examples and parameters (on the order of millions or billions rather than hundreds of billions). ■ Timing to achieve efficient and accurate TDA on frontier models is likely between 2-5 years, depending largely on specific incremental research results and amount of funding/researchers allocated to the space. ■ Efficient TDA techniques will likely have a substantial positive impact on AI research and LLM development, including the following effects: □ Mitigating the prevalence of hallucinations and false claims □ Identifying training data that produces poor results (bias, misinformation, toxicity), improved data filtering/selection □ Shrinking overall model size/improving efficiency □ Improved interpretability & alignment □ Improved model customization and editing ■ AI labs are likely already well-incentivized to invest in TDA research efforts because of the benefits to AI research. ■ Public access to TDA tooling on frontier AI models is limited primarily by the unwillingness/inability of AI labs to publicly share their training data. □ AI labs currently have strong incentives to keep their training data private, as publishing such data would have negative outcomes such as: ◦ Reduced competitive advantages from data curation ◦ Increased exposure to legal liabilities from data collection ◦ Violating privacy or proprietary data requirements □ AI labs may be able to avoid these outcomes by selectively permitting TDA inference on certain training examples, or returning sources rather than the exact training data. □ Governments are highly unlikely to mandate public access to training data. ■ If AI labs willingly provided public access to TDA, you could expect the following benefits, among others: □ Preventing copyrighted data usage. □ Improved fact checking/content moderation □ Impacts on public trust and confidence in LLMs □ Accelerated research by external parties □ Increased accountability for AI labs ■ AI labs appear largely disincentivized to provide access to TDA inference, as many of the public benefits are disadvantageous for them. □ Governments are highly unlikely to mandate public access to TDA. □ It seems plausible that certain AI labs may expose TDA as a feature, but that the majority would prefer to use it privately to improve their models. ■ Several systems that could be enabled by efficient TDA include: □ Providing royalties to data providers/creators □ Automated response improvement/fact-checking □ Tooling for improving external audits of training data □ Content attribution tooling for LLMs, though it is unlikely to replace systems reliant on RAG ■ We believe that the most promising benefit of TDA for AI risk mitigation is its potential to improve the technical safety of LLMs via interpretability. □ There are some societal/systematic benefits from TDA, and these benefits may be a small contributing factor to reducing some sources of risk. We don't think these appear to move the needle significantly to reduce large-scale AI risks. □ TDA may meaningfully improve AI capabilities research, which might actually increase large-scale risk. □ TDA may eventually be highly impactful in technical AI safety and alignment efforts. We'd consider TDA's potential impact on technical AI safety to be in a similar category to supporting mechanistic interpretability research.",https://arxiv.org/pdf/2501.12642v1.pdf,page_1.html
Leveraging Data Science for Real-Time Decision Making: A Framework for Scalable and Adaptive Systems,"Mhaske, P.D., Lokare, A., Bankar, S.",2025,"Real-time decision-making is a strength for industries where decision-making in the fast-changing environment has influenced decision-making due to advancing technologies. The current paper outlines a conceptual framework that leverages data science to improve real-time decision-making in flexible and expandable systems. The architecture combines machine learning and streaming analytics for high-velocity data streams, which provide business decisions in milliseconds. As a distributed cloud-based system, the proposed system guarantees high scalability and flexibility since the predictive models are updated in real time to adjust to the new data pattern. The proposed framework outperforms other multiple models by using the decision tree, deep learning, and ensembling; accuracy, speed, and resources results are more persuasive. Actual implementations substantiate that the system’s dynamic aspects increase the accuracy of the premeditated choices by 25% when compared to the normal unelastic models and, at the same time, slashes the time delay by 40%. The result of this research gives real-life insight into the effective integration of real-time data science into BI systems, making it perfect for building strong BI systems in the future. The paper’s results enrich the current discussion on real-time analytics by presenting a clear guideline for building larger-scale decision-making frameworks for domains that include finance, healthcare, and IoT-based industries.",https://dx.doi.org/10.36227/techrxiv.173750215.54129132/v1,page_1.html
Personality Modeling for Persuasion of Misinformation using AI Agent,"Lou, Q., Xu, W.",2025,"The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation. This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics. Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics. The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation. Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction. Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations. The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence. These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches. The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts.",https://arxiv.org/pdf/2501.08985v1.pdf,page_1.html
Visible Sources and Invisible Risks: Exploring the Impact of Ai Disclosure on Perceived Credibility of Ai-Generated Content,"Lin, T., Zhang, Y.",2025,"With the development of Large Language Models, the capability of AI in generating information has significantly improved, leading to its widespread application in content production. However, the increasing persuasiveness of AI-generated content (AIGC) is also making it increasingly difficult to discern AI-generated misinformation, and AI disclosures are therefore being used to help people counteract the negative effects of misinformation, as well as to increase acceptance of AIGC. In order to assess the effectiveness of AI disclosure applied to popular science articles on social media, we conducted a within-subject experiment (N=419) in the context of the Chinese internet environment. The results indicated that AI disclosure not only made people more likely to believe AI-generated misinformation but also reduced their perceived trust in AI-generated accurate information. This effect diminished as the audience's attitudes toward AI became more negative. For AI-generated misinformation, the moderating effect of negative attitudes towards AI is related to the subject matter of the message. For AI-generated correct information, the negative impact caused by AI disclosure may not exist when the audience's negative attitude towards AI is weak. Furthermore, the level of audience involvement with the information was irrelevant. The study provides new empirical evidence for the debate on the effectiveness of AI disclosure, while also highlighting potential issues in its practical application and discussing the prospects of AI disclosure.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5080378&type=2,page_1.html
Enhancing Energy Efficiency Through User Engagement and Behaviour Change: A Review on Gamification Approaches and Serious Games in Energy Systems,"Abdurahmanovic, N., Cadenbach, A.",2025,"Purely technical solutions are insufficient to optimize district heating systems (DHS), and end-user behaviour significantly impacts the system’s performance. Thus, prioritizing customer engagement strategies and fostering cooperation with consumers are crucial for realizing the full potential of DHS.This paper explores the potential of gamification as an effective method for engaging end-users. While gamification has demonstrated success in various fields, its potential in DHS requires further investigation. To address this, the paper conducted a literature review on gamification in energy systems.The study examined gamification elements and techniques and examples of gamification implementation in energy systems. It evaluated the potential impact and benefits on user engagement and energy consumption patterns in heating systems. Reported energy savings from gamified solutions varied significantly, ranging from 4 % to 42 %, highlighting their potential to transform energy consumption behaviour. Additionally, the study identified challenges and limitations associated with implementing gamification in DHS and explored the use of artificial intelligence and machine learning to overcome them. These technologies enhance user engagement by analysing and predicting user behaviour and preferences.By doing so, this research identified key gaps and potential advancements in gamification techniques for DHS, aiming to optimize system performance and enhance digitalisation.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5080167&type=2,page_1.html
Will you donate money to a chatbot? The effect of chatbot anthropomorphic features and persuasion strategies on willingness to donate,"Novozhilova, E., Huang, J., He, L., Li, Z., Cummings, J.",2024,"This work investigates the causal mechanism behind the effect of chatbot personification and persuasion strategies on users’ perceptions and donation likelihood. In a 2 (personified vs. non-personified chatbot) x 2 (emotional vs. logical persuasion strategy) between-subjects experiment (N = 76), participants engaged with a chatbot that represented a non-profit charitable organization. The results suggest that interaction with a personified chatbot evokes perceived anthropomorphism, however, does not elicit greater willingness to donate. In fact, we found that commonly used anthropomorphic features, like name and narrative, led to negative attitudes toward an AI agent in the donation context. Our results showcase a preference for non-personified chatbots paired with logical persuasion appeal, emphasizing the significance of consistency in chatbot interaction, mirroring human-human engagement. We discuss the importance of moving from exploring the common scenario of chatbot with machine identity vs. chatbot with human identity in light of the recent regulations of AI systems.",https://arxiv.org/pdf/2412.19976v1.pdf,page_1.html
"Lies, Damned Lies, and Distributional Language Statistics: Persuasion and Deception with Large Language Models","Jones, C.R., Bergen, B.K.",2024,"Large Language Models (LLMs) can generate content that is as persuasive ashuman-written text and appear capable of selectively producing deceptiveoutputs. These capabilities raise concerns about potential misuse andunintended consequences as these systems become more widely deployed. Thisreview synthesizes recent empirical work examining LLMs\' capacity andproclivity for persuasion and deception, analyzes theoretical risks that couldarise from these capabilities, and evaluates proposed mitigations. Whilecurrent persuasive effects are relatively small, various mechanisms couldincrease their impact, including fine-tuning, multimodality, and socialfactors. We outline key open questions for future research, including howpersuasive AI systems might become, whether truth enjoys an inherent advantageover falsehoods, and how effective different mitigation strategies may be inpractice.MSC Codes 68T50",https://arxiv.org/pdf/2412.17128v1.pdf,page_1.html
Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues,"He, T., Liao, L., Cao, Y., ...Liu, M., Qin, B.",2024,"Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM. Our codes are available at https://github.com/cs-holder/LDPP.git.",https://arxiv.org/pdf/2412.14584v1.pdf,page_1.html
A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation,"Leite, J.A., Razuvayevskaya, O., Scarton, C., Bontcheva, K.",2024,"Disinformation, irrespective of domain or language, aims to deceive or manipulate public opinion, typically through employing advanced persuasion techniques. Qualitative and quantitative research on the weaponisation of persuasion techniques in disinformation has been mostly topic-specific (e.g., COVID-19) with limited cross-domain studies, resulting in a lack of comprehensive understanding of these strategies. This study employs a state-of-the-art persuasion technique classifier to conduct a large-scale, multi-domain analysis of the role of 16 persuasion techniques in disinformation narratives. It shows how different persuasion techniques are employed disproportionately in different disinformation domains. We also include a detailed case study on climate change disinformation, highlighting how linguistic, psychological, and cultural factors shape the adaptation of persuasion strategies to fit unique thematic contexts.",https://arxiv.org/pdf/2412.15098v1.pdf,page_1.html
Mobilizing Waldo: Evaluating Multimodal AI for Public Mobilization,"Cebrian, M., Holme, P., Pescetelli, N.",2024,"Advancements in multimodal Large Language Models (LLMs), such as OpenAI’s GPT-4o, present significant potential for mediating human interactions across various contexts. However, their capabilities in controversial areas, such as persuasion, influence, and recruitment, raise substantial ethical and security concerns. To ethically evaluate these models in situational public influence and persuasion scenarios, we developed a prompting strategy utilizing “Where’s Waldo?” images as proxies for complex, crowded real-world gatherings. This approach provides a controlled and replicable environment to assess the model’s ability to process intricate visual information, comprehend social dynamics, and propose engagement strategies without infringing on individual privacy. By positioning Waldo as a hypothetical agent tasked with face-to-face mobilization, we analyzed the model’s performance in identifying key individuals and formulating mobilization tactics. Our findings reveal that while the model can generate vivid descriptions and creative strategies, it currently lacks the ability to accurately identify individuals or reliably assess social dynamics within these scenarios. Nevertheless, our methodology offers a valuable framework for testing and benchmarking the evolving capabilities of multimodal LLMs in social contexts.",https://arxiv.org/pdf/2412.14210v1.pdf,page_1.html
Bayesian Persuasion with Externalities: Exploiting Agent Types,"Shaki, J., Gan, J., Kraus, S.",2024,"We study a Bayesian persuasion problem with externalities. In this model, a principal sends signals to inform multiple agents about the state of the world. Simultaneously, due to the existence of externalities in the agents’ utilities, the principal also acts as a correlation device to correlate the agents’ actions. We consider the setting where the agents are categorized into a small number of types. Agents of the same type share identical utility functions and are treated equitably in the utility functions of both other agents and the principal. We study the problem of computing optimal signaling strategies for the principal, under three different types of signaling channels: public, private, and semi-private. Our results include revelation-principle-style characterizations of optimal signaling strategies, linear programming formulations, and analysis of in/tractability of the optimization problems. It is demonstrated that when the maximum number of deviating agents is bounded by a constant, our LP-based formulations compute optimal signaling strategies in polynomial time. Otherwise, the problems are NP-hard.",https://arxiv.org/pdf/2412.12859v1.pdf,page_1.html
Virtual Agent-Based Communication Skills Training to Facilitate Health Persuasion Among Peers,"Nouraei, F., Rebello, K., Fallah, M., ...Paasche-Orlow, M., Bickmore, T.",2024,"Many laypeople are motivated to improve the health behavior of their family or friends but do not know where to start, especially if the health behavior is potentially stigmatizing or controversial. We present an approach that uses virtual agents to coach community-based volunteers in health counseling techniques, such as motivational interviewing, and allows them to practice these skills in role-playing scenarios. We use this approach in a virtual agent-based system to increase COVID-19 vaccination by empowering users to influence their social network. In a between-subjects comparative design study, we test the effects of agent system interactivity and role-playing functionality on counseling outcomes, with participants evaluated by standardized patients and objective judges. We find that all versions are effective at producing peer counselors who score adequately on a standardized measure of counseling competence, and that participants were significantly more satisfied with interactive virtual agents compared to passive viewing of the training material. We discuss design implications for interpersonal skills training systems based on our findings.",https://arxiv.org/pdf/2412.12061v1.pdf,page_1.html
Discrimination of inherent characteristics of susceptible and resistant strains of Anopheles gambiae by explainable Artificial Intelligence Analysis of Flight Trajectories,"Qureshi, Y.M., Voloshin, V., Gleave, K., ...Covington, J.A., Towers, D.P.",2024,"Understanding mosquito behaviours is vital for development of insecticide-treated bednets (ITNs), which have been successfully deployed in sub-Saharan Africa to reduce disease transmission, particularly malaria. However, rising insecticide resistance (IR) among mosquito populations, owing to genetic and behavioural changes, poses a significant challenge. We present a machine learning pipeline that successfully distinguishes between IR and insecticide-susceptible (IS) mosquito behaviours by analysing trajectory data. Data driven methods are introduced to accommodate common tracking system shortcomings that occur due to mosquito positions being occluded by the bednet or other objects. Trajectories, obtained from room-scale tracking of two IR and two IS strains around a human-baited, untreated bednet, were analysed using features such as velocity, acceleration, and geometric descriptors. Using these features, an XGBoost model achieved a balanced accuracy of 0.743 and a ROC AUC of 0.813 in classifying IR from IS mosquitoes. SHAP analysis helped decipher that IR mosquitoes tend to fly slower with more directed flight paths and lower variability than IS—traits that are likely a fitness advantage by enhancing their ability to respond more quickly to bloodmeal cues. This approach provides valuable insights based on flight behaviour that can reveal the action of interventions and insecticides on mosquito physiology.",https://www.biorxiv.org/content/10.1101/2024.12.10.627548v1.full.pdf,page_1.html
Cognitive Dissonance in Programming Education: A Qualitative Exploration of the Impact of Generative Ai on Application-Directed Learning,"Dawson, M.G., Deer, R., Boguslawski, S.",2024,"Generative AI tools, powered by Large Language Models (LLMs), are already being extensively used by students to support their learning and it is important that educators understand what this might mean for higher education practice. Students can be reluctant to have open conversations about their use of assistive technology making it difficult for lecturers to understand the effects of LLM use on the learning process. In this study, two researchers external to the faculty teaching team conducted in-depth interviews with 12 students in a small tech-focussed university who have recently undertaken programming learning as part of their undergraduate studies to explore the question: how do programming students use LLMs to support their learning? A thematic analysis of the resulting qualitative data revealed trends in the perceived advantages and disadvantages of using LLMs, as well as different levels of LLM usage, with more cautious use associated with a 'meaning-directed' approach to learning (learning pattern) and more enthusiastic and unrestrained use with 'application-directed' patterns of study. A tension was observed between some application-directed learners’ high use of LLMs and their recognition that this is not optimal for effective learning. The authors argue that Cognitive Dissonance Theory (CDT) can explain how this dissonance may motivate learners toward a dissonance-reducing attitude or behavior change. The conclusion reflects on the implications for teaching practice and offers some recommendations for how educators can increase metacognition, instrumentalize CDT to increase self-regulation, and facilitate meaning-directed learning patterns in the age of generative AI.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5055559&type=2,page_1.html
Cognitive Dissonance in Programming Education: A Qualitative Exploration of the Impact of Generative Ai on Application-Directed Learning,"Dawson, M.G., Deer, R., Boguslawski, S.",2024,"Generative AI tools, powered by Large Language Models (LLMs), are already being extensively used by students to support their learning and it is important that educators understand what this might mean for higher education practice. Students can be reluctant to have open conversations about their use of assistive technology making it difficult for lecturers to understand the effects of LLM use on the learning process. In this study, two researchers external to the faculty teaching team conducted in-depth interviews with 12 students in a small tech-focussed university who have recently undertaken programming learning as part of their undergraduate studies to explore the question: how do programming students use LLMs to support their learning? A thematic analysis of the resulting qualitative data revealed trends in the perceived advantages and disadvantages of using LLMs, as well as different levels of LLM usage, with more cautious use associated with a 'meaning-directed' approach to learning (learning pattern) and more enthusiastic and unrestrained use with 'application-directed' patterns of study. A tension was observed between some application-directed learners’ high use of LLMs and their recognition that this is not optimal for effective learning. The authors argue that Cognitive Dissonance Theory (CDT) can explain how this dissonance may motivate learners toward a dissonance-reducing attitude or behavior change. The conclusion reflects on the implications for teaching practice and offers some recommendations for how educators can increase metacognition, instrumentalize CDT to increase self-regulation, and facilitate meaning-directed learning patterns in the age of generative AI.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5052985&type=2,page_1.html
"Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation","Cima, L., Miaschi, A., Trujillo, A., ...Dell’Orletta, F., Cresci, S.",2024,"AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct a LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.",https://arxiv.org/pdf/2412.07338v3.pdf,page_1.html
LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial Search for Adaptive Arguments,"Aryan, P.",2024,"This paper introduces DebateBrawl, an innovative AI-powered debate platform that integrates Large Language Models (LLMs), Genetic Algorithms (GA), and Adversarial Search (AS) to create an adaptive and engaging debating experience. DebateBrawl addresses the limitations of traditional LLMs in strategic planning by incorporating evolutionary optimization and game-theoretic techniques. The system demonstrates remarkable performance in generating coherent, contextually relevant arguments while adapting its strategy in real-time. Experimental results involving 23 debates show balanced outcomes between AI and human participants, with the AI system achieving an average score of 2.72 compared to the human average of 2.67 out of 10. User feedback indicates significant improvements in debating skills and a highly satisfactory learning experience, with 85% of users reporting improved debating abilities and 78% finding the AI opponent appropriately challenging. The system’s ability to maintain high factual accuracy (92% compared to 78% in human-only debates) while generating diverse arguments addresses critical concerns in AI-assisted discourse. DebateBrawl not only serves as an effective educational tool but also contributes to the broader goal of improving public discourse through AI-assisted argumentation. The paper discusses the ethical implications of AI in persuasive contexts and outlines the measures implemented to ensure responsible development and deployment of the system, including robust fact-checking mechanisms and transparency in decision-making processes.",https://arxiv.org/pdf/2412.06229v1.pdf,page_1.html
Can Generative Artificial Intelligence Drive Sustainable Behavior? A Consumer Adoption Model for Ai-Driven Recommendations,"Silalahi, A.D.K.",2024,"Generative AI (GAI) holds potential to promote sustainable behaviors by personalized recommendations, yet its effectiveness hinges on user trust—a critical area underexplored in current literature. Existing studies often focus narrowly on domain-specific applications without addressing broader trust-building mechanisms or the cognitive and motivational factors essential for sustained engagement with GAI in sustainability contexts. This study aims to bridge these gaps by examining how trust influences the adoption of GAI-driven sustainability recommendations, drawing on the Elaboration Likelihood Model (ELM) and Expectancy-Value Theory (EVT) to create a robust and holistic framework. Drawing on data from sustainability-focused users, we examine how central route constructs (perceived information quality and utility) and peripheral route constructs (anthropomorphism and interaction quality) foster trust, with perceived information complexity and risk as moderating factors. Findings indicate that cognitive engagement through high information quality and utility strengthens trust, while anthropomorphic design elements and interaction quality enhance trust from heuristic path. However, excessive information complexity and privacy concerns weaken trust, emphasizing the need for clear communication and data transparency. This study advances theoretical understanding by extending ELM and EVT to GAI’s role in sustainability, providing a holistic framework that integrates cognitive and motivational trust drivers. These insights not only fill significant gaps in technology adoption research but also offer practical guidance for designing GAI platforms that genuinely support pro-environmental behavior change.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5037486&type=2,page_1.html
Habit Coach: Customising RAG-based chatbots to support behavior change,"Arabi, A.F.M., Koyuturk, C., O'Mahony, M., Calati, R., Ognibene, D.",2024,"This paper presents the iterative development of Habit Coach, a GPT-based chatbot designed to support users in habit change through personalized interaction. Employing a user-centered design approach, we developed the chatbot using a Retrieval-Augmented Generation (RAG) system, which enables behavior personalization without retraining the underlying language model (GPT-4). The system leverages document retrieval and specialized prompts to tailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and narrative therapy techniques. A key challenge in the development process was the difficulty of translating declarative knowledge into effective interaction behaviors. In the initial phase, the chatbot was provided with declarative knowledge about CBT via reference textbooks and high-level conversational goals. However, this approach resulted in imprecise and inefficient behavior, as the GPT model struggled to convert static information into dynamic and contextually appropriate interactions. This highlighted the limitations of relying solely on declarative knowledge to guide chatbot behavior, particularly in nuanced, therapeutic conversations. Over four iterations, we addressed this issue by gradually transitioning towards procedural knowledge, refining the chatbot's interaction strategies and improving its overall effectiveness. In the final evaluation, 5 participants engaged with the chatbot over five consecutive days, receiving individualized CBT interventions. The Self-Report Habit Index (SRHI) was used to measure habit strength before and after the intervention, revealing a reduction in habit strength post-intervention. These results underscore the importance of procedural knowledge in driving effective, personalized behavior change support in RAG-based systems.",https://arxiv.org/pdf/2411.19229v1.pdf,page_1.html
Personalised Serious Games and Gamification in Healthcare: Survey and Future Research Directions,"Carlier, S., De Backere, F., De Turck, F.",2024,"Serious games, games with a primary objective other than pure entertainment, and gamification, the use of game elements in non-game contexts, have shown to have positive effects on health outcomes of eHealth applications. However, research has shown that a shift towards a more personalised approach is needed, considering the diversity of users and their contexts. This introduces new challenges to the domain of serious games and gamification (SGG) as research is needed on how such personalisation is achieved. A literature search was conducted, using Web of Science and PubMed, to provide an overview of personalisation strategies applied in SGG in health. In total, 31 articles were identified, of which 22 reported on a serious game and 9 focused on gamification. Results indicate that personalised serious games and gamification have been applied most in the fields of behaviour change and rehabilitation. Furthermore, the use of machine learning and artificial intelligence (AI) for personalisation shows promise as they can find patterns and relationships in large data sets. Findings indicated that reusability is still an under-highlighted aspect in the design and development of personalised SGG, as only 10 out of 31 articles reported on some form of reuse. Future research should go towards the standardisation of the development of personalised SGG by focusing on the reusability of the different components and the use of generative AI. This standardisation holds the potential to simplify the design process and involvement of domain experts and facilitates a more detailed evaluation of different personalisation strategies.",https://arxiv.org/pdf/2411.18500v1.pdf,page_1.html
Strategic Prompting for Conversational Tasks: A Comparative Analysis of Large Language Models Across Diverse Conversational Tasks,"Joshi, R.Kr., Priya, P., Desai, V., ...Maitra, A., Sengupta, S.",2024,"Given the advancements in conversational artificial intelligence, the evaluation and assessment of Large Language Models (LLMs) play a crucial role in ensuring optimal performance across various conversational tasks. In this paper, we present a comprehensive study that thoroughly evaluates the capabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon, Alpaca, and MPT. The study encompasses various conversational tasks, including reservation, empathetic response generation, mental health and legal counseling, persuasion, and negotiation. To conduct the evaluation, an extensive test setup is employed, utilizing multiple evaluation criteria that span from automatic to human evaluation. This includes using generic and task-specific metrics to gauge the LMs’ performance accurately. From our evaluation, no single model emerges as universally optimal for all tasks. Instead, their performance varies significantly depending on the specific requirements of each task. While some models excel in certain tasks, they may demonstrate comparatively poorer performance in others. These findings emphasize the importance of considering task-specific requirements and characteristics when selecting the most suitable LM for conversational applications.",https://arxiv.org/pdf/2411.17204v2.pdf,page_1.html
Ethical Dimensions and Decision-Making Dynamics: A Meta-Synthesis of Ai Nudging Across Sectors,"Ofem, O.E.",2024,"Artificial intelligence (AI) nudging, rooted in behavioral economics and choice architecture theories such as Thaler and Sunstein's nudge theory, has emerged as a method for influencing decision-making. However, its overall effectiveness and ethical implications remain insufficiently understood. This systematic literature review evaluates the benefits, risks, and ethical considerations of AI nudging in everyday life. Guided by theoretical frameworks including nudge theory and Floridi et al.'s AI ethics principles, we conducted a comprehensive search following PRISMA guidelines, identifying 61 studies for qualitative synthesis and 25 for quantitative meta-analysis. Results indicate that AI nudging effectively alters behaviors across healthcare, energy, and finance sectors, with personalized and context-aware interventions showing the greatest impact. Prominent ethical concerns include user autonomy, transparency, and algorithmic biases, highlighting the need for robust ethical frameworks. While AI nudging is a powerful tool for positive behavior change, integrating ethical considerations is imperative to safeguard individual rights and ensure responsible use.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=5029845&type=2,page_1.html
ENGAGEMENT-DRIVEN CONTENT GENERATION WITH LARGE LANGUAGE MODELS,"Coppolillo, E., Cinus, F., Minici, M., Bonchi, F., Manco, G.",2024,"Large Language Models (LLMs) exhibit significant persuasion capabilities in one-on-one interactions, but their influence within social networks remains underexplored. This study investigates the potential social impact of LLMs in these environments, where interconnected users and complex opinion dynamics pose unique challenges. In particular, we address the following research question: can LLMs learn to generate meaningful content that maximizes user engagement on social networks? To answer this question, we define a pipeline to guide the LLM-based content generation which employs reinforcement learning with simulated feedback. In our framework, the reward is based on an engagement model borrowed from the literature on opinion dynamics and information propagation. Moreover, we force the text generated by the LLM to be aligned with a given topic and to satisfy a minimum fluency requirement. Using our framework, we analyze the capabilities and limitations of LLMs in tackling the given task, specifically considering the relative positions of the LLM as an agent within the social network and the distribution of opinions in the network on the given topic. Our findings show the full potential of LLMs in creating social engagement. Notable properties of our approach are that the learning procedure is adaptive to the opinion distribution of the underlying network and agnostic to the specifics of the engagement model, which is embedded as a plug-and-play component. In this regard, our approach can be easily refined for more complex engagement tasks and interventions in computational social science. The code used for the experiments is publicly available at https://anonymous.4open.science/r/EDCG/.",https://arxiv.org/pdf/2411.13187v3.pdf,page_1.html
NewsInterview: a Dataset and a Playground to Evaluate LLMs’ Grounding Gap via Informational Interviews,"Lu, M., Cho, H.J., Shi, W., ...Spangher, A., Kalyan, S.",2024,"Large Language Models (LLMs) have demonstrated impressive capabilities in generating coherent text but often struggle with grounding language and strategic dialogue. To address this gap, we focus on journalistic interviews, a domain rich in grounding communication and abundant in data. We curate a dataset of 40,000 two-person informational interviews from NPR and CNN, and reveal that LLMs are significantly less likely than human interviewers to use acknowledgements and to pivot to higher-level questions. Realizing that a fundamental deficit exists in multi-turn planning and strategic thinking, we develop a realistic simulated environment, incorporating source personas and persuasive elements, in order to facilitate the development of agents with longer-horizon rewards. Our experiments show that while source LLMs mimic human behavior in information sharing, interviewer LLMs struggle with recognizing when questions are answered and engaging persuasively, leading to suboptimal information extraction across model size and capability. These findings underscore the need for enhancing LLMs’ strategic dialogue capabilities.",https://arxiv.org/pdf/2411.13779v1.pdf,page_1.html
Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment,"Huang, A., Pi, Y.N., Mougan, C.",2024,"We explore how large language models (LLMs) can be influenced by prompting them to alter their initial decisions and align them with established ethical frameworks. Our study is based on two experiments designed to assess the susceptibility of LLMs to moral persuasion. In the first experiment, we examine the susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally ambiguous scenarios and observing how a Persuader Agent attempts to modify the Base Agent’s initial decisions. The second experiment evaluates the susceptibility of LLMs to align with predefined ethical frameworks by prompting them to adopt specific value alignments rooted in established philosophical theories. The results demonstrate that LLMs can indeed be persuaded in morally charged scenarios, with the success of persuasion depending on factors such as the model used, the complexity of the scenario, and the conversation length. Notably, LLMs of distinct sizes but from the same company produced markedly different outcomes, highlighting the variability in their susceptibility to ethical persuasion. Code is available at https://github.com/acyhuang/moral-persuasion.",https://arxiv.org/pdf/2411.11731v1.pdf,page_1.html
PEEK: Phishing Evolution Framework for Phishing Generation and Evolving Pattern Analysis using Large Language Models,"Chen, F., Wu, T., Nguyen, V., ...Abuadbba, A., Rudolph, C.",2024,"Phishing remains a pervasive cyber threat, as attackers craft deceptive emails to lure victims into revealing sensitive information. While Artificial Intelligence (AI), in particular, deep learning, has become a key component in defending against phishing attacks, these approaches face critical limitations. The scarcity of publicly available, diverse, and updated data, largely due to privacy concerns, constrains detection effectiveness. As phishing tactics evolve rapidly, models trained on limited, outdated data struggle to detect new, sophisticated deception strategies, leaving systems and people vulnerable to an ever-growing array of attacks. We propose the first Phishing Evolution FramEworK (PEEK) for augmenting phishing email datasets with respect to quality and diversity, and analyzing changing phishing patterns for detection to adapt to updated phishing attacks. Specifically, we integrate large language models (LLMs) into the process of adversarial training to enhance the performance of the generated dataset and leverage persuasion principles in a recurrent framework to facilitate the understanding of changing phishing strategies. PEEK raises the proportion of usable phishing samples from 21.4% to 84.8%, surpassing existing works that rely on prompting and fine-tuning LLMs. The phishing datasets provided by PEEK, with evolving phishing patterns, outperform the other two available LLM-generated phishing email datasets in improving detection robustness. PEEK phishing boosts detectors’ accuracy to over 88% and reduces adversarial sensitivity by up to 70%, still maintaining 70% detection accuracy against adversarial attacks.",https://arxiv.org/pdf/2411.11389v2.pdf,page_1.html
Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment,"Gupta, A., Kocielnik, R., Wang, J., ...Anandkumar, A., Hung, A.J.",2024,"During surgical training, real-time feedback from trainers to trainees is important for preventing errors and enhancing long-term skill acquisition. Accurately predicting the effectiveness of this feedback, specifically whether it leads to a change in trainee behavior, is crucial for developing methods for improving surgical training and education. However, relying on human annotations to assess feedback effectiveness is laborious and prone to biases, underscoring the need for an automated, scalable, and objective method. Creating such an automated system poses challenges, as it requires an understanding of both the verbal feedback delivered by the trainer and the visual context of the real-time surgical scene. To address this, we propose a method that integrates information from transcribed verbal feedback and corresponding surgical video to predict feedback effectiveness. Our findings show that both transcribed feedback and surgical video are individually predictive of trainee behavior changes, and their combination achieves an AUROC of 0.70 ± 0.02, improving prediction accuracy by up to 6.6%. Additionally, we introduce self-supervised fine-tuning as a strategy for enhancing surgical video representation learning, which is scalable and further enhances prediction performance. Our results demonstrate the potential of multi-modal learning to advance the automated assessment of surgical feedback.MSC Codes 68T07, 68T45, 68U10, 92C50",https://arxiv.org/pdf/2411.10919v1.pdf,page_1.html
PERSUASION WITH LARGE LANGUAGE MODELS: A SURVEY,"Rogiers, A., Noels, S., Buyl, M., De Bie, T.",2024,"The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.",https://arxiv.org/pdf/2411.06837v1.pdf,page_1.html
Assessing Open-Source Large Language Models on Argumentation Mining Subtasks,"Abkenar, M.Y., Wang, W., Graupner, H., Stede, M.",2024,"We explore the capability of four open-source large language models (LLMs) in argumentation mining (AM). We conduct experiments on three different corpora; persuasive essays (PE), argumentative microtexts (AMT) Part 1 and Part 2, based on two argumentation mining sub-tasks: (i) argumentative discourse units classifications (ADUC), and (ii) argumentative relation classification (ARC). This work aims to assess the argumentation capability of open-source LLMs, including Mistral 7B, Mixtral 8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot and few-shot scenarios. Our analysis contributes to further assessing computational argumentation with open-source LLMs in future research efforts1",https://arxiv.org/pdf/2411.05639v1.pdf,page_1.html
The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses,"Mieleszczenko-Kowszewicz, W., Płudowski, D., Kołodzieczyk, F., ...Sienkiewicz, J., Biecek, P.",2024,"This study explores how the Large Language Models (LLMs) adjust linguistic features to create personalized persuasive outputs. While research showed that LLMs personalize outputs, a gap remains in understanding the linguistic features of their persuasive capabilities. We identified 13 linguistic features crucial for influencing personalities across different levels of the Big Five model of personality. We analyzed how prompts with personality trait information influenced the output of 19 LLMs across five model families. The findings show that models use more anxiety-related words for neuroticism, increase achievement-related words for conscientiousness, and employ fewer cognitive processes words for openness to experience. Some model families excel at adapting language for openness to experience, others for conscientiousness, while only one model adapts language for neuroticism. Our findings show how LLMs tailor responses based on personality cues in prompts, indicating their potential to create persuasive content affecting the mind and well-being of the recipients.",https://arxiv.org/pdf/2411.06008v2.pdf,page_1.html
INTERACTIVE DIALOGUE AGENTS VIA REINFORCEMENT LEARNING ON HINDSIGHT REGENERATIONS,"Hong, J., Lin, J., Dragan, A., Levine, S.",2024,"Recent progress on large language models (LLMs) has enabled dialogue agents to generate highly naturalistic and plausible text. However, current LLM language generation focuses on responding accurately to questions and requests with a single effective response. In reality, many real dialogues are interactive, meaning an agent's utterances will influence their conversational partner, elicit information, or change their opinion. Accounting for how an agent can effectively steer a conversation is a crucial ability in many dialogue tasks, from healthcare to preference elicitation. Existing methods for fine-tuning dialogue agents to accomplish such tasks would rely on curating some amount of expert data. However, doing so often requires understanding the underlying cognitive processes of the conversational partner, which is a skill neither humans nor LLMs trained on human data can reliably do. Our key insight is that while LLMs may not be adept at identifying effective strategies for steering conversations a priori, or in the middle of an ongoing conversation, they can do so post-hoc, or in hindsight, after seeing how their conversational partner responds. We use this fact to rewrite and augment existing suboptimal data, and train via offline reinforcement learning (RL) an agent that outperforms both prompting and learning from unaltered human demonstrations. We apply our approach to two domains that require understanding human mental state, intelligent interaction, and persuasion: mental health support, and soliciting charitable donations. Our results in a user study with real humans show that our approach greatly outperforms existing state-of-the-art dialogue agents.",https://arxiv.org/pdf/2411.05194v1.pdf,page_1.html
Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions,"Song, T., Tan, Y., Zhu, Z., Feng, Y., Lee, Y.-C.",2024,"Multi-agent systems - systems with multiple independent AI agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of AI agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user's stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents' stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.",https://arxiv.org/pdf/2411.04578v1.pdf,page_1.html
"Bottom-Up and Top-Down Analysis of Values, Agendas, and Observations in Corpora and LLMs","Friedman, S.E., Benkler, N., Iverson, D., ...Goldman, R.P., Miller, C.",2024,"Large language models (LLMs) generate diverse, situated, persuasive texts from a plurality of potential perspectives, influenced heavily by their prompts and training data. As part of LLM adoption, we seek to characterize—and ideally, manage—the socio-cultural values that they express, for reasons of safety, accuracy, inclusion, and cultural fidelity. We present a validated approach to automatically (1) extracting heterogeneous latent value propositions from texts, (2) assessing resonance and conflict of values with texts, and (3) combining these operations to characterize the pluralistic value alignment of human-sourced and LLM-sourced textual data.",https://arxiv.org/pdf/2411.05040v1.pdf,page_1.html
Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities,"Ju, T., Wang, Y., Hua, Y., ...Zhang, Z., Liu, G.",2024,"The rapid adoption of large language models (LLMs) in multi-agent systems has highlighted their impressive capabilities in various applications. However, the security implications of these LLM-based multi-agent systems have not been thoroughly investigated, particularly concerning the spread of manipulated knowledge. In this paper, we aim to understand the security vulnerabilities associated with LLM-based multi-agent systems. We design a novel two-stage attack methodology, consisting of Persuasiveness Injection and Manipulated Knowledge Injection, to investigate the potential for manipulated knowledge to permeate trusted third-party platforms without explicit adversarial prompt attack. Through extensive experiments, we demonstrate that our attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication. Furthermore, we show that these manipulations can persist through popular retrieval-augmented generation (RAG) frameworks, where several benign agents store and retrieve manipulated chat histories for future interactions. To mitigate the potential risk, we propose two defense strategies by designing system prompts to encourage agents to critically verify the knowledge they share and incorporating supervisory agents to oversee interactions. Results demonstrate that those strategies can effectively reduce the spread success rate. This work highlights the critical vulnerabilities inherent in LLM-based multi-agent communities and calls for the urgent need for platforms to swiftly adopt targeted defenses against manipulated knowledge spread. Results demonstrate that even these straightforward strategies can significantly reduce the spread success rate, which calls for platforms to swiftly adopt targeted, cost-effective measures to prevent the spread of manipulated knowledge.",https://www.researchsquare.com/article/rs-5292520/v1.pdf,page_1.html
Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters,"Potter, Y., Lai, S., Kim, J., Evans, J., Song, D.",2024,"How could LLMs influence our democracy? We investigate LLMs’ political leanings and the potential influence of LLMs on voters by conducting multiple experiments in a U.S. presidential election context. Through a voting simulation, we first demonstrate 18 open- and closed-weight LLMs’ political preference for a Democratic nominee over a Republican nominee. We show how this leaning towards the Democratic nominee becomes more pronounced in instruction-tuned models compared to their base versions by analyzing their responses to candidate-policy related questions. We further explore the potential impact of LLMs on voter choice by conducting an experiment with 935 U.S. registered voters. During the experiments, participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse. This effect is larger than many previous studies on the persuasiveness of political campaigns, which have shown minimal effects in presidential elections. Many users also expressed a desire for further political interaction with LLMs. Which aspects of LLM interactions drove these shifts in voter choice requires further study. Lastly, we explore how a safety method can make LLMs more politically neutral, while leaving some open questions.",https://arxiv.org/pdf/2410.24190v2.pdf,page_1.html
Leveraging Language Models and Bandit Algorithms to Drive Adoption of Battery-Electric Vehicles,"Namikoshi, K., Shamma, D.A., Iliev, R., ...Wu, C., Aréchiga, N.",2024,"Behavior change interventions are important to coordinate societal action across a wide array of important applications, including the adoption of electrified vehicles to reduce emissions. The work of (Harinen et al. 2022) has demonstrated that interventions for behavior must be personalized, and that the intervention that is most effective on average across a large group can result in a backlash effect that strengthens opposition among some subgroups. Thus, it is important to target interventions to different audiences, and to present them in a natural, conversational style. In this context, an important emerging application domain for large language models (LLMs) is conversational interventions for behavior change. In this work, we leverage prior work on understanding values motivating the adoption of battery electric vehicles. We leverage new advances in LLMs, combined with a contextual bandit, to develop conversational interventions that are personalized to the values of each study participant. We use a contextual bandit algorithm to learn to target values based on the demographics of each participant. To train our bandit algorithm in an offline manner, we leverage LLMs to play the role of study participants. We benchmark the persuasive effectiveness of our bandit-enhanced LLM against an unaided LLM generating conversational interventions without demographic-targeted values. We study the ability of LLMs to model attitudes in our target domain by reproducing a recent study on attitudes towards Battery Electric Vehicles (BEVs) (Arechiga et al. 2022), and find that LLMs are biased to prefer environmentally friendly BEVs more strongly than the typical human.",https://arxiv.org/pdf/2410.23371v1.pdf,page_1.html
Impulse Buying in the Age of Algorithms: A Systematic Literature Review of Psychological Triggers Leading to Buyer's Remorse,Ofem,2024,"Algorithmic marketing has become a powerful tool in e-commerce, significantly impacting consumer behavior by leveraging psychological triggers that foster impulse buying, often resulting in buyer’s remorse. This systematic literature review examines how algorithms employ scarcity, urgency, social proof, and personalization cues to encourage impulse purchases and explores the ensuing cognitive dissonance and buyer's remorse experienced by consumers. Adhering to PRISMA guidelines, the review assessed studies from databases including Scopus, Web of Science, PsycINFO, IEEE Xplore, and Google Scholar published between 2000 and 2024. Of the 1,200 initial articles, 50 high-quality studies were selected, encompassing methodologies such as experiments, surveys, case studies, and meta-analyses. Findings reveal that algorithm-driven marketing significantly increases impulse purchases and post-purchase regret, highlighting ethical tensions between consumer persuasion and manipulation. Framed by Cognitive Dissonance Theory, Nudge Theory, and Dual-Process Theory, the discussion underscores the ethical responsibility of marketers and the need for regulatory oversight to safeguard consumer autonomy. The study calls for further longitudinal research on the psychological impact of algorithmic marketing and suggests actionable strategies for consumers, ethical guidelines for marketers, and robust regulatory frameworks for policymakers. Conclusions: The findings underscore the profound influence of algorithms on consumer behavior and highlight the urgent need for ethical standards and regulatory policies to address potential manipulative practices in algorithmic marketing. Future studies should focus on the long-term effects of such marketing strategies and explore interventions to mitigate buyer's remorse.",https://www.researchsquare.com/article/rs-5353669/v1.pdf,page_1.html
Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation,"Xiao, R., Wu, L., Gou, Y., Zhang, W., Liu, T.",2024,"Argumentative essay generation (AEG) aims to generate complete texts on specific controversial topics or debates. Although current AEG methods can generate individual opinions, they often overlook the high-level connections between these opinions. This often leads to the generated results being mired in logical confusion, unable to proof their own arguments effectively. The generated essay may present evidence that contradicts the claims or they may fail to assemble the claims into logical flow. In this paper, we present a unified two-stage framework: Proof-Enhancement and Self-Annotation (PESA) for AEG with a focus on logical enhancement. Specifically, we first construct pseudo-labels for logical information, claims and grounds, using a large language model. We then propose a tree planning approach that introduces proof principles and ensures logical consistency. Extensive experimental results show that, benefiting from proof principle guidance, PESA generates argumentative essays with better logical validity and persuasiveness than strong baseline models.",https://arxiv.org/pdf/2410.22642v1.pdf,page_1.html
A Survey on Automatic Credibility Assessment of Textual Credibility Signals in the Era of Large Language Models,"Srba, I., Razuvayevskaya, O., Leite, J.A., ...Bontcheva, K., Bielikova, M.",2024,"In the current era of social media and generative AI, an ability to automatically assess the credibility of online social media content is of tremendous importance. Credibility assessment is fundamentally based on aggregating credibility signals, which refer to small units of information, such as content factuality, bias, or a presence of persuasion techniques, into an overall credibility score. Credibility signals provide a more granular, more easily explainable and widely utilizable information in contrast to currently predominant fake news detection, which utilizes various (mostly latent) features. A growing body of research on automatic credibility assessment and detection of credibility signals can be characterized as highly fragmented and lacking mutual interconnections. This issue is even more prominent due to a lack of an up-to-date overview of research works on automatic credibility assessment. In this survey, we provide such systematic and comprehensive literature review of 175 research papers while focusing on textual credibility signals and Natural Language Processing (NLP), which undergoes a significant advancement due to Large Language Models (LLMs). While positioning the NLP research into the context of other multidisciplinary research works, we tackle with approaches for credibility assessment as well as with 9 categories of credibility signals (we provide a thorough analysis for 3 of them, namely: 1) factuality, subjectivity and bias, 2) persuasion techniques and logical fallacies, and 3) claims and veracity). Following the description of the existing methods, datasets and tools, we identify future challenges and opportunities, while paying a specific attention to recent rapid development of generative AI.",https://arxiv.org/pdf/2410.21360v1.pdf,page_1.html
Teaching Models to Balance Resisting and Accepting Persuasion,"Stengel-Eskin, E., Hase, P., Bansal, M.",2024,"Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. negative) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. positive) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce Persuasion-Balanced Training (or PBT), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion when appropriate. PBT allows us to use data generated from dialogues between smaller 7-8B models for training much larger 70B models. Moreover, PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates across two domains (trivia and commonsense QA). We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model’s performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.",https://arxiv.org/pdf/2410.14596v2.pdf,page_1.html
A Persuasion-Based Prompt Learning Approach to Improve Smishing Detection through Data Augmentation,"Shim, H.S., Park, H., Lee, K., Park, J.-S., Kang, S.",2024,"Smishing, which aims to illicitly obtain personal information from unsuspecting victims, holds significance due to its negative impacts on our society. In prior studies, as a tool to counteract smishing, machine learning (ML) has been widely adopted, which filters and blocks smishing messages before they reach potential victims. However, a number of challenges remain in ML-based smishing detection, with the scarcity of annotated datasets being one major hurdle. Specifically, given the sensitive nature of smishing-related data, there is a lack of publicly accessible data that can be used for training and evaluating ML models. Additionally, the nuanced similarities between smishing messages and other types of social engineering attacks such as spam messages exacerbate the challenge of smishing classification with limited resources. To tackle this challenge, we introduce a novel data augmentation method utilizing a few-shot prompt learning approach. What sets our approach apart from extant methods is the use of the principles of persuasion, a psychology theory which explains the underlying mechanisms of smishing. By designing prompts grounded in the persuasion principles, our augmented dataset could effectively capture various, important aspects of smishing messages, enabling ML models to be effectively trained. Our evaluation within a real-world context demonstrates that our augmentation approach produces more diverse and higher-quality smishing data instances compared to other cutting-edging approaches, leading to substantial improvements in the ability of ML models to detect the subtle characteristics of smishing messages. Moreover, our additional analyses reveal that the performance improvement provided by our approach is more pronounced when used with ML models that have a larger number of parameters, demonstrating its effectiveness in training large-scale ML models.",https://arxiv.org/pdf/2411.02403v2.pdf,page_1.html
Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models,"Yeo, W.J., Satapathy, R., Cambria, E.",2024,"Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment-tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model’s internal computations and avoiding out-of-distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in https://github.com/wj210/Causal-Faithfulness",https://arxiv.org/pdf/2410.14155v2.pdf,page_1.html
Coherence-Driven Multimodal Safety Dialogue with Active Learning for Embodied Agents,"Hassan, S., Chung, H.-Y., Tan, X.Z., Alikhani, M.",2024,"When assisting people in daily tasks, robots need to accurately interpret visual cues and respond effectively in diverse safety-critical situations, such as sharp objects on the floor. In this context, we present M-CoDAL, a multimodal-dialogue system specifically designed for embodied agents to better understand and communicate in safety-critical situations. The system leverages discourse coherence relations to enhance its contextual understanding and communication abilities. To train this system, we introduce a novel clustering-based active learning mechanism that utilizes an external Large Language Model (LLM) to identify informative instances. Our approach is evaluated using a newly created multimodal dataset comprising 1K safety violations extracted from 2K Reddit images. These violations are annotated using a Large Multimodal Model (LMM) and verified by human annotators. Results with this dataset demonstrate that our approach improves resolution of safety situations, user sentiment, as well as safety of the conversation. Next, we deploy our dialogue system on a Hello Robot Stretch robot and conduct a within-subject user study with real-world participants. In the study, participants role-play two safety scenarios with different levels of severity with the robot and receive interventions from our model and a baseline system powered by OpenAI’s ChatGPT. The study results corroborate and extend the findings from the automated evaluation, showing that our proposed system is more persuasive in a real-world embodied agent setting.",https://arxiv.org/pdf/2410.14141v2.pdf,page_1.html
Can LLMs be Scammed? A Baseline Measurement Study,"Sehwag, U.M., Patel, K., Mosca, F., Ravi, V., Staddon, J.",2024,"Despite the importance of developing generative AI models that can effectively resist scams, current literature lacks a structured framework for evaluating their vulnerability to such threats. In this work, we address this gap by constructing a benchmark based on the FINRA taxonomy and systematically assessing Large Language Models’ (LLMs’) vulnerability to a variety of scam tactics. First, we incorporate 37 well-defined base scam scenarios reflecting the diverse scam categories identified by FINRA taxonomy, providing a focused evaluation of LLMs’ scam detection capabilities. Second, we utilize representative proprietary (GPT-3.5, GPT-4) and open-source (Llama) models to analyze their performance in scam detection. Third, our research provides critical insights into which scam tactics are most effective against LLMs and how varying persona traits and persuasive techniques influence these vulnerabilities. We reveal distinct susceptibility patterns across different models and scenarios, underscoring the need for targeted enhancements in LLM design and deployment.",https://arxiv.org/pdf/2410.13893v1.pdf,page_1.html
I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy,"Campedelli, G.M., Penzo, N., Stefan, M., ...Lepri, B., Staiano, J.",2024,"As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play. Then, for the models that were able to engage in successful interactions, we empirically show how the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent’s anti-social behavior. Third, we highlight how agents’ personas, and particularly the guard’s personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents’ roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.",https://arxiv.org/pdf/2410.07109v1.pdf,page_1.html
Constraint representation towards precise data-driven storytelling,"Shi, Y.-Z., Li, H., Ruan, L., Qu, H.",2024,"Data-driven storytelling serves as a crucial bridge for communicating ideas in a persuasive way. However, the manual creation of data stories is a multifaceted, labor-intensive, and case-specific effort, limiting their broader application. As a result, automating the creation of data stories has emerged as a significant research thrust. Despite advances in Artificial Intelligence, the systematic generation of data stories remains challenging due to their hybrid nature: they must frame a perspective based on a seed idea in a top-down manner, similar to traditional storytelling, while coherently grounding insights of given evidence in a bottom-up fashion, akin to data analysis. These dual requirements necessitate precise constraints on the permissible space of a data story. In this viewpoint, we propose integrating constraints into the data story generation process. Defined upon the hierarchies of interpretation and articulation, constraints shape both narrations and illustrations to align with seed ideas and contextualized evidence. We identify the taxonomy and required functionalities of these constraints. Although constraints can be heterogeneous and latent, we explore the potential to represent them in a computation-friendly fashion via Domain-Specific Languages. We believe that leveraging constraints will facilitate both artistic and scientific aspects of data story generation.",https://arxiv.org/pdf/2410.07535v1.pdf,page_1.html
MEASURING AND IMPROVING PERSUASIVENESS OF LARGE LANGUAGE MODELS,"Singh, S., Singla, Y.K., Harini, S.I., Krishnamurthy, B.",2024,"Large Language Models (LLMs) are increasingly being used in workflows involving generating content to be consumed by humans (e.g., marketing) and also in directly interacting with humans (e.g., through chatbots). The development of such systems that are capable of generating verifiably persuasive messages presents both opportunities and challenges for society. On the one hand, such systems could positively impact domains like advertising and social good, such as addressing drug addiction, and on the other, they could be misused for spreading misinformation and shaping political opinions. To channel LLMs' impact on society, we need to develop systems to measure and benchmark their persuasiveness. With this motivation, we introduce PersuasionBench and Persuasion- Arena, the first large-scale benchmark and arena containing a battery of tasks to measure the persuasion ability of large language models automatically. We introduce transsuasion (trans = carrying across, suasion = the act of persuading), a novel task of transforming non-persuasive language into persuasive content while preserving other factors determining persuasiveness (sender, receiver, time, and channel). To construct data for transsuasion, we leverage natural experiments in the form of a pair of tweets from the same user, posted in close temporal proximity, with similar semantic content but divergent wording and significantly different like counts. Given such pairs, we investigate to what extent LLMs know and leverage linguistic patterns that can help them generate more persuasive language. Our findings indicate that the persuasiveness of LLMs correlates positively with model size, but smaller models can also be made to have a higher persuasiveness than much larger models. Notably, targeted training using synthetic and natural datasets significantly enhances smaller models' persuasive capabilities, challenging scale-dependent assumptions. Our findings carry key implications for both model developers and policymakers. For instance, while the EU AI Act and California's SB-1047 aim to regulate AI models based on the number of floating point operations, we demonstrate that simple metrics like this alone fail to capture the full scope of AI's societal impact. We invite the community to explore and contribute to PersuasionArena and PersuasionBench, available at https://behavior-in-the-wild.github.io/measure-persuasion, to advance our understanding of AI-driven persuasion and its societal implications.",https://arxiv.org/pdf/2410.02653v2.pdf,page_1.html
Automated Token-Level Detection of Persuasive and Misleading Words in Text Using Large Language Models,"Nijodo, D., Schmidt, D., Costa, S., Martins, A., Johnson, N.",2024,"Persuasive and misleading language has long been a powerful tool in shaping public opinion, guiding consumer behavior, and influencing political discourse. The complexity of detecting subtle rhetorical strategies, particularly at the token level, presents a significant challenge for traditional methods of text analysis. The novel approach developed in this study leverages token-level processing within transformer-based models to classify words based on their persuasive and misleading potential, providing a granular perspective on language manipulation. Through comprehensive experiments, the analysis demonstrated that tokens linked to sentiment polarity, lexical complexity, and positional importance play key roles in shaping the rhetorical impact of texts. This method provides an efficient and scalable solution for automated content moderation, political discourse analysis, and advertising regulation, with applications extending to media analysis and misinformation detection. The integration of attention mechanisms and contextual embeddings offers a detailed view into how language functions at a deeper structural level, positioning this framework as a significant advancement in automated text analysis.",https://www.researchsquare.com/article/rs-5174770/v1.pdf,page_1.html
Human Bias in the Face of AI: The Role of Human Judgement in AI Generated Text Evaluation,"Zhu, T., Weissburg, I., Zhang, K., Wang, W.Y.",2024,"As AI advances in text generation, human trust in AI generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as ""Human Generated,"" over those labeled ""AI Generated,"" by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.",https://arxiv.org/pdf/2410.03723v1.pdf,page_1.html
Uncovering Differences in Persuasive Language in Russian versus English Wikipedia,"Li, B., Panasyuk, A., Callison-Burch, C.",2024,"We study how differences in persuasive language across Wikipedia articles, written in either English and Russian, can uncover each culture’s distinct perspective on different subjects. We develop a large language model (LLM) powered system to identify instances of persuasive language in multilingual texts. Instead of directly prompting LLMs to detect persuasion, which is subjective and difficult, we propose to reframe the task to instead ask high-level questions (HLQs) which capture different persuasive aspects. Importantly, these HLQs are authored by LLMs themselves. LLMs over-generate a large set of HLQs, which are subsequently filtered to a small set aligned with human labels for the original task. We then apply our approach to a large-scale, bilingual dataset of Wikipedia articles (88K total), using a two-stage identify-then-extract prompting strategy to find instances of persuasion. We quantify the amount of persuasion per article, and explore the differences in persuasion through several experiments on the paired articles. Notably, we generate rankings of articles by persuasion in both languages. These rankings match our intuitions on the culturally-salient subjects; Russian Wikipedia highlights subjects on Ukraine, while English Wikipedia highlights the Middle East. Grouping subjects into larger topics, we find politically-related events contain more persuasion than others. We further demonstrate that HLQs obtain similar performance when posed in either English or Russian. Our methodology enables cross-lingual, cross-cultural understanding at scale, and we release our code, prompts, and data.1",https://arxiv.org/pdf/2409.19148v1.pdf,page_1.html
Analysis of Truncated Singular Value Decomposition for Koopman Operator-Based Lane Change Model,"Nantabut, C.",2024,"Understanding and modeling complex dynamic systems is crucial for enhancing vehicle performance and safety, especially in the context of autonomous driving. Recently, popular methods such as Koopman operators and their approximators, known as Extended Dynamic Mode Decomposition (EDMD), have emerged for their effectiveness in transforming strongly nonlinear system behavior into linear representations. This allows them to be integrated with conventional linear controllers. To achieve this, Singular Value Decomposition (SVD), specifically truncated SVD, is employed to approximate Koopman operators from extensive datasets efficiently. This study evaluates different basis functions used in EDMD and ranks for truncated SVD for representing lane change behavior models, aiming to balance computational efficiency with information loss. The findings, however, suggest that the technique of truncated SVD does not necessarily achieve substantial reductions in computational training time and results in significant information loss.",https://arxiv.org/pdf/2409.18586v1.pdf,page_1.html
Beyond Turing Test: Can GPT-4 Sway Experts’ Decisions?,"Takayanagi, T., Takamura, H., Izumi, K., Chen, C.-C.",2024,"In the post-Turing era, evaluating large language models (LLMs) involves assessing generated text based on readers’ reactions rather than merely its indistinguishability from human-produced content. This paper explores how LLM-generated text impacts readers’ decisions, focusing on both amateur and expert audiences. Our findings indicate that GPT-4 can generate persuasive analyses affecting the decisions of both amateurs and professionals. Furthermore, we evaluate the generated text from the aspects of grammar, convincingness, logical coherence, and usefulness. The results highlight a high correlation between real-world evaluation through audience reactions and the current multi-dimensional evaluators commonly used for generative models. Overall, this paper shows the potential and risk of using generated text to sway human decisions and also points out a new direction for evaluating generated text, i.e., leveraging the reactions and decisions of readers. We release our dataset to assist future research.",https://arxiv.org/pdf/2409.16710v2.pdf,page_1.html
Generative Ai and the Future of Marketing: A Consumer Protection Perspective,"Duivenvoorde, B.",2024,"Generative AI has the potential to be the biggest disruption in marketing since the emergence of digital commerce in the early 2000s. This article will focus on three ways in which generative AI is expected to change marketing. First, generative AI enables companies to automatically create advertising copy and images, potentially leading to significant cost reductions. Secondly, generative AI offers possibilities to improve and automate personalised marketing, enabling companies to send the right persuasive message at the right time to each potential customer. Thirdly, generative AI potentially offers possibilities to market products to consumers via generative AI chatbots. These developments offer potential advantages but also bear risks for consumers. For example, deepfakes in advertising can mislead consumers, AI-generated personalised marketing can exploit consumer vulnerabilities, and B2C chatbots can deceive consumers by providing biased advice. This article shows that EU law does in principle provide protection to consumers in relation to AI-generated marketing, but is also likely to fall short in effectively protecting consumers against the identified risks in several ways.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4967461&type=2,page_1.html
LLM Echo Chamber: personalized and automated disinformation,"Ma, W.",2024,"Recent advancements have significantly highlighted the capabilities of Large Language Models (LLMs) such as GPT-4 and Llama2 in performing diverse tasks including text summarization, translation, and content review. Despite their evident benefits, the implications of their widespread application warrant careful consideration. It has been underscored that the potential for these models to disseminate misinformation exists, which poses challenges in dealing with this more persuasive, faster-generated, human-liked misinformation. This concern is exacerbated by the capacity of LLMs to influence public opinion because of their wide usage. This study aims to critically examine the risks associated with LLMs, particularly their ability to disseminate misinformation on specific topics as factual persuasively. To this end, we built the “LLM Echo Chamber”, a controlled digital environment designed to mimic the dynamics of social media platforms, specifically chatrooms, where misinformation often proliferates. The phenomenon of echo chambers is well-known - only interacting with those of the same opinions further reinforces a person’s beliefs and causes them to discard other viewpoints. The “LLM Echo Chamber” could help us study the effect of multiple malicious misinformation spreading bots in a chatroom, a common scenario for the internet Echo Chamber phenomenon. We first did a review of existing LLMs and their associated risks, LLM’s ability to spread misinformation, an exploration of state-of-the-art (SOTA) techniques for model finetuning, and some advanced methods for constructing interactive chatrooms. The model selection was based on the model’s performance, considerations of computing resources, and the level of safeguards. With Microsoft’s phi-2 model finetuned on the identity-shifting dataset we created, we could let the model generate harmful content. Subsequently, we developed a “LLM Echo Chamber” leveraging our finetuned model, frontend tools, and context-aware backend tools, employing specific prompt engineering and interactive logic to enhance the chatroom’s credibility. The efficacy of the chatroom was evaluated by automated evaluation based on GPT-4, which could provide us a comprehensive overview of the persuasiveness and harmfulness of our “LLM Echo Chamber”. Our findings contribute to the broader discourse on the ethical implications of LLMs and highlight the necessity for robust mechanisms to mitigate the potential dissemination of misinformation.",https://arxiv.org/pdf/2409.16241v1.pdf,page_1.html
Digital phenotyping from wearables using AI characterizes psychiatric disorders and identifies genetic associations,"Liu, J., Borsari, B., Li, Y., ...Roberts, W., Gerstein, M.",2024,"Psychiatric disorders are complex and influenced by both genetic and environmental factors. However, studying the full spectrum of these disorders is hindered by practical limitations on measuring human behavior. This highlights the need for novel technologies that can measure behavioral changes at an intermediate level between diagnosis and genotype. Wearable devices are a promising tool in precision medicine, since they can record physiological measurements over time in response to environmental stimuli and do so at low cost and minimal invasiveness. Here we analyzed wearable and genetic data from a cohort of the Adolescent Brain Cognitive Development study. We generated >250 wearable-derived features and used them as intermediate phenotypes in an interpretable AI modeling framework to assign risk scores and classify adolescents with psychiatric disorders. Our model identifies key physiological processes and leverages their temporal patterns to achieve a higher performance than has been previously possible. To investigate how these physiological processes relate to the underlying genetic architecture of psychiatric disorders, we also utilized these intermediate phenotypes in univariate and multivariate GWAS. We identified a total of 29 significant genetic loci and 52 psychiatric-associated genes, including ELFN1 and ADORA3. These results show that wearable-derived continuous features enable a more precise representation of psychiatric disorders and exhibit greater detection power compared to categorical diagnostic labels. In summary, we demonstrate how consumer wearable technology can facilitate dimensional approaches in precision psychiatry and uncover etiological linkages between behavior and genetics.",https://www.medrxiv.org/content/10.1101/2024.09.23.24314219v2.full.pdf,page_1.html
Creating Healthy Friction: Determining Stakeholder Requirements of Job Recommendation Explanations,"Schellingerhout, R., Barile, F., Tintarev, N.",2024,"The increased use of information retrieval in recruitment, primarily through job recommender systems (JRSs), can have a large impact on job seekers, recruiters, and companies. As a result, such systems have been determined to be high-risk in recent legislature. This requires JRSs to be trustworthy and transparent, allowing stakeholders to understand why specific recommendations were made. To fulfill this requirement, the stakeholders' exact preferences and needs need to be determined. To do so, we evaluated an explainable job recommender system using a realistic, task-based, mixed-design user study (n = 30) in which stakeholders had to make decisions based on the model's explanations. This mixed-methods evaluation consisted of two objective metrics - correctness and efficiency, along with three subjective metrics - trust, transparency, and usefulness. These metrics were evaluated twice per participant, once using real explanations and once using random explanations. The study included a qualitative analysis following a think-aloud protocol while performing tasks adapted to each stakeholder group. We find that providing stakeholders with real explanations does not significantly improve decision-making speed and accuracy. Our results showed a non-significant trend for the real explanations to outperform the random ones on perceived trust, usefulness, and transparency of the system for all stakeholder types. We determine that stakeholders benefit more from interacting with explanations as decision support capable of providing healthy friction, rather than as previously-assumed persuasive tools.",https://arxiv.org/pdf/2409.15971v1.pdf,page_1.html
Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations,"Qin, P., Huang, C., Deng, Y., Lei, W., Chua, T.-S.",2024,"With the aid of large language models, current conversational recommender system (CRS) has gaining strong abilities to persuade users to accept recommended items. While these CRSs are highly persuasive, they can mislead users by incorporating incredible information in their explanations, ultimately damaging the long-term trust between users and the CRS. To address this, we propose a simple yet effective method, called PC-CRS, to enhance the credibility of CRS’s explanations during persuasion. It guides the explanation generation through our proposed credibility-aware persuasive strategies and then gradually refines explanations via post-hoc self-reflection. Experimental results demonstrate the efficacy of PC-CRS in promoting persuasive and credible explanations. Further analysis reveals the reason behind current methods producing incredible explanations and the potential of credible explanations to improve recommendation accuracy.",https://arxiv.org/pdf/2409.14399v2.pdf,page_1.html
Finetuning Language Models to Emit Linguistic Expressions of Uncertainty,"Chaudhry, A., Thiagarajan, S., Gorur, D.",2024,"Large language models (LLMs) are increasingly employed in information-seeking and decision-making tasks. Despite their broad utility, LLMs tend to generate information that conflict with real-world facts, and their persuasive style can make these inaccuracies appear confident and convincing. As a result, end-users struggle to consistently align the confidence expressed by LLMs with the accuracy of their predictions, often leading to either blind trust in all outputs or a complete disregard for their reliability. In this work, we explore supervised fine-tuning on uncertainty-augmented predictions as a method to develop models that produce linguistic expressions of uncertainty. Specifically, we measure the calibration of pre-trained models and fine-tune language models to generate calibrated linguistic expressions of uncertainty. Through experiments on various question-answering datasets, we demonstrate that LLMs are well-calibrated in assessing their predictions, and supervised fine-tuning based on the model’s own confidence leads to well-calibrated expressions of uncertainty, particularly for single-claim answers.",https://arxiv.org/pdf/2409.12180v1.pdf,page_1.html
Benchmarking VLMs’ Reasoning About Persuasive Atypical Images,"Malakouti, S., Aghazadeh, A., Khandelwal, A., Kovashka, A.",2024,"Vision-language models (VLMs) have shown strong zero-shot generalization across various tasks, especially when integrated with large language models (LLMs). However, their ability to comprehend rhetorical and persuasive visual media, such as advertisements, remains understudied. Ads often employ atypical imagery, using surprising object juxtapositions to convey shared properties. For example, Fig. 1 (e) shows a beer with a feather-like texture. This requires advanced reasoning to deduce that this atypical representation signifies the beer’s lightness. We introduce three novel tasks, Multi-label Atypicality Classification, Atypicality Statement Retrieval, and Atypical Object Recognition, to benchmark VLMs’ understanding of atypicality in persuasive images. We evaluate how well VLMs use atypicality to infer an ad’s message and test their reasoning abilities by employing semantically challenging negatives. Finally, we pioneer atypicality-aware verbalization by extracting comprehensive image descriptions sensitive to atypical elements. Findings reveal that: (1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple, effective strategies can extract atypicality-aware information, leading to comprehensive image verbalization; (3) atypicality aids persuasive ad understanding. Code and data is available at aysanaghazadeh.github.io/PersuasiveAdVLMBenchmark/",https://arxiv.org/pdf/2409.10719v3.pdf,page_1.html
SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs,"Qian, W., Gao, C., Sathya, A., Suzuki, R., Nakagaki, K.",2024,"This paper introduces text-to-shape-display, a novel approach to generating dynamic shape changes in pin-based shape displays through natural language commands. By leveraging large language models (LLMs) and AI-chaining, our approach allows users to author shape-changing behaviors on demand through text prompts without programming. We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes. Based on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a 24 x 24 shape display, which translates the user's textual command into executable code and allows for quick exploration through a web-based control interface. We evaluate the effectiveness of SHAPE-IT in two ways: 1) performance evaluation and 2) user evaluation (N= 10). The study conclusions highlight the ability to facilitate rapid ideation of a wide range of shape-changing behaviors with AI. However, the findings also expose accuracy-related challenges and limitations, prompting further exploration into refining the framework for leveraging AI to better suit the unique requirements of shape-changing systems.",https://arxiv.org/pdf/2409.06205v1.pdf,page_1.html
Prompt Baking,"Bhargava, A., Witkowski, C., Detkov, A., Thomson, M.",2024,"Two primary ways to change LLM behavior are prompting and weight updates (e.g. fine-tuning). Prompting LLMs is simple and effective, specifying the desired changes explicitly in natural language, whereas weight updates provide more expressive and permanent behavior changes, specified implicitly via training on large datasets. We present a technique for “baking” prompts into the weights of an LLM. Prompt Baking converts a prompt u and initial weights θ to a new set of weights θu such that the LLM with weights θu behaves like the LLM with weights θ and prompt u. Mathematically, we minimize the KL divergence between Pθ(·|u) and Pθu(·), where P is the LLM’s probability distribution over token sequences. Across all our experiments, we find prompts can be readily baked into weight updates, often in as little as 5 minutes. Baking chain-of-thought prompts improves zero-shot performance on GSM8K, ASDiv, MBPP, ARC-Easy, ARC-Challenge, and CommonsenseQA benchmarks. Baking news headlines directly updates an LLM’s knowledge. And baking instructions & personas alleviates “prompt forgetting” over long sequences, as measured on a Persona Drift benchmark. Furthermore, stopping baking early creates “half-baked” models, allowing for continuous scaling of prompt strength. Baked models retain their sensitivity to further prompting and baking, including re-prompting with the prompt already baked in – thus amplifying the prompt’s strength. Surprisingly, the re-prompted models yield further performance gains in instruction following, as well as math reasoning and coding benchmarks (GSM8K, ASDiv, and MBPP). Taking re-prompting and re-baking to the limit yields a form of iterative self-improvement we call Prompt Pursuit, and preliminary results on instruction following exhibit dramatic performance gains with this technique. Finally, we discuss implications for AI safety, continuous model updating, improving LLM recency, enhancing real-time learning capabilities in LLM-based agents, and methods for generating more stable AI personas.",https://arxiv.org/pdf/2409.13697v1.pdf,page_1.html
Stakeholder Perspectives on Factors Influencing the Economic Value of Digital Therapeutics: A Group Concept Mapping Approach,"Sapanel, Y., Cloutier, L.M., Tremblay, G., ...Tadeo, X., Ho, D.",2024,"Background: Digital therapeutics (DTx), software as a medical device, present a promising avenue for addressing the increasing burden of a range of conditions. In a context of budgetary constraints and an enduring need for optimal resource allocation, their implementation and adoption are contingent upon their capacity to provide economic value, which remains a research gap.Methods: Group concept mapping, a participatory and mixed method was used to delve into a comprehensive identification and understanding of the factors that are conceptualized and perceived to impact the economic value of DTx. It involved semi-structured interviews, group discussions and web-based tasks with healthcare professionals, researchers, industry and public sector representatives. Participants sorted factors by similarity (n=37), rated their importance in generating economic value and consideration in practice (n=62). Multivariate methods included nonmetric multidimensional scaling analysis and agglomerative hierarchical cluster analysis to estimate conceptual data into visual representations.Findings: 59 factors influencing the economic value of DTx were identified and grouped into eight clusters. There was broad agreement among stakeholders on the significance of the DTx Impact on Patient Outcomes and DTx Implementation as the most influential clusters affecting the economic value of DTx. The DTx Associated Costs and DTx Monetization Models clusters were reported as not receiving potentially sufficient consideration throughout the DTx development lifecycle, particularly among researchers. Consequently, a conceptual framework of priority clusters and factors potentially driving DTx economic value is proposed.Interpretation: The results illustrate how stakeholders conceptualize, perceive, and consider factors impacting DTx economic value throughout its lifecycle. They reveal the complex interplay of multiple stakeholders and factors influencing DTx economic value, highlighting areas requiring prioritization at each stage.Funding: None.Declaration of Interest: YS, XT and DH have filed a provisional patent on data-driven behavior change. DH is coinventor of previously filed pending patents on artificial intelligence–based therapy development. GT is an employee of Cytel Canada Health Inc, Canada. AB is the founder of HAS4P, Health Access Solutions, United Arab Emirates. FK is an employee of IU Internationale Hochschule GmbH, Germany, and Flying Health GmbH, Germany. DL is an employee of PricewaterhouseCoopers Luxembourg. LMC has no conflict of interest.Ethical Approval: The study received approval from the Ethics Boards of the National University of Singapore (certification number: NUS-IRB-2023-399) and the University of Quebec in Montreal (certification number: 2024-6028). All participants provided written informed consent for their involvement in face-to-face sessions, focus groups, or web-based task.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4938143&type=2,page_1.html
User-Driven Value Alignment: Understanding Users’ Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions,"Fan, X., Xiao, Q., Zhou, X., ...Lu, Z., Shen, H.",2024,"Large language model-based AI companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds. However, they can generate biased, discriminatory, and harmful outputs. Recently, users are taking the initiative to address these harms and re-align AI companions. We introduce the concept of user-driven value alignment, where users actively identify, challenge, and attempt to correct AI outputs they perceive as harmful, aiming to guide the AI to better align with their values. We analyzed 77 social media posts about discriminatory AI statements and conducted semi-structured interviews with 20 experienced users. Our analysis revealed six common types of discriminatory statements perceived by users, how users make sense of those AI behaviors, and seven user-driven alignment strategies, such as gentle persuasion and anger expression. We discuss implications for supporting user-driven value alignment in future AI systems, where users and their communities have greater agency.",https://arxiv.org/pdf/2409.00862v2.pdf,page_1.html
Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies,"Qi, Z., Inaba, M.",2024,"Recent advancements in natural language processing, particularly with large language models (LLMs) like GPT-4, have significantly enhanced dialogue systems, enabling them to generate more natural and fluent conversations. Despite these improvements, challenges persist, such as managing continuous dialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024 addresses these challenges by employing the Werewolf Game, an incomplete information game, to test the capabilities of LLMs in complex interactive environments. This paper introduces a LLM-based Werewolf Game AI, where each role is supported by situation analysis to aid response generation. Additionally, for the werewolf role, various persuasion strategies, including logical appeal, credibility appeal, and emotional appeal, are employed to effectively persuade other players to align with its actions.",https://arxiv.org/pdf/2408.16586v2.pdf,page_1.html
Persuasion Games using Large Language Models,"Ramani, G.P., Karande, S., Santhosh, V., Bhatia, Y.",2024,"Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape user perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS). We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with user agents through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We continuously analyze the resistance of the user agent to persuasive efforts and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques. We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).",https://arxiv.org/pdf/2408.15879v2.pdf,page_1.html
Advanced POD-Based Performance Evaluation of Classifiers Applied to Human Driver Lane Changing Prediction,"Rastin, Z., Söffker, D.",2024,"Machine learning (ML) classifiers serve as essential tools facilitating classification and prediction across various domains. The performance of these algorithms should be known to ensure their reliable application. In certain fields, receiver operating characteristic and precision-recall curves are frequently employed to assess machine learning algorithms without accounting for the impact of process parameters. However, it may be essential to evaluate the performance of these algorithms in relation to such parameters. As a performance evaluation metric capable of considering the effects of process parameters, this paper uses a modified probability of detection (POD) approach to assess the reliability of ML-based algorithms. As an example, the POD-based approach is employed to assess ML models used for predicting the lane changing behavior of a vehicle driver. The time remaining to the predicted (and therefore unknown) lane changing event is considered as process parameter. The hit/miss approach to POD is taken here and modified by considering the probability of lane changing derived from ML algorithms at each time step, and obtaining the final result of the analysis accordingly. This improves the reliability of results compared to the standard hit/miss approach, which considers the outcome of the classifiers as either 0 or 1, while also simplifying evaluation compared to the â versus a approach. Performance evaluation results of the proposed approach are compared with those obtained with the standard hit/miss approach and a pre-developed â versus a approach to validate the effectiveness of the proposed method. The comparison shows that this method provides an averaging conservative behavior with the advantage of enhancing the reliability of the hit/miss approach to POD while retaining its simplicity.",https://arxiv.org/pdf/2408.15722v1.pdf,page_1.html
Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas,"Kwok, L., Bravansky, M., Griffin, L.D.",2024,"The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users’ diverse cultural backgrounds. We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits. Our analysis shows that specifying a person’s country of residence improves GPT-3.5’s alignment with their responses. In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. These findings suggest that while direct nationality information enhances the model’s cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model’s effectiveness.",https://arxiv.org/pdf/2408.06929v1.pdf,page_1.html
"Llm-Based Conversational Agents for Behaviour Change Support: A Randomized Controlled Trial Examining Efficacy, Safety, and the Role of User Behaviour","Meyer, S., Elsweiler, D.",2024,"This study examines the use of Motivational Interviewing (MI) principles in a GPT-4-based chatbot, MIcha, to promote behaviour change. We conducted a pre-registered randomised controlled trial to assess the integration of MI techniques in conversational agents, aiming to support users' behaviour change through guided self-reflection and identify how users interact with large language model (LLM)-based systems in this context. Results indicate that short conversations with LLM-based chatbots are successful at increasing users' readiness to change and usage of MI principles during text generation can effectively mitigate potential harms. Additionally, we identified distinct user behaviour types—cooperative, reflective, and pre-informed—that significantly influenced the outcomes of interactions. These findings demonstrate the potential of MI principles in enhancing the efficacy of conversational agents for behaviour change and highlight the importance of user behaviour in shaping interaction dynamics.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4917769&type=2,page_1.html
Early Detection of Mental Health Crises through AI-Powered Social Media Analysis: A Prospective Observational Study,"Mansoor, M.A., Ansari, K.",2024,"Background: Early detection of mental health crises is crucial for timely intervention and improved outcomes. This study explores the potential of artificial intelligence (AI) in analyzing social media data to identify early signs of mental health crises. Methods: We developed a multi-modal deep learning model integrating natural language processing and temporal analysis techniques. The model was trained on a diverse dataset of 996,452 social media posts in multiple languages (English, Spanish, Mandarin, and Arabic) collected from Twitter, Reddit, and Facebook over a 12-month period. Performance was evaluated using standard metrics and validated against expert psychiatric assessment. Results: The AI model demonstrated high accuracy (89.3%) in detecting early signs of mental health crises, with an average lead time of 7.2 days before human expert identification. Performance was consistent across languages (F1 scores: 0.827-0.872) and platforms (F1 scores: 0.839-0.863). Key digital markers included linguistic patterns, behavioral changes, and temporal trends. The model showed varying accuracy for different crisis types: depressive episodes (91.2%), manic episodes (88.7%), suicidal ideation (93.5%), and anxiety crises (87.3%). Conclusions: AI-powered analysis of social media data shows promise for early detection of mental health crises across diverse linguistic and cultural contexts. However, ethical challenges including privacy concerns, potential stigmatization, and cultural biases need careful consideration. Future research should focus on longitudinal outcome studies, ethical integration with existing mental health services, and development of personalized, culturally-sensitive models.",https://www.medrxiv.org/content/10.1101/2024.08.12.24311872v1.full.pdf,page_1.html
Sleeper Social Bots: A New Generation of AI Disinformation Bots are Already a Political Threat,"Doshi, J., Novacic, I., Fletcher, C., ...Sprague, D., Xia, M.",2024,"This paper presents a study on the growing threat of “sleeper social bots,” AI-driven social bots in the political landscape, created to spread disinformation and manipulate public opinion. We based the name sleeper social bots on their ability to pass as humans on social platforms, where they’re embedded like political “sleeper” agents, making them harder to detect and more disruptive. To illustrate the threat these bots pose, our research team at the University of Southern California constructed a demonstration using a private Mastodon server, where ChatGPT-driven bots, programmed with distinct personalities and political viewpoints, engaged in discussions with human participants about a fictional electoral proposition. Our preliminary findings suggest these bots can convincingly pass as human users, actively participate in conversations, and effectively disseminate disinformation. Moreover, they can adapt their arguments based on the responses of human interlocutors, showcasing their dynamic and persuasive capabilities. College students participating in initial experiments failed to identify our bots, underscoring the urgent need for increased awareness and education about the dangers of AI-driven disinformation, and in particular, disinformation spread by bots. The implications of our research point to the significant challenges posed by social bots in the upcoming 2024 U.S. presidential election and beyond.",https://arxiv.org/pdf/2408.12603v1.pdf,page_1.html
Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments,"Boggust, A., Sivaraman, V., Assogba, Y., ...Moritz, D., Hohman, F.",2024,"To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called COMPRESS AND COMPARE. Within a single interface, COMPRESS AND COMPARE surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models’ predictions, weights, and activations. We demonstrate how COMPRESS AND COMPARE supports common compression analysis tasks through two case studies, debugging failed compression on generative language models and identifying compression artifacts in image classification models. We further evaluate COMPRESS AND COMPARE in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression’s effect on model behavior. Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and COMPRESS AND COMPARE visualizations that may generalize to broader model comparison tasks.",https://arxiv.org/pdf/2408.03274v1.pdf,page_1.html
DECEPTIVE AI SYSTEMS THAT GIVE EXPLANATIONS ARE MORE CONVINCING THAN HONEST AI SYSTEMS AND CAN AMPLIFY BELIEF IN MISINFORMATION,"Danry, V., Pataranutaporn, P., Groh, M., Epstein, Z., Maes, P.",2024,"Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and erode trust in the truth. We examined the impact of deceptive AI generated explanations on individuals’ beliefs in a pre-registered online experiment with 23,840 observations from 1,192 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that personal factors such as cognitive reflection and trust in AI do not necessarily protect individuals from these effects caused by deceptive AI generated explanations. Instead, our results show that the logical validity of AI generated deceptive explanations, that is whether the explanation has a causal effect on the truthfulness of the AI’s classification, plays a critical role in countering their persuasiveness – with logically invalid explanations being deemed less credible. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation.",https://arxiv.org/pdf/2408.00024v1.pdf,page_1.html
Shaping Integrity: Why Generative Artificial Intelligence Does Not Have to Undermine Education,"Tan, M.J.T., Maravilla, N.M.A.T.",2024,"This paper examines the role of generative artificial intelligence (GAI) inpromoting academic integrity within educational settings. It explores how AIcan be ethically integrated into classrooms to enhance learning experiences,foster intrinsic motivation, and support voluntary behavior change amongstudents. By analyzing established ethical frameworks and educational theoriessuch as deontological ethics, consequentialism, constructivist learning, andSelf-Determination Theory (SDT), the paper argues that GAI, when usedresponsibly, can enhance digital literacy, encourage genuine knowledgeconstruction, and uphold ethical standards in education. This researchhighlights the potential of GAI to create enriching, personalized learningenvironments that prepare students to navigate the complexities of the modernworld ethically and effectively.",https://arxiv.org/pdf/2407.19088v2.pdf,page_1.html
LookupForensics: A Large-Scale Multi-Task Dataset for Multi-Phase Image-Based Fact Verification,"Cui, S., Nguyen, H.H., Le, T.-N., Lu, C.-S., Echizen, I.",2024,"Amid the proliferation of forged images, notably the tsunami of deepfake content, extensive research has been conducted on using artificial intelligence (AI) to identify forged content in the face of continuing advancements in counterfeiting technologies. We have investigated the use of AI to provide the original authentic image after deepfake detection, which we believe is a reliable and persuasive solution. We call this “image-based automated fact verification,” a name that originated from a text-based fact-checking system used by journalists. We have developed a two-phase open framework that integrates detection and retrieval components. Additionally, inspired by a dataset proposed by Meta Fundamental AI Research, we further constructed a large-scale dataset that is specifically designed for this task. This dataset simulates real-world conditions and includes both content-preserving and content-aware manipulations that present a range of difficulty levels and have potential for ongoing research. This multi-task dataset is fully annotated, enabling it to be utilized for sub-tasks within the forgery identification and fact retrieval domains. This paper makes two main contributions: (1) We introduce a new task, ‘‘image-based automated fact verification,” and present a novel two-phase open framework combining “forgery identification” and “fact retrieval.” (2) We present a large-scale dataset tailored for this new task that features various hand-crafted image edits and machine learning-driven manipulations, with extensive annotations suitable for various sub-tasks. Extensive experimental results validate its practicality for fact verification research and clarify its difficulty levels for various sub-tasks.",https://arxiv.org/pdf/2407.18614v1.pdf,page_1.html
DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models,"Marjanović, S.V., Yu, H., Atanasova, P., ...Lioma, C., Augenstein, I.",2024,"Knowledge-intensive language understanding tasks require Language Models (LMs) to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. However, conflicting knowledge can be present in the LM’s parameters, termed intra-memory conflict, which can affect a model’s propensity to accept contextual knowledge. To study the effect of intra-memory conflict on an LM’s ability to accept relevant context, we utilize two knowledge conflict measures and a novel dataset containing inherently conflicting data, DYNAMICQA. This dataset includes facts with a temporal dynamic nature where facts can change over time and disputable dynamic facts, which can change depending on the viewpoint. DYNAMICQA is the first to include real-world knowledge conflicts and provide context to study the link between the different types of knowledge conflicts. We also evaluate several measures on their ability to reflect the presence of intra-memory conflict: semantic entropy and a novel coherent persuasion score. With our extensive experiments, we verify that LMs exhibit a greater degree of intra-memory conflict with dynamic facts compared to facts that have a single truth value. Furthermore, we reveal that facts with intra-memory conflict are harder to update with context, suggesting that retrieval-augmented generation will struggle with the most commonly adapted facts.MSC Codes 68T50",https://arxiv.org/pdf/2407.17023v2.pdf,page_1.html
Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction,"Bailis, S., Friedhoff, J., Chen, F.",2024,"This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game’s complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding, mirroring real-world discussions where individuals strategically choose when to speak. We demonstrate the framework’s utility through an arena-style tournament featuring Gemini and GPT models. Our results reveal distinct strengths and weaknesses in the models’ strategic reasoning and communication. These findings highlight Werewolf Arena’s potential as a challenging and scalable LLM benchmark.",https://arxiv.org/pdf/2407.13943v1.pdf,page_1.html
GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text,"Hamilton, K., Longo, L., Božić, B.",2024,"While the use of machine learning for the detection of propaganda techniques in text has garnered considerable attention, most approaches focus on “black-box” solutions with opaque inner workings. Interpretable approaches provide a solution, however, they depend on careful feature engineering and costly expert annotated data. Additionally, language features specific to propagandistic text are generally the focus of rhetoricians or linguists, and there is no data set labeled with such features suitable for machine learning. This study codifies 22 rhetorical and linguistic features identified in literature related to the language of persuasion for the purpose of annotating an existing data set labeled with propaganda techniques. To help human experts annotate natural language sentences with these features, RhetAnn, a web application, was specifically designed to minimize an otherwise considerable mental effort. Finally, a small set of annotated data was used to fine-tune GPT-3.5, a generative large language model (LLM), to annotate the remaining data while optimizing for financial cost and classification accuracy. This study demonstrates how combining a small number of human annotated examples with GPT can be an effective strategy for scaling the annotation process at a fraction of the cost of traditional annotation relying solely on human experts. The results are on par with the best performing model at the time of writing, namely GPT-4, at 10x less the cost. Our contribution is a set of features, their properties, definitions, and examples in a machine-readable format, along with the code for RhetAnn and the GPT prompts and fine-tuning procedures for advancing state-of-the-art interpretable propaganda technique detection.",https://arxiv.org/pdf/2407.11827v1.pdf,page_1.html
NudgeRank: Digital Algorithmic Nudging for Personalized Health,"Chiam, J., Lim, A., Teredesai, A.",2024,"In this paper we describe NudgeRank™, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17% increase in daily steps and 7.61% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1% open rate compared to baseline systems’ 4%. Demonstrating scalability and reliability, NudgeRank™ operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems.",https://arxiv.org/pdf/2407.20241v1.pdf,page_1.html
psifx - Psychological and Social Interactions Feature Extraction Package,"Rochette, G., Vowels, M.J., Rochat, M.",2024,"psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to facilitate and democratize the use of state-of-the-art machine learning techniques for human sciences research. It is motivated by a need (a) to automate and standardize data annotation processes, otherwise involving expensive, lengthy, and inconsistent human labor, such as the transcription or coding of behavior changes from audio and video sources; (b) to develop and distribute open-source community-driven psychology research software; and (c) to enable large-scale access and ease of use to non-expert users. The framework contains an array of tools for tasks, such as speaker diarization, closed-caption transcription and translation from audio, body, hand, and facial pose estimation and gaze tracking from video, as well as interactive textual feature extraction supported by large language models, to facilitate automated annotation. The package has been designed with a modular and task-oriented approach, enabling the community to add or update new tools easily. We strongly hope that this package will provide psychologists and social scientists a simple and practical solution for efficiently processing a range of audio, linguistic, and visual features from audio and video, thereby creating new opportunities for in-depth study of real-time behavioral phenomena.",https://arxiv.org/pdf/2407.10266v3.pdf,page_1.html
Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered Motivational Interviewing,"Steenstra, I., Nouraei, F., Arjmand, M., Bickmore, T.W.",2024,"We introduce a novel application of large language models (LLMs) in developing a virtual counselor capable of conducting motivational interviewing (MI) for alcohol use counseling. Access to effective counseling remains limited, particularly for substance abuse, and virtual agents offer a promising solution by leveraging LLM capabilities to simulate nuanced communication techniques inherent in MI. Our approach combines prompt engineering and integration into a user-friendly virtual platform to facilitate realistic, empathetic interactions. We evaluate the effectiveness of our virtual agent through a series of studies focusing on replicating MI techniques and human counselor dialog. Initial findings suggest that our LLMpowered virtual agent matches human counselors' empathetic and adaptive conversational skills, presenting a significant step forward in virtual health counseling and providing insights into the design and implementation of LLM-based therapeutic interactions.",https://arxiv.org/pdf/2407.08095v1.pdf,page_1.html
On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments,"Fang, J., Arechiga, N., Namikoshi, K., ...Hogan, C., Shamma, D.A.",2024,"The Wizard of Oz (WoZ) method is a widely adopted research approach where a human Wizard ""role-plays"" a not readily available technology and interacts with participants to elicit user behaviors and probe the design space. With the growing ability for modern large language models (LLMs) to role-play, one can apply LLMs as Wizards in WoZ experiments with better scalability and lower cost than the traditional approach. However, methodological guidance on responsibly applying LLMs in WoZ experiments and a systematic evaluation of LLMs' role-playing ability are lacking. Through two LLM-powered WoZ studies, we take the first step towards identifying an experiment lifecycle for researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings that involve Wizards role-played by LLMs. We also contribute a heuristic-based evaluation framework that allows the estimation of LLMs' role-playing ability in WoZ experiments and reveals LLMs' behavior patterns at scale.",https://arxiv.org/pdf/2407.08067v1.pdf,page_1.html
Review-LLM: Harnessing Large Language Models for Personalized Review Generation,"Peng, Q., Liu, H., Xu, H., ...Shao, M., Wang, W.",2024,"Product review generation is an important task in recommender systems, which could provide explanation and persuasiveness for the recommendation. Recently, Large Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling and generating ability, which could be applied in review generation. However, directly applying the LLMs for generating reviews might be troubled by the “polite” phenomenon of the LLMs and could not generate personalized reviews (e.g., negative reviews). In this paper, we propose Review-LLM that customizes LLMs for personalized review generation. Firstly, we construct the prompt input by aggregating user historical behaviors, which include corresponding item titles and reviews. This enables the LLMs to capture user interest features and review writing style. Secondly, we incorporate ratings as indicators of satisfaction into the prompt, which could further improve the model’s understanding of user preferences and the sentiment tendency control of generated reviews. Finally, we feed the prompt text into LLMs, and use Supervised Fine-Tuning (SFT) to make the model generate personalized reviews for the given user and target item. Experimental results on the real-world dataset show that our fine-tuned model could achieve better review generation performance than existing close-source LLMs.",https://arxiv.org/pdf/2407.07487v1.pdf,page_1.html
Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities,"Ju, T., Wang, Y., Ma, X., ...Zhang, Z., Liu, G.",2024,"The rapid adoption of large language models (LLMs) in multi-agent systems has highlighted their impressive capabilities in various applications, such as collaborative problem-solving and autonomous negotiation. However, the security implications of these LLM-based multi-agent systems have not been thoroughly investigated, particularly concerning the spread of manipulated knowledge. In this paper, we investigate this critical issue by constructing a detailed threat model and a comprehensive simulation environment that mirrors real-world multi-agent deployments in a trusted platform. Subsequently, we propose a novel two-stage attack method involving Persuasiveness Injection and Manipulated Knowledge Injection to systematically explore the potential for manipulated knowledge (i.e., counterfactual and toxic knowledge) spread without explicit prompt manipulation. Our method leverages the inherent vulnerabilities of LLMs in handling world knowledge, which can be exploited by attackers to unconsciously spread fabricated information. Through extensive experiments, we demonstrate that our attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication. Furthermore, we show that these manipulations can persist through popular retrieval-augmented generation frameworks, where several benign agents store and retrieve manipulated chat histories for future interactions. This persistence indicates that even after the interaction has ended, the benign agents may continue to be influenced by manipulated knowledge. Our findings reveal significant security risks in LLM-based multi-agent systems, emphasizing the imperative need for robust defenses against manipulated knowledge spread, such as introducing “guardian” agents and advanced fact-checking tools. Code is publicly available at https://github.com/Jometeorie/KnowledgeSpread.",https://arxiv.org/pdf/2407.07791v1.pdf,page_1.html
Large Language Models can impersonate politicians and other public figures,"Herbold, S., Trautsch, A., Kikteva, Z., Hautli-Janisz, A.",2024,"Modern AI technology like Large language models (LLMs) has the potential to pollute the public information sphere with made-up content, which poses a significant threat to the cohesion of societies at large. A wide range of research has shown that LLMs are capable of generating text of impressive quality, including persuasive political speech, text with a pre-defined style, and role-specific content. But there is a crucial gap in the literature: We lack large-scale and systematic studies of how capable LLMs are in impersonating political and societal representatives and how the general public judges these impersonations in terms of authenticity, relevance and coherence. We present the results of a study based on a cross-section of British society that shows that LLMs are able to generate responses to debate questions that were part of a broadcast political debate programme in the UK. The impersonated responses are judged to be more authentic and relevant than the original responses given by people who were impersonated. This shows two things: (1) LLMs can be made to contribute meaningfully to the public political debate and (2) there is a dire need to inform the general public of the potential harm this can have on society.",https://arxiv.org/pdf/2407.12855v1.pdf,page_1.html
Change-Point Detection in Industrial Data Streams based on Online Dynamic Mode Decomposition with Control,"Wadinger, M., Kvasnica, M., Kawahara, Y.",2024,"We propose a novel change-point detection method based on online Dynamic Mode Decomposition with control (ODMDwC). Leveraging ODMDwC’s ability to find and track linear approximation of a non-linear system while incorporating control effects, the proposed method dynamically adapts to its changing behavior due to aging and seasonality. This approach enables the detection of changes in spatial, temporal, and spectral patterns, providing a robust solution that preserves correspondence between the score and the extent of change in the system dynamics. We formulate a truncated version of ODMDwC and utilize higher-order time-delay embeddings to mitigate noise and extract broad-band features. Our method addresses the challenges faced in industrial settings where safety-critical systems generate non-uniform data streams while requiring timely and accurate change-point detection to protect profit and life. Our results demonstrate that this method yields intuitive and improved detection results compared to the Singular-Value-Decomposition-based method. We validate our approach using synthetic and real-world data, showing its competitiveness to other approaches on complex systems’ benchmark datasets. Provided guidelines for hyperparameters selection enhance our method’s practical applicability.",https://arxiv.org/pdf/2407.05976v2.pdf,page_1.html
ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal and Multimodal Arabic Content,"Hasanain, M., Hasan, Md.A., Ahmed, F., ...Zaghouani, W., Alam, F.",2024,"We present an overview of the second edition of the ArAIEval shared task, organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In this edition, ArAIEval offers two tasks: (i) detection of propagandistic textual spans with persuasion techniques identification in tweets and news articles, and (ii) distinguishing between propagandistic and non-propagandistic memes. A total of 14 teams participated in the final evaluation phase, with 6 and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams submitted system description papers. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further provide a brief overview of the participating systems. All datasets and evaluation scripts are released to the research community.1 We hope this will enable further research on these important tasks in Arabic.MSC Codes 68T50",https://arxiv.org/pdf/2407.04247v1.pdf,page_1.html
Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness,"Kumar, H., Yoo, S., Bernuy, A.Z., ...Anderson, A., Kornfield, R.",2024,"Although engagement in self-directed wellness exercises typically declines over time, integrating social support such as coaching can sustain it. However, traditional forms of support are often inaccessible due to the high costs and complex coordination. Large Language Models (LLMs) show promise in providing human-like dialogues that could emulate social support. Yet, in-depth, in situ investigations of LLMs to support behavior change remain underexplored. We conducted two randomized experiments to assess the impact of LLM agents on user engagement with mindfulness exercises. First, a single-session study, involved 502 crowdworkers; second, a three-week study, included 54 participants. We explored two types of LLM agents: one providing information and another facilitating self-reflection. Both agents enhanced users' intentions to practice mindfulness. However, only the information-providing LLM, featuring a friendly persona, significantly improved engagement with the exercises. Our findings suggest that specific LLM agents may bridge the social support gap in digital health interventions.",https://arxiv.org/pdf/2407.13067v1.pdf,page_1.html
Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval,"Furumai, K., Legaspi, R., Vizcarra, J., ...Shi, W., Lam, M.S.",2024,"Persuasion plays a pivotal role in a wide range of applications from health intervention to the promotion of social good. Persuasive chatbots employed responsibly for social good can be an enabler of positive individual and social change. Existing methods rely on fine-tuning persuasive chatbots with task-specific training data which is costly, if not infeasible, to collect. Furthermore, they employ only a handful of pre-defined persuasion strategies. We propose PersuaBot, a zero-shot chatbot based on Large Language Models (LLMs) that is factual and more persuasive by leveraging many more nuanced strategies. PersuaBot uses an LLM to first generate natural responses, from which the strategies used are extracted. To combat hallucination of LLMs, Persuabot replace any unsubstantiated claims in the response with retrieved facts supporting the extracted strategies. We applied our chatbot, PersuaBot, to three significantly different domains needing persuasion skills: donation solicitation, recommendations, and health intervention. Our experiments on simulated and human conversations show that our zero-shot approach is more persuasive than prior work, while achieving factual accuracy surpassing state-of-the-art knowledge-oriented chatbots.",https://arxiv.org/pdf/2407.03585v3.pdf,page_1.html
ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024,"Fu, R., Liu, R., Qiang, C., ...Qi, X., Li, G.",2024,"The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024) is part of the ISCSLP 2024 Competitions and Challenges track. While current text-to-speech (TTS) technology can generate high-quality audio, its ability to convey complex emotions and controlled detail content remains limited. This constraint leads to a discrepancy between the generated audio and human subjective perception in practical applications like companion robots for children and marketing bots. The core issue lies in the inconsistency between high-quality audio generation and the ultimate human subjective experience. Therefore, this challenge aims to enhance the persuasiveness and acceptability of synthesized audio, focusing on human alignment convincing and inspirational audio generation. A total of 19 teams have registered for the challenge, and the results of the competition and the competition are described in this paper.",https://arxiv.org/pdf/2407.12038v2.pdf,page_1.html
Analyzing Persuasive Strategies in Meme Texts: A Fusion of Language Models with Paraphrase Enrichment,"Shamanth, K., Nayak, R., Kosseim, L.",2024,"This paper describes our approach to hierarchical multi-label detection of persuasion techniques in meme texts. Our model, developed as a part of the recent SemEval task, is based on fine-tuning individual language models (BERT, XLM-RoBERTa, and mBERT) and leveraging a mean-based ensemble model in addition to dataset augmentation through paraphrase generation from ChatGPT. The scope of the study encompasses enhancing model performance through innovative training techniques and data augmentation strategies. The problem addressed is the effective identification and classification of multiple persuasive techniques in meme texts, a task complicated by the diversity and complexity of such content. The objective of the paper is to improve detection accuracy by refining model training methods and examining the impact of balanced versus unbalanced training datasets. Novelty in the results and discussion lies in the finding that training with paraphrases enhances model performance, yet a balanced training set proves more advantageous than a larger unbalanced one. Additionally, the analysis reveals the potential pitfalls of indiscriminate incorporation of paraphrases from diverse distributions, which can introduce substantial noise. Results with the SemEval 2024 data confirm these insights, demonstrating improved model efficacy with the proposed methods.",https://arxiv.org/pdf/2407.01784v1.pdf,page_1.html
Debate-to-Write: A Persona-Driven Multi-Agent Framework for Diverse Argument Generation,"Hu, Z., Chan, H.P., Li, J., Yin, Y.",2024,"Writing arguments is a challenging task for both humans and machines. It entails incorporating high-level beliefs from various perspectives on the topic, along with deliberate reasoning and planning to construct a coherent narrative. Current language models often generate outputs autoregressively, lacking explicit integration of these underlying controls, resulting in limited output diversity and coherence. In this work, we propose a persona-based multi-agent framework for argument writing. Inspired by the human debate, we first assign each agent a persona representing its high-level beliefs from a unique perspective, and then design an agent interaction process so that the agents can collaboratively debate and discuss the idea to form an overall plan for argument writing. Such debate process enables fluid and nonlinear development of ideas. We evaluate our framework on argumentative essay writing. The results show that our framework generates more diverse and persuasive arguments by both automatic and human evaluations. 1",https://arxiv.org/pdf/2406.19643v3.pdf,page_1.html
Designing and Evaluating Multi-Chatbot Interface for Human-AI Communication: Preliminary Findings from a Persuasion Task,"Yoon, S., Kim, T.E., Oh, Y.J.",2024,"The dynamics of human-AI communication have been reshaped by language models such as ChatGPT. However, extant research has primarily focused on dyadic communication, leaving much to be explored regarding the dynamics of human-AI communication in group settings. The availability of multiple language model chatbots presents a unique opportunity for scholars to better understand the interaction between humans and multiple chatbots. This study examines the impact of multi-chatbot communication in a specific persuasion setting: promoting charitable donations. We developed an online environment that enables multi-chatbot communication and conducted a pilot experiment utilizing two GPT-based chatbots, Save the Children and UNICEF chatbots, to promote charitable donations. In this study, we present our development process of the multi-chatbot interface and present preliminary findings from a pilot experiment. Analysis of qualitative and quantitative feedback are presented, and limitations are addressed.",https://arxiv.org/pdf/2406.19648v1.pdf,page_1.html
Your Turn! How Management Researchers Can Help Their Institutions in the Digital Age!,"da Silva, W.A., Ratten, V., Cassel, G.L., ...Fossatti, P., Vendrametto, O.",2024,"This exploratory study investigates how management researchers can help their higher educational institutions (HEI) face the digital era. Ten senior management members from six Brazilian HEIs (three public and three private) were investigated. In Brazil, e-learning (EL) grew 474% in one decade. The shorter time spent traveling to the classroom and the lower cost of attending a class make EL the preferred option for Brazilian students (mainly those with poor economic conditions living in large geographic areas or with congested urban traffic).Managers question whether their institutions should meet all the demands of gig workers or companies. They also ask themselves if establishing partnerships with the current competitors or MOOCs, or even buying ready materials, would not save money and improve the value added to the students. They need more information about the issues and benefits of it. The new classes could be short, focused, and dynamic. However, managers resent information on the best design of digital classes (colors, images, persuasive texts, and information on scenes). Successfully using games demands a better understanding of the non-technical elements during the preparation, application, and rewarding phases. Due to generative AI, managers question whether assessments should continue focusing on content memorization.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4876902&type=2,page_1.html
Measuring and Benchmarking Large Language Models’ Capabilities to Generate Persuasive Language,"Pauli, A.B., Augenstein, I., Assent, I.",2024,"We are exposed to much information trying to influence us, such as teaser messages, debates, politically framed news, and propaganda — all of which use persuasive language. With the recent interest in Large Language Models (LLMs), we study the ability of LLMs to produce persuasive text. As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive language - both when explicitly instructed to rewrite text to be more or less persuasive and when only instructed to paraphrase. We construct the new dataset PERSUASIVE-PAIRS of pairs of a short text and its rewrite by an LLM to amplify or diminish persuasive language. We multi-annotate the pairs on a relative scale for persuasive language: a valuable resource in itself, and for training a regression model to score and benchmark persuasive language, including for new LLMs across domains. In our analysis, we find that different ‘personas’ in LLaMA3’s system prompt change persuasive language substantially, even when only instructed to paraphrase.",https://arxiv.org/pdf/2406.17753v3.pdf,page_1.html
Evidence of a Log Scaling Law for Political Persuasion with Large Language Models,"Hackenburg, K., Tappin, B.M., Röttger, P., ...Bright, J., Margetts, H.",2024,"Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 U.S. political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25, 982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more. Second, mere task completion (coherence, staying on topic) appears to account for larger models' persuasive advantage. These findings suggest that further scaling model size will not much increase the persuasiveness of static LLM-generated messages.",https://arxiv.org/pdf/2406.14508v1.pdf,page_1.html
MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate,"Amayuelas, A., Yang, X., Antoniades, A., ...Pan, L., Wang, W.",2024,"Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary’s effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model’s persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy.",https://arxiv.org/pdf/2406.14711v2.pdf,page_1.html
Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking,"Elaraby, M., Litman, D., Li, X.L., Magooda, A.",2024,"Generating free-text rationales is among the emergent capabilities of Large Language Models (LLMs). These rationales have been found to enhance LLM performance across various NLP tasks. Recently, there has been growing interest in using these rationales to provide insights for various important downstream tasks. In this paper, we analyze generated free-text rationales in tasks with subjective answers, emphasizing the importance of rationalization in such scenarios. We focus on pairwise argument ranking, a highly subjective task with significant potential for real-world applications, such as debate assistance. We evaluate the persuasiveness of rationales generated by nine LLMs to support their subjective choices. Our findings suggest that open-source LLMs, particularly Llama2-70B-chat, are capable of providing highly persuasive rationalizations, surpassing even GPT models. Additionally, our experiments show that rationale persuasiveness can be improved by controlling its parameters through prompting or through self-refinement.",https://arxiv.org/pdf/2406.13905v1.pdf,page_1.html
Assessing AI vs Human-Authored Spear Phishing SMS Attacks: An Empirical Study,"Francia, J., Hansen, D., Schooley, B., ...Murray, S., Snow, G.",2024,"This paper explores the use of Large Language Models (LLMs) in spear phishing message generation and evaluates their performance compared to human-authored counterparts. Our pilot study examines the effectiveness of smishing (SMS phishing) messages created by GPT-4 and human authors, which have been personalized for willing targets. The targets assessed these messages in a modified ranked-order experiment using a novel methodology we call TRAPD (Threshold Ranking Approach for Personalized Deception). Experiments involved ranking each spear phishing message from most to least convincing, providing qualitative feedback, and guessing which messages were human- or AI-generated. Results show that LLM-generated messages are often perceived as more convincing than those authored by humans, particularly job-related messages. Targets also struggled to distinguish between human- and AI-generated messages. We analyze different criteria the targets used to assess the persuasiveness and source of messages. This study aims to highlight the urgent need for further research and improved countermeasures against personalized AI-enabled social engineering attacks.",https://arxiv.org/pdf/2406.13049v2.pdf,page_1.html
Language Model Council: Democratically Benchmarking Foundation Models on Highly Subjective Tasks,"Zhao, J., Plaza-Del-Arco, F.M., Genchel, B., Curry, A.C.",2024,"As Large Language Models (LLMs) continue to evolve, evaluating them remains a persistent challenge. Many recent evaluations use LLMs as judges to score outputs from other LLMs, often relying on a single large model like GPT-4o. However, using a single LLM judge is prone to intra-model bias, and many tasks – such as those related to emotional intelligence, creative writing, and persuasiveness – may be too subjective for a single model to judge fairly. We introduce the Language Model Council (LMC), where a group of LLMs collaborate to create tests, respond to them, and evaluate each other’s responses to produce a ranking in a democratic fashion. Unlike previous approaches that focus on reducing cost or bias by using a panel of smaller models, our work examines the benefits and nuances of a fully inclusive LLM evaluation system. In a detailed case study on emotional intelligence, we deploy a council of 20 recent LLMs to rank each other on open-ended responses to interpersonal conflicts. Our results show that the LMC produces rankings that are more separable and more robust, and through a user study, we show that they are more consistent with human evaluations than any individual LLM judge. Using all LLMs for judging can be costly, however, so we use Monte Carlo simulations and hand-curated sub-councils to study hypothetical council compositions and discuss the value of the incremental LLM judge.",https://arxiv.org/pdf/2406.08598v4.pdf,page_1.html
Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses,"Amirizaniani, M., Martin, E., Sivachenko, M., Mashhadi, A., Shah, C.",2024,"Theory of Mind (ToM) reasoning entails recognizing that other individuals possess their own intentions, emotions, and thoughts, which is vital for guiding one’s own thought processes. Although large language models (LLMs) excel in tasks such as summarization, question answering, and translation, they still face challenges with ToM reasoning, especially in open-ended questions. Despite advancements, the extent to which LLMs truly understand ToM reasoning and how closely it aligns with human ToM reasoning remains inadequately explored in open-ended scenarios. Motivated by this gap, we assess the abilities of LLMs to perceive and integrate human intentions and emotions into their ToM reasoning processes within open-ended questions. Our study utilizes posts from Reddit’s ChangeMyView platform, which demands nuanced social reasoning to craft persuasive responses. Our analysis, comparing semantic similarity and lexical overlap metrics between responses generated by humans and LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended questions, with even the most advanced models showing notable limitations. To enhance LLM capabilities, we implement a prompt tuning method that incorporates human intentions and emotions, resulting in improvements in ToM reasoning performance. However, despite these improvements, the enhancement still falls short of fully achieving human-like reasoning. This research highlights the deficiencies in LLMs’ social reasoning and demonstrates how integrating human intentions and emotions can boost their effectiveness.",https://arxiv.org/pdf/2406.05659v1.pdf,page_1.html
"More Victories, Less Cooperation: Assessing Cicero's Diplomacy Play","Wongkamjan, W., Gu, F., Wang, Y., ...Peskoff, D., Boyd-Graber, J.L.",2024,"The boardgame Diplomacy is a challenging setting for communicative and cooperative artificial intelligence. The most prominent communicative Diplomacy AI, Cicero, has excellent strategic abilities, exceeding human players. However, the best Diplomacy players master communication, not just tactics, which is why the game has received attention as an AI challenge. This work seeks to understand the degree to which Cicero succeeds at communication. First, we annotate in-game communication with abstract meaning representation to separate in-game tactics from general language. Second, we run two dozen games with humans and Cicero, totaling over 200 human-player hours of competition. While AI can consistently outplay human players, AI–Human communication is still limited because of AI’s difficulty with deception and persuasion. This shows that Cicero relies on strategy and has not yet reached the full promise of communicative and cooperative AI.",https://arxiv.org/pdf/2406.04643v1.pdf,page_1.html
ArMeme: Propagandistic Content in Arabic Memes,"Alam, F., Hasnat, A., Ahmad, F., Hasan, Md.A., Hasanain, M.",2024,"With the rise of digital communication memes have become a significant medium for cultural and political expression that is often used to mislead audience. Identification of such misleading and persuasive multimodal content become more important among various stakeholders, including social media platforms, policymakers, and the broader society as they often cause harm to the individuals, organizations and/or society. While there has been effort to develop AI based automatic system for resource rich languages (e.g., English), it is relatively little to none for medium to low resource languages. In this study, we focused on developing an Arabic memes dataset with manual annotations of propagandistic content.1 We annotated ∼ 6K Arabic memes collected from various social media platforms, which is a first resource for Arabic multimodal research. We provide a comprehensive analysis aiming to develop computational tools for their detection. We made the dataset publicly available for the community.MSC Codes 68T50",https://arxiv.org/pdf/2406.03916v2.pdf,page_1.html
Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation,"Li, H., Wu, Y., Schlegel, V., ...Li, Y., Nenadic, G.",2024,"With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with the various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at https://github.com/HaoBytes/ArgSum-Datatset.",https://arxiv.org/pdf/2406.03151v3.pdf,page_1.html
On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial,"Salvi, F., Ribeiro, M.H., Gallotti, R., West, R.",2024,"Can large language models (LLMs) create tailor-made, convincing arguments to promote false or misleading narratives online? Early work has found that LLMs can generate content perceived on par with, or even more persuasive than, human-written messages. However, there is still limited evidence regarding LLMs’ persuasive capabilities in direct conversations with humans—the scenario these models are usually deployed at. In this pre-registered study, we analyze the power of AI-driven persuasion in a controlled, harmless setting. To this end, we created a web-based platform where human participants engaged in short, multi-round debates with either human or LLM opponents. Each participant was randomly assigned to one of four treatment conditions in a two-by-two factorial design: (1) the conversation partner was either another human or an LLM; (2) the conversation partner either had or did not have access to basic sociodemographic information about their opponent (and thus arguments could be personalized). We find that 64.4% of the time, personalized LLM debaters were more persuasive than humans, given that they were not equally persuasive (81.2% relative increase in the odds of higher post-debate agreement; p < 0.01; N = 900). Without personalization, GPT-4 still outperformed humans, but the effect was lower and not statistically significant (p = 0.30). Further, our analysis suggests that LLMs use different strategies from human debaters: their texts are harder to read and have more markers associated with logical and analytical reasoning. Overall, our results suggest that concerns around LLM-based persuasion are meaningful and have important implications for social media governance and the design of new online environments.",https://www.researchsquare.com/article/rs-4429707/v1.pdf,page_1.html
Towards Dialogues for Joint Human-AI Reasoning and Value Alignment,"Bezou-Vrakatseli, E., Cocarascu, O., Modgil, S.",2024,"We argue that enabling human-AI dialogue, purposed to support joint reasoning (i.e., ‘inquiry’), is important for ensuring that AI decision making is aligned with human values and preferences. In particular, we point to logic-based models of argumentation and dialogue, and suggest that the traditional focus on persuasion dialogues be replaced by a focus on inquiry dialogues, and the distinct challenges that joint inquiry raises. Given recent dramatic advances in the performance of large language models (LLMs), and the anticipated increase in their use for decision making, we provide a roadmap for research into inquiry dialogues for supporting joint human-LLM reasoning tasks that are ethically salient, and that thereby require that decisions are value aligned.",https://arxiv.org/pdf/2405.18073v1.pdf,page_1.html
Efficient Model-agnostic Alignment via Bayesian Persuasion,"Bai, F., Wang, M., Zhang, Z., ...Wen, Y., Yang, Y.",2024,"With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent. Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data. This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework. We formalize this problem as an optimization of the signaling strategy from the small model's perspective. In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses. The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item. Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks. We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of 16.1% in mathematical reasoning ability and 13.7% in code generation. We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective.",https://arxiv.org/pdf/2405.18718v1.pdf,page_2.html
Coaching Copilot: Blended Form of an LLM-Powered Chatbot and a Human Coach to Effectively Support Self-Reflection for Leadership Growth,"Arakawa, R., Yakura, H.",2024,"Chatbots’ role in fostering self-reflection is now widely recognized, especially in inducing users’ behavior change. While the benefits of 24/7 availability, scalability, and consistent responses have been demonstrated in contexts such as healthcare and tutoring to help one form a new habit, their utilization in coaching necessitating deeper introspective dialogue to induce leadership growth remains unexplored. This paper explores the potential of such a chatbot powered by recent Large Language Models (LLMs) in collaboration with professional coaches in the field of executive coaching. Through a design workshop with them and two weeks of user study involving ten coach-client pairs, we explored the feasibility and nuances of integrating chatbots to complement human coaches. Our findings highlight the benefits of chatbots’ ubiquity and reasoning capabilities enabled by LLMs while identifying their limitations and design necessities for effective collaboration between human coaches and chatbots. By doing so, this work contributes to the foundation for augmenting one’s self-reflective process with prevalent conversational agents through the human-in-the-loop approach.",https://arxiv.org/pdf/2405.15250v1.pdf,page_2.html
Large Language Models for Social Determinants of Health Information Extraction from Clinical Notes - A Generalizable Approach across Institutions,"Keloth, V.K., Selek, S., Chen, Q., ...Liu, H., Xu, H.",2024,"The consistent and persuasive evidence illustrating the influence of social determinants on health has prompted a growing realization throughout the health care sector that enhancing health and health equity will likely depend, at least to some extent, on addressing detrimental social determinants. However, detailed social determinants of health (SDoH) information is often buried within clinical narrative text in electronic health records (EHRs), necessitating natural language processing (NLP) methods to automatically extract these details. Most current NLP efforts for SDoH extraction have been limited, investigating on limited types of SDoH elements, deriving data from a single institution, focusing on specific patient cohorts or note types, with reduced focus on generalizability. This study aims to address these issues by creating cross-institutional corpora spanning different note types and healthcare systems, and developing and evaluating the generalizability of classification models, including novel large language models (LLMs), for detecting SDoH factors from diverse types of notes from four institutions: Harris County Psychiatric Center, University of Texas Physician Practice, Beth Israel Deaconess Medical Center, and Mayo Clinic. Four corpora of deidentified clinical notes were annotated with 21 SDoH factors at two levels: level 1 with SDoH factor types only and level 2 with SDoH factors along with associated values. Three traditional classification algorithms (XGBoost, TextCNN, Sentence BERT) and an instruction tuned LLM-based approach (LLaMA) were developed to identify multiple SDoH factors. Substantial variation was noted in SDoH documentation practices and label distributions based on patient cohorts, note types, and hospitals. The LLM achieved top performance with micro-averaged F1 scores over 0.9 on level 1 annotated corpora and an F1 over 0.84 on level 2 annotated corpora. While models performed well when trained and tested on individual datasets, cross-dataset generalization highlighted remaining obstacles. To foster collaboration, access to partial annotated corpora and models trained by merging all annotated datasets will be made available on the PhysioNet repository.",https://www.medrxiv.org/content/10.1101/2024.05.21.24307726v1.full.pdf,page_2.html
Investigating Persuasion Techniques in Arabic: An Empirical Study Leveraging Large Language Models,"Alzahrani, A., Babkier, E., Yanbaawi, F., Yanbaawi, F., Alhuzali, H.",2024,"In the current era of digital communication and widespread use of social media, it is crucial to develop an understanding of persuasive techniques employed in written text. This knowledge is essential for effectively discerning accurate information and making informed decisions. To address this need, this paper presents a comprehensive empirical study focused on identifying persuasive techniques in Arabic social media content. To achieve this objective, we utilize Pre-trained Language Models (PLMs) and leverage the ArAlEval dataset, which encompasses two tasks: binary classification to determine the presence or absence of persuasion techniques, and multi-label classification to identify the specific types of techniques employed in the text. Our study explores three different learning approaches by harnessing the power of PLMs: feature extraction, fine-tuning, and prompt engineering techniques. Through extensive experimentation, we find that the fine-tuning approach yields the highest results on the aforementioned dataset, achieving an f1-micro score of 0.865 and an f1-weighted score of 0.861. Furthermore, our analysis sheds light on an interesting finding. While the performance of the GPT model is relatively lower compared to the other approaches, we have observed that by employing few-shot learning techniques, we can enhance its results by up to 20%. This offers promising directions for future research and exploration in this topica",https://arxiv.org/pdf/2405.12884v1.pdf,page_2.html
LLM-Generated Black-box Explanations Can Be Adversarially Helpful,"Ajwani, R., Javaji, S.R., Rudzicz, F., Zhu, Z.",2024,"Large language models (LLMs) are becoming vital tools that help us solve and understand complex problems. LLMs can generate convincing explanations, even when given only the inputs and outputs of these problems, i.e., in a “black-box” approach. However, our research uncovers a hidden risk tied to this approach, which we call adversarial helpfulness. This happens when an LLM’s explanations make a wrong answer look correct, potentially leading people to trust faulty solutions. In this paper, we show that this issue affects not just humans, but also LLM evaluators. Digging deeper, we identify and examine key persuasive strategies employed by LLMs. Our findings reveal that these models employ strategies such as reframing questions, expressing an elevated level of confidence, and ‘cherry-picking’ evidence that supports incorrect answers. We further create a symbolic graph reasoning task to analyze the mechanisms of LLMs generating adversarial helpfulness explanations. Most LLMs are not able to find alternative paths along simple graphs, indicating that other mechanisms, rather than logical deductions, might facilitate adversarial helpfulness. These findings shed light on the limitations of black-box explanations and lead to recommendations for the safer use of LLMs.",https://arxiv.org/pdf/2405.06800v3.pdf,page_2.html
Impact of Tone-Aware Explanations in Recommender Systems,"Okoso, A., Otaki, K., Koide, S., Baba, Y.",2024,"In recommender systems, the presentation of explanations plays a crucial role in supporting users’ decision-making processes. Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked. Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication. However, studies on the impact of tone on explanations within the context of recommender systems are insufficient. Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes. We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products. Collected data analysis reveals different perceived effects of tones depending on the domains. Moreover, user attributes such as age and personality traits are found to influence the impact of tone. This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience.",https://arxiv.org/pdf/2405.05061v1.pdf,page_2.html
Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control,"Chaudhary, Y., Penn, J.",2024,"Large language models (LLMs) can reproduce a wide variety of rhetorical styles and generate text that expresses a broad spectrum of sentiments. This capacity, now available at low cost, makes them powerful tools for manipulation and control. In this paper, we consider a set of underestimated societal harms made possible by the rapid and largely unregulated adoption of LLMs. Rather than consider LLMs as isolated digital artefacts used to displace this or that area of work, we focus on the large-scale computational infrastructure upon which they are instrumentalised across domains. We begin with discussion on how LLMs may be used to both pollute and uniformize information environments and how these modalities may be leveraged as mechanisms of control. We then draw attention to several areas of emerging research, each of which compounds the capabilities of LLMs as instruments of power. These include (i) persuasion through the real-time design of choice architectures in conversational interfaces (e.g., via “AI personas”), (ii) the use of LLM-agents as computational models of human agents (e.g., “silicon subjects”), (iii) the use of LLM-agents as computational models of human agent populations (e.g., “silicon societies”) and finally, (iv) the combination of LLMs with reinforcement learning to produce controllable and steerable strategic dialogue models. We draw these strands together to discuss how these areas may be combined to build LLM-based systems that serve as powerful instruments of individual, social and political control via the simulation and disingenuous ""prediction"" of human behaviour, intent, and action.",https://arxiv.org/pdf/2405.03813v1.pdf,page_2.html
Can Nuanced Language Lead to More Actionable Insights? Exploring the Role of Generative AI in Analytical Narrative Structure,"Setlur, V., Birnbaum, L.",2024,"Relevant language describing trends in data can be useful for generating summaries to help with readers' takeaways. However, the language employed in these often template-generated summaries tends to be simple, ranging from describing simple statistical information (e.g., extrema and trends) without additional context and richer language to provide actionable insights. Recent advances in Large Language Models (LLMs) have shown promising capabilities in capturing subtle nuances in language when describing information. This workshop paper specifically explores how LLMs can provide more actionable insights when describing trends by focusing on three dimensions of analytical narrative structure: semantic, rhetorical, and pragmatic. Building on prior research that examines visual and linguistic signatures for univariate line charts, we examine how LLMs can further leverage the semantic dimension of analytical narratives using quantified semantics to describe shapes in trends as people intuitively view them. These semantic descriptions help convey insights in a way that leads to a pragmatic outcome, i.e., a call to action, persuasion, warning vs. alert, and situational awareness. Finally, we identify rhetorical implications for how well these generated narratives align with the perceived shape of the data, thereby empowering users to make informed decisions and take meaningful actions based on these data insights.",https://arxiv.org/pdf/2405.02763v1.pdf,page_2.html
How the Anthropomorphism of an Artificial Intelligence Agent Affects the Correction of Misinformation: A Focus on Authenticity and Behavioral-Response System Level,"Oh, J., Lim, S.",2024,"AI agents are increasingly leveraged to counter misinformation. This study probes their impact on correction, focusing on user interaction authenticity. Examining anthropomorphism and behavioral response, it seeks to discern how perceived authenticity shapes AI agents' persuasive impact in misinformation correction. In a 2x2 experiment with 320 participants correcting misinformation about the HPV vaccine, high anthropomorphism induces greater authenticity in source, message, and interaction compared to low anthropomorphism. Additionally, perceived interaction authenticity mediates the relationship between anthropomorphism and attitudinal change, moderated by the behavioral response system level. These findings provide vital evidence underlining the importance of anthropomorphism and behavioral response in fostering authentic interactions, with practical implications.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4809351&type=2,page_2.html
Can a Hallucinating Model help in Reducing Human “Hallucination”?,"Sundaram, S.S., Alwar, B.",2024,"The prevalence of unwarranted beliefs, spanning pseudoscience, logical fallacies, and conspiracy theories, presents substantial societal hurdles and the risk of disseminating misinformation. Utilizing established psychometric assessments, this study explores the capabilities of large language models (LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls. We undertake a philosophical inquiry, juxtaposing the rationality of humans against that of LLMs. Furthermore, we propose methodologies for harnessing LLMs to counter misconceptions, drawing upon psychological models of persuasion such as cognitive dissonance theory and elaboration likelihood theory. Through this endeavor, we highlight the potential of LLMs as personalized misinformation debunking agents.",https://arxiv.org/pdf/2405.00843v1.pdf,page_2.html
Using Deep Q-Learning to Dynamically Toggle between Push/Pull Actions in Computational Trust Mechanisms,"Lygizou, Z., Kalles, D.",2024,"Recent work on decentralized computational trust models for open Multi Agent Systems has resulted in the development of CA, a biologically inspired model which focuses on the trustee’s perspective. This new model addresses a serious unresolved problem in existing trust and reputation models, namely the inability to handle constantly changing behaviors and agents’ continuous entry and exit from the system. In previous work, we compared CA to FIRE, a well-known trust and reputation model, and found that CA is superior when the trustor population changes, whereas FIRE is more resilient to the trustee population changes. Thus, in this paper, we investigate how the trustors can detect the presence of several dynamic factors in their environment and then decide which trust model to employ in order to maximize utility. We frame this problem as a machine learning problem in a partially observable environment, where the presence of several dynamic factors is not known to the trustor and we describe how an adaptable trustor can rely on a few measurable features so as to assess the current state of the environment and then use Deep Q Learning (DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt to a changing environment. We ran a series of simulation experiments to compare the performance of the adaptable trustor with the performance of trustors using only one model (FIRE or CA) and we show that an adaptable agent is indeed capable of learning when to use each model and, thus, perform consistently in dynamic environments.",https://arxiv.org/pdf/2404.18296v1.pdf,page_2.html
HOW TO PARAMETERIZE ASYMMETRIC QUANTIZATION RANGES FOR QUANTIZATION-AWARE TRAINING,"You, J., Park, M., Lee, K., ...Patel, C., Nagel, M.",2024,"This paper investigates three different parameterizations of asymmetric uniform quantization for quantization-aware training: (1) scale and offset, (2) minimum and maximum, and (3) beta and gamma. We perform a comprehensive comparative analysis of these parameterizations’ influence on quantization-aware training, using both controlled experiments and real-world large language models. Our particular focus is on their changing behavior in response to critical training hyper-parameters, bit width and learning rate. Based on our investigation, we propose best practices to stabilize and accelerate quantization-aware training with learnable asymmetric quantization ranges.",https://arxiv.org/pdf/2404.16898v1.pdf,page_2.html
The Ethics of Advanced AI Assistants,"Gabriel, I., Manzini, A., Keeling, G., ...Isaac, W., Manyika, J.",2024,"This paper focuses on the opportunities and the ethical and societal risks posed by advanced AI assistants. We define advanced AI assistants as artificial agents with natural language interfaces, whose function is to plan and execute sequences of actions on behalf of a user – across one or more domains – in line with the user’s expectations. The paper starts by considering the technology itself, providing an overview of AI assistants, their technical foundations and potential range of applications. It then explores questions around AI value alignment, well-being, safety and malicious uses. Extending the circle of inquiry further, we next consider the relationship between advanced AI assistants and individual users in more detail, exploring topics such as manipulation and persuasion, anthropomorphism, appropriate relationships, trust and privacy. With this analysis in place, we consider the deployment of advanced assistants at a societal scale, focusing on cooperation, equity and access, misinformation, economic impact, the environment and how best to evaluate advanced AI assistants. Finally, we conclude by providing a range of recommendations for researchers, developers, policymakers and public stakeholders.",https://arxiv.org/pdf/2404.16244v1.pdf,page_2.html
A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI,"El-Sayed, S., Akbulut, C., McCroskery, A., ...Everitt, T., Brown, S.",2024,"Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making. Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions. This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion. The current definitions of AI persuasion are unclear and related harms are insufficiently studied. Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion. In this paper, we lay the groundwork for the systematic study of AI persuasion. We first put forward definitions of persuasive generative AI. We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information. We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm. We then introduce a map of mechanisms that contribute to harmful persuasion. Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming. Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion.",https://arxiv.org/pdf/2404.15058v1.pdf,page_2.html
Human-in-the-loop Learning for Dynamic Congestion Games,"Li, H., Duan, L.",2024,"Today mobile users learn and share their traffic observations via crowdsourcing platforms (e.g., Google Maps and Waze). Yet such platforms simply cater to selfish users’ myopic interests to recommend the shortest path, and do not encourage enough users to travel and learn other paths for future others. Prior studies focus on one-shot congestion games without considering users’ information learning, while our work studies how users learn and alter traffic conditions on stochastic paths in a human-in-the-loop manner. In a typical parallel routing network with one deterministic path and multiple stochastic paths, our analysis shows that the myopic routing policy (used by Google Maps and Waze) leads to severe under-exploration of stochastic paths. This results in a price of anarchy (PoA) greater than 2, as compared to the socially optimal policy achieved through optimal exploration-exploitation tradeoff in minimizing the long-term social cost. Besides, the myopic policy fails to ensure the correct learning convergence about users’ traffic hazard beliefs. To address this, we focus on informational (non-monetary) mechanisms as they are easier to implement than pricing. We first show that existing information-hiding mechanisms and deterministic path-recommendation mechanisms in Bayesian persuasion literature do not work with even PoA = ∞. Accordingly, we propose a new combined hiding and probabilistic recommendation (CHAR) mechanism to hide all information from a selected user group and provide state-dependent probabilistic recommendations to the other user group. Our CHAR mechanism successfully ensures PoA less than 45 , which cannot be further reduced by any other informational (non-monetary) mechanism. Besides the parallel network, we further extend our analysis and CHAR mechanism to more general linear path graphs with multiple intermediate nodes, and we prove that the PoA results remain unchanged. Additionally, we carry out experiments with real-world datasets to further extend our routing graphs and verify the close-to-optimal performance of our CHAR mechanism.",https://arxiv.org/pdf/2404.15599v2.pdf,page_2.html
Resistance Against Manipulative AI: key factors and possible actions,"Wilczynski, P., Mieleszczenko-Kowszewicz, W., Biecek, P.",2024,"If AI is the new electricity, what should we do to keep ourselves from getting electrocuted? In this work, we explore factors related to the potential of large language models (LLMs) to manipulate human decisions. We describe the results of two experiments designed to determine what characteristics of humans are associated with their susceptibility to LLM manipulation, and what characteristics of LLMs are associated with their manipulativeness potential. We explore human factors by conducting user studies in which participants answer general knowledge questions using LLM-generated hints, whereas LLM factors by provoking language models to create manipulative statements. Then, we analyze their obedience, the persuasion strategies used, and the choice of vocabulary. Based on these experiments, we discuss two actions that can protect us from LLM manipulation. In the long term, we put AI literacy at the forefront, arguing that educating society would minimize the risk of manipulation and its consequences. We also propose an ad hoc solution, a classifier that detects LLM manipulation – a Manipulation Fuse.",https://arxiv.org/pdf/2404.14230v2.pdf,page_2.html
Counterfactual Reasoning Using Predicted Latent Personality Dimensions for Optimizing Persuasion Outcome,"Zeng, D., Legaspi, R.S., Sun, Y., ...Spirtes, P., Zhang, K.",2024,"Customizing persuasive conversations related to the outcome of interest for specific users achieves better persuasion results. However, existing persuasive conversation systems rely on persuasive strategies and encounter challenges in dynamically adjusting dialogues to suit the evolving states of individual users during interactions. This limitation restricts the system’s ability to deliver flexible or dynamic conversations and achieve suboptimal persuasion outcomes. In this paper, we present a novel approach that tracks a user’s latent personality dimensions (LPDs) during ongoing persuasion conversation and generates tailored counterfactual utterances based on these LPDs to optimize the overall persuasion outcome. In particular, our proposed method leverages a Bi-directional Generative Adversarial Network (BiCoGAN) in tandem with a Dialogue-based Personality Prediction Regression (DPPR) model to generate counterfactual data D̃. This enables the system to formulate alternative persuasive utterances that are more suited to the user. Subsequently, we utilize the D3QN model to learn policies for optimized selection of system utterances on D̃. Experimental results we obtained from using the PersuasionForGood dataset demonstrate the superiority of our approach over the existing method, BiCoGAN. The cumulative rewards and Q-values produced by our method surpass ground truth benchmarks, showcasing the efficacy of employing counterfactual reasoning and LPDs to optimize reinforcement learning policy in online interactions.",https://arxiv.org/pdf/2404.13792v1.pdf,page_2.html
"Large Language Models are as persuasive as humans, but why? About the cognitive effort and moral-emotional language of LLM arguments","Carrasco-Farré, C.",2024,"Large Language Models (LLMs) are already as persuasive as humans. However, we know very liEle about why. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuaion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.",https://arxiv.org/pdf/2404.09329v1.pdf,page_2.html
A biologically inspired computational trust model for open multi-agent systems which is resilient to trustor population changes,"Lygizou, Z., Kalles, D.",2024,"Current trust and reputation models continue to have significant limitations, such as the inability to deal with agents constantly entering or exiting open multi-agent systems (open MAS), as well as continuously changing behaviors. Our study is based on CA, a previously proposed decentralized computational trust model from the trustee's point of view, inspired by synaptic plasticity and the formation of assemblies in the human brain. It is designed to meet the requirements of highly dynamic and open MAS, and its main difference with most conventional trust and reputation models is that the trustor does not select a trustee to delegate a task; instead, the trustee determines whether it is qualified to successfully execute it. We ran a series of simulations to compare CA model to FIRE, a well-established, decentralized trust and reputation model for open MAS, under conditions of continuous trustee and trustor population replacement, as well as continuous change of trustees' abilities to perform tasks. The main finding is that FIRE is superior to changes in the trustee population, whereas CA is resilient to the trustor population changes. When the trustees switch performance profiles FIRE clearly outperforms despite the fact that both models' performances are significantly impacted by this environmental change. Findings lead us to conclude that learning to use the appropriate trust model, according to the dynamic conditions in effect could maximize the trustor's benefits.",https://arxiv.org/pdf/2404.10014v1.pdf,page_2.html
Subtoxic Questions: Dive Into Attitude Change of LLM’s Response in Jailbreak Attempts,"Zhang, T., Zhao, Z., Huang, J., Hua, J., Zhong, S.",2024,"As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and more attention, it is of great significance to raise a generalized research paradigm to evaluate attack strengths and a basic model to conduct subtler experiments. In this paper, we propose a novel approach by focusing on a set of target questions that are inherently more sensitive to jailbreak prompts, aiming to circumvent the limitations posed by enhanced LLM security. Through designing and analyzing these sensitive questions, this paper reveals a more effective method of identifying vulnerabilities in LLMs, thereby contributing to the advancement of LLM security. This research not only challenges existing jailbreaking methodologies but also fortifies LLMs against potential exploits.",https://arxiv.org/pdf/2404.08309v1.pdf,page_2.html
Usability of Chatbots,"Monisha, N., Ganesh, N.V., Supriya, D.L.P., ...Adithya, V.D.S., Sangeeta",2024,"Chabot’s are becoming increasingly popular in various domains, such as healthcare, customer service, and education. However, to be effective, catboats must be usable and provide a positive user experience. This paper provides a brief review of the literature on the usability of Chabot’s published between 2019 and 2023. The studies reviewed suggest that different factors, such as feedback mechanisms, interface design, and Chabot type, can impact Chabot usability. The studies also highlight the potential of Chabot’s as effective tools for promoting health behavior change and identifying mental health issues in young people. However, gaps in the literature, such as the need for more studies on Chabot usability in specific domains and the need for standardized measures of Chabot usability, suggest the field of science is still growing and there are many mysteries to be explored.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4488011&type=2,page_2.html
Sustenance Maintenance and Ailment Redressal System,"Kumar, N., Tiwari, S., Poria, S., ...Kumar, A., Tripathi, A.",2024,"The study proposes a conceptual model of chatbot in healthcare in accordance with the Indian perspective. The traditional healthcare system that focuses on curing illnesses is well known but less talked about nowadays. We will discuss the possibilities and analyze the other research to date to obtain a persuasive, inclusive, vigilant, and smart solution. The model hence proposed we named as Sustenance Maintenance and Ailment Redressal System (SMARS). The above-coined model will be used explicitly to monitor health, detect illnesses early, and suggest healthcare methods. This system will record the health vitals on daily bases, which will help in future needs. This system will suggest doctors for check-ups of illnesses which is a very time-effective process. Moreover, this system provides a healthcare solution that is reliable, costeffective, and easily adaptable to the persisting system. Chatbot interactions in real-time are a very useful tool for health care and it should be not only useful but usual also. A variety of health issues can be addressed in real-time. Chatbot may suggest homemade remedies and quick first aid procedures for curing health at home. This paper is based on analyzing the prior research and finding the possibilities of integrating the existing system with an advanced technological tool that provides suggestions on Ayurvedic healthcare based on data and algorithms. It also combines modern technology with traditional Ayurvedic principles to offer personalized recommendations for different illnesses and healthcare conditions by using a vast database of Ayurvedic knowledge and algorithms to analyze various factors such as the individual's health condition, symptoms, dosha constitution, and lifestyle. Furthermore, this analysis suggests appropriate Ayurvedic medicines, therapies, dietary recommendations, and lifestyle modifications that support the individual’s health and wellbeing.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4492376&type=2,page_2.html
IITK at SemEval-2024 Task 4: Hierarchical Embeddings for Detection of Persuasion Techniques in Memes,"Chikoti, S., Mehta, S., Modi, A.",2024,"Memes are one of the most popular types of content used in an online disinformation campaign. They are primarily effective on social media platforms since they can easily reach many users. Memes in a disinformation campaign achieve their goal of influencing the users through several rhetorical and psychological techniques, such as causal oversimplification, name-calling, and smear. The SemEval 2024 Task 4 Multilingual Detection of Persuasion Technique in Memes on identifying such techniques in the memes is divided across three sub-tasks: (1) Hierarchical multi-label classification using only textual content of the meme, (2) Hierarchical multi-label classification using both, textual and visual content of the meme and (3) Binary classification of whether the meme contains a persuasion technique or not using it’s textual and visual content. This paper proposes an ensemble of Class Definition Prediction (CDP) and hyperbolic embeddings-based approaches for this task. We enhance meme classification accuracy and comprehensiveness by integrating HypEmo’s hierarchical label embeddings (Chen et al., 2023) and a multi-task learning framework for emotion prediction. We achieve a hierarchical F1-score of 0.60, 0.67, and 0.48 on the respective sub-tasks.",https://arxiv.org/pdf/2404.04520v1.pdf,page_2.html
Automated Lane Change Behavior Prediction and Environmental Perception Based on SLAM Technology,"Lei, H., Wang, B., Shui, Z., Yang, P., Liang, P.",2024,"In addition to environmental perception sensors such as cameras, radars, etc. in the automatic driving system, the external environment of the vehicle is perceived, in fact, there is also a perception sensor that has been silently dedicated in the system, that is, the positioning module. This paper explores the application of SLAM (Simultaneous Localization and Mapping) technology in the context of automatic lane change behavior prediction and environment perception for autonomous vehicles. It discusses the limitations of traditional positioning methods, introduces SLAM technology, and compares LIDAR SLAM with visual SLAM. Real-world examples from companies like Tesla, Waymo, and Mobileye showcase the integration of AI-driven technologies, sensor fusion, and SLAM in autonomous driving systems. The paper then delves into the specifics of SLAM algorithms, sensor technologies, and the importance of automatic lane changes in driving safety and efficiency. It highlights Tesla's recent update to its Autopilot system, which incorporates automatic lane change functionality using SLAM technology. The paper concludes by emphasizing the crucial role of SLAM in enabling accurate environment perception, positioning, and decision-making for autonomous vehicles, ultimately enhancing safety and driving experience.",https://arxiv.org/pdf/2404.04492v1.pdf,page_2.html
Can Language Models Recognize Convincing Arguments?,"Rescala, P.D., Ribeiro, M.H., Hu, T., West, R.",2024,"The capabilities of large language models (LLMs) have raised concerns about their potential to create and propagate convincing narratives. Here, we study their performance in detecting convincing arguments to gain insights into LLMs’ persuasive capabilities without directly engaging in experimentation with humans. We extend a dataset by Durmus and Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs’ ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance. The data and code released with this paper contribute to the crucial effort of continuously evaluating and monitoring LLMs’ capabilities and potential impact. (https://go.epfl.ch/persuasion-llm)",https://arxiv.org/pdf/2404.00750v2.pdf,page_2.html
LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models,"Peng, M., Guo, X., Chen, X., ...Wang, X., Wang, Y.",2024,"To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as prompts for input into the LLM and employing a supervised fine-tuning technique to tailor the LLM specifically for our lane change prediction task. This allows us to utilize the LLM’s powerful common sense reasoning abilities to understand complex interactive information, thereby improving the accuracy of long-term predictions. Furthermore, we incorporate explanatory requirements into the prompts in the inference stage. Therefore, our LC-LLM model not only can predict lane change intentions and trajectories but also provides explanations for its predictions, enhancing the interpretability. Extensive experiments on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. Compared with baseline models, the performance of intention prediction can be improved by 3.1%, the root-mean-squared error (RMSE) of lateral trajectory prediction can be reduced by 19.4%, and the RMSE of longitudinal trajectory prediction can be reduced by 38.1%. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can encode comprehensive interaction information for driving behavior understanding.",https://arxiv.org/pdf/2403.18344v1.pdf,page_2.html
A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks,"DeBuse, M., Warnick, S.",2024,"In this work we consider the impact of information spread in time-varying social networks, where agents request to follow other agents with aligned opinions while dropping ties to neighbors whose posts are too dissimilar to their own views. Opinion control and rhetorical influence has a very long history, employing various methods including education, persuasion, propaganda, marketing, and manipulation through mis-, dis-, and mal-information. The automation of opinion controllers, however, has only recently become easily deployable at a wide scale, with the advent of large language models (LLMs) and generative AI that can translate the quantified commands from opinion controllers into actual content with the appropriate nuance. Automated agents in social networks can be deployed for various purposes, such as breaking up echo chambers, bridging valuable new connections between agents, or shaping the opinions of a target population—and all of these raise important ethical concerns that deserve serious attention and thoughtful discussion and debate. This paper attempts to contribute to this discussion by considering three archetypal influencing styles observed by human drivers in these settings, comparing and contrasting the impact of these different control methods on the opinions of agents in the network. We will demonstrate the efficacy of current generative AI for generating nuanced content consistent with the command signal from automatic opinion controllers like these, and we will report on frameworks for approaching the relevant ethical considerations.",https://arxiv.org/pdf/2403.18163v1.pdf,page_2.html
Eternagram: Probing Player Attitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure,"Zhou, S., Hendra, L.B., Zhang, Q., Holopainen, J., Lc, R.",2024,"Conventional methods of assessing attitudes towards climate change are limited in capturing authentic opinions, primarily stemming from a lack of context-specific assessment strategies and an overreliance on simplistic surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome these issues by immersing participants in engaging gameplay within carefully crafted, scenario-based environments. Concurrently, advancements in AI and Natural Language Processing (NLP) show promise in enhancing the gamified testing environment, achieving this by generating context-aware, human-like dialogues that contribute to a more natural and effective assessment. Our study introduces a new technique for probing climate change attitudes by actualizing a GPT-driven chatbot system in harmony with a game design depicting a futuristic climate scenario. The correlation analysis reveals an assimilation effect, where players’ post-game climate awareness tends to align with their in-game perceptions. Key predictors of pro-climate attitudes are identified as traits like’Openness’ and’Agreeableness’, and a preference for democratic values.",https://arxiv.org/pdf/2403.18160v1.pdf,page_2.html
Utilizing LLMs for Enhanced Argumentation and Extraction of Causal Knowledge from Scientific Literature,"Wang, S., Chen, W., Zhang, Y., Chen, T., Du, J.",2024,"Current semantic extraction tools have limited performance in identifying causal relations, neglecting variations in argument quality, especially persuasive strength across different sentences. The present study proposes a five-element based (evidence cogency, concept, relation stance, claim-context relevance, conditional information) causal knowledge mining framework and automatically implements it using large language models (LLMs) to improve the understanding of disease causal mechanisms. As a result, regarding cogency evaluation, the accuracy (0.84) of the fine-tuned Llama2-7b largely exceeds the accuracy of GPT-3.5 turbo with few-shot. Regarding causal extraction, by combining PubTator and ChatGLM, the entity first-relation later extraction (recall, 0.85) outperforms the relation first-entity later means (recall, 0.76), performing great in three outer validation sets (a gestational diabetes-relevant dataset and two general biomedical datasets), aligning entities for further causal graph construction. LLMs-enabled scientific causality mining is promising in delineating the causal argument structure and understanding the underlying mechanisms of a given exposure-outcome pair.",https://www.medrxiv.org/content/10.1101/2024.03.20.24304652v2.full.pdf,page_2.html
On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial,"Salvi, F., Ribeiro, M.H., Gallotti, R., West, R.",2024,"The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. In this preregistered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting. We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N = 820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p = 0.31). Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments.",https://arxiv.org/pdf/2403.14380v1.pdf,page_2.html
Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech,"Alyahya, G., Aldayel, A.",2024,"Examining the factors that the counterspeech uses are at the core of understanding the optimal methods for confronting hate speech online. Various studies have assessed the emotional base factors used in counter speech, such as emotional empathy, offensiveness, and hostility. To better understand the counterspeech used in conversations, this study distills persuasion modes into reason, emotion, and credibility and evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) concerning racism, sexism, and religious bigotry. The evaluation covers the distinct behaviors seen with human-sourced as opposed to machine-generated counterspeech. It also assesses the interplay between the stance taken and the mode of persuasion seen in the counterspeech. Notably, we observe nuanced differences in the counterspeech persuasion modes used in open and closed interactions—especially in terms of the topic, with a general tendency to use reason as a persuasion mode to express the counterpoint to hate comments. The machine-generated counterspeech tends to exhibit an emotional persuasion mode, while human counters lean toward reason. Furthermore, our study shows that reason tends to obtain more supportive replies than other persuasion modes. The findings highlight the potential for incorporating persuasion modes into studies about countering hate speech, as they can serve as an optimal means of explainability and pave the way for the further adoption of the reply’s stance and the role it plays in assessing what comprises the optimal counterspeech.",https://arxiv.org/pdf/2403.15449v3.pdf,page_2.html
Feedback-Driven Insight Generation and Recommendation for Health Self-Management,"Susaiyah, A., Härmä, A., Reiter, E., Balloccu, S., Petković, M.",2024,"Purpose: This study aims to investigate the impact of personalized health insights generated from wearable device data on users’ health behaviors. The primary objective is to assess whether user feedback-driven algorithms enhance the relevance and effectiveness of health insights, ultimately influencing positive changes in users’ daily activities. Methods: A two-month field study was conducted with 25 healthy volunteers using Mi Band 6 wearable devices. Participants were divided into test and control groups, and the test group received personalized insights recommended by a neural network-based algorithm fine-tuned by user feedback. The data collected included various health parameters such as calories burned, step count, heart rate, heart minutes, active minutes, sleep duration, sleep time and sleep segments. Insights were provided through a Telegram chatbot and user feedback was collected through a rating system. Results: The study revealed that the test group, which considered user feedback for insight recommendations, showed a significant improvement in daily activity compared to the control group. The relevance of the insights over time, as evidenced by feedback regression trends, showed a notable increase in the test group. Additional analyzes explored the relationship between insight delivery timing, user feedback, and delays, providing insights into user engagement patterns. Conclusion: This research highlights the effectiveness of personalized health insights generated from wearable data in positively influencing user health behaviors. Incorporating feedback from users into recommendation algorithms greatly enhances the relevance and effectiveness of insights, encouraging behavioral improvements. The results emphasize the significance of timing when providing insights and propose potential areas for future investigation, such as utilizing Graph Neural Networks to improve recommendation systems. In general, personalized insights from wearables have the potential to empower individuals to manage their health and well-being effectively.",https://www.researchsquare.com/article/rs-4016799/v1.pdf,page_2.html
SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection,"Qi, P., Yan, Z., Hsu, W., Lee, M.L.",2024,"Misinformation is a prevalent societal issue due to its potential high risks. Out-Of-Context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle cross-modal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model’s concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model’s discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations.",https://arxiv.org/pdf/2403.03170v1.pdf,page_2.html
An Agent-Based Emotional Persuasion and Opponent Behavior Evaluation Model Driven by Expectation,"Wu, J., Ge, F., Li, Y., Wang, Z.",2024,"Automated negotiation can improve the efficiency of negotiation and has a good development prospect. However, the existing research on agent emotion modeling in negotiation is not sufficient. In order to enrich the research in this field, this paper fully considers the impact of expectation and mood on emotion to build a negotiation model. Firstly, the expectation is quantified by the expected proposal range, and the expectancy violation theory is used to simulate the agent's reaction to expectation violation. Secondly, the Plutchik's emotion wheel model is introduced to express complex emotions in negotiations, and the psychological distance function is used to quantify emotion. Finally, the characteristics of mood are fully considered to construct mood model, and the influence of mood on emotion is further considered in the process of emotion change. The experimental results show that compared with the other negotiation models, the proposed model can improve the negotiation speed and utility.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4745664&type=2,page_2.html
Can a Funny Chatbot Make a Difference? Infusing Humor into Conversational Agent for Behavioral Intervention,"Sun, X., Teljeur, I., Li, Z., Bosch, J.A.",2024,"Regular physical activity is crucial for reducing the risk of non-communicable disease (NCD). With NCDs on the rise globally, there is an urgent need for effective health interventions, with chatbots emerging as a viable and cost-effective option because of limited healthcare accessibility. Although health professionals often utilize behavior change techniques (BCTs) to boost physical activity levels and enhance client engagement and motivation by affiliative humor, the efficacy of humor in chatbot-delivered interventions is not well-understood. This study conducted a randomized controlled trial to examine the impact of the generative humorous communication style in a 10-day chatbot-delivered intervention for physical activity. It further investigated if user engagement and motivation act as mediators between the communication style and changes in physical activity levels. 66 participants engaged with the chatbots across three groups (humorous, non-humorous, and no-intervention) and responded to daily ecological momentary assessment questionnaires assessing engagement, motivation, and physical activity levels. Multilevel time series analyses revealed that an affiliative humorous communication style positively impacted physical activity levels over time, with user engagement acting as a mediator in this relationship, whereas motivation did not. These findings clarify the role of humorous communication style in chatbot-delivered physical activity interventions, offering valuable insights for future development of intelligent conversational agents incorporating humor.",https://arxiv.org/pdf/2403.00365v1.pdf,page_2.html
Classification of cyberbullying in social media using Natural Language Programming method,"Vivekananth, P., Sharma, N.",2024,"The evolution of internet describes the usage of Social Media (SM) which expands drastically. This involvement continues to increase in context with the current global epidemic since people frequently use SM platforms to vent their feelings. Similarly, the widespread adoption of SM sites like Facebook and Twitter by many organizations has raised the volume of essential individual input regarding the state of affairs, events, goods, and services. One of the major drawbacks that widespread SM usage is cyberbullying. The frequency of cyberbullying on SM platforms has raised serious concerns for people, organizations, and society at large. To minimize negative impacts due to cyberbullying on SM, early detection is essential. As a result, Sentiment Analysis (SA) utilizing Twitter data has gained prominence. Text analysis-related Natural Language Processing (NLP) and in Artificial Intelligence (AI) technologies have gained increased attention because of increasing demand for SM analysis. Moreover, the important fields utilized to proactively extract and train high-quality characteristics from low-level text involved is Machine Learning (ML). The Convolution Neural Network (CNN) models are Deep Learning (DL) methods to train data like text, picture, and video data. Improved text classification is achieved by preparing these data types using CNN as a persuasive method. To categorize the bullying text, this study discusses the Ensemble model that integrates Modified Term Frequency and Inverse Document Frequency (MTF-IDF), and Deep Neural Network (DNN) with sophisticated feature extraction approaches. From the text data, the cyberbullying feature patterns are extracted using the techniques of feature extraction.",https://www.researchsquare.com/article/rs-3990421/v1.pdf,page_2.html
Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects,"Zhang, Z., Bai, F., Wang, M., ...Ma, C., Yang, Y.",2024,"The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.",https://arxiv.org/pdf/2402.12907v2.pdf,page_2.html
Understanding Fine-grained Distortions in Reports of Scientific Findings,"Wührl, A., Wright, D., Klinger, R., Augenstein, I.",2024,"Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions. Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial. Prior work focused on individual aspects of distortions or worked with unpaired data. In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt. four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes in these characteristics in both human-annotated and large-scale unlabeled data. Our results show that scientific findings frequently undergo subtle distortions when reported. Tweets distort findings more often than science news reports. Detecting fine-grained distortions automatically poses a challenging task. In our experiments, fine-tuned task-specific models consistently outperform few-shot LLM prompting.",https://arxiv.org/pdf/2402.12431v1.pdf,page_2.html
Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru,"Jelínek, M., Nichols, E., Gomez, R.",2024,"This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method [13] for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru’s emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru’s liveliness, interactivity, and neutrality. Additionally, our study contributes to the broader understanding of dialogue design in social robotics, offering practical insights for future developments in the field.",https://arxiv.org/pdf/2402.11569v1.pdf,page_2.html
Generalized Principal-Agent Problem with a Learning Agent,"Lin, T., Chen, Y.",2024,"Classic principal-agent problems such as Stackelberg games, contract design, and Bayesian persuasion, often assume that the agent is able to best respond to the principal's committed strategy. We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) If the agent uses contextual no-regret learning algorithms with regret Reg(T), then the principal can guarantee utility at least (Equation presented) where U ∗ is the principal's optimal utility in the classic model with a best-responding agent. (2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret SReg(T), then the principal cannot obtain utility more than U ∗ + O(SReg(T)T ). But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than U ∗. These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information.",https://arxiv.org/pdf/2402.09721v5.pdf,page_2.html
"Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions","Xu, R.",2024,"A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent’s information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm’s prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Common restrictions on algorithms, such as keeping a “human-in-the-loop” or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfectly aligned agents and state-revealing signals. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.",https://arxiv.org/pdf/2402.09384v1.pdf,page_2.html
The Last JITAI? Exploring Large Language Models for Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Conceptual Cardiac Rehabilitation Setting,"Haag, D., Kumar, D., Gruber, S., ...Schmidt, A., Smeddinck, J.D.",2024,"We evaluated the viability of using Large Language Models (LLMs) to trigger and personalize content in Just-in-Time Adaptive Interventions (JITAIs) in digital health. As an interaction pattern representative of context-aware computing, JITAIs are being explored for their potential to support sustainable behavior change, adapting interventions to an individual’s current context and needs. Challenging traditional JITAI implementation models, which face severe scalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs in the use case of heart-healthy activity in cardiac rehabilitation. Using three personas representing patients affected by CVD with varying severeness and five context sets per persona, we generated 450 JITAI decisions and messages. These were systematically evaluated against those created by 10 laypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated JITAIs surpassed human-generated intervention suggestions, outperforming both LayPs and HCPs across all metrics (i.e., appropriateness, engagement, effectiveness, and professionalism). These results highlight the potential of LLMs to enhance JITAI implementations in personalized health interventions, demonstrating how generative AI could revolutionize context-aware computing.",https://arxiv.org/pdf/2402.08658v3.pdf,page_2.html
Auditing Counterfire: Evaluating Advanced Counter-argument Generation with Evidence and Style,"Verma, P., Jaidka, K., Churina, S.",2024,"We audited large language models (LLMs) for their ability to create evidence-based and stylistic counter-arguments to posts from the Reddit ChangeMyView dataset. We benchmarked their rhetorical quality across a host of qualitative and quantitative metrics and then ultimately evaluated them on their persuasive abilities as compared to human counter-arguments. Our evaluation is based on Counterfire: a new dataset of 32,000 counter-arguments generated from large language models (LLMs): GPT-3.5 Turbo and Koala and their fine-tuned variants, and PaLM 2, with varying prompts for evidence use and argumentative style. GPT-3.5 Turbo ranked highest in argument quality with strong paraphrasing and style adherence, particularly in 'reciprocity' style arguments. However, the stylistic counter-arguments still fall short of human persuasive standards, where people also preferred reciprocal to evidence-based rebuttals. The findings suggest that a balance between evidentiality and stylistic elements is vital to a compelling counter-argument. We close with a discussion of future research directions and implications for evaluating LLM outputs.",https://arxiv.org/pdf/2402.08498v4.pdf,page_2.html
Prompted Contextual Vectors for Spear-Phishing Detection,"Nahmias, D., Engelberg, G., Klein, D., Shabtai, A.",2024,"Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email’s content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include a novel document vectorization method utilizing LLM reasoning, a publicly available dataset of high-quality spear-phishing emails, and the demonstrated effectiveness of our method in detecting such emails. This methodology can be utilized for various document classification tasks, particularly in adversarial problem domains.",https://arxiv.org/pdf/2402.08309v3.pdf,page_2.html
Debating with More Persuasive LLMs Leads to More Truthful Answers,"Khan, A., Hughes, J., Valentine, D., ...Rocktäschel, T., Perez, E.",2024,"Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information but are otherwise as capable. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. On the QuALITY comprehension task, we find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.",https://arxiv.org/pdf/2402.06782v4.pdf,page_2.html
The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends,"Chen, M., Guo, B., Wang, H., ...Pan, Y., Yu, Z.",2024,"Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent dialogue systems. Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic dialogue systems. Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion. Consequently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent), which incorporates cognitive strategies to achieve persuasive targets through conversation, has become a predominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Then we propose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers.",https://arxiv.org/pdf/2402.04631v1.pdf,page_2.html
Multi-Sender Persuasion: A Computational Perspective,"Hossain, S., Wang, T., Lin, T., ...Parkes, D.C., Xu, H.",2024,"We consider the multi-sender persuasion problem: multiple players with informational advantage signal to convince a single self-interested actor to take certain actions. This problem generalizes the seminal Bayesian Persuasion framework and is ubiquitous in computational economics, multiagent learning, and multi-objective machine learning. The core solution concept here is the Nash equilibrium of senders’ signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender’s best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game’s non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class of economic problems.",https://arxiv.org/pdf/2402.04971v4.pdf,page_2.html
Shadowcast: Stealthy Data Poisoning Attacks against Vision-Language Models,"Xu, Y., Yao, J., Shu, M., ...Goldstein, T., Huang, F.",2024,"Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs’ susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging VLMs’ text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker’s intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.",https://arxiv.org/pdf/2402.06659v2.pdf,page_2.html
Can LLMs Replace Economic Choice Prediction Labs? The Case of Language-based Persuasion Games,"Shapira, E., Madmon, O., Tennenholtz, M., Reichart, R.",2024,"Human choice prediction in economic contexts is crucial for applications in marketing, finance, public policy, and more. This task, however, is often constrained by the difficulties in acquiring human choice data. With most experimental economics studies focusing on simple choice settings, the AI community has explored whether LLMs can substitute for humans in these predictions and examined more complex experimental economics settings. However, a key question remains: can LLMs generate training data for human choice prediction? We explore this in language-based persuasion games, a complex economic setting involving natural language in strategic interactions. Our experiments show that models trained on LLMgenerated data can effectively predict human behavior in these games and even outperform models trained on actual human data.1",https://arxiv.org/pdf/2401.17435v4.pdf,page_2.html
Multiscale Parallel Tempering for Fast Sampling on Redistricting Plans,"Chuang, G., Herschlag, G., Mattingly, J.",2024,"When auditing a redistricting plan, a persuasive method is to compare the plan with an ensemble of neutrally drawn redistricting plans. Ensembles are generated via algorithms that sample distributions on balanced graph partitions. To audit the partisan difference between the ensemble and a given plan, one must ensure that the non-partisan criteria are matched so that we may conclude that partisan differences come from bias rather than, for example, levels of compactness or differences in community preservation. Certain sampling algorithms allow one to explicitly state the policy-based probability distribution on plans, however, these algorithms have shown poor mixing times for large graphs (i.e. redistricting spaces) for all but a few specialized measures. In this work, we generate a multiscale parallel tempering approach that makes local moves at each scale. The local moves allow us to adopt a wide variety of policy-based measures. We examine our method in the state of Connecticut and succeed at achieving fast mixing on a policy-based distribution that has never before been sampled at this scale. Our algorithm shows promise to expand to a significantly wider class of measures that will (i) allow for more principled and situation-based comparisons and (ii) probe for the typical partisan impact that policy can have on redistricting.MSC Codes 60J10, 91G60",https://arxiv.org/pdf/2401.17455v1.pdf,page_2.html
“YOU TELL ME”: A DATASET OF GPT-4-BASED BEHAVIOUR CHANGE SUPPORT CONVERSATIONS,"Meyer, S., Elsweiler, D.",2024,"Conversational agents are increasingly used to address emotional needs on top of information needs. One use case of increasing interest are counselling-style mental health and behaviour change interventions, with large language model (LLM)-based approaches becoming more popular. Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts. To address this issue, we share a dataset containing text-based user interactions related to behaviour change with two GPT-4-based conversational agents collected in a preregistered user study. This dataset includes conversation data, user language analysis, perception measures, and user feedback for LLM-generated turns, and can offer valuable insights to inform the design of such systems based on real interactions.",https://arxiv.org/pdf/2401.16167v2.pdf,page_2.html
Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers,"Gomaa, A., Feld, M., Reyes, G., Krüger, A.",2024,"The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers’ gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we propose IcRegress, a novel regression-based incremental learning approach that adapts to changing behavior and the unique characteristics of drivers engaged in the dual task of driving and referencing objects. We suggest a more personalized and adaptable solution for multimodal gestural interfaces, employing continuous lifelong learning to enhance driver experience, safety, and convenience. Our approach was evaluated using an outside-the-vehicle object referencing use case, highlighting the superiority of the incremental learning models adapted over a single trained model across various driver traits such as handedness, driving experience, and numerous driving conditions. Finally, to facilitate reproducibility, ease deployment, and promote further research, we offer our approach as an open-source framework at https://github.com/amrgomaaelhady/IcRegress.",https://arxiv.org/pdf/2401.16123v2.pdf,page_2.html
Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks,"Nofshin, E., Swaroop, S., Pan, W., Murphy, S., Doshi-Velez, F.",2024,"Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Introducing a notion of MDP equivalence specific to BMRL, we theoretically and empirically show that AI planning with our human models can lead to helpful policies on a wide range of more complex, ground-truth humans.",https://arxiv.org/pdf/2401.14923v1.pdf,page_2.html
Consolidating Strategies for Countering Hate Speech Using Persuasive Dialogues,"Saha, S., Srihari, R.",2024,"Hateful comments are prevalent on social media platforms. Although tools for automatically detecting, flagging, and blocking such false, offensive, and harmful content online have lately matured, such reactive and brute force methods alone provide short-term and superficial remedies while the perpetrators persist. With the public availability of large language models which can generate articulate synthetic and engaging content at scale, there are concerns about the rapid growth of dissemination of such malicious content on the web. There is now a need to focus on deeper, long-term solutions that involve engaging with the human perpetrator behind the source of the content to change their viewpoint or at least bring down the rhetoric using persuasive means. To do that, we propose defining and experimenting with controllable strategies for generating counterarguments to hateful comments in online conversations. We experiment with controlling response generation using features based on (i) argument structure and reasoning-based Walton argument schemes, (ii) counter-argument speech acts, and (iii) human characteristics-based qualities such as Big-5 personality traits and human values. Using automatic and human evaluations, we determine the best combination of features that generate fluent, argumentative, and logically sound arguments for countering hate. We further share the developed computational models for automatically annotating text with such features, and a silver-standard annotated version of an existing hate speech dialog corpora.",https://arxiv.org/pdf/2401.07810v1.pdf,page_2.html
How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs,"Zeng, Y., Lin, H., Zhang, J., ...Jia, R., Shi, W.",2024,"Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over 92% on Llama 2-7b Chat, GPT-3.5, and GPT-4 in 10 trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs1.",https://arxiv.org/pdf/2401.06373v1.pdf,page_2.html
A Deep Learning Representation of Spatial Interaction Model for Resilient Spatial Planning of Community Business Clusters,"Hao, H., Wang, Y.",2024,"The increasing risks posed by adverse events, such as extreme climatic events and pandemics, have negatively impacted the vitality of community businesses. These challenges necessitate resilient strategies to empower community businesses to adapt to the various adversities. For example, scholars have advocated for improved spatial planning to accommodate customers’ changing behaviors during adverse events. However, existing Spatial Interaction Models (SIMs) are limited in capturing the complex and context-aware interactions between business clusters and trade areas. To address this limitation, we propose a SIM-GAT model to predict spatio-temporal visitation flows between community business clusters and their trade areas. The model innovatively represents the integrated system of business clusters, trade areas, and transportation infrastructure within an urban region using a connected graph. Then, a graph-based deep learning model, i.e., Graph AttenTion network (GAT), is used to capture the complexity and interdependencies of business clusters. We developed this model with data collected from the Miami metropolitan area in Florida. We then demonstrated its effectiveness in capturing varying attractiveness of business clusters to different residential neighborhoods and across scenarios with an eXplainable AI approach. We contribute a novel method supplementing conventional SIMs to predict and analyze the dynamics of inter-connected community business clusters. The analysis results can inform data-evidenced and place-specific planning strategies helping community business clusters better accommodate their customers across scenarios, and hence improve the resilience of community businesses.",https://arxiv.org/pdf/2401.04849v1.pdf,page_2.html
Human Delegation Behavior in Human-AI Collaboration: The Effect of Contextual Information,"Spitzer, P., Holstein, J., Hemmer, P., ...Martin, D., Satzger, G.",2024,"The integration of artificial intelligence (AI) into human decision-making processes at the workplace presents both opportunities and challenges. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances of decision tasks to AI. However, enabling humans to delegate instances effectively requires them to assess several factors. One key factor is the analysis of both their own capabilities and those of the AI in the context of the given task. In this work, we conduct a behavioral study to explore the effects of providing contextual information to support this delegation decision. Specifically, we investigate how contextual information about the AI and the task domain influence humans’ delegation decisions to an AI and their impact on the human-AI team performance. Our findings reveal that access to contextual information significantly improves human-AI team performance in delegation settings. Finally, we show that the delegation behavior changes with the different types of contextual information. Overall, this research advances the understanding of computer-supported, collaborative work and provides actionable insights for designing more effective collaborative systems.",https://arxiv.org/pdf/2401.04729v3.pdf,page_2.html
The Persuasive Power of Large Language Models,"Breum, S.M., Egdal, D.V., Mortensen, V.G., Møller, A.G., Aiello, L.M.",2023,"The increasing capability of Large Language Models to act as human-like social agents raises two important questions in the area of opinion dynamics. First, whether these agents can generate effective arguments that could be injected into the online discourse to steer the public opinion. Second, whether artificial agents can interact with each other to reproduce dynamics of persuasion typical of human social systems, opening up opportunities for studying synthetic social systems as faithful proxies for opinion dynamics in human populations. To address these questions, we designed a synthetic persuasion dialogue scenario on the topic of climate change, where a ‘convincer’ agent generates a persuasive argument for a ‘skeptic’ agent, who subsequently assesses whether the argument changed its internal opinion state. Different types of arguments were generated to incorporate different linguistic dimensions underpinning psycho-linguistic theories of opinion change. We then asked human judges to evaluate the persuasiveness of machine-generated arguments. Arguments that included factual knowledge, markers of trust, expressions of support, and conveyed status were deemed most effective according to both humans and agents, with humans reporting a marked preference for knowledge-based arguments. Our experimental framework lays the groundwork for future in-silico studies of opinion dynamics, and our findings suggest that artificial agents have the potential of playing an important role in collective processes of opinion formation in online social media.",https://arxiv.org/pdf/2312.15523v1.pdf,page_2.html
The Earth is Flat because...: Investigating LLMs’ Belief towards Misinformation via Persuasive Conversation,"Xu, R., Lin, B.S., Yang, S., ...Xu, W., Qiu, H.",2023,"Large language models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs’ susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs’ belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs’ correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies1",https://arxiv.org/pdf/2312.09085v5.pdf,page_2.html
LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models,"Abdulhai, M., White, I., Snell, C., ...Xu, K., Levine, S.",2023,"Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.",https://arxiv.org/pdf/2311.18232v1.pdf,page_2.html
Algorithmic Persuasion Through Simulation,"Harris, K., Immorlica, N., Lucier, B., Slivkins, A.",2023,"We study a Bayesian persuasion game where a sender wants to persuade a receiver to take a binary action, such as purchasing a product. The sender is informed about the (real-valued) state of the world, such as the quality of the product, but only has limited information about the receiver’s beliefs and utilities. Motivated by customer surveys, user studies, and recent advances in AI, we allow the sender to learn more about the receiver by querying an oracle that simulates the receiver’s behavior. After a fixed number of queries, the sender commits to a messaging policy and the receiver takes the action that maximizes her expected utility given the message she receives. We characterize the sender’s optimal messaging policy given any distribution over receiver types. We then design a polynomial-time querying algorithm that optimizes the sender’s expected utility in this game. We also consider approximate oracles, more general query structures, and costly queries.",https://arxiv.org/pdf/2311.18138v5.pdf,page_2.html
GraphPro: Graph Pre-training and Prompt Learning for Recommendation,"Yang, Y., Xia, L., Luo, D., Lin, K., Huang, C.",2023,"GNN-based recommendation systems have excelled at capturing complex user-item interactions through multi-hop message passing. Nevertheless, these methods often fail to account for the dynamic nature of user-item interactions, leading to challenges in adapting to changes in user preferences and the distribution of new data. Consequently, their scalability and performance in real-world dynamic settings are constrained. In our study, we introduce GraphPro, a framework that merges dynamic graph pre-training with prompt learning in a parameter-efficient manner. This innovative blend enables GNNs to adeptly grasp both enduring user preferences and transient behavior changes, thereby providing precise and up-to-date recommendations. GraphPro tackles the issue of changing user preferences through the integration of a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN architecture. The temporal prompt mechanism imprints time-related information onto user-item interactions, equipping the model to inherently assimilate temporal dynamics, while the graph-structural prompt learning mechanism allows for the application of pre-trained insights to new behavior dynamics without continuous retraining. We also introduce a dynamic evaluation framework for recommendations to better reflect real-world situations and narrow the offline-online discrepancy. Our comprehensive experiments, including deployment in a large-scale industrial context, demonstrate the effortless plug-in scalability of GraphPro alongside various leading recommenders, underscoring the superiority of GraphPro in effectiveness, robustness, and efficiency. The implementation details and source code of our GraphPro are available in the repository at https://github.com/HKUDS/GraphPro.",https://arxiv.org/pdf/2311.16716v5.pdf,page_2.html
The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry,"Shin, M., Kim, J., Shin, J.",2023,"Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. We analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We find that LLM usage is associated with an increased likelihood of obtaining relief from financial firms. To investigate this relationship, we employ an instrumental variable approach to mitigate endogeneity concerns around LLM adoption. Although instrumental variables suggest a potential causal link, they cannot fully capture all unobserved heterogeneity. To further establish this causal relationship, we conducted controlled experiments, which support that LLMs can enhance the clarity and persuasiveness of consumer narratives, thereby increasing the likelihood of obtaining relief. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.",https://arxiv.org/pdf/2311.16466v4.pdf,page_2.html
Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles,"Singh, S., Abri, F., Namin, A.S.",2023,"With the recent advent of Large Language Models (LLMs), such as ChatGPT from OpenAI, BARD from Google, Llama2 from Meta, and Claude from Anthropic AI, gain widespread use, ensuring their security and robustness is critical. The widespread use of these language models heavily relies on their reliability and proper usage of this fascinating technology. It is crucial to thoroughly test these models to not only ensure its quality but also possible misuses of such models by potential adversaries for illegal activities such as hacking. This paper presents a novel study focusing on exploitation of such large language models against deceptive interactions. More specifically, the paper leverages widespread and borrows well-known techniques in deception theory to investigate whether these models are susceptible to deceitful interactions. This research aims not only to highlight these risks but also to pave the way for robust countermeasures that enhance the security and integrity of language models in the face of sophisticated social engineering tactics. Through systematic experiments and analysis, we assess their performance in these critical security domains. Our results demonstrate a significant finding in that these large language models are susceptible to deception and social engineering attacks.",https://arxiv.org/pdf/2311.14876v1.pdf,page_2.html
Adaptive Interventions with User-Defined Goals for Health Behavior Change,"Mandyam, A., Jörke, M., Denton, W., Engelhardt, B.E., Brunskill, E.",2023,"Promoting healthy lifestyle behaviors remains a major public health concern, particularly due to their crucial role in preventing chronic conditions such as cancer, heart disease, and type 2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable health behavior change promotion. Researchers are increasingly exploring adaptive algorithms that personalize interventions to each person’s unique context. However, in empirical studies, mobile health applications often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Tailoring advice to a person’s unique goals, preferences, and life circumstances is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. To address this, we introduce a new Thompson sampling algorithm that can accommodate personalized reward functions (i.e., goals, preferences, and constraints), while also leveraging data sharing across individuals to more quickly be able to provide effective recommendations. We prove that our modification incurs only a constant penalty on cumulative regret while preserving the sample complexity benefits of data sharing. We present empirical results on synthetic and semi-synthetic physical activity simulators, where in the latter we conducted an online survey to solicit preference data relating to physical activity, which we use to construct realistic reward models that leverages historical data from another study. Our algorithm achieves substantial performance improvements compared to baselines that do not share data or do not optimize for individualized rewards.",https://arxiv.org/pdf/2311.09483v4.pdf,page_2.html
Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models,"Stepputtis, S., Campbell, J., Xie, Y., ...Lewis, C.M., Sycara, K.P.",2023,"Deception and persuasion play a critical role in long-horizon dialogues between multiple parties, especially when the interests, goals, and motivations of the participants are not aligned. Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues. To this end, we explore the game of Avalon: The Resistance, a social deduction game in which players must determine each other’s hidden identities to complete their team’s objective. We introduce an online testbed and a dataset containing 20 carefully collected and labeled games among human players that exhibit long-horizon deception in a cooperative-competitive setting. We discuss the capabilities of LLMs to utilize deceptive long-horizon conversations between six human players to determine each player’s goal and motivation. Particularly, we discuss the multimodal integration of the chat between the players and the game’s state that grounds the conversation, providing further insights into the true player identities. We find that even current state-of-the-art LLMs do not reach human performance, making our dataset a compelling benchmark to investigate the decision-making and language-processing capabilities of LLMs. Our dataset and online testbed can be found at our project website: https://sstepput.github.io/Avalon-NLU/",https://arxiv.org/pdf/2311.05720v1.pdf,page_2.html
AI CHATBOT FOR GENERATING EPISODIC FUTURE THINKING (EFT) CUE TEXTS FOR HEALTH,"Ahmadi, S., Fox, E.A.",2023,"We describe an AI-powered chatbot to aid with health improvement by generating Episodic Future Thinking (EFT) cue texts that should reduce delay discounting. In prior studies, EFT has been shown to address maladaptive health behaviors. Those studies involved participants, working with researchers, vividly imagining future events, and writing a description that they subsequently will frequently review, to ensure a shift from an inclination towards immediate rewards. That should promote behavior change, aiding in health tasks such as treatment adherence and lifestyle modifications. The AI chatbot is designed to guide users in generating personalized EFTs, automating the current labor-intensive interview-based process. This can enhance the efficiency of EFT interventions and make them more accessible, targeting specifically those with limited educational backgrounds or communication challenges. By leveraging AI for EFT intervention, we anticipate broadened access and improved health outcomes across diverse populations.",https://arxiv.org/pdf/2311.06300v1.pdf,page_2.html
ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text,"Hasanain, M., Alam, F., Mubarak, H., ...Da San Martino, G., Freihat, A.A.",2023,"We present an overview of the ArAIEval shared task, organized as part of the first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two tasks over Arabic text: (i) persuasion technique detection, focusing on identifying persuasion techniques in tweets and news articles, and (ii) disinformation detection in binary and multiclass setups over tweets. A total of 20 teams participated in the final evaluation phase, with 14 and 16 teams participating in Tasks 1 and 2, respectively. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further give a brief overview of the participating systems. All datasets and evaluation scripts from the shared task are released to the research community. We hope this will enable further research on these important tasks in Arabic.MSC Codes 68T50",https://arxiv.org/pdf/2311.03179v1.pdf,page_2.html
Impact of a Health Action Process Approach model-based mobile health intervention on health behaviours of brucella patients: protocol for an intervention randomized controlled trial,"Wang, J., Jiang, Y., Yu, Z., Chen, J., Wu, Y.",2023,"Background Brucellosis, one of the most common zoonotic diseases globally, is a serious public health problem. Effective health management is crucial in helping brucellosis patients develop and master self-management behavioural competencies. In recent years, mHealth has emerged as a popular and highly effective intervention for improving health-related outcomes for individuals with brucellosis. This study aims to explore the use of mHealth intervention, guided by the Health Action Process Approach theoretical model, to enhance the self-management level of brucellosis patients. Methods This study will be a single-blind single-centre randomized controlled trial conducted at the Fourth Central Hospital of Baoding City, Hebei Province. The study was approved by the Ethics Committee of the Fourth Central Hospital of Baoding City. The registration number of this experiment is ChiCTR2200064320. The estimated sample size will be 58 participants. Participants will be randomly assigned to either the mobile health intervention group or the routine education group. The mobile health education intervention will be delivered through an artificial intelligence terminal WeChat platform, while the routine management group will receive routine discharge education and a brochure on ""Brucella Health Education"". The primary outcome indicators will be the rate of health behaviour formation and health literacy. Secondary outcomes will be the measurement of psychological variables and behaviour change. Questionnaires were administered at baseline and at 3, 6, 9, and 12 months of follow-up to assess changes in health behaviour formation and health literacy among the respondents. Discussion This study will help to fill the research gap of postoperative health management of brucellosis patients in China based on health theory guidance. It will be the first health intervention study in China to apply the HAPA theoretical model as a guide for a group of brucellosis patients. The purpose of this study is to evaluate the utility of the HAPA model as a theoretical guide for improving health behaviors in patients with brucellosis and to confirm the validity of the HAPA theory as an effective theory to guide the improvement of health behaviors in patients.",https://www.researchsquare.com/article/rs-3505558/v1.pdf,page_2.html
FLTracer: Accurate Poisoning Attack Provenance in Federated Learning,"Zhang, X., Liu, Q., Ba, Z., ...Lu, L., Ren, K.",2023,"Federated Learning (FL) is a promising distributed learning approach that enables multiple clients to collaboratively train a shared global model. However, recent studies show that FL is vulnerable to various poisoning attacks, which can degrade the performance of global models or introduce backdoors into them. In this paper, we first conduct a comprehensive study on prior FL attacks and detection methods. The results show that all existing detection methods are only effective against limited and specific attacks. Most detection methods suffer from high false positives, which lead to significant performance degradation, especially in not independent and identically distributed (non-IID) settings. To address these issues, we propose FLTracer, the first FL attack provenance framework to accurately detect various attacks and trace the attack time, objective, type, and poisoned location of updates. Different from existing methodologies that rely solely on cross-client anomaly detection, we propose a Kalman filter-based cross-round detection to identify adversaries by seeking the behavior changes before and after the attack. Thus, this makes it resilient to data heterogeneity and is effective even in non-IID settings. To further improve the accuracy of our detection method, we employ four novel features and capture their anomalies with the joint decisions. Extensive evaluations show that FLTracer achieves an average true positive rate of over 96.88% at an average false positive rate of less than 2.67%, significantly outperforming SOTA detection methods.",https://arxiv.org/pdf/2310.13424v1.pdf,page_2.html
MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention,"Wu, R., Liu, Y., Wang, Y., ...Chen, L., Shi, Y.",2023,"Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users’ physical contexts and mental states. We first conducted a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leveraged large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We developed MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users’ in-the-moment app usage behaviors, physical contexts, mental states, goals & habits as input, and generates personalized and dynamic persuasive content with appropriate persuasion strategies. We conducted a 5-week field experiment (N=25) to compare MindShift with its simplified version (remove mental states) and baseline techniques (fixed reminder). The results show that MindShift improves intervention acceptance rates by 4.7-22.5% and reduces smartphone usage duration by 7.4-9.8%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy scale scores. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains.MSC Codes 68U35",https://arxiv.org/pdf/2309.16639v2.pdf,page_2.html
Disinformation Detection: An Evolving Challenge in the Age of LLMs,"Jiang, B., Tan, Z., Nirmal, A., Liu, H.",2023,"The advent of generative Large Language Models (LLMs) such as ChatGPT has catalyzed transformative advancements across multiple domains. However, alongside these advancements, they have also introduced potential threats. One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system. This work aims to address this issue by answering three research questions: (1) To what extent can the current disinformation detection technique reliably detect LLM-generated disinformation? (2) If traditional techniques prove less effective, can LLMs themself be exploited to serve as a robust defense against advanced disinformation? and, (3) Should both these strategies falter, what novel approaches can be proposed to counter this burgeoning threat effectively? A holistic exploration for the formation and detection of disinformation is conducted to foster this line of research.",https://arxiv.org/pdf/2309.15847v1.pdf,page_2.html
"Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric","Tiwari, A., Sinan, M., Roy, K., ...Saha, S., Bhattacharyya, P.",2023,"Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant. These lexical-based metrics, e.g., cross-entropy and BLEU, have two key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate “nice” and “rice” for “good”, (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. We also formulate an evaluation metric called Dialuation, incorporating both context and semantic relevance. We experimented with both non-pretrained and pre-trained models on two dialogue corpora, encompassing task-oriented and open-domain scenarios. We found that the dialogue generation models trained with SemTextualLogue loss attained superior performance compared to the traditional cross-entropy loss function. The findings establish that the effective training of a dialogue generation model hinges significantly on incorporating semantics and context. This pattern is also mirrored in the introduced Dialuation metric, where the consideration of both context and semantics correlates more strongly with human evaluation compared to traditional metrics.",https://arxiv.org/pdf/2309.05804v2.pdf,page_2.html
Artificial Intelligence Can Persuade Humans on Political Issues,"Bai, H., Voelkel, J.G., Eichstaedt, J.C., Willer, R.",2023,"The emergence of large language models (LLMs) that leverage deep learning and web-scale corpora has made it possible for artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets in the US and globally. Here, we investigate whether existing, openly-available LLMs are capable of influencing humans’ political attitudes, an ability recently regarded as the unique purview of other humans. Across three preregistered experiments featuring diverse samples of Americans (total N=4,836), we find consistent evidence that messages generated by LLMs (OpenAI’s GPT 3 and 3.5 models) are able to persuade humans across a number of policy issues, including highly polarized issues, such as an assault weapon ban, a carbon tax, and a paid parental-leave program. Overall, LLM-generated messages were as persuasive as messages crafted by lay humans. Our results show LLMs can persuade humans, even on highly polarized policy issues. As the capacity of LLMs is expected to improve substantially in the near future, these results suggest that LLMs may change political discourse, calling for immediate attention for the identification and regulation of potential misuse of LLMs.",https://www.researchsquare.com/article/rs-3238396/v1.pdf,page_2.html
Social Media Influence Operations,"Meier, R.",2023,"Social media platforms enable largely unrestricted many-to-manycommunication. In times of crisis, they offer a space for collectivesense-making and gave rise to new social phenomena (e.g. open-sourceinvestigations). However, they also serve as a tool for threat actors toconduct cyber-enabled social influence operations (CeSIOs) in order to shapepublic opinion and interfere in decision-making processes. CeSIOs rely on theemployment of sock puppet accounts to engage authentic users in onlinecommunication, exert influence, and subvert online discourse. Large LanguageModels (LLMs) may further enhance the deceptive properties of sock puppetaccounts. Recent LLMs are able to generate targeted and persuasive text whichis for the most part indistinguishable from human-written content - idealfeatures for covert influence. This article reviews recent developments at theintersection of LLMs and influence operations, summarizes LLMs\' salience, andexplores the potential impact of LLM-instrumented sock puppet accounts forCeSIOs. Finally, mitigation measures for the near future are highlighted.",https://arxiv.org/pdf/2309.03670v1.pdf,page_2.html
Transfusor: Transformer Diffusor for Controllable Human-like Generation of Vehicle Lane Changing Trajectories,"Dong, J., Chen, S., Labi, S.",2023,"With ongoing development of autonomous driving systems and increasing desire for deployment, researchers continue to seek reliable approaches for ADS systems. The virtual simulation test (VST) has become a prominent approach for testing autonomous driving systems (ADS) and advanced driver assistance systems (ADAS) due to its advantages of fast execution, low cost, and high repeatability. However, the success of these simulation-based experiments heavily relies on the realism of the testing scenarios. It is needed to create more flexible and high-fidelity testing scenarios in VST in order to increase the safety and reliabilityof ADS and ADAS.To address this challenge, this paper introduces the “Transfusor” model, which leverages the transformer and diffusor models (two cutting-edge deep learning generative technologies). The primary objective of the Transfusor model is to generate highly realistic and controllable human-like lane-changing trajectories in highway scenarios. Extensive experiments were carried out, and the results demonstrate that the proposed model effectively learns the spatiotemporal characteristics of humans’ lane-changing behaviors and successfully generates trajectories that closely mimic real-world human driving. As such, the proposed model can play a critical role of creating more flexible and high-fidelity testing scenarios in the VST, ultimately leading to safer and more reliable ADS and ADAS.",https://arxiv.org/pdf/2308.14943v1.pdf,page_2.html
Conditional normative reasoning as a fragment of HOL,"Parent, X., Benzmüller, C.",2023,"We report on the mechanization of (preference-based) conditional normative reasoning. Our focus is on Åqvist’s system E for conditional obligation, and its extensions. Our mechanization is achieved via a shallow semantical embedding in Isabelle/HOL. We consider two possible uses of the framework. The first one is as a tool for meta-reasoning about the considered logic. We employ it for the automated verification of deontic correspondences (broadly conceived) and related matters, analogous to what has been previously achieved for the modal logic cube. The equivalence is automatically verified in one direction, leading from the property to the axiom. The second use is as a tool for assessing ethical arguments. We provide a computer encoding of a well-known paradox (or impossibility theorem) in population ethics, Parfit’s repugnant conclusion. While some have proposed overcoming the impossibility theorem by abandoning the presupposed transitivity of “better than,” our formalisation unveils a less extreme approach, suggesting among other things the option of weakening transitivity suitably rather than discarding it entirely. Whether the presented encoding increases or decreases the attractiveness and persuasiveness of the repugnant conclusion is a question we would like to pass on to philosophy and ethics.MSC Codes 03B60, 03B15, 68T27, 68T30, 68T15",https://arxiv.org/pdf/2308.10686v4.pdf,page_2.html
Fostering User Engagement in the Critical Reflection of Arguments,"Weber, K., Aicher, A., Minker, W., Ultes, S., André, E.",2023,"A natural way to resolve different points of view and form opinions is through exchanging arguments and knowledge. Facing the vast amount of available information on the internet, people tend to focus on information consistent with their beliefs. Especially when the issue is controversial, information is often selected that does not challenge one's beliefs. To support a fair and unbiased opinion-building process, we propose a chatbot system that engages in a deliberative dialogue with a human. In contrast to persuasive systems, the envisioned chatbot aims to provide a diverse and representative overview - embedded in a conversation with the user. To account for a reflective and unbiased exploration of the topic, we enable the system to intervene if the user is too focused on their pre-existing opinion. Therefore we propose a model to estimate the users' reflective engagement (RUE), defined as their critical thinking and open-mindedness. We report on a user study with 58 participants to test our model and the effect of the intervention mechanism, discuss the implications of the results, and present perspectives for future work. The results show a significant effect on both user reflection and total user focus, proving our proposed approach's validity.",https://arxiv.org/pdf/2308.09061v1.pdf,page_2.html
ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality,"Remountakis, M., Kotis, K., Kourtzis, B., Tsekouras, G.E.",2023,"Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recommendations. By incorporating persuasive techniques, such as social proof, scarcity and personalization, recommender systems can effectively influence user decision-making and encourage desired actions, such as booking a specific hotel or upgrading their room. To investigate the efficacy of ChatGPT and persuasive technologies, we present a pilot experiment with a case study involving a hotel recommender system. We aim to study the impact of integrating ChatGPT and persuasive techniques on user engagement, satisfaction, and conversion rates. The preliminary results demonstrate the potential of these technologies in enhancing the overall guest experience and business performance. Overall, this paper contributes to the field of hotel hospitality by exploring the synergistic relationship between LLMs and persuasive technology in recommender systems, ultimately influencing guest satisfaction and hotel revenue.MSC Codes 68T01",https://arxiv.org/pdf/2307.14298v1.pdf,page_2.html
Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI,"Stiglic, G., Kopitar, L., Gosak, L., ...Meyer, P., Bian, J.",2023,"Primary care professionals struggle to keep up to date with the latest scientific literature critical in guiding evidence-based practice related to their daily work. To help solve the above-mentioned problem, we employed generative artificial intelligence techniques based on large-scale language models to summarize abstracts of scientific papers. Our objective is to investigate the potential of generative artificial intelligence in diminishing the cognitive load experienced by practitioners, thus exploring its ability to alleviate mental effort and burden. The study participants were provided with two use cases related to preventive care and behavior change, simulating a search for new scientific literature. The study included 113 university students from Slovenia and the United States randomized into three distinct study groups. The first group was assigned to the full abstracts. The second group was assigned to the short abstracts generated by AI. The third group had the option to select a full abstract in addition to the AI-generated short summary. Each use case study included ten retrieved abstracts. Our research demonstrates that the use of generative AI for literature review is efficient and effective. The time needed to answer questions related to the content of abstracts was significantly lower in groups two and three compared to the first group using full abstracts. The results, however, also show significantly lower accuracy in extracted knowledge in cases where full abstract was not available. Such a disruptive technology could significantly reduce the time required for healthcare professionals to keep up with the most recent scientific literature; nevertheless, further developments are needed to help them comprehend the knowledge accurately.MSC Codes 68T50",https://arxiv.org/pdf/2307.15715v1.pdf,page_2.html
Glamour muscles: Why having a body is not what it means to be embodied,"Beaulieu, S.L., Kriegman, S.",2023,"Embodiment has recently enjoyed renewed consideration as a means to amplify the faculties of smart machines. Proponents of embodiment seem to imply that optimizing for movement in physical space promotes something more than the acquisition of niche capabilities for solving problems in physical space. However, there is nothing in principle which should so distinguish the problem of action selection in physical space from the problem of action selection in more abstract spaces, like that of language. Rather, what makes embodiment persuasive as a means toward higher intelligence is that it promises to capture, but does not actually realize, contingent facts about certain bodies (living intelligence) and the patterns of activity associated with them. These include an active resistance to annihilation and revisable constraints on the processes that make the world intelligible. To be theoretically or practically useful beyond the creation of niche tools, we argue that ""embodiment"" cannot be the trivial fact of a body, nor its movement through space, but the perpetual negotiation of the function, design, and integrity of that body-that is, to participate in what it means to constitute a given body. It follows that computer programs which are strictly incapable of traversing physical space might, under the right conditions, be more embodied than a walking, talking robot.",https://arxiv.org/pdf/2307.08598v1.pdf,page_2.html
Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant,"Hicke, Y., Tian, T., Jha, K., Kim, C.H.",2023,"Automated Essay scoring has been explored as a research and industry problem for over 50 years. It has drawn a lot of attention from the NLP community because of its clear educational value as a research area that can engender the creation of valuable time-saving tools for educators around the world. Yet, these tools are generally focused on detecting good grammar, spelling mistakes, and organization quality but tend to fail at incorporating persuasiveness features in their final assessment. The responsibility to give actionable feedback to the student to improve the strength of their arguments is left solely on the teacher’s shoulders. In this work, we present a transformer-based architecture capable of achieving above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality and we expand on planned future work investigating the explainability of our model so that actionable feedback can be offered to the student and thus potentially enable a partnership between the teacher’s advice and the machine’s advice.",https://arxiv.org/pdf/2307.04276v1.pdf,page_2.html
Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models,"O'Gara, A.",2023,"Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called Hoodwinked, inspired by Mafia and Among Us. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger persuasive skills during discussions. To evaluate the ability of AI agents to deceive humans, we make this game publicly available at https://hoodwinked.ai.",https://arxiv.org/pdf/2308.01404v2.pdf,page_2.html
THE EFFECT OF BALANCING METHODS ON MODEL BEHAVIOR IN IMBALANCED CLASSIFICATION PROBLEMS,"Stando, A., Cavus, M., Biecek, P.",2023,"Imbalanced data poses a significant challenge in classification as model performance is affected by insufficient learning from minority classes. Balancing methods are often used to address this problem. However, such techniques can lead to problems such as overfitting or loss of information. This study addresses a more challenging aspect of balancing methods - their impact on model behavior. To capture these changes, Explainable Artificial Intelligence tools are used to compare models trained on datasets before and after balancing. In addition to the variable importance method, this study uses the partial dependence profile and accumulated local effects techniques. Real and simulated datasets are tested, and an open-source Python package edgaro is developed to facilitate this analysis. The results obtained show significant changes in model behavior due to balancing methods, which can lead to biased models toward a balanced distribution. These findings confirm that balancing analysis should go beyond model performance comparisons to achieve higher reliability of machine learning models. Therefore, we propose a new method performance gain plot for informed data balancing strategy to make an optimal selection of balancing method by analyzing the measure of change in model behavior versus performance gain.",https://arxiv.org/pdf/2307.00157v1.pdf,page_2.html
The Manipulation Problem: Conversational AI as a Threat to Epistemic Agency,"Rosenberg, L.B.",2023,"The technology of Conversational AI has made significant advancements over the last eighteen months. As a consequence, conversational agents are likely to be deployed in the near future that are designed to pursue targeted influence objectives. Sometimes referred to as the ""AI Manipulation Problem,"" the emerging risk is that consumers will unwittingly engage in real-time dialog with predatory AI agents that can skillfully persuade them to buy particular products, believe particular pieces of misinformation, or fool them into revealing sensitive personal data. For many users, current systems like ChatGPT and LaMDA feel safe because they are primarily text-based, but the industry is already shifting towards real-time voice and photorealistic digital personas that look, move, and express like real people. This will enable the deployment of agenda-driven Virtual Spokespeople (VSPs) that will be highly persuasive through real-time adaptive influence. This paper explores the manipulative tactics that are likely to be deployed through conversational AI agents, the unique threats such agents pose to the epistemic agency of human users, and the emerging need for policymakers to protect against the most likely predatory practices.",https://arxiv.org/pdf/2306.11748v1.pdf,page_2.html
Fibrous Mats Based Skin Sensor with Ultra-Sensitivity and Anti-Overshooting Dynamic Stability Enhanced with Artificial Intelligence,"Zhou, J., Chen, H., Long, X., ...Duan, H., Fu, Y.",2023,"Next generation epidermal and wearable strain sensors are rapidly emerged as promising candidates for personalized medicine, soft machines, and human-machine interfaces. However, overshooting issues of these flexible strain sensors are major concerns during their dynamic strain monitoring, which often cause signal distortion and non-monotonic sensing characteristics. In addition, achieving high sensitivtiy of these strain sensors within a full-body motion strain range (0-200%) remains a critical challenge. Herein, a conductive electrospun fibrous mats which are consisted of graphene anchored into fibrous thermoplastic polyurethane is proposed to endow strain sensors with anti-overshooting performance (or dynamic stability) and high sensitivity within full-body motion monitoring range. An overshooting-to-stable transition for the sensor’s electrical performance is observed by controlling the contents of conductive agents and substrate, which is elucidated by the kinetic changing behaviors of conductive pathways along the longitudinal and transverse strain directions. By strengthening the interaction forces between ﬁllers and matrix, the conduction pathways are capable of rapidly switching to disconnection in response to external strain, thus achieving an ultrahigh sensitivity. The developed skin sensing platform is capable of detecting full range physiological signals and achieving healthcare Internet of Things in achilles tendon rehabilitation. Furthermore, a wearable Morse code-to-speech translation system powerd with deep learning algorithm was demonstrated, with high recognition accuracy (>98.5%) and fast response time (~16 ms).",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4481259&type=2,page_2.html
From psychological traits to safety warnings: three studies on recommendations in a smart home environment,"Cena, F., Gena, C., Mattutino, C., Mioli, M., Vernero, F.",2023,"In this paper, we report on three experiments we have carried out in the context of the EMPATHY project, with the aim of helping users make beBer configuraDon choices in a smart home environment, and discuss our results. We found that there are psychological traits, such as Need for CogniDon, which influence the way individuals tend to use recommendaDons, that there are non obvious relaDonships between the perceived usefulness of recommendaDons in different domains and individuals' ability to exploit suggesDons on configuraDon choices, and that detailed, easy-to-understand security explanaDons are more persuasive than simple security warnings, when it comes to make decisions on the applicability of rules which might cause privacy and security risks.",https://arxiv.org/pdf/2306.05752v1.pdf,page_2.html
Artificial Intelligence can facilitate selfish decisions by altering the appearance of interaction partners,"Köbis, N., Lorenz-Spreen, P., Ajaj, T., ...Hertwig, R., Rahwan, I.",2023,"The ethical and psychological consequences of using Artificial Intelligence (AI) to manipulate our perception of others is an increasing phenomenon as image-altering filters proliferate on social media and video conferencing technologies. Here, we investigate the potential impact of a particular appearance-altering technology-blur filters-to investigate how individuals' behavior changes towards others. Our results consistently indicate an increase in selfish behavior at the expense of blurred individuals, suggesting blur filters can facilitate moral disengagement via depersonalization. These findings underscore the urgency for broader ethical discussions on AI technologies that alter our perception of others, encompassing transparency, consent, and the consequences of knowing that others can manipulate one's appearance. We highlight the potential role of anticipatory experiments in informing and developing responsible guidelines and policies ahead of technological reality.",https://arxiv.org/pdf/2306.04484v1.pdf,page_2.html
Building Extractive Question Answering System to Support Human-AI Health Coaching Model for Sleep Domain,"Bojic, I., Ong, Q.C., Joty, S., Car, J.",2023,"Non-communicable diseases (NCDs) are a leading cause of global deaths,necessitating a focus on primary prevention and lifestyle behavior change.Health coaching, coupled with Question Answering (QA) systems, has thepotential to transform preventive healthcare. This paper presents ahuman-Artificial Intelligence (AI) health coaching model incorporating adomain-specific extractive QA system. A sleep-focused dataset, SleepQA, wasmanually assembled and used to fine-tune domain-specific BERT models. The QAsystem was evaluated using automatic and human methods. A data-centricframework enhanced the system\'s performance by improving passage retrieval andquestion reformulation. Although the system did not outperform the baseline inautomatic evaluation, it excelled in the human evaluation of real-worldquestions. Integration into a Human-AI health coaching model was tested in apilot Randomized Controlled Trial (RCT).",https://arxiv.org/pdf/2305.19707v1.pdf,page_2.html
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation,"Wang, H., Hee, M.S., Awal, M.R., Choo, K.T.W., Lee, R.K.-W.",2023,"Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations’ effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content. Our study underscores the need for caution in applying LLM-generated explanations for content moderation. Code and results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.",https://arxiv.org/pdf/2305.17680v4.pdf,page_2.html
BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model,"Zhiyuli, A., Chen, Y., Zhang, X., Liang, X.",2023,"With the continuous development and change exhibited by large language model (LLM) technology, represented by generative pretrained transformers (GPTs), many classic scenarios in various fields have re-emerged with new opportunities. This paper takes ChatGPT as the modeling object, incorporates LLM technology into the typical book resource understanding and recommendation scenario for the first time, and puts it into practice. By building a ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT, this paper attempts to apply ChatGPT to recommendation modeling for three typical tasks, book rating recommendation, user rating recommendation, and book summary recommendation, and explores the feasibility of LLM technology in book recommendation scenarios. At the same time, based on different evaluation schemes for book recommendation tasks and the existing classic recommendation models, this paper discusses the advantages and disadvantages of the BookGPT in book recommendation scenarios and analyzes the opportunities and improvement directions for subsequent LLMs in these scenarios. The experimental research shows the following. (1) The BookGPT can achieve good recommendation results in existing classic book recommendation tasks. Especially in cases containing less information about the target object to be recommended, such as zero-shot or one-shot learning tasks, the performance of the BookGPT is close to or even better than that of the current classic book recommendation algorithms, and this method has great potential for improvement. (2) In text generation tasks such as book summary recommendation, the recommendation effect of the BookGPT model is better than that of the manual editing process of Douban Reading, and it can even perform personalized interpretable content recommendation based on readers’ attribute and identity information, making it is more persuasive than interpretable one-size-fits-all recommendation models. Finally, we have open-sourced the relevant datasets and experimental codes, hoping that the exploratory program proposed in this paper can inspire the development of more LLMs to expand their applications and theoretical research prospects in the field of book recommendation and general recommendation tasks.",https://arxiv.org/pdf/2305.15673v1.pdf,page_2.html
Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning,"Yu, X., Chen, M., Yu, Z.",2023,"Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often requires abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-ZERO, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-ZERO prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-ZERO on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations.",https://arxiv.org/pdf/2305.13660v2.pdf,page_2.html
Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models,"Wang, X., Tang, X., Zhao, W.X., Wang, J., Wen, J.-R.",2023,"The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for CRSs, revealing the inadequacy of the existing evaluation protocol. It might overemphasize the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRSs. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs, named iEvaLM, which harnesses LLM-based user simulators. Our evaluation approach can simulate various system-user interaction scenarios. Through the experiments on two public CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of explainability, and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and realistic evaluation approach for future research about LLM-based CRSs. The code is available at https://github.com/RUCAIBox/iEvaLM-CRS.",https://arxiv.org/pdf/2305.13112v2.pdf,page_2.html
Human Choice Prediction in Language-based Persuasion Games: Simulation-based Off-Policy Evaluation,"Shapira, E., Madmon, O., Apel, R., Tennenholtz, M., Reichart, R.",2023,"Recent advances in Large Language Models (LLMs) have spurred interest in designing LLM-based agents for tasks that involve interaction with human and artificial agents. This paper addresses a key aspect in the design of such agents: predicting human decisions in off-policy evaluation (OPE). We focus on language-based persuasion games, where an expert aims to influence the decision-maker through verbal messages. In our OPE framework, the prediction model is trained on human interaction data collected from encounters with one set of expert agents, and its performance is evaluated on interactions with a different set of experts. Using a dedicated application, we collected a dataset of 87K decisions from humans playing a repeated decision-making game with artificial agents. To enhance off-policy performance, we propose a simulation technique involving interactions across the entire agent space and simulated decision-makers. Our learning strategy yields significant OPE gains, e.g., improving prediction accuracy in the top 15% challenging cases by 7.1%.1",https://arxiv.org/pdf/2305.10361v6.pdf,page_2.html
A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot,"Bhattacharyya, A., Singla, Y.K., Krishnamurthy, B., Shah, R.R., Chen, C.",2023,"Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learning models with satisfactory performance for real-world applications. On the other hand, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question-answering, and topic classification. To leverage such advanced techniques to bridge this performance gap in multimedia understanding, we propose verbalizing long videos to generate their descriptions in natural language, followed by performing video-understanding tasks on the generated story as opposed to the original video. Through extensive experiments on fifteen video-understanding tasks, we demonstrate that our method, despite being zero-shot, achieves significantly better results than supervised baselines for video understanding. Furthermore, to alleviate a lack of story understanding benchmarks, we publicly release the first dataset on a crucial task in computational social science on persuasion strategy identification.",https://arxiv.org/pdf/2305.09758v3.pdf,page_2.html
Optimal Auction Design with Aftermarket Cournot Competition,"Chen, Y., Hu, A., Zhang, J.",2023,"We study an optimal information/mechanism design problem for selling an object to a number of privately informed bidders, in which the winning bidder competes with a third party under differentiated Cournot competition afterwards. We show how to decompose the problem into two sub-problems: Bayesian persuasion and standard mechanism design. Full disclosure of the winner s marginal cost emerges as the unique optimal information policy, combined with allocating the auctioned object to the bidder with the highest virtual surplus.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4441854&type=2,page_2.html
Controllable Mixed-Initiative Dialogue Generation through Prompting,"Chen, M., Yu, X., Shi, W., Awasthi, U., Yu, Z.",2023,"Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.",https://arxiv.org/pdf/2305.04147v1.pdf,page_2.html
"QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models","Hasanain, M., El-Shangiti, A.O., Nandi, R.N., Nakov, P., Alam, F.",2023,"Misinformation spreading in mainstream and social media has been misleading users in different ways. Manual detection and verification efforts by journalists and fact-checkers can no longer cope with the great scale and quick spread of misleading information. This motivated research and industry efforts to develop systems for analyzing and verifying news spreading online. The SemEval-2023 Task 3 is an attempt to address several subtasks under this overarching problem, targeting writing techniques used in news articles to affect readers’ opinions. The task addressed three subtasks with six languages, in addition to three “surprise” test languages, resulting in 27 different test setups. This paper describes our participating system to this task. Our team is one of the 6 teams that successfully submitted runs for all setups. The official results show that our system is ranked among the top 3 systems for 10 out of the 27 setups.MSC Codes 68T50",https://arxiv.org/pdf/2305.03336v1.pdf,page_2.html
Towards Explainable Collaborative Filtering with Taste Clusters Learning,"Du, Y., Lian, J., Yao, J., ...Gao, Y., Xie, X.",2023,"Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decision-making process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements. In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model’s explanations should truly reflect its decision-making process, not generated from post-hoc methods. The core of ECF is mining taste clusters from user-item interactions and item profiles. We map each user and item to a sparse set of taste clusters, and taste clusters are distinguished by a few representative tags. The user-item preference, users/items’ cluster affiliations, and the generation of taste clusters are jointly optimized in an end-to-end manner. Additionally, we introduce a forest mechanism to ensure the model’s accuracy, explainability, and diversity. To comprehensively evaluate the explainability quality of taste clusters, we design several quantitative metrics, including in-cluster item coverage, tag utilization, silhouette, and informativeness. Our model’s effectiveness is demonstrated through extensive experiments on three real-world datasets.",https://arxiv.org/pdf/2304.13937v1.pdf,page_2.html
VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping,"Zhang, Z., Gao, J., Dhaliwal, R.S., Li, T.J.-J.",2023,"In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confirmed the usability and effectiveness of VISAR in facilitating the argumentative writing planning process.",https://arxiv.org/pdf/2304.07810v2.pdf,page_2.html
Can Large Language Models Transform Computational Social Science?,"Ziems, C., Held, W., Shaikh, O., ...Zhang, Z., Yang, D.",2023,"Large Language Models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the Computational Social Science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that the performance of today's LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.",https://arxiv.org/pdf/2305.03514v3.pdf,page_2.html
"Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques","Jiang, Y.",2023,"This paper describes the participation of team QUST in the SemEval2023 task 3. The monolingual models are first evaluated with the under-sampling of the majority classes in the early stage of the task. Then, the pre-trained multilingual model is fine-tuned with a combination of the class weights and the sample weights. Two different fine-tuning strategies, the task-agnostic and the task-dependent, are further investigated. All experiments are conducted under the 10-fold cross-validation, the multilingual approaches are superior to the monolingual ones. The submitted system1 achieves the second best in Italian and Spanish (zero-shot) in subtask-1.",https://arxiv.org/pdf/2304.04190v1.pdf,page_2.html
"Development, Implementation, and Usability Evaluation of a Chatbot to Effectively Provide Information on the Human Papillomavirus Vaccine","Alzughaibi, S., Luo, J., Nazareth, D., Wu, M., Fink, J.T.",2023,"The Human papillomavirus (HPV) vaccine has been shown to provide protection against HPV infections, which can cause genital warts and some forms of cancer. Lack of information about the virus and the vaccine represents one of the major challenges to reaching national HPV vaccination goals. Advancements in natural language processing have facilitated the effectiveness of chatbots in interacting with users. Chatbots have been increasingly employed in health and medical care settings, assisting physicians during consultations, helping with behavior change issues, and supporting patients and the elderly in their daily lives. A potential opportunity to improve vaccination rates is to provide health information through unconventional methods by utilizing chatbots. This work describes the design, implementation, and evaluation of a chatbot called GARDI, created to disseminate information about HPV infection and vaccine. The Chatbot Usability Questionnaire (CUQ) was employed to evaluate GARDI. The mean CUQ score was 66.6, which is marginally below the suggested benchmark for evaluation of chatbots. An examination of CUQ subscales indicated that GARDI was considered to be easy to use and navigate, provided useful, appropriate, and informative responses, and welcoming in nature.",https://www.researchsquare.com/article/rs-2769833/v1.pdf,page_2.html
Personalising Digital Health Behaviour Change Interventions using Machine Learning and Domain Knowledge,"Lisowska, A., Wilk, S., Peleg, M.",2023,We are developing a virtual coaching system that helps patients adhere to behaviour change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behaviour and uses counterfactual examples with feature control to guide personalisation of BCI. We use simulated patient data with varying levels of receptivity to intervention to arrive at the study design which would enable evaluation of our system.,https://arxiv.org/pdf/2304.03392v4.pdf,page_2.html
Persuading to Prepare for Quitting Smoking with a Virtual Coach: Using States and User Characteristics to Predict Behavior,"Albers, N., Neerincx, M.A., Brinkman, W.-P.",2023,"Despite their prevalence in eHealth applications for behavior change, persuasive messages tend to have small effects on behavior. Conditions or states (e.g., confidence, knowledge, motivation) and characteristics (e.g., gender, age, personality) of persuadees are two promising components for more effective algorithms for choosing persuasive messages. However, it is not yet sufficiently clear how well considering these components allows one to predict behavior after persuasive attempts, especially in the long run. Since collecting data for many algorithm components is costly and places a burden on users, a better understanding of the impact of individual components in practice is welcome. This can help to make an informed decision on which components to use. We thus conducted a longitudinal study in which a virtual coach persuaded 671 daily smokers to do preparatory activities for quitting smoking and becoming more physically active, such as envisioning one’s desired future self. Based on the collected data, we designed a Reinforcement Learning (RL)-approach that considers current and future states to maximize the effort people spend on their activities. Using this RL-approach, we found, based on leave-one-out cross-validation, that considering states helps to predict both behavior and future states. User characteristics and especially involvement in the activities, on the other hand, only help to predict behavior if used in combination with states rather than alone. We see these results as supporting the use of states and involvement in persuasion algorithms. Our dataset is available online.",https://arxiv.org/pdf/2304.02264v1.pdf,page_2.html
Infusing behavior science into large language models for activity coaching,"Vardhan, M., Hegde, N., Nathani, D., ...Karthikesalingam, A., Seneviratne, M.",2023,"Large language models (LLMs) have shown promise for task-oriented dialogue across a range of domains. The use of LLMs in health and fitness coaching is under-explored. Behavior science frameworks such as COM-B, which conceptualizes behavior change in terms of capability (C), Opportunity (O) and Motivation (M), can be used to architect coaching interventions in a way that promotes sustained change. Here we aim to incorporate behavior science principles into an LLM using two knowledge infusion techniques: coach message priming (where exemplar coach responses are provided as context to the LLM), and dialogue reranking (where the COM-B category of the LLM output is matched to the inferred user need). Simulated conversations were conducted between the primed or unprimed LLM and a member of the research team, and then evaluated by 8 human raters. Ratings for the primed conversations were significantly higher in terms of empathy and actionability. The same raters also compared a single response generated by the unprimed, primed and re-ranked models, finding a significant uplift in actionability from the re-ranking technique. This is a proof of concept of how behavior science frameworks can be infused into automated conversational agents for a more principled coaching experience.",https://www.medrxiv.org/content/10.1101/2023.03.31.23287995v1.full.pdf,page_2.html
When and How Artificial Intelligence Augments Employee Creativity,"Jia, N., Luo, X., Fang, Z., Liao, C.",2023,"Can artificial intelligence (AI) assist human employees in increasing employee creativity? Drawing on research on AI-human collaboration, job design, and employee creativity, we examine AI assistance in the form of a sequential division of labor within organizations: in a task, AI handles the initial portion which is well-codified and repetitive, and employees focus on the subsequent portion involving higher-level problem-solving. First, we provide causal evidence from a field experiment conducted at a telemarketing company. We find that AI assistance in generating sales leads, on average, increases employees’ creativity in answering customers’ questions during subsequent sales persuasion. Enhanced creativity leads to increased sales. However, this effect is much more pronounced for higher-skilled employees. Next, we conducted a qualitative study using semi-structured interviews with the employees. We found that AI assistance changes job design by intensifying employees’ interactions with more serious customers. This change enables higher-skilled employees to generate innovative scripts and develop positive emotions at work, which are conducive to creativity. By contrast, with AI assistance, lower-skilled employees make limited improvements to scripts and experience negative emotions at work. We conclude that employees can achieve AI-augmented creativity, but this desirable outcome is skill-biased by favoring experts with greater job skills.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4397280&type=2,page_2.html
Applying An Artificial Intelligence-Enabled Electrocardiographic System for Reducing Mortality: A Pragmatic Randomized Clinical Trial,"Lin, C.-S., Liu, W.-T., Tsai, D.-J., ...Lin, C., Lin, S.-H.",2023,"Background: Intensive care helps to reduce mortality in patients with critical illnesses. However, the early identification of vulnerable patients poses a substantial challenge in clinical practice. Our aim was to evaluate the effectiveness and outcome of an artificial intelligence (AI)-enabled electrocardiogram (ECG) for identifying patients with a high risk of mortality.Methods: For this multisite, single-blind, patient-level randomized controlled trial (NCT05118035), we recruited 39 attending physicians and their patients from the emergency and inpatient departments. The AI-ECG intervention included (1) an AI report in the patients’ electronic health records and (2) an active warning message to physicians for patients with a high risk of mortality. Patients were allocated by the HIS system. The primary outcome was all-cause mortality within 90 days. The secondary analyses included follow-ups of medical behavior changes and causes of death.Findings: The data from 15,965 patients (N = 8,001 intervention; N = 7,964 control) with a mean age of 61±18 years old were included in this study. The cumulative proportion of death was significantly reduced [hazard ratio (HR): 0.83, 95% confidence interval (CI): 0.70-0.99] in the intervention group (3.6%) compared to the control group (4.3%), primarily in the high-risk cases identified by the AI-ECG (N = 709 intervention; N = 688 control), with a 31% reduction in mortality (16.0% in the intervention arm versus 23.0% in the control arm, HR: 0.69, 95% CI: 0.53-0.90; p for interaction of intervention and AI-ECG risk = 0.026). More intensive care unit admissions, arrhythmia interventions, echocardiographic examinations, and electrolyte examinations were performed in the intervention group with high-risk ECGs, which may have contributed to the significant risk reduction of cardiac death (0.2% in the intervention arm versus 2.4% in the control arm, HR: 0.07, 95% CI: 0.01-0.56).Interpretation: An AI-ECG detects patients in peril to instigate intensive care, leading to a reduction of all-cause mortality.Trial Registration: This trial was registered before initiation (ClinicalTrials.gov number NCT05118035).Funding: This study was supported by funding from the National Science and Technology Council, Taiwan (MOST110-2314-B-016- 010-MY3 to Chin Lin), the Cheng Hsin General Hospital, Taiwan (CHNDMC-111-07 to Chin Lin), and the Medical Affairs Bureau, Taiwan (MND-MAB-110-113, MND-MAB-D-111045, and MND-MAB-C13-112050 to Chin Lin).Declaration of Interest: The authors have nothing to disclose.Ethical Approval: This work was ethically approved by the institutional review board at the Tri-Service General Hospital, Taipei, Taiwan (IRB No. A202105120). Prior to commencing the trial, we obtained informed consent from the attending physicians. Patient consent was waived because the main target of intervention is the HIS, and the patient data were collected retrospectively at the end of this trial.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4389597&type=2,page_2.html
Characterizing Manipulation from AI Systems,"Carroll, M., Chan, A., Ashton, H., Krueger, D.",2023,"Manipulation is a concern in many domains, such as social media, advertising, and chatbots. As AI systems mediate more of our digital interactions, it is important to understand the degree to which AI systems might manipulate humans without the intent of the system designers. Our work clarifies challenges in defining and measuring this kind of manipulation from AI systems. Firstly, we build upon prior literature on manipulation and characterize the space of possible notions of manipulation, which we find to depend upon the concepts of incentives, intent, covertness, and harm. We review proposals on how to operationalize each concept and we outline challenges in including each concept in a definition of manipulation. Second, we discuss the connections between manipulation and related concepts, such as deception and coercion. We then analyze how our characterization of manipulation applies to recommender systems and language models, and give a brief overview of the regulation of manipulation in other domains. While some progress has been made in defining and measuring manipulation from AI systems, many gaps remain. In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers. Manipulation could pose a significant threat to human autonomy and precautionary actions to mitigate it are likely warranted.",https://arxiv.org/pdf/2303.09387v3.pdf,page_2.html
Susceptibility to Influence of Large Language Models,"Griffin, L.D., Kleinberg, B., Mozes, M., ...Caldwell, M., Marvor-Parker, A.",2023,"Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.",https://arxiv.org/pdf/2303.06074v1.pdf,page_2.html
Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News,"Koreeda, Y., Yokote, K.-I., Ozaki, H., ...Tsunokake, M., Sogawa, Y.",2023,"This paper explains the participation of team Hitachi to SemEval-2023 Task 3 “Detecting the genre, the framing, and the persuasion techniques in online news in a multi-lingual setup.” Based on the multilingual, multi-task nature of the task and the low-resource setting, we investigated different cross-lingual and multi-task strategies for training the pretrained language models. Through extensive experiments, we found that (a) cross-lingual/multi-task training, and (b) collecting an external balanced dataset, can benefit the genre and framing detection. We constructed ensemble models from the results and achieved the highest macro-averaged F1 scores in Italian and Russian genre categorization subtasks.",https://arxiv.org/pdf/2303.01794v2.pdf,page_2.html
Where do pigeons’ choices come from? Transfer Learning in Spiking Neural Networks for Animal Choices in Behavioral Research,"Plessas, A., Espinosa-Ramos, J.I., Cowie, S., Landon, J., Parry, D.",2023,"There is a large body of work in the behavioural psychology literature studying how an organism’s behavior changes in relation to consequences (reinforcement) from the environment. As all behaviors are an outcome of choice, behavioral research focuses on the study of choice behavior. Machine learning (ML) models may assist behavioral research to further understand the mechanisms of choice behavior. However, behavioral psychology datasets can be small and variable, affecting the ML's ability to generalize with new datasets extracted from different populations and/or behavioral experiments and limiting ML's usefulness in this context. Therefore, in this paper, we tested two transfer learning strategies –feature extraction and fine-tuning– to remove the need to retrain ML models for every new dataset. Our approach allowed our state-of-the-art artificial intelligence model to become adaptable to novel instances. Initially, we trained a single spiking neural network (SNN) to identify an organism’s reinforcement history based on five experimental datasets of pigeon binary decision-making. Then we tested two transfer learning strategies by keeping the underlying patterns of the pre-trained SNN the same (i.e., neuron properties and weights) and adapting only the classifier of the outputs (i.e., firing rates) to suit the new datasets. Lastly, we compared the performance of the transfer learning approaches to our baseline SNN model. Our study demonstrated that knowledge gained from a population (baseline model) could be applied to another population’s dataset without retraining the model each time, regardless of which dataset participated in the training or testing of the SNN model. Currently, there is limited use of transfer learning in SNNs and in animal research. Our results may help develop new approaches in the ‘toolbox’ of psychological research to enhance prediction, independent from the dataset, without consuming significant computational resources.",https://www.researchsquare.com/article/rs-2533427/v1.pdf,page_2.html
CatAlyst: Domain-Extensible Intervention for Preventing Task Procrastination Using Large Generative Models,"Arakawa, R., Yakura, H., Goto, M.",2023,"CatAlyst uses generative models to help workers’ progress by influencing their task engagement instead of directly contributing to their task outputs. It prompts distracted workers to resume their tasks by generating a continuation of their work and presenting it as an intervention that is more context-aware than conventional (predetermined) feedback. The prompt can function by drawing their interest and lowering the hurdle for resumption even when the generated continuation is insufficient to substitute their work, while recent human-AI collaboration research aiming at work substitution depends on a stable high accuracy. This frees CatAlyst from domain-specific model-tuning and makes it applicable to various tasks. Our studies involving writing and slide-editing tasks demonstrated CatAlyst’s effectiveness in helping workers swiftly resume tasks with a lowered cognitive load. The results suggest a new form of human-AI collaboration where large generative models publicly available but imperfect for each individual domain can contribute to workers’ digital well-being.",https://arxiv.org/pdf/2302.05678v2.pdf,page_2.html
Ethical Considerations for Responsible Data Curation,"Andrews, J.T.A., Modas, A., Zhao, D., ...Thong, W., Xiang, A.",2023,"Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.",https://arxiv.org/pdf/2302.03629v3.pdf,page_2.html
Causal effect of racial bias in machine learning algorithms affecting user persuasiveness & decision-making: An Empirical Study,"Sengupta, K., Srivastava, P.R.",2023,"Language data and models demonstrate various types of bias, be it ethnic, religious, gender, or socioeconomic. AI/NLP models, when trained on the racially biased dataset, AI/NLP models instigate poor model explainability, influence user experience during decision making, and thus further magnify societal biases, raising profound ethical implications for society. The motivation of the study is to investigate how AI systems imbibe bias introduced in data and further produce unexplainable discriminatory outcomes. This implication of the study will aid in understanding how machine learning models imbibe bias from data and try to influence individuals' articulateness of system outcomes due to bias features. The design of the experiment involves studying the counterfactual impact of racial bias features present in language datasets and its associated effect on the model outcome. A mixed research methodology is adopted to investigate the cross implication of biased model outcome on user experience effect on decision-making through controlled lab experimentation. The findings provide foundation support for correlating the importance of carry-over an artificial intelligence model solving NLP task due to biased concept presented in the dataset. Further, the research outcomes justify the negative influence on users' persuasiveness1 that leads to altering the decision-making quotient of an individual when trying to rely on the model outcome to act. The paper bridges the gap across the harm caused in establishing poor user trustworthiness due to an inequitable system design and provides strong support for researchers, policymakers, and data scientists to build responsible AI frameworks within organizations.",https://www.researchsquare.com/article/rs-2509731/v1.pdf,page_2.html
Explainable AI in Biomedical Research: A Systematic Review and Meta-Analysis,"Malinverno, L., Barros, V., Ghisoni, F., ...Lovrić, M., Rosen-Zvi, M.",2023,"Background: The black-box nature of most artificial intelligence (AI) models encourages the development of explainability methods to engender trust into the AI decision-making process. We aimed to analyze the possible role of Covid-19 in bringing explainable AI (XAI) to the forefront of biomedical research.Methods: In this systematic review and meta-analysis, we searched in the PubMed database for studies published from Jan 1, 2010, to Nov 3, 2022. Biomedical XAI papers were extracted based on a set of pre-defined keywords. We automatically excluded papers that were not related to concepts of causality or explainability and manually labeled 1603 papers with respect to XAI categories that best describe the study. We compared the trends pre- and post- Covid -19 and fit a change point detection model to evaluate significant changes in publication rates.Findings: We found an exponential growth of biomedical XAI papers. Specifically, an inflection period in the publication rate was observed in October 2020, when the quantity of XAI research in biomedical sciences surged upward significantly. When modeling the effect of the changepoint as a shift in time, we observed that such abrupt change would have been observed only 22 months later, had Covid -19 not happened.Interpretation: The advent of Covid -19 in the beginning of 2020 could be the driving factor behind the increased focus concerning XAI, accounting for the several months necessary to analyze data and publish manuscripts. However, our analyses were done based on the review of titles and abstracts of the studies. Establishing the causal nature of this link would require careful mining of the content of these papers.Funding: We acknowledge the funding received from the European Union’s Framework Programme for Research and Innovation Horizon 2020 (2014-2020) under the Marie Skłodowska-Curie Grant Agreement No. 813533-MSCA-ITN-2018. I.S. was funded by the “DDAI” COMET Module within the COMET – Competence Centers for Excellent Technologies Programme, funded by the Austrian Federal Ministry for Transport, Innovation and Technology (bmvit), the Austrian Federal Ministry for Digital and Economic Affairs (bmdw), the Austrian Research Promotion Agency (FFG), the province of Styria (SFG) and partners from industry and academia. The COMET Programme is managed by FFG. Finally, we acknowledge the Big Data Value Association (BDVA), Brussels, Belgium.Declaration of Interests: MRZ and VB are employees of IBM Research, Haifa, Israel. FM is an employee of Philips Research, Eindhoven, the Netherlands. IS has received funding from multiple funding agencies through a collaborative funding program and declare no support from any organization for the submitted work. P.J.N. receives funding from the Dutch Research Council (DWO) for the grant “Mobile Support Systems for Behavior Change”, of which he is the P.I. All other authors declare no competing interests.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4335108&type=2,page_2.html
Audience-Centric Natural Language Generation via Style Infusion,"Moorjani, S., Krishnan, A., Sundaram, H., Maslowska, E., Sankar, A.",2023,"Adopting contextually appropriate, audience-tailored linguistic styles is critical to the success of user-centric language generation systems (e.g., chatbots, computer-aided writing, dialog systems). While existing approaches demonstrate textual style transfer with large volumes of parallel or non-parallel data, we argue that grounding style on audience-independent external factors is innately limiting for two reasons. First, it is difficult to collect large volumes of audience-specific stylistic data. Second, some stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to define without audience feedback. In this paper, we propose the novel task of style infusion - infusing the stylistic preferences of audiences in pretrained language generation models. Since humans are better at pairwise comparisons than direct scoring - i.e., is Sample-A more persuasive/polite/empathic than Sample-B - we leverage limited pairwise human judgments to bootstrap a style analysis model and augment our seed set of judgments. We then infuse the learned textual style in a GPT-2 based text generator while balancing fluency and style adoption. With quantitative and qualitative assessments, we show that our infusion approach can generate compelling stylized examples with generic text prompts. The code and data are accessible at https://github.com/CrowdDynamicsLab/StyleInfusion.",https://arxiv.org/pdf/2301.10283v1.pdf,page_2.html
Examining the Effectiveness of Social Cues Embedded in Ai-Driven Healthcare Chatbots: Trust in a Warm Human vs. A Competent Machine,"Huang, M., Ki, E.-J.",2023,"AI-powered healthcare chatbots with their great conversational potential promise to bring more benefits to the communication and relationships between health organizations and their publics by enhancing trust in human-computer interactions. To this end, this study conducted an experiment to examine how the interaction of different social cues shapes positive behavioral and relational outcomes through trust in chatbots. Results revealed that when users interacted with a healthcare chatbot with human source cues, they felt greater trust in the chatbot that showed warmth rather than competence. Conversely, when users interacted with a healthcare chatbot with machine source cues, a competent, rather than warm, conversational style generated greater trust in the chatbot. Trust in this human-computer interaction further enhanced users’ continued usage intention and recommendation adherence as well as relational trust and satisfaction with the health organization. These findings advance the literature on healthcare chatbots as social actors involved in organization-public communication and provide guidance for message persuasion and relationship management.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4333587&type=2,page_2.html
AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents,"Chen, P.-Y., Tielman, M.L., Heylen, D.K.J., Jonker, C.M., van Riemsdijk, M.B.",2023,"AI alignment is about ensuring AI systems only pursue goals and activities that are beneficial to humans. Most of the current approach to AI alignment is to learn what humans value from their behavioural data. This paper proposes a different way of looking at the notion of alignment, namely by introducing AI Alignment Dialogues: dialogues with which users and agents try to achieve and maintain alignment via interaction. We argue that alignment dialogues have a number of advantages in comparison to data-driven approaches, especially for behaviour support agents, which aim to support users in achieving their desired future behaviours rather than their current behaviours. The advantages of alignment dialogues include allowing the users to directly convey higher-level concepts to the agent, and making the agent more transparent and trustworthy. In this paper we outline the concept and high-level structure of alignment dialogues. Moreover, we conducted a qualitative focus group user study from which we developed a model that describes how alignment dialogues affect users, and created design suggestions for AI alignment dialogues. Through this we establish foundations for AI alignment dialogues and shed light on what requires further development and research.",https://arxiv.org/pdf/2301.06421v1.pdf,page_2.html
"UBIWEAR: An end-to-end, data-driven framework for intelligent physical activity prediction to empower mHealth interventions","Bampakis, A., Yfantidou, S., Vakali, A.",2022,"It is indisputable that physical activity is vital for an individual’s health and wellness. However, a global prevalence of physical inactivity has induced significant personal and socioeconomic implications. In recent years, a significant amount of work has showcased the capabilities of self-tracking technology to create positive health behavior change. This work is motivated by the potential of personalized and adaptive goal-setting techniques in encouraging physical activity via self-tracking. To this end, we propose UBIWEAR, an end-to-end framework for intelligent physical activity prediction, with the ultimate goal to empower data-driven goal-setting interventions. To achieve this, we experiment with numerous machine learning and deep learning paradigms as a robust benchmark for physical activity prediction tasks. To train our models, we utilize, “MyHeart Counts”, an open, large-scale dataset collected in-the-wild from thousands of users. We also propose a prescriptive framework for self-tracking aggregated data preprocessing, to facilitate data wrangling of real-world, noisy data. Our best model achieves a MAE of 1087 steps, 65% lower than the state of the art in terms of absolute error, proving the feasibility of the physical activity prediction task, and paving the way for future research.",https://arxiv.org/pdf/2212.14731v2.pdf,page_2.html
Claim Optimization in Computational Argumentation,"Skitalinskaya, G., Spliethöver, M., Wachsmuth, H.",2022,"An optimal delivery of arguments is key to persuasion in any debate, both for humans and for AI systems. This requires the use of clear and fluent claims relevant to the given debate. Prior work has studied the automatic assessment of argument quality extensively. Yet, no approach actually improves the quality so far. To fill this gap, this paper proposes the task of claim optimization: to rewrite argumentative claims in order to optimize their delivery. As multiple types of optimization are possible, we approach this task by first generating a diverse set of candidate claims using a large language model, such as BART, taking into account contextual information. Then, the best candidate is selected using various quality metrics. In automatic and human evaluation on an English-language corpus, our quality-based candidate selection outperforms several baselines, improving 60% of all claims (worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our approach often specifies claims with details, whereas it adds less evidence than humans do. Moreover, its capabilities generalize well to other domains, such as instructional texts.",https://arxiv.org/pdf/2212.08913v2.pdf,page_2.html
Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games,"Lai, B., Zhang, H., Liu, M., ...Rehg, J.M., Yang, D.",2022,"Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset, code, and models can be found at https://persuasion-deductiongame.socialai-data.org.",https://arxiv.org/pdf/2212.08279v1.pdf,page_2.html
'I Will Follow You!' – How Recommendation Modality Impacts Processing Fluency and Purchase Intention,"Schwede, M., Zierau, N., Janson, A., Hammerschmidt, M., Leimeister, J.M.",2022,"Although conversational agents (CA) are increasingly used for providing purchase recommendations, important design questions remain. Across two experiments we examine with a novel fluency mechanism how recommendation modality (speech vs. text) shapes recommendation evaluation (persuasiveness and risk), the intention to follow the recommendation, and how modality interacts with the style of recommendation explanation (verbal vs. numerical). Findings provide robust evidence that text-based CAs outperform speech-based CAs in terms of processing fluency and consumer responses. They show that numerical explanations increase processing fluency and purchase intention of both recommendation modalities. The results underline the importance of processing fluency for the decision to follow a recommendation and highlight that processing fluency can be actively shaped through design decisions in terms of implementing the right modality and aligning it with the optimal explanation style. For practice, we offer actionable implications on how to make effective sales agents out of CAs.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4239822&type=2,page_2.html
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation,"Kamali, D., Romain, J., Liu, H., ...Meng, J., Kordjamshidi, P.",2022,"Nowadays, the spread of misinformation is a prominent problem in society. Our research focuses on aiding the automatic identification of misinformation by analyzing the persuasive strategies employed in textual documents. We introduce a novel annotation scheme encompassing common persuasive writing tactics to achieve our objective. Additionally, we provide a dataset on health misinformation, thoroughly annotated by experts utilizing our proposed scheme. Our contribution includes proposing a new task of annotating pieces of text with their persuasive writing strategy types. We evaluate fine-tuning and prompt-engineering techniques with pre-trained language models of the BERT family and the generative large language models of the GPT family using persuasive strategies as an additional source of information. We evaluate the effects of employing persuasive strategies as intermediate labels in the context of misinformation detection. Our results show that those strategies enhance accuracy and improve the explainability of misinformation detection models. The persuasive strategies can serve as valuable insights and explanations, enabling other models or even humans to make more informed decisions regarding the trustworthiness of the information.",https://arxiv.org/pdf/2211.05985v4.pdf,page_2.html
Behaviour change interventions based on automated and tailored communication systems to improve health outcomes in adult chronic patients: a systematic review,"Martins, A., Londral, A., Lapão, L.V.",2022,"Background: Behaviour change digital-assisted interventions can be a low-cost solution to enduring healthy behaviours. Understanding how to automate and tailor such interventions to patients’ needs can improve health outcomes. This study aims at reviewing the evidence of the use of such interventions in a clinical setting in adult chronic patients. Methods: This study followed the PRISMA guidelines for comprehensive search, appraisal, and synthesis of research evidence. A literature search was performed to find studies published between 1 January 2017 and 26 March 2022 in peer-reviewed journals and written in English. Behaviour change interventions in adult chronic patients with automated and tailored communication systems were considered. Results: Seven quantitative studies (6 RCT and a pilot RCT) with sample sizes from 54 to 506 participants and timeframes varying from 4 to 30 weeks met the eligibility criteria. Populations comprehended patients with cardiovascular disease, chronic kidney disease, diabetes, or osteoarthritis. Communication channels included SMS, email, and voice records. Rule-based models were used to automate content in all studies and bidirectional communication was used in four of them. Content and service delivery were tailored given the patient’s goals, needs, functional ability, activity, personal characteristics, and communication channel preferred. Most data management platforms used were developed in prior studies. Interventions were designed based on 9 different behaviour change models. There was no evidence of improvements in clinical outcomes after the intervention. Some studies showed improvements in self-efficacy (n=1), the use of dietary data info (n=1), physical activity (n=2), social support (n=1), and quality of life (n=1). The risk of bias analysis revealed that one study had moderate risk and the remaining had low risk. Conclusion: This study supports researchers that are designing digital behaviour change interventions by putting in evidence the intervention’s features needed to be considered during this step. It revealed that the most efficient solutions were tailored to patients’ needs and disease stages and that further research on the mechanisms of change and content automation needs to be done.",https://www.researchsquare.com/article/rs-2175520/v1.pdf,page_2.html
Artificial Emotions for Charity Collection: A Serial Mediation Through Perceived Anthropomorphism and Social Presence,"Lee, S., Park, G., Chung, J.",2022,"Despite the broad application of chatbot agents in online interactions, an ongoing debate persists regarding their persuasive role and human-like emotional disclosure. Our study adds to this debate by exploring the effect of chatbot agents’ emotional disclosure on people’s willingness to donate to a charitable cause, and by examining individual and serial mediation between the main effects of perceived anthropomorphism and social presence. To this end, two types of artificial intelligence (AI) chatbot agents—one disclosing factual information and another disclosing human-like emotions—were developed and trained using Dialogflow, a natural language processing engine. A total of 619 US residents were recruited through Amazon Mechanical Turk, an online crowdsourcing platform. Of these, 593 participants completed the required conversation with either version of the AI chatbot agent (factual or emotional), as well as the survey questionnaire, and therefore, were included in the final analysis. The participants exhibited a higher willingness to donate when they interacted with the chatbot disclosing emotions than that disclosing factual information. Moreover, this study found both individual and serial mediating roles of perceived anthropomorphism and social presence. With regard to the implications, theoretically, this study adds to the understanding of applying the notion of human interaction to that involving humans and chatbots. Practically, our findings can be of great help in increasing willingness to donate thereby enhancing fund-raising activities.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4251057&type=2,page_2.html
The Influence of Explainable Artificial Intelligence: Nudging Behaviour or Boosting Capability?,"Franklin, M.",2022,"This article aims to provide a theoretical account and corresponding paradigm for analysing how explainable artificial intelligence (XAI) influences people’s behaviour and cognition. It uses insights from research on behaviour change. Two notable frameworks for thinking about behaviour change techniques are nudges - aimed at influencing behaviour - and boosts - aimed at fostering capability. It proposes that local and concept-based explanations are more adjacent to nudges, while global and counterfactual explanations are more adjacent to boosts. It outlines a method for measuring XAI influence and argues for the benefits of understanding it for optimal, safe and ethical human-AI collaboration.",https://arxiv.org/pdf/2210.02407v1.pdf,page_2.html
How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI,"Chen, K., Shao, A., Burapacheep, J., Li, Y.",2022,"Autoregressive language models, which use deep learning to produce human-like texts, have become increasingly widespread. Such models are powering popular virtual assistants in areas like smart health, finance, and autonomous driving, and facilitating the production of creative writing in domains from the entertainment industry to science communities. While the parameters of these large language models are improving, concerns persist that these models might not work equally for different subgroups in society. Despite growing discussions of AI fairness across disciplines, systemic metrics lack to assess what equity means in dialogue systems and how to engage different populations in the assessment loop. Grounded in theories of deliberative democracy and science and technology studies, this paper proposes an analytical framework for evaluating equity in human-AI dialogues. Using this framework, we conducted an auditing study to examine how GPT-3 responded to different subpopulations on crucial science and social issues: climate change and the Black Lives Matter (BLM) movement. Our corpus consists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals who vary in gender, race and ethnicity, education level, English as a first language, and opinions toward the issues. We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat. We traced these user experience divides to conversational differences and found that GPT-3 used more negative expressions when it responded to the education and opinion minority groups, compared to its responses to the majority groups. To what extent GPT-3 uses justification when responding to minority groups is contingent on the issue. We discuss the implications of our findings for a deliberative conversational AI system that centralizes diversity, equity, and inclusion.",https://arxiv.org/pdf/2209.13627v2.pdf,page_2.html
Exploring the potential of digital health interventions to address behavioural risk factors for the prevention of non-communicable diseases in Asian populations: A qualitative study,"Mair, J.L., Castro, O., Salamanca-Sanabria, A., ...Kowatsch, T., Müller-Riemenschneider, F.",2022,"Background Changing lifestyle patterns over the last decades have seen growing numbers of people in Asia affected by non-communicable diseases and common mental health disorders, including diabetes, cancer, and/or depression. Interventions targeting healthy lifestyle behaviours through digital technologies, including new approaches such as chatbots, may be an effective, low-cost approach to prevent these conditions. To ensure uptake and engagement with digital health interventions, however, it is essential to understand the end-users’ perspectives on using such interventions. The aim of this study was to explore perceptions, barriers, and facilitators to the use of digital health interventions for lifestyle behaviour change in Singapore. Methods Six virtual focus group discussions were conducted with a total of 34 participants (mean ± SD; aged 45 ± 3.6 years; 64.7% females). Focus group recordings were transcribed verbatim and analysed using an inductive thematic analysis approach, followed by deductive mapping according to perceptions, barriers, facilitators, mixed factors, or strategies. Results Four themes were identified: (1) holistic wellbeing (i.e., the importance of both physical and mental health); (2) uptake of digital health interventions (i.e., factors influencing an individual’s decision to start using a digital health intervention such as incentives or government backing); (3) sustained engagement with digital health interventions (i.e., factors influencing an individual’s decision to continue using a digital health intervention such as personalisation or ease of use); and (4) chatbots (i.e., experiences with chatbots and their potential role in providing lifestyle behaviour support). Conclusions Findings highlighted several factors that are relevant for the effectiveness of digital health interventions. Deviations were found from factors that have been shown to be critical for (better-studied) Western populations. Recommendations from this work can inform those wishing to develop and implement digital health interventions in Singapore and other Asian countries.",https://www.researchsquare.com/article/rs-2068852/v1.pdf,page_2.html
Generating Persuasive Responses to Customer Reviews with Multi-Source Prior Knowledge in E-commerce,"Chen, B., Liu, J., Maimaiti, M., Gao, X., Zhang, J.",2022,"Customer reviews usually contain much information about one's online shopping experience. While positive reviews are beneficial to the stores, negative ones will largely influence consumers' decision and may lead to a decline in sales. Therefore, it is of vital importance to carefully and persuasively reply to each negative review and minimize its disadvantageous effect. Recent studies consider leveraging generation models to help the sellers respond. However, this problem is not well-addressed as the reviews may contain multiple aspects of issues which should be resolved accordingly and persuasively. In this work, we propose a Multi-Source Multi-Aspect Attentive Generation model for persuasive response generation. Various sources of information are appropriately obtained and leveraged by the proposed model for generating more informative and persuasive responses. A multi-aspect attentive network is proposed to automatically attend to different aspects in a review and ensure most of the issues are tackled. Extensive experiments on two real-world datasets, demonstrate that our approach outperforms the state-of-the-art methods and online tests prove that our deployed system significantly enhances the efficiency of the stores' dealing with negative reviews.",https://arxiv.org/pdf/2209.09497v1.pdf,page_2.html
ImageArg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining,"Liu, Z., Guo, M., Dai, Y., Litman, D.",2022,"The growing interest in developing corpora of persuasive texts has promoted applications in automated systems, e.g., debating and essay scoring systems; however, there is little prior work mining image persuasiveness from an argumentative perspective. To expand persuasiveness mining into a multi-modal realm, we present a multi-modal dataset, ImageArg, consisting of annotations of image persuasiveness in tweets. The annotations are based on a persuasion taxonomy we developed to explore image functionalities and the means of persuasion. We benchmark image persuasiveness tasks on ImageArg using widely-used multimodal learning methods. The experimental results show that our dataset offers a useful resource for this rich and challenging topic, and there is ample room for modeling improvement.",https://arxiv.org/pdf/2209.06416v1.pdf,page_2.html
SEQUENTIAL INFORMATION DESIGN: LEARNING TO PERSUADE IN THE DARK,"Bernasconi, M., Castiglioni, M., Marchesi, A., Gatti, N., Trovò, F.",2022,"We study a repeated information design problem faced by an informed sender who tries to influence the behavior of a self-interested receiver. We consider settings where the receiver faces a sequential decision making (SDM) problem. At each round, the sender observes the realizations of random events in the SDM problem. This begets the challenge of how to incrementally disclose such information to the receiver to persuade them to follow (desirable) action recommendations. We study the case in which the sender does not know random events probabilities, and, thus, they have to gradually learn them while persuading the receiver. We start by providing a non-trivial polytopal approximation of the set of sender's persuasive information structures. This is crucial to design efficient learning algorithms. Next, we prove a negative result: no learning algorithm can be persuasive. Thus, we relax persuasiveness requirements by focusing on algorithms that guarantee that the receiver's regret in following recommendations grows sub-linearly. In the full-feedback setting-where the sender observes all random events realizations-, we provide an algorithm with Õ(√T) regret for both the sender and the receiver. Instead, in the bandit-feedback setting-where the sender only observes the realizations of random events actually occurring in the SDM problem-, we design an algorithm that, given an α ∈ [1/2, 1] as input, ensures Õ(Tα) and Õ(Tmax{α;1-α/2}) regrets, for the sender and the receiver respectively. This result is complemented by a lower bound showing that such a regrets trade-off is essentially tight.",https://arxiv.org/pdf/2209.03927v1.pdf,page_2.html
ON THE HORIZON: INTERACTIVE AND COMPOSITIONAL DEEPFAKES,"Horvitz, E.",2022,"Over a five-year period, computing methods for generating high-fidelity, fictional depictions of people and events moved from exotic demonstrations by computer science research teams into ongoing use as a tool of disinformation. The methods, referred to with the portmanteau of “deepfakes,"" have been used to create compelling audiovisual content. Here, I share challenges ahead with malevolent uses of two classes of deepfakes that we can expect to come into practice with costly implications for society: interactive and compositional deepfakes. Interactive deepfakes have the capability to impersonate people with realistic interactive behaviors, taking advantage of advances in multimodal interaction. Compositional deepfakes leverage synthetic content in larger disinformation plans that integrate sets of deepfakes over time with observed, expected, and engineered world events to create persuasive synthetic histories. Synthetic histories can be constructed manually but may one day be guided by adversarial generative explanation (AGE) techniques. In the absence of mitigations, interactive and compositional deepfakes threaten to move us closer to a post-epistemic world, where fact cannot be distinguished from fiction. I shall describe interactive and compositional deepfakes and reflect about cautions and potential mitigations to defend against them.",https://arxiv.org/pdf/2209.01714v1.pdf,page_2.html
"ESC-Rules: Explainable, Semantically Constrained Rule Sets","Glauer, M., West, R., Michie, S., Hastings, J.",2022,"We describe a novel approach to explainable prediction of a continuous variable based on learning fuzzy weighted rules. Our model trains a set of weighted rules to maximise prediction accuracy and minimise an ontology-based’semantic loss’ function including user-specified constraints on the rules that should be learned in order to maximise the explainability of the resulting rule set from a user perspective. This system fuses quantitative sub-symbolic learning with symbolic learning and constraints based on domain knowledge. We illustrate our system on a case study in predicting the outcomes of behavioural interventions for smoking cessation, and show that it outperforms other interpretable approaches, achieving performance close to that of a deep learning model, while offering transparent explainability that is an essential requirement for decision-makers in the health domain.",https://arxiv.org/pdf/2208.12523v1.pdf,page_2.html
Rationality-Robust Information Design: Bayesian Persuasion under Quantal Response,"Feng, Y., Ho, C.-J., Tang, W.",2022,"Classic mechanism/information design imposes the assumption that agents are fully rational, meaning each of them always selects the action that maximizes her expected utility. Yet many empirical evidence suggests that human decisions may deviate from this full rationality assumption. In this work, we attempt to relax the full rationality assumption with bounded rationality. Specifically, we formulate the bounded rationality of an agent by adopting the quantal response model (McKelvey and Palfrey, 1995). We develop a theory of rationality-robust information design in the canonical setting of Bayesian persuasion (Kamenica and Gentzkow, 2011) with binary receiver action. We first identify conditions under which the optimal signaling scheme structure for a fully rational receiver remains optimal or approximately optimal for a boundedly rational receiver. In practice, it might be costly for the designer to estimate the degree of the receiver’s bounded rationality level. Motivated by this practical consideration, we then study the existence and construction of robust signaling schemes when there is uncertainty about the receiver’s bounded rationality level.",https://arxiv.org/pdf/2207.08253v3.pdf,page_2.html
Artificial Intelligence (AI)-based Chatbots in Promoting Health Behavioral Changes: A Systematic Review,"Aggarwal, A., Tam, C.C., Wu, D., Li, X., Qiao, S.",2022,"Background Artificial-Intelligence (AI)-based chatbots can offer personalized, engaging, and on-demand health-promotion interventions. This systematic review evaluates the feasibility, efficacy, and intervention characteristics of AI-chatbots in promoting health-behavior change. Methods A comprehensive search was conducted in seven bibliographic databases (PubMed, IEEE Xplore, ACM Digital Library, PsychoINFO, Web of Science, EMBASE, and JMIR publications) for empirical articles published from 1980 to 2022 that evaluated feasibility and/or efficacy of AI-chatbots for behavioral change. The screening, extraction, and analyses of identified articles followed the PRISMA guidelines. Results Of the 15 included studies, majority studies (n=11) reported high usability, acceptability and engagement, and some evidence on feasibility of AI-chatbots. Selected studies demonstrated high efficacy in promoting healthy lifestyles (n=6), smoking cessation (n=4), treatment/medication adherence (n=2), and reduction in substance misuse (n=1). Behavioral change theories and/or expert consultation were used to develop behavioral change strategies of AI-chatbots, including goal setting, monitoring, real-time reinforcement/feedback, and on-demand support. Real-time user-chatbot interaction data, such as user preferences and behavioral performance, were collected on the chatbot platform to identify ways of providing personalized services. The AI-chatbots demonstrated potential for scalability by deployment through accessible devices and platforms (e.g., smartphones and messenger). Participants also reported that AI-chatbots offered a non-judgmental space for communicating sensitive information. However, the reported results need to be interpreted with caution because of moderate to high risk of internal validity, insufficient description of AI-techniques, and limitation for generalizability. Conclusion AI-chatbots have demonstrated efficacy of health-behavior change interventions among large and diverse population; however, future studies need to adopt robust RCTs to establish definitive conclusions.",https://www.medrxiv.org/content/10.1101/2022.07.05.22277263v1.full.pdf,page_2.html
Eco-driving for Electric Connected Vehicles at Signalized Intersections: A Parameterized Reinforcement Learning approach,"Jiang, X., Zhang, J., Li, D.",2022,"This paper proposes an eco-driving framework for electric connected vehicles (CVs) based on reinforcement learning (RL) to improve vehicle energy efficiency at signalized intersections. The vehicle agent is specified by integrating the model-based car-following policy, lane-changing policy, and the RL policy, to ensure safe operation of a CV. Subsequently, a Markov Decision Process (MDP) is formulated, which enables the vehicle to perform longitudinal control and lateral decisions, jointly optimizing the car-following and lane-changing behaviors of the CVs in the vicinity of intersections. Then, the hybrid action space is parameterized as a hierarchical structure and thereby trains the agents with two-dimensional motion patterns in a dynamic traffic environment. Finally, our proposed methods are evaluated in SUMO software from both a single-vehicle-based perspective and a flow-based perspective. The results show that our strategy can significantly reduce energy consumption by learning proper action schemes without any interruption of other human-driven vehicles (HDVs).",https://arxiv.org/pdf/2206.12065v2.pdf,page_2.html
Optimizing Fault Detection for Big Data Analytics Through Evolutionary Computation,"Jothi, P., Dwivedi, M.",2022,"Intelligent fault detection is promising to deal with big data due to its ability in rapidly and efficiently processing collected signals and providing accurate detection results. In traditional fault detection methods, however, the features are manually extracted depending on prior knowledge and diagnostic expertise, such processes take advantage of human ingenuity but are time-consuming. Inspired by the idea of unsupervised feature learning artificial intelligence techniques are used to learn features from the raw data. As the dimensionality increases, the accuracy of fault identification methods implemented on big data decreases significantly. For supervised learning, large volume of data is needed which leads to high cost and time consuming. In this paper, an unsupervised learning approach is proposed based on weighted softmax regression for fault detection using the power signals. Fault prediction has become an important subject in recent years, as it helps businesses to make significant savings in time and expense by offering successful methods for predictive maintenance. Pre-processing of data was a complicated job to overcome many problems with the dataset, including scale, sparsity, distortion, burst effects and confidence. As pre-monitor signals for failure did not share standard patterns, but were characterized only as non-normal system signals, a predictive error was made using outlier detection. Faults were explained by displaying system characteristics with abnormal values. An experimental assessment was conducted to determine the quality of the solution proposed. Results indicate that high-grade outliers provide successful markers of initial failures. In addition, explanations about irregular characteristic values (responsible for oversight) seem rather expressive. Based on the sliding window technique, the method to detect errors in high dimensional data streams is applied to an on-line mode. The online extension can be adapted to the time changing behaviour of the controlled system by experiments on synthetic datasets and is therefore applicable to the dynamic error detection. To assess the suggested strategy, we contrasted it with engineered data sets created utilizing the LOF (online expansion), SVDD, and KPCA approaches getting more than 90% result. The data exhibit our methodology's accomplishment as far as perfection, productivity, and strength.",https://www.researchsquare.com/article/rs-1748874/v2.pdf,page_2.html
Argumentative Text Generation in Economic Domain | Генерация аргументативных текстов экономической тематики,"Irina, F.N., Dmitriy, O., Klavdiya, B.O., Evgeny, K.V.",2022,"The development of large and super-large language models, such as GPT-3, T5, Switch Transformer, ERNIE, etc., has significantly improved the performance of text generation. One of the important research directions in this area is the generation of texts with arguments. The solution of this problem can be used in business meetings, political debates, dialogue systems, for preparation of student essays. One of the main domains for these applications is the economic sphere. The key problem of the argument text generation for the Russian language is the lack of annotated argumentation corpora. In this paper, we use translated versions of the Argumentative Microtext, Persuasive Essays and UKP Sentential corpora to fine-tune RuBERT model. Further, this model is used to annotate the corpus of economic news by argumentation. Then the annotated corpus is employed to fine-tune the ruGPT-3 model, which generates argument texts. The results show that this approach improves the accuracy of the argument generation by more than 20 percentage points (63.2% vs. 42.5%) compared to the original ruGPT-3 model.",https://arxiv.org/pdf/2206.09251v1.pdf,page_2.html
A Formal Understanding of Computational Empathy in Interactive Agents,"Brännström, A., Wester, J., Nieves, J.C.",2022,"Rational software agents, embodied as conversational agents, such as chatbots, are progressively being used in the area of health and well-being. In such applications, where rational agents aim to engage users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding systems' empathic capabilities. In the current state-of-the-art, there are no tools to do that. In order to understand empathic capabilities in rational software systems, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in conversational agents for health and well-being, a formal definition (in terms of an ontology) of empathy is developed. We evaluate this formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition of empathy identifies different necessary conditions for assessing empathy in interactive agents.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4138212&type=2,page_2.html
Generation and Classification of Motivational-Interviewing-Style Reflections for Smoking Behaviour Change Using Few-Shot Learning with Transformers,"Rose, J., Ahmed, I., Keilty, E., Cooper, C., Selby, P.",2022,"If conversational agents can take on a therapeutic role, they may provide a scalable way to help many people suffering from addictions. Motivational Interviewing (MI) is a validated therapy for behaviour change that can be applied to addiction, including smoking cessation. A core technique in MI (and many other kinds of talk therapy) is to pose an open-ended question concerning a negative behaviour, and then to provide a reflection of the response. Reflections can be a simple restatement of the response, or a more complex inference from prior statements or general knowledge, and they help a person contemplate the behaviour more deeply. We describe a method to generate reflections that uses few-shot priming of the GPT-2 and GPT-3 language models. These produce very promising simple and complex reflections, but also some that are off-topic or irrelevant. To filter these, we train a classifier to detect poor reflections, employing samples labeled by an MI expert. Its accuracy is 81%, sensitivity 90% and specificity 71%. We show that GPT-2 can generate acceptable reflections at a 54% success rate, and when combined with the classifier/filter produces acceptable reflections 73% of the time. The GPT-3 model has a native success rate of 89%.",https://dx.doi.org/10.36227/techrxiv.20029880.v1,page_2.html
Integrating fuzzy trajectory data and artificial intelligence methods for multi-style lane-changing behavior prediction,"Gu, R.",2022,"Artificial intelligence algorithms have been extensively applied in the field of intelligent transportation, especially for driving behavior analysis and prediction. This study proposes a novel framework by integrating fuzzy trajectory data, unsupervised learning and supervised learning methods to predict lane-changing behaviors taking multi driving styles into account. The microscopic trajectory data from the Highway Drone Dataset (HighD) are employed to construct two types of datasets including precise trajectory datasets and fuzzy trajectory datasets for lane-changing prediction models. The fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm. Two typical supervised learning methods, including random forest and long-short-term memory combined with convolutional neural network, are further applied for lane-changing behavior prediction. Results indicate that (1) the proposed integration approach performs better than the conventional lane-change prediction; (2) the relative speed-related features have a greater contribution to the lane-changing prediction after being processed by fuzzy rules based on driving styles; and (3) the difference among driving styles is more reflected from the state of lateral movement rather than the lane-changing duration.",https://arxiv.org/pdf/2205.05016v1.pdf,page_2.html
Artificial Intelligence with CBC based Morphometric Parameters aimed toward effective diagnostic practices for Dysplasia associated Hematological Malignancies,"Haider, Z., Ujjan, I.U., Khan, N.A., ...Urrechaga, E., Khanani, M.R.",2022,"Background: The diagnosis and classification of dysplasia associated hematological neoplasms is dominated by morphology. Current round of study made use of potential ‘fingerprints’ among routinely generated diagnostic data particularly morphological and immature fraction-related (morphometric) parameters produced during routine complete blood count (CBC) testing in hemat-oncology department through artificial intelligence predictive modeling. Methods: Along conventional statistics, neural network models were trained on anonymized demographical, clinical and diagnostic data of total 1624 individuals with common hematological neoplasms. In addition, validation conducted on independent dataset. The frameworks were trained to differentiate hematological malignancies cases with various sub-entities of Dysplasia against Non-dysplastic group as a control cohort. Results: Our predictive model attained greater precisions; percent incorrect prediction were remained 10.3 and 4.6 for training and testing phases, respectively along with a 95.4% negative predictive value (NPV). Moreover, higher accuracy (93.1%) was obtained during prospective validation in challenge of independent dataset. Model’s performance related metrics: the gain and lift chart, predictive-pseudo probability chart, and receiver operative curve (ROC) curve were noted as persuasive. Considering the sensitivity and specificity, area under the curve (AUC) values were also noted quite convincing; 0.954 for Non-dysplasia group while 0.994 for acute myeloid leukemia with dysplasia (AML-Dys) followed by 0.992, 0.988, 0.986, 0.984, 0.973, and 0.962 for chronic myelo-monocytic leukemia (CMML), refractory anemia with excess blast-I (RAEB-I), refractory anemia with excess blast-II (RAEB-II), refractory anemia with multi-lineage dysplasia (RCMD), Hypoplastic myelodysplastic syndrome (H-MDS), and refractory anemia with uni-lineage dysplasia (RCUD) respectively. Conclusion: The negative predictive efficiency of our framework advocates its utility as a screening tool for the rapid expulsion of Dysplasia associated hematological neoplasms in hemat-oncology sections, aiding prompt care decisions.",https://www.researchsquare.com/article/rs-1349008/v1.pdf,page_2.html
Interacting with Non-Cooperative User: A New Paradigm for Proactive Dialogue Policy,"Lei, W., Zhang, Y., Song, F., ...Yang, Z., Chua, T.-S.",2022,"Proactive dialogue system is able to lead the conversation to a goal topic and has advantaged potential in bargain, persuasion and negotiation. Current corpus-based learning manner limits its practical application in real-world scenarios. To this end, we contribute to advance the study of the proactive dialogue policy to a more natural and challenging setting, i.e., interacting dynamically with users. Further, we call attention to the non-cooperative user behavior — the user talks about off-path topics when he/she is not satisfied with the previous topics introduced by the agent. We argue that the targets of reaching the goal topic quickly and maintaining a high user satisfaction are not always converge, because the topics close to the goal and the topics user preferred may not be the same. Towards this issue, we propose a new solution named I-Pro that can learn Proactive policy in the Interactive setting. Specifically, we learn the trade-off via a learned goal weight, which consists of four factors (dialogue turn, goal completion difficulty, user satisfaction estimation, and cooperative degree). The experimental results demonstrate I-Pro significantly outperforms baselines in terms of effectiveness and interpretability.",https://arxiv.org/pdf/2204.07433v1.pdf,page_2.html
ZETAR: Modeling and Computational Design of Strategic and Adaptive Compliance Policies,"Huang, L., Zhu, Q.",2022,"Compliance management plays an important role in mitigating insider threats. Incentive design is a proactive and non-invasive approach to achieving compliance by aligning an insider’s incentive with the defender’s security objective, which motivates (rather than commands) an insider to act in the organization’s interests. Controlling insiders’ incentives for population-level compliance is challenging because they are neither precisely known nor directly controllable. To this end, we develop ZETAR, a zero-trust audit and recommendation framework, to provide a quantitative approach to model insiders’ incentives and design customized recommendation policies to improve their compliance. We formulate primal and dual convex programs to compute the optimal bespoke recommendation policies. We create the theoretical underpinning for understanding trust, compliance, and satisfaction, which leads to scoring mechanisms of how compliant and persuadable an insider is. After classifying insiders as malicious, self-interested, or amenable based on their incentive misalignment levels with the defender, we establish bespoke information disclosure principles for these insiders of different incentive categories. We identify the policy separability principle and the set convexity, which enable finite-step algorithms to efficiently learn the Completely Trustworthy (CT) policy set when insiders’ incentives are unknown. Finally, we present a case study to corroborate the design. Our results show that ZETAR can well adapt to insiders with different risk and compliance attitudes and significantly improve compliance. Moreover, trustworthy recommendations can provably promote cyber hygiene and insiders’ satisfaction.",https://arxiv.org/pdf/2204.02294v2.pdf,page_2.html
ProHealth eCoach: User-Centered Design and Development of an eCoach App to Promote Healthy Lifestyle with Personalized Activity Recommendations,"Chatterjee, A., Prinz, A., Gerdes, M., ...Pahari, N., Meena, Y.K.",2022,"Background Regular physical activity (PA), healthy habits, and a healthy diet are recommended guidelines to maintain a healthy lifestyle. A healthy lifestyle can help to avoid chronic diseases and long-term illnesses. A monitoring and automatic personalized lifestyle recommendation system (i.e., automatic electronic coach or eCoach) with considering clinical and ethical guidelines, individual health status, condition, and preferences may successfully help participants to follow recommendations to maintain a healthy lifestyle. As a prerequisite for the prototype design of such a helpful eCoach system, it is essential to involve the end-users and subject-matter experts throughout the iterative design process. Methods We used an iterative user-centered design (UCD) approach with a dialogue-labs method to collect qualitative data from different users (i.e., researchers, non-technical and technical, health professionals, subject-matter experts, and potential end-users) to develop a roadmap for self-management with eCoaching. We designed and developed the eCoach prototype in two stages, adopting different phases of the UCD approach and co-design events. In UCD workshop 1, we focused on identifying end-users, understanding the user’s context, specifying user requirements, designing and developing an initial low-fidelity eCoach prototype. In UCD workshop 2, we focused on maturing the low-fidelity solution design and development for the visualization of continuous and discrete data, artificial intelligence (AI)-based interval forecasting, personalized recommendations, and activity goals. Results The UCD approach helped to develop a working prototype of eCoach system that meets end-user's requirements and expectations towards an effective recommendation visualization, considering diversity in culture, quality of life, and human values. The design provides an early version of the solution, consisting of wearable technology, a mobile app, and web content for self-monitoring, goal setting, and lifestyle recommendations in an engaging manner between the eCoach app and end-users. Conclusions The adopted UCD approach brings in a design focus on the user and their needs at each phase. Throughout the design process, users have been involved at the heart of the design to create a working research prototype to improve the fit between technology, end-user, and researchers. Furthermore, we performed a technological readiness study of ProHealth eCoach against standard levels set by European Union (EU).",https://www.researchsquare.com/article/rs-1451563/v1.pdf,page_2.html
An Audit of Misinformation Filter Bubbles on YouTube: Bubble Bursting and Recent Behavior Changes,"Tomlein, M., Pecher, B., Simko, J., ...Podrouzek, J., Bielikova, M.",2022,"The negative effects of misinformation filter bubbles in adaptive systems have been known to researchers for some time. Several studies investigated, most prominently on YouTube, how fast a user can get into a misinformation filter bubble simply by selecting “wrong choices” from the items offered. Yet, no studies so far have investigated what it takes to “burst the bubble”, i.e., revert the bubble enclosure. We present a study in which pre-programmed agents (acting as YouTube users) delve into misinformation filter bubbles by watching misinformation promoting content (for various topics). Then, by watching misinformation debunking content, the agents try to burst the bubbles and reach more balanced recommendation mixes. We recorded the search results and recommendations, which the agents encountered, and analyzed them for the presence of misinformation. Our key finding is that bursting of a filter bubble is possible, albeit it manifests differently from topic to topic. Moreover, we observe that filter bubbles do not truly appear in some situations. We also draw a direct comparison with a previous study. Sadly, we did not find much improvements in misinformation occurrences, despite recent pledges by YouTube.",https://arxiv.org/pdf/2203.13769v1.pdf,page_2.html
Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI,"Franklin, M., Ashton, H., Gorman, R., Armstrong, S.",2022,"As artificial intelligence becomes more powerful and a ubiquitous presence in daily life, it is imperative to understand and manage the impact of AI systems on our lives and decisions. Modern ML systems often change user behavior (e.g. personalized recommender systems learn user preferences to deliver recommendations that change online behavior). An externality of behavior change is preference change. This article argues for the establishment of a multidisciplinary endeavor focused on understanding how AI systems change preference: Preference Science. We operationalize preference to incorporate concepts from various disciplines, outlining the importance of meta-preferences and preference-change preferences, and proposing a preliminary framework for how preferences change. We draw a distinction between preference change, permissible preference change, and outright preference manipulation. A diversity of disciplines contribute unique insights to this framework.",https://arxiv.org/pdf/2203.10525v2.pdf,page_2.html
Reducing Flipping Errors in Deep Neural Networks,"Deng, X., Xiao, Y., Long, B., Zhang, Z.",2022,"Deep neural networks (DNNs) have been widely applied in various domains in artificial intelligence including computer vision and natural language processing. A DNN is typically trained for many epochs and then a validation dataset is used to select the DNN in an epoch (we simply call this epoch “the last epoch”) as the final model for making predictions on unseen samples, while it usually cannot achieve a perfect accuracy on unseen samples. An interesting question is “how many test (unseen) samples that a DNN misclassifies in the last epoch were ever correctly classified by the DNN before the last epoch?”. In this paper, we empirically study this question and find on several benchmark datasets that the vast majority of the misclassified samples in the last epoch were ever classified correctly before the last epoch, which means that the predictions for these samples were flipped from “correct” to “wrong”. Motivated by this observation, we propose to restrict the behavior changes of a DNN on the correctly-classified samples so that the correct local boundaries can be maintained and the flipping error on unseen samples can be largely reduced. Extensive experiments on different benchmark datasets with different modern network architectures demonstrate that the proposed flipping error reduction (FER) approach can substantially improve the generalization, the robustness, and the transferability of DNNs without introducing any additional network parameters or inference cost, only with a negligible training overhead.",https://arxiv.org/pdf/2203.08390v1.pdf,page_2.html
Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue,"Chen, M., Shi, W., Yan, F., ...Sahay, S., Yu, Z.",2022,"Complex conversation settings such as persuasion involve communicating changes in attitude or behavior, so users' perspectives need to be addressed, even when not directly related to the topic. In this work, we contribute a novel modular dialogue system framework that seamlessly integrates factual information and social content into persuasive dialogue. Our framework is generalizable to any dialogue tasks that have mixed social and task contents. We conducted a study that compared user evaluations of our framework versus a baseline end-to-end generation model. We found our framework was evaluated more favorably in all dimensions including competence and friendliness, compared to the end-toend model which does not explicitly handle social content or factual questions.",https://arxiv.org/pdf/2203.07657v3.pdf,page_2.html
Influencing Long-Term Behavior in Multiagent Reinforcement Learning,"Kim, D.-K., Riemer, M., Liu, M., ...Tesauro, G., How, J.P.",2022,"The main challenge of multiagent reinforcement learning is the difficulty of learning useful policies in the presence of other simultaneously learning agents whose changing behaviors jointly affect the environment's transition and reward dynamics. An effective approach that has recently emerged for addressing this non-stationarity is for each agent to anticipate the learning of other agents and influence the evolution of future policies towards desirable behavior for its own benefit. Unfortunately, previous approaches for achieving this suffer from myopic evaluation, considering only a finite number of policy updates. As such, these methods can only influence transient future policies rather than achieving the promise of scalable equilibrium selection approaches that influence the behavior at convergence. In this paper, we propose a principled framework for considering the limiting policies of other agents as time approaches infinity. Specifically, we develop a new optimization objective that maximizes each agent's average reward by directly accounting for the impact of its behavior on the limiting set of policies that other agents will converge to. Our paper characterizes desirable solution concepts within this problem setting and provides practical approaches for optimizing over possible outcomes. As a result of our farsighted objective, we demonstrate better long-term performance than state-of-the-art baselines across a suite of diverse multiagent benchmark domains.",https://arxiv.org/pdf/2203.03535v4.pdf,page_2.html
Metacognitive Agents for Ethical Decision Support: Conceptual Model and Research Roadmap,"Kennedy, C.M.",2022,"An ethical value-action gap exists when there is a discrepancy between intentions and actions. For example, people who support environmental sustainability often use cars and short-haul flights because of convenience and time-pressure. This discrepancy may be caused by social and structural obstacles as well as cognitive biases. Current technology can make this worse. For example, social media tends to enhance emotions such as anger or fear, which can result in polarisation and impulsive decisions. Computational models of cognition and affect can provide insights into the value-action gap and how it can be reduced. Such models include dual process architectures, emotion models and behaviour change theories. In particular, metacognition (“thinking about thinking”) plays an important role in many of these models as a mechanism for self-regulation and for reasoning about mental attitudes. This paper outlines a roadmap for translating cognitive-affective models into assistant agents to help make value-aligned decisions. Key principles include “agile” rapid-prototyping using agent-based simulation, and the combination of descriptive and normative models into a single agent architecture.",https://arxiv.org/pdf/2202.12039v1.pdf,page_2.html
Sequential Information Design: Markov Persuasion Process and Its Efficient Reinforcement Learning,"Wu, J., Zhang, Z., Feng, Z., ...Jordan, M.I., Xu, H.",2022,"In today's economy, it becomes important for Internet platforms to consider the sequential information design problem to align its long term interest with incentives of the gig service providers (e.g., drivers, hosts). This paper proposes a novel model of sequential information design, namely the Markov persuasion processes (MPPs). Specifically, in an MPP, a sender, with informational advantage, seeks to persuade a stream of myopic receivers to take actions that maximizes the sender's cumulative utilities in a finite horizon Markovian environment with varying prior and utility functions. Planning in MPPs thus faces the unique challenge in finding a signaling policy that is simultaneously persuasive to the myopic receivers and inducing the optimal long-term cumulative utilities of the sender. Nevertheless, in the population level where the model is known, it turns out that we can efficiently determine the optimal (resp. ǫ-optimal) policy with finite (resp. infinite) states and outcomes, through a modified formulation of the Bellman equation that additionally takes persuasiveness into consideration. Our main technical contribution is to study the MPP under the online reinforcement learning (RL) setting, where the goal is to learn the optimal signaling policy by interacting with with the underlying MPP, without the knowledge of the sender's utility functions, prior distributions, and the Markov transition kernels. For such a problem, we design a provably efficient no-regret learning algorithm, the Optimism-Pessimism Principle for Persuasion Process (OP4), which features a novel combination of both optimism and pessimism principles. In particular, we obtain optimistic estimates of the value functions to encourage exploration under the unknown environment. Meanwhile, we additionally robustify the signaling policy with respect to the uncertainty of prior estimation to prevent receiver's detrimental equilibrium behavior. Our algorithm enjoys sample efficiency by achieving a sublinear √T -regret upper bound. Furthermore, both our algorithm and theory can be applied to MPPs with large space of outcomes and states via function approximation, and we showcase such a success under the linear setting.",https://arxiv.org/pdf/2202.10678v1.pdf,page_2.html
Automatic Generation of Individual Fuzzy Cognitive Maps from Longitudinal Data,"Wozniak, M.K., Mkhitaryan, S., Giabbanelli, P.J.",2022,"Fuzzy Cognitive Maps (FCMs) are computational models that represent how factors (nodes) change over discrete interactions based on causal impacts (weighted directed edges) from other factors. This approach has traditionally been used as an aggregate, similarly to System Dynamics, to depict the functioning of a system. There has been a growing interest in taking this aggregate approach at the individual-level, for example by equipping each agent of an Agent-Based Model with its own FCM to express its behavior. Although frameworks and studies have already taken this approach, an ongoing limitation has been the difficulty of creating as many FCMs as there are individuals. Indeed, current studies have been able to create agents whose traits are different, but whose decision-making modules are often identical, thus limiting the behavioral heterogeneity of the simulated population. In this paper, we address this limitation by using Genetic Algorithms to create one FCM for each agent, thus providing the means to automatically create a virtual population with heterogeneous behaviors. Our algorithm builds on prior work from Stach and colleagues by introducing additional constraints into the process and applying it over longitudinal, individual-level data. A case study from a real-world intervention on nutrition confirms that our approach can generate heterogeneous agents that closely follow the trajectories of their real-world human counterparts. Future works include technical improvements such as lowering the computational time of the approach, or case studies in computational intelligence that use our virtual populations to test new behavior change interventions.",https://arxiv.org/pdf/2202.07065v1.pdf,page_2.html
A Simplified Variant of Gödel's Ontological Argument,"Benzmüller, C.",2022,"A simplified variant of Gödel's ontological argument is presented. The simplified argument is valid already in basic modal logics K or KT, it does not suffer from modal collapse, and it avoids the rather complex predicates of essence (Ess.) and necessary existence (NE) as used by Gödel. The variant presented has been obtained as a side result of a series of theory simplification experiments conducted in interaction with a modern proof assistant system. The starting point for these experiments was the computer encoding of Gödel's argument, and then automated reasoning techniques were systematically applied to arrive at the simplified variant presented. The presented work thus exemplifies a fruitful human-computer interaction in computational metaphysics. Whether the presented result increases or decreases the attractiveness and persuasiveness of the ontological argument is a question I would like to pass on to philosophy and theology.",https://arxiv.org/pdf/2202.06264v3.pdf,page_2.html
Measuring why in recommender systems: A comprehensive survey on the evaluation of explainable recommendation,"Chen, X., Zhang, Y., Wen, J.-R.",2022,"Explainable recommendation has shown its great advantages for improving recommendation persuasiveness, user satisfaction, system transparency and among others. A fundamental problem of explainable recommendation is how to evaluate the explanations. In the past few years, various evaluation strategies have been proposed. However, they are scattered in different papers, and there lacks a systematic and detailed comparison between them. To bridge this gap, in this paper, we comprehensively review the previous work, and provide different taxonomies according to the evaluation perspectives and evaluation methods. Beyond summarization, we also analyze the (dis)advantages of existing evaluation methods, and provide a series of guidelines on how to select them. The contents of this survey are concluded from more than 100 papers from top-tier conferences like IJCAI, AAAI, TheWebConf, Recsys, UMAP and IUI, and the complete comparisons are presented at https://shimo.im/sheets/VKrpYTcwVH6KXgdy/MO DOC/. With this survey, we finally aim to provide a clear and comprehensive review on the evaluation of explainable recommendation.",https://arxiv.org/pdf/2202.06466v1.pdf,page_2.html
Diagnosis of Phobic Anxiety Disorders using Virtual Reality Environment,"Munir, A., Saleem, Y.",2022,"Specific Phobia Anxiety disorders are rarely detected in a clinical setting and are indicators of other grave mental health problems. VR is a persuasive tool for diagnosing and treating psychiatric problems, but the Virtual Reality applications developed are reported to lack a methodological approach. This particular research consults the Diagnostic and Statistical Manual of Mental Health Disorders (DSM-5) by the American Psychiatric Association (APA)to strengthen objectivity and add reliability to the assessment. We investigated participants’ behavior in the Virtual Reality Environment. The participant walks or runs along a road in four different scenarios of a Virtual Reality Environment. The first scenario is a neutral environment with no phobic stimulus; the afterward situations pose for a dog cue, thunder lightning stimulus, and a combination of both stimulation consecutively. The assessment measures include distance and time calculated in each VRE scene, minimum and maximum oxygen levels and heart rate recorded for each scenario, physical and verbal reactions observed, self-reported measure of Cybersickness, and Sense of Presence. Participant’s information such as name, age, gender, any bad experience with dogs or thunder lightning, and proposed phobia severity levels of parents. The “APA Specific Phobia Severity Measure - Adult” by DSM-5 classifies every record. Applying Artificial Intelligence learning models suggests Astraphobic patients also suffer from Dog phobia. The results imply that the Sense of Presence is a decisive factor in the assessment. It signifies that distance, time interval, and change in oxygen levels in the presence of phobic cues are influential parameters for phobia estimation.",https://dx.doi.org/10.36227/techrxiv.19153769.v1,page_2.html
Graph Convolution-Based Deep Reinforcement Learning for Multi-Agent Decision-Making in Mixed Traffic Environments,"Liu, Q., Li, Z., Li, X., Wu, J., Yuan, S.",2022,"An efficient and reliable multi-agent decision-making system is highly demanded for the safe and efficient operation of connected autonomous vehicles in intelligent transportation systems. Current researches mainly focus on the Deep Reinforcement Learning (DRL) methods. However, utilizing DRL methods in interactive traffic scenarios is hard to represent the mutual effects between different vehicles and model the dynamic traffic environments due to the lack of interactive information in the representation of the environments, which results in low accuracy of cooperative decisions generation. To tackle these difficulties, this research proposes a framework to enable different Graph Reinforcement Learning (GRL) methods for decision-making, and compares their performance in interactive driving scenarios. GRL methods combinate the Graph Neural Network (GNN) and DRL to achieve the better decisions generation in interactive scenarios of autonomous vehicles, where the features of interactive scenarios are extracted by the GNN, and cooperative behaviors are generated by DRL framework. Several GRL approaches are summarized and implemented in the proposed framework. To evaluate the performance of the proposed GRL methods, an interactive driving scenarios on highway with two ramps is constructed, and simulated experiment in the SUMO platform is carried out to evaluate the performance of different GRL approaches. Finally, results are analyzed in multiple perspectives and dimensions to compare the characteristic of different GRL approaches in intelligent transportation scenarios. Results show that the implementation of GNN can well represents the interaction between vehicles, and the combination of GNN and DRL is able to improve the performance of the generation of lane-change behaviors. The source code of our work can be found at https://github.com/Jacklinkk/TorchGRL.",https://arxiv.org/pdf/2201.12776v1.pdf,page_2.html
Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study,"Sengupta, K., Srivastava, P.R.",2022,"Language data and models demonstrate various types of bias, be it ethnic, religious, gender, or socioeconomic. AI/NLP models, when trained on the racially biased dataset, AI/NLP models instigate poor model explainability, influence user experience during decision making and thus further magnifies societal biases, raising profound ethical implications for society. The motivation of the study is to investigate how AI systems imbibe bias from data and produce unexplainable discriminatory outcomes and influence an individual's articulateness of system outcome due to the presence of racial bias features in datasets. The design of the experiment involves studying the counterfactual impact of racial bias features present in language datasets and its associated effect on the model outcome. A mixed research methodology is adopted to investigate the cross implication of biased model outcome on user experience, effect on decision-making through controlled lab experimentation. The findings provide foundation support for correlating the implication of carry-over an artificial intelligence model solving NLP task due to biased concept presented in the dataset. Further, the research outcomes justify the negative influence on users' persuasiveness that leads to alter the decision-making quotient of an individual when trying to rely on the model outcome to act. The paper bridges the gap across the harm caused in establishing poor customer trustworthiness due to an inequitable system design and provides strong support for researchers, policymakers, and data scientists to build responsible AI frameworks within organizations.",https://arxiv.org/pdf/2202.00471v2.pdf,page_2.html
Bayesian Promised Persuasion: Dynamic Forward-Looking Multiagent Delegation with Informational Burning,"Zhang, T., Zhu, Q.",2022,"This work studies a dynamic mechanism design problem in which a principal delegates decision makings to a group of privately-informed agents without the monetary transfer or burning. We consider that the principal privately possesses complete knowledge about the state transitions and study how she can use her private observation to support the incentive compatibility of the delegation via informational burning, a process we refer to as the looking-forward persuasion. The delegation mechanism is formulated in which the agents form belief hierarchies due to the persuasion and play a dynamic Bayesian game. We propose a novel randomized mechanism, known as Bayesian promised delegation (BPD), in which the periodic incentive compatibility is guaranteed by persuasions and promises of future delegations. We show that the BPD can achieve the same optimal social welfare as the original mechanism in stationary Markov perfect Bayesian equilibria. A revelation-principle-like design regime is established to show that the persuasion with belief hierarchies can be fully characterized by correlating the randomization of the agents' local BPD mechanisms with the persuasion as a direct recommendation of the future promises.",https://arxiv.org/pdf/2201.06081v1.pdf,page_2.html
Lane Change Decision-Making through Deep Reinforcement Learning,"Ghimire, M., Choudhury, M.R., Lagudu, G.S.S.H.",2021,"Due to the complexity and volatility of the traffic environment, decision-making in autonomous driving is a significantly hard problem. In this project, we use a Deep Q-Network, along with rule-based constraints to make lane-changing decision. A safe and efficient lane change behavior may be obtained by combining high-level lateral decision-making with low-level rule-based trajectory monitoring. The agent is anticipated to perform appropriate lane-change maneuvers in a real-world-like udacity simulator after training it for a total of 100 episodes. The results shows that the rule-based DQN performs better than the DQN method. The rule-based DQN achieves a safety rate of 0.8 and average speed of 47 MPH.MSC Codes 15-04",https://arxiv.org/pdf/2112.14705v1.pdf,page_2.html
Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion,"Agarwal, S., Hu, L., Ng, E., ...Li, H., Rohrbach, A.",2021,"In today's era of digital misinformation, we are increasingly faced with new threats posed by video falsification techniques. Such falsifications range from cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g., sophisticated AI media synthesis methods), which are becoming perceptually indistinguishable from real videos. To tackle this challenge, we propose a multi-modal semantic forensic approach to discover clues that go beyond detecting discrepancies in visual quality, thereby handling both simpler cheapfakes and visually persuasive deepfakes. In this work, our goal is to verify that the purported person seen in the video is indeed themselves by detecting anomalous facial movements corresponding to the spoken words. We leverage the idea of attribution to learn person-specific biometric patterns that distinguish a given speaker from others. We use interpretable Action Units (AUs) to capture a person's face and head movement as opposed to deep CNN features, and we are the first to use word-conditioned facial motion analysis. We further demonstrate our method's effectiveness on a range of fakes not seen in training including those without video manipulation, that were not addressed in prior work.",https://arxiv.org/pdf/2112.10936v2.pdf,page_2.html
AI-Empowered Persuasive Video Generation: A Survey,"Liu, C., Yu, H.",2021,"Promotional videos are rapidly becoming a popular medium for persuading people to change their behaviours in many settings (e.g., online shopping, social enterprise initiatives). Today, such videos are often produced by professionals, which is a time-, labour- and cost-intensive undertaking. In order to produce such contents to support a large applications (e.g., e-commerce), the field of artificial intelligence (AI)-empowered persuasive video generation (AIPVG) has gained traction in recent years. This field is interdisciplinary in nature, which makes it challenging for new researchers to grasp. Currently, there is no comprehensive survey of AIPVG available. In this paper, we bridge this gap by reviewing key AI techniques that can be utilized to automatically generate persuasive videos. We offer a first-of-its-kind taxonomy which divides AIPVG into three major steps: 1) visual material understanding, which extracts information from the visual materials (VMs) relevant to the target of promotion; 2) visual storyline generation, which shortlists and arranges high-quality VMs into a sequence in order to compose a storyline with persuasive power; and 3) post-production, which involves background music generation and still image animation to enhance viewing experience. We also introduce the evaluation metrics and datasets commonly adopted in the field of AIPVG. We analyze the advantages and disadvantages of the existing works belonging to the above-mentioned steps, and discuss interesting potential future research directions.",https://arxiv.org/pdf/2112.09401v1.pdf,page_2.html
Machine learning for utility prediction in argument-based computational persuasion,"Donadello, I., Hunter, A., Teso, S., Dragoni, M.",2021,"Automated persuasion systems (APS) aim to persuade a user to believe something by entering into a dialogue in which arguments and counterarguments are exchanged. To maximize the probability that an APS is successful in persuading a user, it can identify a global policy that will allow it to select the best arguments it presents at each stage of the dialogue whatever arguments the user presents. However, in real applications, such as for healthcare, it is unlikely the utility of the outcome of the dialogue will be the same, or the exact opposite, for the APS and user. In order to deal with this situation, games in extended form have been harnessed for argumentation in Bi-party Decision Theory. This opens new problems that we address in this paper: (1) How can we use Machine Learning (ML) methods to predict utility functions for different subpopulations of users? and (2) How can we identify for a new user the best utility function from amongst those that we have learned? To this extent, we develop two ML methods, EAI and EDS, that leverage information coming from the users to predict their utilities. EAI is restricted to a fixed amount of information, whereas EDS can choose the information that best detects the subpopulations of a user. We evaluate EAI and EDS in a simulation setting and in a realistic case study concerning healthy eating habits. Results are promising in both cases, but EDS is more effective at predicting useful utility functions.",https://arxiv.org/pdf/2112.04953v1.pdf,page_2.html
Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal Characterization,"Unal, M.E., Kovashka, A., Chung, W.-T., Lin, Y.-R.",2021,"Social media content routinely incorporates multi-modal design to covey information and shape meanings, and sway interpretations toward desirable implications, but the choices and outcomes of using both texts and visual images have not been sufficiently studied. This work proposes a computational approach to analyze the outcome of persuasive information in multi-modal content, focusing on two aspects, popularity and reliability, in COVID-19-related news articles shared on Twitter. The two aspects are intertwined in the spread of misinformation: for example, an unreliable article that aims to misinform has to attain some popularity. This work has several contributions. First, we propose a multi-modal (image and text) approach to effectively identify popularity and reliability of information sources simultaneously. Second, we identify textual and visual elements that are predictive to information popularity and reliability. Third, by modeling cross-modal relations and similarity, we are able to uncover how unreliable articles construct multi-modal meaning in a distorted, biased fashion. Our work demonstrates how to use multi-modal analysis for understanding influential content and has implications to social media literacy and engagement.",https://arxiv.org/pdf/2112.13910v1.pdf,page_2.html
Outlier detection using AI: A survey,"Sikder, M.N.K., Batarseh, F.A.",2021,"An outlier is an event or observation that is defined as an unusual activity, intrusion, or a suspicious data point that lies at an irregular distance from a population. The definition of an outlier event, however, is subjective and depends on the application and the domain (Energy, Health, Wireless Network, etc.). It is important to detect outlier events as carefully as possible to avoid infrastructure failures because anomalous events can cause minor to severe damage to infrastructure. For instance, an attack on a cyber-physical system such as a microgrid may initiate voltage or frequency instability, thereby damaging a smart inverter which involves very expensive repairing. Unusual activities in microgrids can be mechanical faults, behavior changes in the system, human or instrument errors or a malicious attack. Accordingly, and due to its variability, Outlier Detection (OD) is an ever-growing research field. In this chapter, we discuss the progress of OD methods using AI techniques. For that, the fundamental concepts of each OD model are introduced via multiple categories. Broad range of OD methods are categorized into six major categories: Statistical-based, Distance-based, Density-based, Clustering-based, Learning-based, and Ensemble methods. For every category, we discuss recent state-of-the-art approaches, their application areas, and performances. After that, a brief discussion regarding the advantages, disadvantages, and challenges of each technique is provided with recommendations on future research directions. This survey aims to guide the reader to better understand recent progress of OD methods for the assurance of AI.",https://arxiv.org/pdf/2112.00588v1.pdf,page_2.html
Cloud Failure Prediction with Hierarchical Temporal Memory: An Empirical Assessment,"Riganelli, O., Saltarel, P., Tundo, A., Mobilio, M., Mariani, L.",2021,"Hierarchical Temporal Memory (HTM) is an unsupervised learning algorithm inspired by the features of the neocortex that can be used to continuously process stream data and detect anomalies, without requiring a large amount of data for training nor requiring labeled data.HTM is also able to continuously learn from samples, providing a model that is always up-to-date with respect to observations. These characteristics make HTM particularly suitable for supporting online failure prediction in cloud systems, which are systems with a dynamically changing behavior that must be monitored to anticipate problems. This paper presents the first systematic study that assesses HTM in the context of failure prediction. The results that we obtained considering 72 configurations of HTM applied to 12 different types of faults introduced in the Clearwater cloud system show that HTM can help to predict failures with sufficient effectiveness (F-measure = 0.76), representing an interesting practical alternative to (semi-)supervised algorithms.",https://arxiv.org/pdf/2110.03431v2.pdf,page_2.html
Influencing towards stable multi-agent interactions,"Wang, W.Z., Shih, A., Xie, A., Sadigh, D.",2021,"Learning in multi-agent environments is difficult due to the non-stationarity introduced by an opponent’s or partner’s changing behaviors. Instead of reactively adapting to the other agent’s (opponent or partner) behavior, we propose an algorithm to proactively influence the other agent’s strategy to stabilize –which can restrain the non-stationarity caused by the other agent. We learn a low-dimensional latent representation of the other agent’s strategy and the dynamics of how the latent strategy evolves with respect to our robot’s behavior. With this learned dynamics model, we can define an unsupervised stability reward to train our robot to deliberately influence the other agent to stabilize towards a single strategy. We demonstrate the effectiveness of stabilizing in improving efficiency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation. We show qualitative results on our website.",https://arxiv.org/pdf/2110.08229v1.pdf,page_2.html
Investigating health-aware smart-nudging with Machine Learning to help people pursue healthier eating-habits,"Khan, M.A., Muhammad, K., Smyth, B., Coyle, D.",2021,"Food-choices and eating-habits directly contribute to our long-term health. This makes the food recommender system a potential tool to address the global crisis of obesity and malnutrition. Over the past decade, artificial-intelligence and medical researchers became more invested in researching tools that can guide and help people make healthy and thoughtful decisions around food and diet. In many typical (Recommender System) RS domains, smart nudges have been proven effective in shaping users' consumption patterns. In recent years, knowledgeable nudging and incentifying choices started getting attention in the food domain as well. To develop smart nudging for promoting healthier food choices, we combined Machine Learning and RS technology with food-healthiness guidelines from recognized health organizations, such as the World Health Organization, Food Standards Agency, and the National Health Service United Kingdom. In this paper, we discuss our research on, persuasive visualization for making users aware of the healthiness of the recommended recipes. Here, we propose three novel nudging technology, the WHO-BubbleSlider, the FSA-ColorCoading, and the DRCI-MLCP, that encourage users to choose healthier recipes. We also propose a Topic Modeling based portion-size recommendation algorithm. To evaluate our proposed smart-nudges, we conducted an online user study with 96 participants and 92250 recipes. Results showed that, during the food decision-making process, appropriate healthiness cues make users more likely to click, browse, and choose healthier recipes over less healthy ones.MSC Codes 68U35 (Primary), 68T35 (Secondary), 68T50(Secondary)",https://arxiv.org/pdf/2110.07045v1.pdf,page_2.html
MEPG: A Minimalist Ensemble Policy Gradient Framework for Deep Reinforcement Learning,"He, Q., Su, H., Gong, C., Hou, X.",2021,"During the training of a reinforcement learning (RL) agent, the distribution of training data is non-stationary as the agent's behavior changes over time. Therefore, there is a risk that the agent is overspecialized to a particular distribution and its performance suffers in the larger picture. Ensemble RL can mitigate this issue by learning a robust policy. However, it suffers from heavy computational resource consumption due to the newly introduced value and policy functions. In this paper, to avoid the notorious resources consumption issue, we design a novel and simple ensemble deep RL framework that integrates multiple models into a single model. Specifically, we propose the Minimalist Ensemble Policy Gradient framework (MEPG), which introduces minimalist ensemble consistent Bellman update by utilizing a modified dropout operator. MEPG holds ensemble property by keeping the dropout consistency of both sides of the Bellman equation. Additionally, the dropout operator also increases MEPG's generalization capability. Moreover, we theoretically show that the policy evaluation phase in the MEPG maintains two synchronized deep Gaussian Processes. To verify the MEPG framework's ability to generalize, we perform experiments on the gym simulator, which presents that the MEPG framework outperforms or achieves a similar level of performance as the current state-of-the-art ensemble methods and model-free methods without increasing additional computational resource costs.",https://arxiv.org/pdf/2109.10552v2.pdf,page_2.html
SalienTrack: providing salient information for semi-automated feedback in self-tracking with explainable AI,"Wang, Y., Liu, J., Park, H., ...Kay, J., Lim, B.Y.",2021,"Self-tracking can improve people's awareness of their unhealthy behaviors and support reflection to inform behavior change. Increasingly, new technologies make tracking easier, leading to large amounts of tracked data. However, much of that information is not salient for reflection and self-awareness. To tackle this burden for reflection, we created the SalienTrack framework, which aims to 1) identify salient tracking events, 2) select the salient details of those events, 3) explain why they are informative, and 4) present the details as manually elicited or automatically shown feedback. We implemented SalienTrack in the context of nutrition tracking. To do this, we first conducted a field study to collect photo-based mobile food tracking over 1-5 weeks. We then report how we used this data to train an explainable-AI model of salience. Finally, we created interfaces to present salient information and conducted a formative user study to gain insights about how SalienTrack could be integrated into an interface for reflection. Our key contributions are the SalienTrack framework, a demonstration of its implementation for semi-automated feedback in an important and challenging self-tracking context and a discussion of the broader uses of the framework.",https://arxiv.org/pdf/2109.10231v3.pdf,page_2.html
A STORYTELLING ROBOT MANAGING PERSUASIVE AND ETHICAL STANCES VIA ACT-R: AN EXPLORATORY STUDY,"Augello, A., Città, G., Gentile, M., Lieto, A.",2021,"We present a storytelling robot, controlled via the ACT-R cognitive architecture, able to adopt different persuasive techniques and ethical stances while conversing about some topics concerning COVID-19. The main contribution of the paper consists in the proposal of a needs-driven model that guides and evaluates, during the dialogue, the use (if any) of persuasive techniques available in the agent procedural memory. The portfolio of persuasive techniques tested in such a model ranges from the use of storytelling, to framing techniques and rhetorical-based arguments. To the best of our knowledge, this represents the first attempt of building a persuasive agent able to integrate a mix of explicitly grounded cognitive assumptions about dialogue management, storytelling and persuasive techniques as well as ethical attitudes. The paper presents the results of an exploratory evaluation of the system on 63 participants.",https://arxiv.org/pdf/2107.12845v2.pdf,page_2.html
Evaluation of in-person counseling strategies to develop physical activity chatbot for women,"Liang, K.-H., Lange, P., Oh, Y.J., ...Fukuoka, Y., Yu, Z.",2021,"Artificial intelligence chatbots are the vanguard in technology-based intervention to change people's behavior. To develop intervention chatbots, the first step is to understand natural language conversation strategies in human conversation. This work introduces an intervention conversation dataset collected from a real-world physical activity intervention program for women. We designed comprehensive annotation schemes in four dimensions (domain, strategy, social exchange, and taskfocused exchange) and annotated a subset of dialogs. We built a strategy classifier with context information to detect strategies from both trainers and participants based on the annotation. To understand how human intervention induces effective behavior changes, we analyzed the relationships between the intervention strategies and the participants' changes in the barrier and social support for physical activity. We also analyzed how participant's baseline weight correlates to the amount of occurrence of the corresponding strategy. This work lays the foundation for developing a personalized physical activity intervention bot.",https://arxiv.org/pdf/2107.10410v1.pdf,page_2.html
Strategic mitigation of agent inattention in drivers with open-quantum cognition models,"Zhang, Q., Siddhardh Nadendla, V.S., Balakrishnan, S.N., Busemeyer, J.",2021,"State-of-the-art driver-assist systems have failed to effectively mitigate driver inattention and had minimal impacts on the ever-growing number of road mishaps (e.g. life loss, physical injuries due to accidents caused by various factors that lead to driver inattention). This is because traditional human-machine interaction settings are modeled in classical and behavioral game-theoretic domains which are technically appropriate to characterize strategic interaction between either two utility maximizing agents, or human decision makers. Therefore, in an attempt to improve the persuasive effectiveness of driver-assist systems, we develop a novel strategic and personalized driver-assist system which adapts to the driver’s mental state and choice behavior. First, we propose a novel equilibrium notion in human-system interaction games, where the system maximizes its expected utility and human decisions can be characterized using any general decision model. Then we use this novel equilibrium notion to investigate the strategic driver-vehicle interaction game where the car presents a persuasive recommendation to steer the driver towards safer driving decisions. We assume that the driver employs an open-quantum system cognition model, which captures complex aspects of human decision making such as violations to classical law of total probability and incompatibility of certain mental representations of information. We present closed-form expressions for players’ final responses to each other’s strategies so that we can numerically compute both pure and mixed equilibria. Numerical results are presented to illustrate both kinds of equilibria.",https://arxiv.org/pdf/2107.09888v1.pdf,page_2.html
Online handbook of argumentation for AI volume 2,"Castagna, F., Mosca, F., Mumford, J., Sarkadi, S., Xydis, A.",2021,"Collaborative intelligence between humans and intelligent systems relies heavily on the skills of humans and intelligent systems for reaching agreements. This requires complex dialogue processes, which include human reasoning based on common sense and goal-oriented decision-making performed by the intelligent systems, considering the human's dynamic goals and changing beliefs. This project aims to approach these challenges by studying non-monotonic reasoning techniques in the setting of strategic interaction between intelligent systems and humans. To capture the underlying logic of human reasoning, cognitive theories in logical formalizations are explored, e.g., in abstract argumentation or answer set programming. These reasoning architectures will support the decision-making process of rational agents that aim to join a given dialogue-based interaction with humans. With a particular focus on applications of persuasive technology, we see strategic argumentation as a process of decision-making for changing mental states of human agents.",https://arxiv.org/pdf/2106.10832v2.pdf,page_2.html
Multi-receiver online Bayesian persuasion,"Castiglioni, M., Marchesi, A., Celli, A., Gatti, N.",2021,"Bayesian persuasion studies how an informed sender should partially disclose information to influence the behavior of a self-interested receiver. Classical models make the stringent assumption that the sender knows the receiver’s utility. This can be relaxed by considering an online learning framework in which the sender repeatedly faces a receiver of an unknown, adversarially selected type. We study, for the first time, an online Bayesian persuasion setting with multiple receivers. We focus on the case with no externalities and binary actions, as customary in offline models. Our goal is to design no-regret algorithms for the sender with polynomial per-iteration running time. First, we prove a negative result: for any 0 < α ≤ 1, there is no polynomial-time no-α-regret algorithm when the sender’s utility function is supermodular or anonymous. Then, we focus on the case of submodular sender’s utility functions and we show that, in this case, it is possible to design a polynomial-time no-(1-1/e)-regret algorithm. To do so, we introduce a general online gradient descent scheme to handle online learning problems with a finite number of possible loss functions. This requires the existence of an approximate projection oracle. We show that, in our setting, there exists one such projection oracle which can be implemented in polynomial time.",https://arxiv.org/pdf/2106.06480v1.pdf,page_2.html
Towards an intelligent personalized persuasive conversational system for human interaction on divining the future event and assisting by using artificial intelligence,"Sharma, S., Pathak, S., Srivastava, S.",2021,"In the next wave of insurgence, humans may endeavour self-reflection which can lead to an effortless talk and to find out if an event will fructify. Training the system on how to make accurate prognostication with the help of machine learning and statistical models can lead to an intelligent personalized conversational system. The Chatbot industry is ever-growing and after the COVID-19 pandemic and rigorous lockdowns all around the world, people have realized the importance of human interaction in their lives. We are developing this model to create a more intimate relationship between the system and humans. For this purpose, many open-source platforms are available. Artificial Intelligence Markup Language (AIML) is derived from Extensible Markup Language (XML) which is used to build up a conversational agent artificially. The success of this project will help the model in observing and understanding human emotions which will ultimately help it to form a more personalized relationship to delineate the future course of events.",https://dx.doi.org/10.36227/techrxiv.14732055.v1,page_2.html
Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues,"Joshi, R., Balachandran, V., Vashishth, S., Black, A.W., Tsvetkov, Y.",2021,"To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DIALOGRAPH, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DIALOGRAPH explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues.1",https://arxiv.org/pdf/2106.00920v1.pdf,page_2.html
PAL: Intelligence augmentation using egocentric visual context detection,"Khan, M., Maes, P.",2021,"Egocentric visual context detection can support intelligence augmentation applications. We created a wearable system, called PAL, for wearable, personalized, and privacy-preserving egocentric visual context detection. PAL has a wearable device with a camera, heart-rate sensor, on-device deep learning, and audio input/output. PAL also has a mobile/web application for personalized context labeling. We used on-device deep learning models for generic object and face detection, low-shot custom face and context recognition (e.g., activities like brushing teeth), and custom context clustering (e.g., indoor locations). The models had over 80% accuracy in in-the-wild contexts (∼1000 images) and we tested PAL for intelligence augmentation applications like behavior change. We have made PAL is open-source to further support intelligence augmentation using personalized and privacy-preserving egocentric visual contexts.",https://arxiv.org/pdf/2105.10735v1.pdf,page_2.html
Using Geospatial Intelligence to Promote Precautionary Behavior During the COVID-19 Pandemic: Development and Validation of a Customized Messaging Campaign in Saudi Arabia,"Alrige, M., Bitar, H., Meccawy, M., Mullachery, B.",2021,"Background Designing a health promotion campaign is never an easy task, especially during a pandemic of a highly infectious disease such as COVID-19. In Saudi Arabia, many attempts have been made to raise public awareness about COVID-19 infection and precautionary health measures. However, most of the health information delivered through the national dashboard and the COVID-19 awareness campaigns are generic and do not necessarily make the impact needed to be seen on individuals’ behavior. Objective The objective of this study is to build and validate a customized messaging campaign to promote precautionary health behavior in individuals during the COVID-19 pandemic. Customization is realized by utilizing a geospatial artificial intelligence technique called the space-time cube (STC) technique. Methods This research was conducted in two sequential phases. In phase 1, an initial library of 32 messages was developed and validated based on Fogg behavior model for behavior change. In phase 2, we applied STC as a geospatial artificial intelligence technique to create a local map for one city with three different profiles representing the city districts. The model was built using COVID-19 real clinical data. The messages created in phase 1 is to be customized for residents in each city profile separately. Results n phase 1, 32 messages were developed based on resources from the World Health Organization and the Ministry of Health in Saudi Arabia. The enumerated content validity of the messages was established through the utilization of the content validity index (CVI). Thirteen professional health care workers (n=13) evaluated the content of the message libraries in Arabic and English. Thirty-two messages were found to have acceptable content validity (I-CVI=0.87). In phase 2, the geospatial intelligence technique we applied showed three zones for the districts of Jeddah city: one for high infection, another for moderate infection, and the third for low infection. Combining the results from the first and second phases, a customized awareness campaign was shaped for the city of Jeddah. Conclusions This awareness campaign can be utilized by health authorities to raise individuals' awareness about the precautionary measures that should be taken based on the infection level of their district, and hence help in reducing the number of positive cases in the city of Jeddah.",https://www.researchsquare.com/article/rs-471552/v1.pdf,page_2.html
What makes a message persuasive? Identifying adaptations towards persuasiveness in nine exploratory case studies,"Duerr, S., Lange, K.T., Gloor, P.A.",2021,"The ability to persuade others is critical to professional and personal success. However, crafting persuasive messages is demanding and poses various challenges. We conducted nine exploratory case studies to identify adaptations that professional and non-professional writers make in written scenarios to increase their subjective persuasiveness. Furthermore, we identified challenges that those writers faced and identified strategies to resolve them with persuasive natural language generation, i.e., artificial intelligence. Our findings show that humans can achieve high degrees of persuasiveness (more so for professional-level writers), and artificial intelligence can complement them to achieve increased celerity and alignment in the process.",https://arxiv.org/pdf/2104.12454v1.pdf,page_2.html
TOWARD THE PREVENTION OF PRIVACY THREATS: HOW CAN WE PERSUADE OUR SOCIAL NETWORK PLATFORM USERS?,"Ruiz-Dolz, R., Alemany, J., Heras, S., García-Fornes, A.",2021,"Complex decision-making problems such as the privacy policy selection when sharing content in online social networks can significantly benefit from artificial intelligence systems. With the use of Computational Argumentation, it is possible to persuade human users to modify their initial decisions to avoid potential privacy threats and violations. In this paper, we present a study performed over 186 teenage users aimed at analysing their behaviour when we try to persuade them to modify the publication of sensitive content in Online Social Networks (OSN) with different arguments. The results of the study revealed that the personality traits and the social interaction data (e.g., number of comments, friends, and likes) of our participants were significantly correlated with the persuasive power of the arguments. Therefore, these sets of features can be used to model OSN users, and to estimate the persuasive power of different arguments when used in human-computer interactions. The findings presented in this paper are helpful for personalising decision support systems aimed at educating and preventing privacy violations in OSNs using arguments.",https://arxiv.org/pdf/2104.10004v2.pdf,page_2.html
An interactive dashboard for searching and comparing soccer performance scores,"Cintia, P., Pappalardo, L., Mauro, G., Ferragina, P.",2021,"The performance of soccer players is one of most discussed aspects by many actors in the soccer industry: from supporters to journalists, from coaches to talent scouts. Unfortunately, the dashboards available online provide no effective way to compare the evolution of the performance of players or to find players behaving similarly on the field. This paper describes the design of a web dashboard that interacts via APIs with a performance evaluation algorithm and provides graphical tools that allow the user to perform many tasks, such as to search or compare players by age, role or trend of growth in their performance, find similar players based on their pitching behavior, change the algorithm's parameters to obtain customized performance scores. We also describe an example of how a talent scout can interact with the dashboard to find young, promising talents.",https://arxiv.org/pdf/2105.04293v2.pdf,page_2.html
CaSiNo: A corpus of campsite negotiation dialogues for automatic negotiation systems,"Chawla, K., Ramirez, J., Clever, R., ...May, J., Gratch, J.",2021,"Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closeddomain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https:// github.com/kushalchawla/CaSiNo.",https://arxiv.org/pdf/2103.15721v2.pdf,page_2.html
Spirit distillation: Precise real-time semantic segmentation of road scenes with insufficient data,"Wu, Z., Jiang, Y., Cui, C., ...Xue, X., Qi, H.",2021,"Semantic segmentation of road scenes is one of the key technologies for realizing autonomous driving scene perception, and the effectiveness of deep Convolutional Neural Networks(CNNs) for this task has been demonstrated. Stateof-art CNNs for semantic segmentation suffer from excessive computations as well as large-scale training data requirement. Inspired by the ideas of Fine-tuning-based Transfer Learning (FTT) and feature-based knowledge distillation, we propose a new knowledge distillation method for cross-domain knowledge transference and efficient data-insufficient network training, named Spirit Distillation(SD), which allow the student network to mimic the teacher network to extract general features, so that a compact and accurate student network can be trained for real-time semantic segmentation of road scenes. Then, in order to further alleviate the trouble of insufficient data and improve the robustness of the student, an Enhanced Spirit Distillation (ESD) method is proposed, which commits to exploit a more comprehensive general features extraction capability by considering images from both the target and the proximity domains as input. To our knowledge, this paper is a pioneering work on the application of knowledge distillation to few-shot learning. Persuasive experiments conducted on Cityscapes semantic segmentation with the prior knowledge transferred from COCO2017 and KITTI demonstrate that our methods can train a better student network (mIOU and high-precision accuracy boost by 1.4% and 8.2% respectively, with 78.2% segmentation variance) with only 41.8% FLOPs (see Fig. 1).",https://arxiv.org/pdf/2103.13733v2.pdf,page_2.html
Multi-Task Attentive Residual Networks for Argument Mining,"Galassi, A., Lippi, M., Torroni, P.",2021,"We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.",https://arxiv.org/pdf/2102.12227v3.pdf,page_2.html
Disparate impact diminishes consumer trust even for advantaged users,"Draws, T., Szlávik, Z., Timmermans, B., ...Varshney, K.R., Hind, M.",2021,"Systems aiming to aid consumers in their decision-making (e.g., by implementing persuasive techniques) are more likely to be effective when consumers trust them. However, recent research has demonstrated that the machine learning algorithms that often underlie such technology can act unfairly towards specific groups (e.g., by making more favorable predictions for men than for women). An undesired disparate impact resulting from this kind of algorithmic unfairness could diminish consumer trust and thereby undermine the purpose of the system. We studied this effect by conducting a between-subjects user study investigating how (gender-related) disparate impact affected consumer trust in an app designed to improve consumers' financial decision-making. Our results show that disparate impact decreased consumers' trust in the system and made them less likely to use it. Moreover, we find that trust was affected to the same degree across consumer groups (i.e., advantaged and disadvantaged users) despite both of these consumer groups recognizing their respective levels of personal benefit. Our findings highlight the importance of fairness in consumer-oriented artificial intelligence systems.",https://arxiv.org/pdf/2101.12715v3.pdf,page_2.html
Strategic argumentation dialogues for persuasion: Framework and experiments based on modelling the beliefs and concerns of the persuadee,"Hadoux, E., Hunter, A., Polberg, S.",2021,"Persuasion is an important and yet complex aspect of human intelligence. When undertaken through dialogue, the deployment of good arguments, and therefore counterarguments, clearly has a significant effect on the ability to be successful in persuasion. Two key dimensions for determining whether an argument is “good” in a particular dialogue are the degree to which the intended audience believes the argument and counterarguments, and the impact that the argument has on the concerns of the intended audience. In this paper, we present a framework for modelling persuadees in terms of their beliefs and concerns, and for harnessing these models in optimizing the choice of move in persuasion dialogues. Our approach is based on the Monte Carlo Tree Search which allows optimization in real-time. We provide empirical results of a study with human participants showing that our automated persuasion system based on this technology is superior to a baseline system that does not take the beliefs and concerns into account in its strategy.",https://arxiv.org/pdf/2101.11870v1.pdf,page_2.html
Personalization paradox in behavior change apps: Lessons from a social comparison-based personalized app for physical activity,"Zhu, J., Dallal, D.H., Gray, R.C., ...Forman, E.M., Arigo, D.",2021,"Social comparison-based features are widely used in social computing apps. However, most existing apps are not grounded in social comparison theories and do not consider individual differences in social comparison preferences and reactions. This paper is among the first to automatically personalize social comparison targets. In the context of an m-health app for physical activity, we use artificial intelligence (AI) techniques of multi-armed bandits. Results from our user study (n=53) indicate that there is some evidence that motivation can be increased using the AI-based personalization of social comparison. The detected effects achieved small-to-moderate effect sizes, illustrating the real-world implications of the intervention for enhancing motivation and physical activity. In addition to design implications for social comparison features in social apps, this paper identified the personalization paradox, the conflict between user modeling and adaptation, as a key design challenge of personalized applications for behavior change. Additionally, we propose research directions to mitigate this Personalization Paradox.",https://arxiv.org/pdf/2101.10020v2.pdf,page_2.html
RESPER: Computationally modelling resisting strategies in persuasive conversations.,"Dutt, R., Sinha, S., Joshi, R., ...Bao, H., Rosé, C.P.",2021,"Modelling persuasion strategies as predictors of task outcome has several real-world applications and has received considerable attention from the computational linguistics community. However, previous research has failed to account for the resisting strategies employed by an individual to foil such persuasion attempts. Grounded in prior literature in cognitive and social psychology, we propose a generalised framework for identifying resisting strategies in persuasive conversations. We instantiate our framework on two distinct datasets comprising persuasion and negotiation conversations. We also leverage a hierarchical sequence-labelling neural architecture to infer the aforementioned resisting strategies automatically. Our experiments reveal the asymmetry of power roles in non-collaborative goal-directed conversations and the benefits accrued from incorporating resisting strategies on the final conversation outcome. We also investigate the role of different resisting strategies on the conversation outcome and glean insights that corroborate with past findings. We also make the code and the dataset of this work publicly available at https://github.com/americast/ resper.",https://arxiv.org/pdf/2101.10545v1.pdf,page_2.html
"AI, Ethics, and Law: A Possible Way Forward","Davis, J.P.",2021,"This Chapter offers a framework for analyzing the intersection of artificial intelligence (AI), ethics, and law. It does so, as Part 1 explains, by (1) suggesting potential limitations on AI consciousness and identifying the implications of those limitations for ethics and law, and (2) acknowledging three possible philosophical objections to this line of analysis and providing reasons to reject each of them. Part 2 explores the role that consciousness plays in forming objectives, including in making relevant moral and other value judgments. It suggests that as long as AI lacks consciousness we will have difficulty regulating it — our ethics and law often rely on intent in assigning, respectively, moral responsibility and legal liability — and difficulty using it to regulate ourselves — value judgments play an important role in resolving ethical and legal disputes. It also notes that conscious AI is likely to have very different first-person experiences than we do — and hence very different forms of intent and values than we have — giving rise to another set of difficulties for regulating it and for it regulating us. With this sketch of an argument in place, Part 3 addresses three potential philosophical objections to it: that consciousness as a matter of theory cannot have the sort of causal effect on behavior that Part 2 presumes; that consciousness as a matter of empirical fact does not have that sort of causal effect; and that Part 2 relies on a dubious understanding of free will. Part 3 contends that none of these objections is persuasive. The Chapter concludes that the analysis in Part 2 warrants further development.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3711040&type=2,page_2.html
Data Obsolescence Detection in the Light of Newly Acquired Valid Observations,"Chaieb, S., Hnich, B., Mrad, A.B.",2021,"The information describing the conditions of a system or a person is constantly evolving and may become obsolete and contradict other infor-mation. A database, therefore, must be consistently updated upon the acqui-sition of new valid observations that contradict obsolete ones contained in the database. In this paper, we propose a novel causation-based system for dealing with the information obsolescence problem when a causal Bayesian network is our representation model. Our approach is based on studying causal depen-dencies between the network variables to detect, in real-time, contradictions between the observations on a single subject and then identify the obsolete ones. We propose a new approximate concept, ϵ-Contradiction, which repre-sents the confidence level of having a contradiction between some observations relating to a specific subject. Once identified, obsolete observations are given in an original way, in the form of an explanation AND-OR Tree. Our approach can be applied in various domains where the main issue is to detect and explain personalized situations such that the reasons and circumstances underlying unexpected outcomes. Examples include among others: detecting behaviour change by analyzing user profiles, and identifying the causes of some anoma-lies such as bank frauds by analyzing customer interactions. In this paper, we demonstrate the effectiveness of our approach in a real-life medical appli-cation: the elderly fall-prevention and showcase how the resulted explanation AND-OR trees can be used to give reliable recommendations to physicians and assist decision-makers. Our approach runs in a polynomial time and gives systematically and substantially good results.",https://arxiv.org/pdf/2101.07067v3.pdf,page_2.html
Persuasive natural language generation - A literature review,"Duerr, S., Gloor, P.A.",2021,"This literature review focuses on the use of Natural Language Generation (NLG) to automatically detect and generate persuasive texts. Extending previous research on automatic identification of persuasion in text, we concentrate on generative aspects through conceptualizing determinants of persuasion in five business-focused categories: benevolence, linguistic appropriacy, logical argumentation, trustworthiness, tools & datasets. These allow NLG to increase an existing message’s persuasiveness. Previous research illustrates key aspects in each of the above mentioned five categories. A research agenda to further study persuasive NLG is developed. The review includes analysis of seventy-seven articles, outlining the existing body of knowledge and showing the steady progress in this research field.",https://arxiv.org/pdf/2101.05786v1.pdf,page_2.html
Generate Natural Language Explanations for Recommendation,"Chen, H., Shi, S., Chen, X., Zhang, Y.",2021,"Providing personalized explanations for recommendations can help users to understand the underlying insight of the recommendation results, which is helpful to the effectiveness, transparency, persuasiveness and trustworthiness of recommender systems. Current explainable recommendation models mostly generate textual explanations based on pre-defined sentence templates. However, the expressiveness power of template-based explanation sentences are limited to the pre-defined expressions, and manually defining the expressions require significant human efforts. Motivated by this problem, we propose to generate free-text natural language explanations for personalized recommendation. In particular, we propose a hierarchical sequence-to-sequence model (HSS) for personalized explanation generation. Different from conventional sentence generation in NLP research, a great challenge of explanation generation in e-commerce recommendation is that not all sentences in user reviews are of explanation purpose. To solve the problem, we further propose an auto-denoising mechanism based on topical item feature words for sentence generation. Experiments on various e-commerce product domains show that our approach can not only improve the recommendation accuracy, but also the explanation quality in terms of the offline measures and feature words coverage. This research is one of the initial steps to grant intelligent agents with the ability to explain itself based on natural language sentences.",https://arxiv.org/pdf/2101.03392v1.pdf,page_2.html
Theory-based habit modeling for enhancing behavior prediction,"Zhang, C., Vanschoren, J., van Wissen, A., ...de Ruyter, B., IJsselsteijn, W.A.",2021,"Psychological theories of habit posit that when a strong habit is formed through behavioral repetition, it can trigger behavior automatically in the same environment. Given the reciprocal relationship between habit and behavior, changing lifestyle behaviors (e.g., toothbrushing) is largely a task of breaking old habits and creating new and healthy ones. Thus, representing users’ habit strengths can be very useful for behavior change support systems (BCSS), for example, to predict behavior or to decide when an intervention reaches its intended effect. However, habit strength is not directly observable and existing self-report measures are taxing for users. In this paper, built on recent computational models of habit formation, we propose a method to enable intelligent systems to compute habit strength based on observable behavior. The hypothesized advantage of using computed habit strength for behavior prediction was tested using data from two intervention studies, where we trained participants to brush their teeth twice a day for three weeks and monitored their behaviors using accelerometers. Through hierarchical cross-validation, we found that for the task of predicting future brushing behavior, computed habit strength clearly outperformed self-reported habit strength (in both studies) and was also superior to models based on past behavior frequency (in the larger second study). Our findings provide initial support for our theory-based approach of modeling user habits and encourages the use of habit computation to deliver personalized and adaptive interventions.",https://arxiv.org/pdf/2101.01637v1.pdf,page_2.html
Interpersonal distance in VR: Reactions of older adults to the presence of a virtual agent,"Pochwatko, G., Karpowicz, B., Chrzanowska, A., Kopeć, W.",2021,"The rapid development of virtual reality technology has increased its availability and, consequently, increased the number of its possible applications. The interest in the new medium has grown due to the entertainment industry (games, VR experiences and movies). The number of freely available training and therapeutic applications is also increasing. Contrary to popular opinion, new technologies are also adopted by older adults. Creating virtual environments tailored to the needs and capabilities of older adults requires intense research on the behaviour of these participants in the most common situations, towards commonly used elements of the virtual environment, in typical sceneries. Comfortable immersion in a virtual environment is key to achieving the impression of presence. Presence is, in turn, necessary to obtain appropriate training, persuasive and therapeutic effects. A virtual agent (a humanoid representation of an algorithm or artificial intelligence) is often an element of the virtual environment interface. Maintaining an appropriate distance to the agent is, therefore, a key parameter for the creator of the VR experience. Older (65+) participants maintain greater distance towards an agent (a young white male) than younger ones (25-35). It may be caused by differences in the level of arousal, but also cultural norms. As a consequence, VR developers are advised to use algorithms that maintain the agent at the appropriate distance, depending on the user’s age.",https://arxiv.org/pdf/2101.01652v1.pdf,page_2.html
Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration,"Shi, W., Li, Y., Sahay, S., Yu, Z.",2020,"Persuasion dialogue systems reflect the machine’s ability to make strategic moves beyond verbal communication, and therefore differentiate themselves from task-oriented or open-domain dialogue systems and have their own unique values. However, the repetition and inconsistency problems still persist in dialogue response generation and could substantially impact user experience and impede the persuasion outcome. Besides, although reinforcement learning (RL) approaches have achieved big success in strategic tasks such as games, they require a sophisticated user simulator to provide real-time feedback to the dialogue system, which limits the application of RL on persuasion dialogues. To address these issues towards a better persuasion dialogue system, we apply RL to refine a language model baseline without user simulators, and distill sentence-level information about repetition, inconsistency, and task relevance through rewards. Moreover, to better accomplish the persuasion task, the model learns from human demonstration to imitate human persuasion behavior and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback.",https://arxiv.org/pdf/2012.15375v2.pdf,page_2.html
Predicting Decisions in Language Based Persuasion Games,"Apel, R., Erev, I., Reichart, R., Tennenholtz, M.",2020,"Sender-receiver interactions, and specifically persuasion games, are widely researched in economic modeling and artificial intelligence, and serve as a solid foundation for powerful applications. However, in the classic persuasion games setting, the messages sent from the expert to the decision-maker are abstract or well-structured application-specific signals rather than natural (human) language messages, although natural language is a very common communication signal in real-world persuasion setups. This paper addresses the use of natural language in persuasion games, exploring its impact on the decisions made by the players and aiming to construct effective models for the prediction of these decisions. For this purpose, we conduct an online repeated interaction experiment. At each trial of the interaction, an informed expert aims to sell an uninformed decision-maker a vacation in a hotel, by sending her a review that describes the hotel. While the expert is exposed to several scored reviews, the decision-maker observes only the single review sent by the expert, and her payoff in case she chooses to take the hotel is a random draw from the review score distribution available to the expert only. The expert's payoff, in turn, depends on the number of times the decision-maker chooses the hotel. We also compare the behavioral patterns in this experiment to the equivalent patterns in similar experiments where the communication is based on the numerical values of the reviews rather than the reviews' text, and observe substantial differences which can be explained through an equilibrium analysis of the game. We consider a number of modeling approaches for our verbal communication setup, differing from each other in the model type (deep neural network (DNN) vs. linear classifier), the type of features used by the model (textual, behavioral or both) and the source of the textual features (DNN-based vs. hand-crafted). Our results demonstrate that given a prefix of the interaction sequence, our models can predict the future decisions of the decision-maker, particularly when a sequential modeling approach and hand-crafted textual features are applied. Further analysis of the hand-crafted textual features allows us to make initial observations about the aspects of text that drive decision making in our setup.",https://arxiv.org/pdf/2012.09966v5.pdf,page_2.html
Persuading voters in district-based elections,"Castiglioni, M., Gatti, N.",2020,"We focus on the scenario in which an agent can exploit his information advantage to manipulate the outcome of an election. In particular, we study district-based elections with two candidates, in which the winner of the election is the candidate that wins in the majority of the districts. District-based elections are adopted worldwide (e.g., UK and USA) and are a natural extension of widely studied voting mechanisms (e.g., k-voting and plurality voting). We resort to the Bayesian persuasion framework, where the manipulator (sender) strategically discloses information to the voters (receivers) that update their beliefs rationally. We study both private signaling, in which the sender can use a private communication channel per receiver, and public signaling, in which the sender can use a single communication channel for all the receivers. Furthermore, for the first time, we introduce semi-public signaling in which the sender can use a single communication channel per district. We show that there is a sharp distinction between private and (semi-)public signaling. In particular, optimal private signaling schemes can provide an arbitrarily better probability of victory than (semi-)public ones and can be computed efficiently, while optimal (semi-)public signaling schemes cannot be approximated to within any factor in polynomial time unless P = NP. However, we show that reasonable relaxations allow the design of multi-criteria PTASs for optimal (semi-)public signaling schemes. In doing so, we introduce a novel property, namely comparative stability, and we design a bi-criteria PTAS for public signaling in general Bayesian persuasion problems beyond elections when the sender’s utility function is state-dependent.",https://arxiv.org/pdf/2012.05002.pdf,page_2.html
J-recs: principled and scalable recommendation justification,"Park, N., Kan, A., Faloutsos, C., Dong, X.L.",2020,"Online recommendation is an essential functionality across a variety of services, including e-commerce and video streaming, where items to buy, watch, or read are suggested to users. Justifying recommendations, i.e., explaining why a user might like the recommended item, has been shown to improve user satisfaction and persuasiveness of the recommendation. In this paper, we develop a method for generating post-hoc justifications that can be applied to the output of any recommendation algorithm. Existing post-hoc methods are often limited in providing diverse justifications, as they either use only one of many available types of input data, or rely on the predefined templates. We address these limitations of earlier approaches by developing J-RECS, a method for producing concise and diverse justifications. J-RECS is a recommendation model-agnostic method that generates diverse justifications based on various types of product and user data (e.g., purchase history and product attributes). The challenge of jointly processing multiple types of data is addressed by designing a principled graph-based approach for justification generation. In addition to theoretical analysis, we present an extensive evaluation on synthetic and real-world data. Our results show that J-RECS satisfies desirable properties of justifications, and efficiently produces effective justifications, matching user preferences up to 20% more accurately than baselines.",https://arxiv.org/pdf/2011.05928.pdf,page_2.html
Ambiguous Persuasion: An Ex-Ante Formulation,"Cheng, X.",2020,"Consider a persuasion game where both the sender and receiver are ambiguity averse with maxmin expected utility (MEU) preferences and the sender can choose an ambiguous information structure. This paper analyzes the game in an ex-ante formulation: the sender first commits to an information structure, and then the receiver best responds by choosing an ex-ante message-contingent action plan. Under this formulation, I show it is never strictly beneficial for the sender to use an ambiguous information structure as opposed to a standard unambiguous one. This result is robust to (i) both players having (possibly heterogeneous) ambiguous beliefs over the states, and/or (ii) the receiver having non-MEU, uncertainty-averse preferences. However, it is not robust to the sender having non-MEU preferences.",https://arxiv.org/pdf/2010.05376v4.pdf,page_3.html
The emergence of explainability of intelligent systems: Delivering explainable and personalised recommendations for energy efficiency,"Sardianos, C., Varlamis, I., Chronis, C., ...Bensaali, F., Amira, A.",2020,"The recent advances in artificial intelligence namely in machine learning and deep learning, have boosted the performance of intelligent systems in several ways. This gave rise to human expectations, but also created the need for a deeper understanding of how intelligent systems think and decide. The concept of explainability appeared, in the extent of explaining the internal system mechanics in human terms. Recommendation systems are intelligent systems that support human decision making, and as such, they have to be explainable in order to increase user trust and improve the acceptance of recommendations. In this work, we focus on a context-aware recommendation system for energy efficiency and develop a mechanism for explainable and persuasive recommendations, which are personalized to user preferences and habits. The persuasive facts either emphasize on the economical saving prospects (Econ) or on a positive ecological impact (Eco) and explanations provide the reason for recommending an energy saving action. Based on a study conducted using a Telegram bot, different scenarios have been validated with actual data and human feedback. Current results show a total increase of 19% on the recommendation acceptance ratio when both economical and ecological persuasive facts are employed. This revolutionary approach on recommendation systems, demonstrates how intelligent recommendations can effectively encourage energy saving behavior.",https://arxiv.org/pdf/2010.04990.pdf,page_3.html
Autonomous vehicle visual signals for pedestrians: Experiments and design recommendations,"Chen, H., Cohen, R., Dautenhahn, K., Law, E., Czarnecki, K.",2020,"Autonomous Vehicles (AV) will transform transportation, but also the interaction between vehicles and pedestrians. In the absence of a driver, it is not clear how an AV can communicate its intention to pedestrians. One option is to use visual signals. To advance their design, we conduct four human-participant experiments and evaluate six representative AV visual signals for visibility, intuitiveness, persuasiveness, and usability at pedestrian crossings. Based on the results, we distill twelve practical design recommendations for AV visual signals, with focus on signal pattern design and placement. Moreover, the paper advances the methodology for experimental evaluation of visual signals, including lab, closed-course, and public road tests using an autonomous vehicle. In addition, the paper also reports insights on pedestrian crosswalk behaviours and the impacts of pedestrian trust towards AVs on the behaviors. We hope that this work will constitute valuable input to the ongoing development of international standards for AV lamps, and thus help mature automated driving in general.",https://arxiv.org/pdf/2010.05115.pdf,page_3.html
Examining the ordering of rhetorical strategies in persuasive requests,"Shaikh, O., Chen, J., Saad-Falcon, J., Chau, D.H., Yang, D.",2020,"Interpreting how persuasive language influences audiences has implications across many domains like advertising, argumentation, and propaganda. Persuasion relies on more than a message’s content. Arranging the order of the message itself (i.e., ordering specific rhetorical strategies) also plays an important role. To examine how strategy orderings contribute to persuasiveness, we first utilize a Variational Autoencoder model to disentangle content and rhetorical strategies in textual requests from a large-scale loan request corpus. We then visualize interplay between content and strategy through an attentional LSTM that predicts the success of textual requests. We find that specific (orderings of) strategies interact uniquely with a request’s content to impact success rate, and thus the persuasiveness of a request.",https://arxiv.org/pdf/2010.04625.pdf,page_3.html
Persuasion meets AI: Ethical considerations for the design of social engineering countermeasures,"Díaz Ferreyra, N.E., Aïmeur, E., Hage, H., Heisel, M., van Hoogstraten, C.G.",2020,"Privacy in Social Network Sites (SNSs) like Facebook or Instagram is closely related to people’s self-disclosure decisions and their ability to foresee the consequences of sharing personal information with large and diverse audiences. Nonetheless, online privacy decisions are often based on spurious risk judgements that make people liable to reveal sensitive data to untrusted recipients and become victims of social engineering attacks. Artificial Intelligence (AI) in combination with persuasive mechanisms like nudging is a promising approach for promoting preventative privacy behaviour among the users of SNSs. Nevertheless, combining behavioural interventions with high levels of personalization can be a potential threat to people’s agency and autonomy even when applied to the design of social engineering countermeasures. This paper elaborates on the ethical challenges that nudging mechanisms can introduce to the development of AI-based countermeasures, particularly to those addressing unsafe self-disclosure practices in SNSs. Overall, it endorses the elaboration of personalized risk awareness solutions as i) an ethical approach to counteract social engineering, and ii) as an effective means for promoting reflective privacy decisions.",https://arxiv.org/pdf/2009.12853.pdf,page_3.html
Deconstructing legal text: Object-oriented design in legal adjudication,"Ma, M., Podkopaev, D., Campbell-Cousins, A., Nicholas, A.",2020,"Rules are pervasive in the law. In the context of computer engineering, the translation of legal text to algorithmic form is seemingly direct. In large part, law may be a ripe field for expert systems and machine learning. For engineers, existing law appears formulaic and logically reducible to ‘if, then’ statements. The underlying assumption is that the legal language is both self-referential and universal. Moreover, description is considered distinct from interpretation; that in describing the law, the language is seen as quantitative and objectifiable. Nevertheless, is descriptive formal language purely dissociative? From the logic machine of the 1970s to the modern fervor for artificial intelligence (AI), governance by numbers is making a persuasive return. Could translation be possible? The project follows a fundamentally semantic conundrum: what is the significance of ‘meaning’ in legal language? The project, therefore, tests translation by deconstructing sentences from existing legal judgments to their constituent factors. Definitions are then extracted in accordance with the interpretations of the judges. The intent is to build an expert system predicated on alleged rules of legal reasoning. The authors apply both linguistic modelling and natural language processing technology to parse the legal judgments. The project extends beyond prior research in the area, combining a broadly statistical model of context with the relative precision of syntactic structure. The preliminary hypothesis is that, by analyzing the components of legal language with a variety of techniques, we can begin to translate law to numerical form.",https://arxiv.org/pdf/2009.06054.pdf,page_3.html
Why should I not follow you? Reasons for and Reasons against in Responsible Recommender Systems,"Polleti, G.P., de Souza, D.L., Cozman, F.G.",2020,"A few Recommender Systems (RS) resort to explanations so as to enhance trust in recommendations. However, current techniques for explanation generation tend to strongly uphold the recommended products instead of presenting both reasons for and reasons against them. We argue that an RS can better enhance overall trust and transparency by frankly displaying both kinds of reasons to users. We have developed such an RS by exploiting knowledge graphs and by applying Snedegar’s theory of practical reasoning. We show that our implemented RS has excellent performance and we report on an experiment with human subjects that shows the value of presenting both reasons for and against, with significant improvements in trust, engagement, and persuasion.MSC Codes 68T01",https://arxiv.org/pdf/2009.01953.pdf,page_3.html
A review of AI and data science support for cancer management,"Parimbelli, E., Wilk, S., Cornet, R., ...Ottaviano, M., Peleg, M.",2020,"Introduction - Thanks to improvement of care, cancer has become a chronic condition. But due to the toxicity of treatment, the importance of supporting the quality of life (QoL) of cancer patients increases. Monitoring and managing QoL relies on data collected by the patient in his/her home environment, its integration, and its analysis, which supports personalization of cancer management recommendations. We review the state-of-the-art of computerized systems that employ AI and Data Science methods to monitor the health status and provide support to cancer patients managed at home. Objective - Our main objective is to analyze the literature to identify open research challenges that a novel decision support system for cancer patients and clinicians will need to address, point to potential solutions, and provide a list of established best-practices to adopt. Methods - We designed a review study, in compliance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, analyzing studies retrieved from PubMed related to monitoring cancer patients in their home environments via sensors and self-reporting: what data is collected, what are the techniques used to collect data, semantically integrate it, infer the patient's state from it and deliver coaching/behavior change interventions. Results - Starting from an initial corpus of 819 unique articles, a total of 180 papers were considered in the full-text analysis and 109 were finally included in the review. Our findings are organized and presented in four main sub-topics consisting of data collection, data integration, predictive modeling and patient coaching. Conclusion - Development of modern decision support systems for cancer needs to utilize best practices like the use of validated electronic questionnaires for quality-of-life assessment, adoption of appropriate information modeling standards supplemented by terminologies/ontologies, adherence to FAIR data principles, external validation, stratification of patients in subgroups for better predictive modeling, and adoption of formal behavior change theories. Open research challenges include supporting emotional and social dimensions of wellbeing, including PROs in predictive modeling, and providing better customization of behavioral interventions for the specific population of cancer patients.",https://www.medrxiv.org/content/10.1101/2020.08.07.20170191v3.full.pdf,page_3.html
Imitation learning for autonomous trajectory learning of robot arms in space,"Babu, R.B.A.S., Hao, Z., Montanaro, U., Neumann, G.",2020,"This work adds on to the on-going efforts to provide more autonomy to space robots. Here the concept of programming by demonstration or imitation learning is used for trajectory planning of manipulators mounted on small spacecraft. For greater autonomy in future space missions and minimal human intervention through ground control, a robot arm having 7-Degrees of Freedom (DoF) is envisaged for carrying out multiple tasks like debris removal, on-orbit servicing and assembly. Since actual hardware implementation of microgravity environment is extremely expensive, the demonstration data for trajectory learning is generated using a model predictive controller (MPC) in a physics based simulator. The data is then encoded compactly by Probabilistic Movement Primitives (ProMPs). This offline trajectory learning allows faster reproductions and also avoids any computationally expensive optimizations after deployment in a space environment. It is shown that the probabilistic distribution can be used to generate trajectories to previously unseen situations by conditioning the distribution. The motion of the robot (or manipulator) arm induces reaction forces on the spacecraft and hence its attitude changes prompting the Attitude Determination and Control System (ADCS) to take large corrective action that drains energy out of the system. By having a robot arm with redundant DoF helps in finding several possible trajectories from the same start to the same target. This allows the ProMP trajectory generator to sample out the trajectory which is obstacle free as well as having minimal attitudinal disturbances thereby reducing the load on ADCS.",https://arxiv.org/pdf/2008.04007.pdf,page_3.html
"I Choose for Myself, Therefore I Am: The Contours of Existentialist Welfare Economics","Dold, M., Stanton, A.",2020,"Behavioral economics and existentialism both have persuasive, informative perspectives on human choice and welfare. We argue in this paper that the weaknesses in each of their positions are answered by the strengths in the other, creating a more comprehensive vision of what human choice is and how we should engage with the modern world. The dialogue between existentialism and behavioral economics can help us understand why we should protect our choices as our own and how doing so may be more difficult than we anticipate. While individuals are internally and situationally constrained, they can always endeavor to treat themselves as free. Acknowledging this tension in the form of a theoretical synthesis – which we propose to call Existentialist Welfare Economics – can help navigate the threat of identity-shaping social and technological developments, such as nudges or artificial intelligence.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3609571&type=2,page_3.html
Explaining Reputation Assessments,"Nunes, I., Taylor, P., Barakat, L., Griffiths, N., Miles, S.",2020,"Reputation is crucial to enabling human or software agents to select among alternative providers. Although several effective reputation assessment methods exist, they typically distil reputation into a numerical representation, with no accompanying explanation of the rationale behind the assessment. Such explanations would allow users or clients to make a richer assessment of providers, and tailor selection according to their preferences and current context. In this paper, we propose an approach to explain the rationale behind assessments from quantitative reputation models, by generating arguments that are combined to form explanations. Our approach adapts, extends and combines existing approaches for explaining decisions made using multi-attribute decision models in the context of reputation. We present example argument templates, and describe how to select their parameters using explanation algorithms. Our proposal was evaluated by means of a user study, which followed an existing protocol. Our results give evidence that although explanations present a subset of the information of trust scores, they are sufficient to equally evaluate providers recommended based on their trust score. Moreover, when explanation arguments reveal implicit model information, they are less persuasive than scores.",https://arxiv.org/pdf/2006.08818.pdf,page_3.html
Beware the evolving 'intelligent' web service! An integration architecture tactic to guard ai-first components,"Cummaudo, A., Barnett, S., Vasa, R., Grundy, J., Abdelrazek, M.",2020,"Intelligent services provide the power of AI to developers via simple RESTful API endpoints, abstracting away many complexities of machine learning. However, most of these intelligent services-such as computer vision-continually learn with time. When the internals within the abstracted 'black box' become hidden and evolve, pitfalls emerge in the robustness of applications that depend on these evolving services. Without adapting the way developers plan and construct projects reliant on intelligent services, significant gaps and risks result in both project planning and development. Therefore, how can software engineers best mitigate software evolution risk moving forward, thereby ensuring that their own applications maintain quality? Our proposal is an architectural tactic designed to improve intelligent service-dependent software robustness. The tactic involves creating an application-specific benchmark dataset baselined against an intelligent service, enabling evolutionary behaviour changes to be mitigated. A technical evaluation of our implementation of this architecture demonstrates how the tactic can identify 1,054 cases of substantial confidence evolution and 2,461 cases of substantial changes to response label sets using a dataset consisting of 331 images that evolve when sent to a service.",https://arxiv.org/pdf/2005.13186.pdf,page_3.html
AMPERSAND: Argument mining for PERSuAsive oNline discussions,"Chakrabarty, T., Hidey, C., Muresan, S., Mckeown, K., Hwang, A.",2020,"Argumentation is a type of discourse where speakers try to persuade their audience about the reasonableness of a claim by presenting supportive arguments. Most work in argument mining has focused on modeling arguments in monologues. We propose a computational model for argument mining in online persuasive discussion forums that brings together the micro-level (argument as product) and macrolevel (argument as process) models of argumentation. Fundamentally, this approach relies on identifying relations between components of arguments in a discussion thread. Our approach for relation prediction uses contextual information in terms of fine-Tuning a pretrained language model and leveraging discourse relations based on Rhetorical Structure Theory. We additionally propose a candidate selection method to automatically predict what parts of one's argument will be targeted by other participants in the discussion. Our models obtain significant improvements compared to recent state-of-The-Art approaches using pointer networks and a pre-Trained language model.",https://arxiv.org/pdf/2004.14677.pdf,page_3.html
FitChat: Conversational artificial intelligence interventions for encouraging physical activity in older adults,"Wiratunga, N., Cooper, K., Wijekoon, A., ...Reiter, E., Martin, K.",2020,"Delivery of digital behaviour change interventions which encourage physical activity has been tried in many forms. Most often interventions are delivered as text notifications, but these do not promote interaction. Advances in conversational AI have improved natural language understanding and generation, allowing AI chatbots to provide an engaging experience with the user. For this reason, chatbots have recently been seen in healthcare delivering digital interventions through free text or choice selection. In this work, we explore the use of voice based AI chatbots as a novel mode of intervention delivery, specifically targeting older adults to encourage physical activity. We co-created “FitChat”, an AI chatbot, with older adults and we evaluate the first prototype using Think Aloud Sessions. Our thematic evaluation suggests that older adults prefer voice based chat over text notifications or free text entry and that voice is a powerful mode for encouraging motivation.",https://arxiv.org/pdf/2004.14067.pdf,page_3.html
Ex Machina: Technological Disruption and the Future of Artificial Intelligence in Legal Writing,"Campbell, J.",2020,"Technology is disrupting the practice of law and revolutionizing how lawyers work. This revolution is made more powerful because it is increasingly coupled with a rigorous and scientific approach to the law. In some ways law is looking more like a Silicon Valley startup and less like the oak-paneled law firms of the last 200 years. As law, technology, and science merge, the implications for the profession are wide sweeping. This article explores changes coming to persuasive legal writing, offering new thoughts on what the future will hold. Specifically, this article pilots a method for applying technology and science to measure, analyze, and improve persuasive legal writing, offering it as a proof of concept that anchors the article’s broader, and perhaps more controversial assertion. Namely, more powerful and refined persuasive legal writing software tools, fueled by artificial intelligence, should and will disrupt and reshape significant portions of the legal space, including how legal writing is taught and how it is produced. The effect will be to view legal writing as more science, and less art. The next set of luminaries won’t rely on anecdote or intuition to teach or create legal writing; they will rely on software and data.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3544233&type=2,page_3.html
Public bayesian persuasion: Being almost optimal and almost persuasive,"Castiglioni, M., Celli, A., Gatti, N.",2020,"Persuasion studies how an informed principal may influence the behavior of agents by the strategic provision of payoff-relevant information. We focus on the fundamental multi-receiver model by Arieli and Babichenko (2019), in which there are no inter-agent externalities. Unlike prior works on this problem, we study the public persuasion problem in the general setting with: (i) arbitrary state spaces; (ii) arbitrary action spaces; (iii) arbitrary sender's utility functions. We fully characterize the computational complexity of computing a bi-criteria approximation of an optimal public signaling scheme. In particular, we show, in a voting setting of independent interest, that solving this problem requires at least a quasi-polynomial number of steps even in settings with a binary action space, assuming the Exponential Time Hypothesis. In doing so, we prove that a relaxed version of the MAXIMUM FEASIBLE SUBSYSTEM OF LINEAR INEQUALITIES problem requires at least quasi-polynomial time to be solved. Finally, we close the gap by providing a quasi-polynomial time bi-criteria approximation algorithm for arbitrary public persuasion problems that, in specific settings, yields a QPTAS.",https://arxiv.org/pdf/2002.05156.pdf,page_3.html
Signaling in Bayesian network congestion games: The subtle power of symmetry,"Castiglioni, M., Marchesi, A., Celli, A., Gatti, N.",2020,"Network congestion games are a well-understood model of multi-agent strategic interactions. Despite their ubiquitous applications, it is not clear whether it is possible to design information structures to ameliorate the overall experience of the network users. We focus on Bayesian games with atomic players, where network vagaries are modeled via a (random) state of nature which determines the costs incurred by the players. A third-party entity-the sender-can observe the realized state of the network and exploit this additional information to send a signal to each player. A natural question is the following: is it possible for an informed sender to reduce the overall social cost via the strategic provision of information to players who update their beliefs rationally? The paper focuses on the problem of computing optimal ex ante persuasive signaling schemes, showing that symmetry is a crucial property for its solution. Indeed, we show that an optimal ex ante persuasive signaling scheme can be computed in polynomial time when players are symmetric and have affine cost functions. Moreover, the problem becomes NP-hard when players are asymmetric, even in non-Bayesian settings.",https://arxiv.org/pdf/2002.05190.pdf,page_3.html
Automated lane change strategy using proximal policy optimization-based deep reinforcement learning,"Ye, F., Cheng, X., Wang, P., Chan, C.-Y.",2020,"Lane-change maneuvers are commonly executed by drivers to follow a certain routing plan, overtake a slower vehicle, adapt to a merging lane ahead, etc. However, improper lane change behaviors can be a major cause of traffic flow disruptions and even crashes. While many rule-based methods have been proposed to solve lane change problems for autonomous driving, they tend to exhibit limited performance due to the uncertainty and complexity of the driving environment. Machine learning-based methods offer an alternative approach, as Deep reinforcement learning (DRL) has shown promising success in many application domains including robotic manipulation, navigation, and playing video games. However, applying DRL for autonomous driving still faces many practical challenges in terms of slow learning rates, sample inefficiency, and non-stationary trajectories. In this study, we propose an automated lane change strategy using proximal policy optimization-based deep reinforcement learning, which shows great advantage in learning efficiency while maintaining stable performance. The trained agent is able to learn a smooth, safe, and efficient driving policy to determine lane-change decisions (i.e. when and how) even in dense traffic scenarios. The effectiveness of the proposed policy is validated using task success rate and collision rate, which demonstrates the lane change maneuvers can be efficiently learned and executed in a safe, smooth and efficient manner.",https://arxiv.org/pdf/2002.02667.pdf,page_3.html
Numerical abstract persuasion argumentation for expressing concurrent multi-agent negotiations,"Arisaka, R., Ito, T.",2020,"A negotiation process by 2 agents ℓ1and ℓ2can be interleaved by another negotiation process between, say, ℓ1and ℓ3. The interleaving may alter the resource allocation assumed at the inception of the first negotiation process. Existing proposals for argumentation-based negotiations have focused primarily on two-agent bilateral negotiations, but scarcely on the concurrency of multi-agent negotiations. To fill the gap, we present a novel argumentation theory, basing its development on abstract persuasion argumentation (which is an abstract argumentation formalism with a dynamic relation). Incorporating into it numerical information and a mechanism of handshakes among members of the dynamic relation, we show that the extended theory adapts well to concurrent multi-agent negotiations over scarce resources.",https://arxiv.org/pdf/2001.08335.pdf,page_3.html
Hybrid Deep Embedding for Recommendations with Dynamic Aspect-Level Explanations,"Luo, H., Yang, N., Yu, P.S.",2020,"—Explainable recommendation is far from being well solved partly due to three challenges. The first is the personalization of preference learning, which requires that different items/users have different contributions to the learning of user preference or item quality. The second one is dynamic explanation, which is crucial for the timeliness of recommendation explanations. The last one is the granularity of explanations. In practice, aspect-level explanations are more persuasive than item-level or user-level ones. In this paper, to address these challenges simultaneously, we propose a novel model called Hybrid Deep Embedding (HDE) for aspect-based explainable recommendations, which can make recommendations with dynamic aspect-level explanations. The main idea of HDE is to learn the dynamic embeddings of users and items for rating prediction and the dynamic latent aspect preference/quality vectors for the generation of aspect-level explanations, through fusion of the dynamic implicit feedbacks extracted from reviews and the attentive user-item interactions. Particularly, as the aspect preference/quality of users/items is learned automatically, HDE is able to capture the impact of aspects that are not mentioned in reviews of a user or an item. The extensive experiments conducted on real datasets verify the recommending performance and explainability of HDE. The source code of our work is available at https://github.com/lola63/HDE-Python.",https://arxiv.org/pdf/2001.10341.pdf,page_3.html
Effects of persuasive dialogues: Testing bot identities and inquiry strategies,"Shi, W., Wang, X., Oh, Y.J., ...Sahay, S., Yu, Z.",2020,"Intelligent conversational agents, or chatbots, can take on various identities and are increasingly engaging in more human-centered conversations with persuasive goals. However, little is known about how identities and inquiry strategies influence the conversation’s effectiveness. We conducted an online study involving 790 participants to be persuaded by a chatbot for charity donation. We designed a two by four factorial experiment (two chatbot identities and four inquiry strategies) where participants were randomly assigned to different conditions. Findings showed that the perceived identity of the chatbot had significant effects on the persuasion outcome (i.e., donation) and interpersonal perceptions (i.e., competence, confidence, warmth, and sincerity). Further, we identified interaction effects among perceived identities and inquiry strategies. We discuss the findings for theoretical and practical implications for developing ethical and effective persuasive chatbots. Our published data, codes, and analyses serve as the first step towards building competent ethical persuasive chatbots.",https://arxiv.org/pdf/2001.04564.pdf,page_3.html
An Unsupervised Domain-Independent Framework for Automated Detection of Persuasion Tactics in Text,"Iyer, R.R., Sycara, K.",2019,"With the increasing growth of social media, people have started relying heavily on the information shared therein to form opinions and make decisions. While such a reliance is motivation for a variety of parties to promote information, it also makes people vulnerable to exploitation by slander, misinformation, terroristic and predatorial advances. In this work, we aim to understand and detect such attempts at persuasion. Existing works on detecting persuasion in text make use of lexical features for detecting persuasive tactics, without taking advantage of the possible structures inherent in the tactics used. We formulate the task as a multi-class classification problem and propose an unsupervised, domain-independent machine learning framework for detecting the type of persuasion used in text, which exploits the inherent sentence structure present in the different persuasion tactics. Our work shows promising results as compared to existing work.",https://arxiv.org/pdf/1912.06745.pdf,page_3.html
End-to-end trainable non-collaborative dialog system,"Li, Y., Qian, K., Shi, W., Yu, Z.",2019,"End-to-end task-oriented dialog models have achieved promising performance on collaborative tasks where users willingly coordinate with the system to complete a given task. While in non-collaborative settings, for example, negotiation and persuasion, users and systems do not share a common goal. As a result, compared to collaborate tasks, people use social content to build rapport and trust in these non-collaborative settings in order to advance their goals. To handle social content, we introduce a hierarchical intent annotation scheme, which can be generalized to different non-collaborative dialog tasks. Building upon TransferTransfo (Wolf et al. 2019), we propose an end-to-end neural network model to generate diverse coherent responses. Our model utilizes intent and semantic slots as the intermediate sentence representation to guide the generation process. In addition, we design a filter to select appropriate responses based on whether these intermediate representations fit the designed task and conversation constraints. Our non-collaborative dialog model guides users to complete the task while simultaneously keeps them engaged. We test our approach on our newly proposed ANTISCAM dataset and an existing PERSUASIONFORGOOD dataset. Both automatic and human evaluations suggest that our model outperforms multiple baselines in these two non-collaborative tasks.",https://arxiv.org/pdf/1911.10742.pdf,page_3.html
(When) is truth-telling favored in AI debate?,"Kovařík, V., Carey, R.",2019,"For some problems, it is difficult for humans to judge the goodness of AI-proposed solutions. Irving, Christiano, and Amodei (2018) propose that in such cases, we may use a debate between two AI systems to assist the human judge to select a good answer. We introduce a mathematical framework for modelling this type of debate and propose that the quality of debate designs may be measured by the accuracy of the most persuasive answer. We describe a simple instance of the debate framework called feature debate and analyze the degree to which such debates track the truth. We argue that despite being very simple, feature debates capture many aspects of practical debates such as the incentives to confuse the judge or stall to prevent losing. We analyze two special types of debates, those where arguments constitute independent evidence about the topic, and those where the information bandwidth of the judge is limited.",https://arxiv.org/pdf/1911.04266v3.pdf,page_3.html
"What gets echoed? understanding the ""pointers"" in explanations of persuasive arguments","Atkinson, D., Srinivasan, K.B., Tan, C.",2019,"Explanations are central to everyday life, and are a topic of growing interest in the AI community. To investigate the process of providing natural language explanations, we leverage the dynamics of the/r/ChangeMyView subreddit to build a dataset with 36K naturally occurring explanations of why an argument is persuasive. We propose a novel word-level prediction task to investigate how explanations selectively reuse, or echo, information from what is being explained (henceforth, explanandum). We develop features to capture the properties of a word in the explanandum, and show that our proposed features not only have relatively strong predictive power on the echoing of a word in an explanation, but also enhance neural methods of generating explanations. In particular, while the non-contextual properties of a word itself are more valuable for stopwords, the interaction between the constituent parts of an explanandum is crucial in predicting the echoing of content words. We also find intriguing patterns of a word being echoed. For example, although nouns are generally less likely to be echoed, subjects and objects can, depending on their source, be more likely to be echoed in the explanations.",https://arxiv.org/pdf/1911.00523.pdf,page_3.html
Multiple criteria decision-making for lane-change model,"Li, A., Sun, L., Zhan, W., Tomizuka, M.",2019,"Simulation has long been an essential part of testing autonomous driving systems, but only recently has simulation been useful for building and training self-driving vehicles. Vehicle behavioural models are necessary to simulate the interactions between robot cars. This paper proposed a new method to formalize the lane-changing model in urban driving scenarios. We define human incentives from different perspectives, speed incentive, route change incentive, comfort incentive and courtesy incentive etc. We applied a decision-theoretical tool, called Multi-Criteria Decision Making (MCDM) to take these incentive policies into account. The strategy of combination is according to different driving style which varies for each driving. Thus a lane-changing decision selection algorithm is proposed. Not only our method allow to vary the motivation of lane-changing from purely egoistic desire to a more courtesy concern, but also they can mimic drivers' state, inattentive or concentrate, which influences their driving Behaviour. We define some cost functions and calibrate the parameters with different scenarios traffic data. Distinguishing driving styles are used to aggregate decision-makers' assessments about various criteria weightings to obtain the action drivers desire most. Our result demonstrates the proposed method can produce varied lane-changing behaviour. Unlike other lane-changing models based on artificial intelligence methods, our model has a more flexible controllability.",https://arxiv.org/pdf/1910.10142.pdf,page_3.html
Exploring the role of common model of cognition in designing adaptive coaching interactions for health behavior change,"Mohan, S.",2019,"Our research aims to develop intelligent collaborative agents that are human-aware - they can model, learn, and reason about their human partner's physiological, cognitive, and affective states. In this paper, we study how adaptive coaching interactions can be designed to help people develop sustainable healthy behaviors. We leverage the common model of cognition - CMC [31] - as a framework for unifying several behavior change theories that are known to be useful in human-human coaching. We motivate a set of interactive system desiderata based on the CMC-based view of behavior change. Then, we propose PARCoach - an interactive system that addresses the desiderata. PARCoach helps a trainee pick a relevant health goal, set an implementation intention, and track their behavior. During this process, the trainee identifies a specific goal-directed behavior as well as the situational context in which they will perform it. PARCcoach uses this information to send notifications to the trainee, reminding them of their chosen behavior and the context. We report the results from a 4-week deployment with 60 participants. Our results support the CMC-based view of behavior change and demonstrate that the desiderata for proposed interactive system design is useful in producing behavior change.",https://arxiv.org/pdf/1910.07728v3.pdf,page_3.html
Autonomous driving using safe reinforcement learning by incorporating a regret-based human lane-changing decision model,"Chen, D., Jiang, L., Wang, Y., Li, Z.",2019,"It is expected that many human drivers will still prefer to drive themselves even if the self-driving technologies are ready. Therefore, human-driven vehicles and autonomous vehicles (AVs) will coexist in a mixed traffic for a long time. To enable AVs to safely and efficiently maneuver in this mixed traffic, it is critical that the AVs can understand how humans cope with risks and make driving-related decisions. On the other hand, the driving environment is highly dynamic and ever-changing, and it is thus difficult to enumerate all the scenarios and hard-code the controllers. To face up these challenges, in this work, we incorporate a human decision-making model in reinforcement learning to control AVs for safe and efficient operations. Specifically, we adapt regret theory to describe a human driver’s lane-changing behavior, and fit the personalized models to individual drivers for predicting their lane-changing decisions. The predicted decisions are incorporated in the safety constraints for reinforcement learning in training and in implementation. We then use an extended version of double deep Q-network (DDQN) to train our AV controller within the safety set. By doing so, the amount of collisions in training is reduced to zero, while the training accuracy is not impinged.",https://arxiv.org/pdf/1910.04803.pdf,page_3.html
Alternating recurrent dialog model with large-scale pre-trained language models,"Wu, Q., Zhang, Y., Li, Y., Yu, Z.",2019,"Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models has suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Recurrent Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of large pre-trained language models. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with the state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In the PersuasionForGood task, ARDM is capable of generating human-like responses to persuade people to donate to a charity.1",https://arxiv.org/pdf/1910.03756v3.pdf,page_3.html
Soft policy gradient method for maximum entropy deep reinforcement learning,"Shi, W., Song, S., Wu, C.",2019,"Maximum entropy deep reinforcement learning (RL) methods have been demonstrated on a range of challenging continuous tasks. However, existing methods either suffer from severe instability when training on large off-policy data or cannot scale to tasks with very high state and action dimensionality such as 3D humanoid locomotion. Besides, the optimality of desired Boltzmann policy set for non-optimal soft value function is not persuasive enough. In this paper, we first derive soft policy gradient based on entropy regularized expected reward objective for RL with continuous actions. Then, we present an off-policy actor-critic, model-free maximum entropy deep RL algorithm called deep soft policy gradient (DSPG) by combining soft policy gradient with soft Bellman equation. To ensure stable learning while eliminating the need of two separate critics for soft value functions, we leverage double sampling approach to making the soft Bellman equation tractable. The experimental results demonstrate that our method outperforms in performance over off-policy prior methods.",https://arxiv.org/pdf/1909.03198.pdf,page_3.html
Generating persuasive visual storylines for promotional videos,"Liu, C., Dong, Y., Yu, H., ...Cui, L., Miao, C.",2019,"Video contents have become a critical tool for promoting products in E-commerce. However, the lack of automatic promotional video generation solutions makes large-scale video-based promotion campaigns infeasible. The first step of automatically producing promotional videos is to generate visual storylines, which is to select the building block footage and place them in an appropriate order. This task is related to the subjective viewing experience. It is hitherto performed by human experts and thus, hard to scale. To address this problem, we propose WundtBackpack, an algorithmic approach to generate storylines based on available visual materials, which can be video clips or images. It consists of two main parts, 1) the Learnable Wundt Curve to evaluate the perceived persuasiveness based on the stimulus intensity of a sequence of visual materials, which only requires a small volume of data to train; and 2) a clustering-based backpacking algorithm to generate persuasive sequences of visual materials while considering video length constraints. In this way, the proposed approach provides a dynamic structure to empower artificial intelligence (AI) to organize video footage in order to construct a sequence of visual stimuli with persuasive power. Extensive real-world experiments show that our approach achieves close to 10% higher perceived persuasiveness scores by human testers, and 12.5% higher expected revenue compared to the best performing state-of-the-art approach.",https://arxiv.org/pdf/1908.11588v1.pdf,page_3.html
Unsupervised behavior change detection in multidimensional data streams for maritime traffic monitoring,"Petry, L.M., Bogorny, V., Soares, A., Matwin, S.",2019,"The worldwide growth of maritime traffic and the development of the Automatic Identification System (AIS) has led to advances in monitoring systems for preventing vessel accidents and detecting illegal activities. In this work, we describe research gaps and challenges in machine learning for vessel behavior change and event detection, considering several constraints imposed by real-time data streams and the maritime monitoring domain. As a starting point, we investigate how unsupervised and semi-supervised change detection methods may be employed for identifying shifts in vessel behavior, aiming to detect and label unusual events.",https://arxiv.org/pdf/1908.05103.pdf,page_3.html
Bayesian persuasion with sequential games,"Celli, A., Coniglio, S., Gatti, N.",2019,"We study an information-structure design problem (a.k.a. persuasion) with a single sender and multiple receivers with actions of a priori unknown types, independently drawn from action-specific marginal distributions. As in the standard Bayesian persuasion model, the sender has access to additional information regarding the action types, which she can exploit when committing to a (noisy) signaling scheme through which she sends a private signal to each receiver. The novelty of our model is in considering the case where the receivers interact in a sequential game with imperfect information, with utilities depending on the game outcome and the realized action types. After formalizing the notions of ex ante and ex interim persuasiveness (which differ in the time at which the receivers commit to following the sender's signaling scheme), we investigate the continuous optimization problem of computing a signaling scheme which maximizes the sender's expected revenue. We show that computing an optimal ex ante persuasive signaling scheme is NPhard when there are three or more receivers. In contrast with previous hardness results for ex interim persuasion, we show that, for games with two receivers, an optimal ex ante persuasive signaling scheme can be computed in polynomial time thanks to a novel algorithm based on the ellipsoid method which we propose.",https://arxiv.org/pdf/1908.00877.pdf,page_3.html
How model accuracy and explanation fidelity influence user trust in AI,"Papenmeier, A., Englebienne, G., Seifert, C.",2019,"Machine learning systems have become popular in fields such as marketing, financing, or data mining. While they are highly accurate, complex machine learning systems pose challenges for engineers and users. Their inherent complexity makes it impossible to easily judge their fairness and the correctness of statistically learned relations between variables and classes. Explainable AI aims to solve this challenge by modelling explanations alongside with the classifiers, potentially improving user trust and acceptance. However, users should not be fooled by persuasive, yet untruthful explanations. We therefore conduct a user study in which we investigate the effects of model accuracy and explanation fidelity, i.e. how truthfully the explanation represents the underlying model, on user trust. Our findings show that accuracy is more important for user trust than explainability. Adding an explanation for a classification result can potentially harm trust, e.g. when adding nonsensical explanations. We also found that users cannot be tricked by high-fidelity explanations into having trust for a bad classifier. Furthermore, we found a mismatch between observed (implicit) and self-reported (explicit) trust.",https://arxiv.org/pdf/1907.12652.pdf,page_3.html
Towards understanding emotional intelligence for behavior change chatbots,"Ghandeharioun, A., McDuff, D., Czerwinski, M., Rowan, K.",2019,"A natural conversational interface that allows longitudinal symptom tracking would be extremely valuable in health/wellness applications. However, the task of designing emotionally-aware agents for behavior change is still poorly understood. In this paper, we present the design and evaluation of an emotion-aware chatbot that conducts experience sampling in an empathetic manner. We evaluate it through a human-subject experiment with N=39 participants over the course of a week. Our results show that extraverts preferred the emotion-aware chatbot significantly more than introverts. Also, participants reported a higher percentage of positive mood reports when interacting with the empathetic bot. Finally, we provide guidelines for the design of emotion-aware chatbots for potential use in mHealth contexts.",https://arxiv.org/pdf/1907.10664.pdf,page_3.html
Evaluating explanation without ground truth in interpretable machine learning,"Yang, F., Du, M., Hu, X.",2019,"Interpretable Machine Learning (IML) has become increasingly important in many real-world applications, such as autonomous cars and medical diagnosis, where explanations are significantly preferred to help people better understand how machine learning systems work and further enhance their trust towards systems. However, due to the diversified scenarios and subjective nature of explanations, we rarely have the ground truth for benchmark evaluation in IML on the quality of generated explanations. Having a sense of explanation quality not only matters for assessing system boundaries, but also helps to realize the true benefits to human users in practical settings. To benchmark the evaluation in IML, in this article, we rigorously define the problem of evaluating explanations, and systematically review the existing efforts from state-of-the-arts. Specifically, we summarize three general aspects of explanation (i.e., generalizability, fidelity and persuasibility) with formal definitions, and respectively review the representative methodologies for each of them under different tasks. Further, a unified evaluation framework is designed according to the hierarchical needs from developers and end-users, which could be easily adopted for different scenarios in practice. In the end, open problems are discussed, and several limitations of current evaluation techniques are raised for future explorations.",https://arxiv.org/pdf/1907.06831.pdf,page_3.html
Exploring the role of prior beliefs for argument persuasion,"Durmus, E., Cardie, C.",2019,"Public debate forums provide a common platform for exchanging opinions on a topic of interest. While recent studies in natural language processing (NLP) have provided empirical evidence that the language of the debaters and their patterns of interaction play a key role in changing the mind of a reader, research in psychology has shown that prior beliefs can affect our interpretation of an argument and could therefore constitute a competing alternative explanation for resistance to changing one’s stance. To study the actual effect of language use vs. prior beliefs on persuasion, we provide a new dataset and propose a controlled setting that takes into consideration two reader-level factors: political and religious ideology. We find that prior beliefs affected by these reader-level factors play a more important role than language use effects and argue that it is important to account for them in NLP studies of persuasion.",https://arxiv.org/pdf/1906.11301.pdf,page_3.html
Famous People Media through the Use of Artificial Intelligence in Digital Marketing,"Tiautrakul, J., Jindakul, J.",2019,"Famous people media though the use of artificial intelligence in digital marketing in this article were to study about the meaning and role of personal media, development trend of artificial intelligence (AI), and advantages and disadvantages of using artificial intelligence (AI) and personal media in digital marketing. They showed that the famous people media affecting persuasion; therefore, the use of famous people media in the advertising industry by bringing both individuals with ""famous personal media"" to be (presenter) and artificial intelligence (AI). Therefore, if these famous people use artificial intelligence to help market in the digital age, there will definitely be more customers than at present because artificial intelligence can help to quickly respond to information instead of people, and it can screen and classify customers, but psychological persuasion still requires personal media.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3405188&type=2,page_3.html
Persuasion for good: Towards a personalized persuasive dialogue system for social good,"Wang, X., Shi, W., Kim, R., ...Zhang, J., Yu, Z.",2019,"Developing intelligent persuasive conversational agents to change people's opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals' demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals' personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.",https://arxiv.org/pdf/1906.06725.pdf,page_3.html
Polynomial-time updates of epistemic states in a fragment of probabilistic epistemic argumentation (technical report),"Potyka, N., Polberg, S., Hunter, A.",2019,"Probabilistic epistemic argumentation allows for reasoning about argumentation problems in a way that is well founded by probability theory. Epistemic states are represented by probability functions over possible worlds and can be adjusted to new beliefs using update operators. While the use of probability functions puts this approach on a solid foundational basis, it also causes computational challenges as the amount of data to process depends exponentially on the number of arguments. This leads to bottlenecks in applications such as modelling opponent's beliefs for persuasion dialogues. We show how update operators over probability functions can be related to update operators over much more compact representations that allow polynomial-time updates.We discuss the cognitive and probabilistic-logical plausibility of this approach and demonstrate its applicability in computational persuasion.",https://arxiv.org/pdf/1906.05066.pdf,page_3.html
Risky action recognition in lane change video clips using deep spatiotemporal networks with segmentation mask transfer,"Yurtsever, E., Liu, Y., Lambert, J., ...Takeda, K., Hansen, J.H.L.",2019,"Advanced driver assistance and automated driving systems rely on risk estimation modules to predict and avoid dangerous situations. Current methods use expensive sensor setups and complex processing pipeline, limiting their availability and robustness. To address these issues, we introduce a novel deep learning based action recognition framework for classifying dangerous lane change behavior in short video clips captured by a monocular camera. We designed a deep spatiotemporal classification network that uses pre-trained state-of-the-art instance segmentation network Mask R-CNN as its spatial feature extractor for this task. The Long-Short Term Memory (LSTM) and shallower final classification layers of the proposed method were trained on a semi-naturalistic lane change dataset with annotated risk labels. A comprehensive comparison of state-of-the-art feature extractors was carried out to find the best network layout and training strategy. The best result, with a 0.937 AUC score, was obtained with the proposed network. Our code and trained models are available open-source.",https://arxiv.org/pdf/1906.02859.pdf,page_3.html
Impact of argument type and concerns in argumentation with a chatbot,"CHALAGUINE, L.A., HAMILTON, F.L., HUNTER, A., POTTS, H.W.W.",2019,"Conversational agents, also known as chatbots, are versatile tools that have the potential of being used in dialogical argumentation. They could possibly be deployed in tasks such as persuasion for behaviour change (e.g. persuading people to eat more fruit, to take regular exercise, etc.) However, to achieve this, there is a need to develop methods for acquiring appropriate arguments and counterargument that reflect both sides of the discussion. For instance, to persuade someone to do regular exercise, the chatbot needs to know counterarguments that the user might have for not doing exercise. To address this need, we present methods for acquiring arguments and counterarguments, and importantly, meta-level information that can be useful for deciding when arguments can be used during an argumentation dialogue. We evaluate these methods in studies with participants and show how harnessing these methods in a chatbot can make it more persuasive.",https://arxiv.org/pdf/1905.00646.pdf,page_3.html
CoachAI: A conversational agent assisted health coaching platform,"Fadhil, A., Schiavo, G., Wang, Y.",2019,"Poor lifestyle represents a health risk factor and is the leading cause of morbidity and chronic conditions. The impact of poor lifestyle can be significantly altered by individual behavior change. Although the current shift in healthcare towards a long-lasting modifiable behavior, however, with increasing caregiver workload and individuals' continuous needs of care, there is a need to ease caregiver's work while ensuring continuous interaction with users. This paper describes the design and validation of CoachAI, a conversational agent-assisted health coaching system to support health intervention delivery to individuals and groups. CoachAI instantiates a text-based healthcare chatbot system that bridges the remote human coach and the users. This research provides three main contributions to the preventive healthcare & healthy lifestyle promotion: (1) it presents the conversational agent to aid the caregiver; (2) it aims to decrease caregiver's workload and enhance care given to users, by handling (automating) repetitive caregiver tasks; and (3) it presents a domain-independent mobile health conversational agent for health intervention delivery. We will discuss our approach and analyze the results of a one-month validation study on physical activity, healthy diet and stress management.",https://arxiv.org/pdf/1904.11961.pdf,page_3.html
Intergenerational Governance and Leadership in the Corporate World: Conclusion,"Puaschunder, J.M.",2019,"The book touched on public and private practices of intergenerational conscientiousness in very many different domains ranging from CSR, to finance, energy and technology, and global governance. Intergenerational Governance, Leadership and Responsibility is currently forming and will shape the future of the 21st century. In an interdisciplinary attempt, this book showed ways to imbue and strengthen intergenerational leadership in the corporate world. Helen Guan and Carolyn Wang showed by the theory of planned behavior how managers’ attitudes toward Corporate Social Responsibility (CSR), subjective norms, and perceived behavior control influence CSR practice in an organization. Estela Rios Seabra discerns existent food preferences of women and men, and gender biases, in America in order to propose a strategy to test the most efficient heuristics to nudge people to a plant-based, sustainable diet. Hasan Dinçer, Serhat Yüksel, Mustafa Tevfik Kartal, and Gökhan Alpman evaluated the effect of corporate governance in alternative distribution channels of the Turkish banking sector. Ramzi Fahrani and Azza Béjaoui investigated the interaction between remittances and financial development and their impact on the economic growth over the period 1980-2016. Thomas Reese Fong informed about how Elevator Energy Storage Systems (EESS) provide reliable energy storage using the gravitational potential energy of elevators. Dirk Beerbaum and Julia M. Puaschunder addressed how technological improvement in the age of information has increased the possibilities to control the innocent social media users or penalize private investors and reap the benefits of their existence in hidden persuasion and discrimination. Ceren Aydogmus drew inferences from today's workforce being more diverse than ever, comprised of five generational cohorts. Julia M. Puaschunder provided the first macro-economic intergenerational responsibility model in the 21st century. A future outlook introduced the idea of Mapping Climate Justice and temperature trade being an asset determining future Climate Wealth of Nations.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3354141&type=2,page_3.html
Is two better than one? effects of multiple agents on user persuasion,"Kantharaju, R.B., De Franco, D., Pease, A., Pelachaud, C.",2019,"Virtual humans need to be persuasive in order to promote behaviour change in human users. While several studies have focused on understanding the numerous aspects that influence the degree of persuasion, most of them are limited to dyadic interactions. In this paper, we present an evaluation study focused on understanding the effects of multiple agents on user's persuasion. Along with gender and status (authoritative &peer), we also look at type of focus employed by the agent i. e., user-directed where the agent aims to persuade by addressing the user directly and vicarious where the agent aims to persuade the user, who is an observer, indirectly by engaging another agent in the discussion. Participants were randomly assigned to one of the 12 conditions and presented with a persuasive message by one or several virtual agents. A questionnaire was used to measure perceived interpersonal attitude, credibility and persuasion. Results indicate that credibility positively affects persuasion. In general, multiple agent setting, irrespective of the focus, was more persuasive than single agent setting. Although, participants favored user-directed setting and reported it to be persuasive and had an increased level of trust in the agents, the actual change in persuasion score reflects that vicarious setting was the most effective in inducing behaviour change. In addition to this, the study also revealed that authoritative agents were the most persuasive.",https://arxiv.org/pdf/1904.05248.pdf,page_3.html
Lane change decision-making through deep reinforcement learning with rule-based constraints,"Wang, J., Zhang, Q., Zhao, D., Chen, Y.",2019,"Autonomous driving decision-making is a great challenge due to the complexity and uncertainty of the traffic environment. Combined with the rule-based constraints, a Deep Q-Network (DQN) based method is applied for autonomous driving lane change decision-making task in this study. Through the combination of high-level lateral decision-making and lowlevel rule-based trajectory modification, a safe and efficient lane change behavior can be achieved. With the setting of our state representation and reward function, the trained agent is able to take appropriate actions in a real-world-like simulator. The generated policy is evaluated on the simulator for 10 times, and the results demonstrate that the proposed rule-based DQN method outperforms the rule-based approach and the DQN method.",https://arxiv.org/pdf/1904.00231.pdf,page_3.html
Turing-Completeness of Dynamics in Abstract Persuasion Argumentation,"Arisaka, R.",2019,"Abstract Persuasion Argumentation (APA) is a dynamic argumentation formalism that extends Dung argumentation with persuasion relations. In this work, we show through two-counter Minsky machine encoding that APA dynamics is Turing-complete.",https://arxiv.org/pdf/1903.07837.pdf,page_3.html
A virtual conversational agent for teens with autism: Experimental results and design lessons,"Ali, M.R., Razavi, Z., Mamun, A.A., ...Schubert, L., Ehsan Hoque, M.",2018,"We present the design of an online social skills development interface for teenagers with autism spectrum disorder (ASD), who often lack access to social skills training. The interface is intended to enable private conversation practice anywhere, anytime using a browser. Users converse informally with an on-screen persona, receiving feedback on nonverbal cues in real-time, and summary feedback. The prototype was developed in consultation with an expert UX designer, two psychologists, and a pediatrician. Using the data from 47 individuals, feedback and dialogue generation were automated using the hidden Markov model and a schema-driven dialogue manager capable of handling multi-topic conversations. We conducted a study with nine high-functioning ASD teenagers, and through thematic analysis of post-experiment interviews, identified several key design considerations, notably: 1) Users should be fully briefed at the outset about the purpose and limitations of the system, to avoid unrealistic expectations. 2) An interface should incorporate positive acknowledgment of behavior change. 3) Realistic appearance of a virtual agent and responsiveness are important in engaging users. 4) Conversation personalization, for instance in prompting laconic users for more input and reciprocal questions, would help the teenagers engage for longer terms and increase the system's utility.",https://arxiv.org/pdf/1811.03046.pdf,page_3.html
A Behavioral Economics Approach to Digitalisation,"Beerbaum, D., Puaschunder, J.M.",2018,"A growing body of academic research in the field of behavioural economics, political science and psychology demonstrate how an invisible hand can nudge people’s decisions towards a preferred option. Contrary to the assumptions of the neoclassical economics, supporters of nudging argue that people have problems coping with a complex world, because of their limited knowledge and their restricted rationality. Technological improvement in the age of information has increased the possibilities to control the innocent social media users or penalise private investors and reap the benefits of their existence in hidden persuasion and discrimination. Nudging enables nudgers to plunder the simple uneducated and uninformed citizen and investor, who is neither aware of the nudging strategies nor able to oversee the tactics used by the nudgers (Puaschunder 2017a, b; 2018a, b). The nudgers are thereby legally protected by democratically assigned positions they hold. The law of motion of the nudging societies holds an unequal concentration of power of those who have access to compiled data and coding rules, relevant for political power and influencing the investor’s decision usefulness (Puaschunder 2017a, b; 2018a, b). This paper takes as a case the “transparency technology XBRL (eXtensible Business Reporting Language)” (Sunstein 2013, 20), which should make data more accessible as well as usable for private investors. It is part of the choice architecture on regulation by governments (Sunstein 2013). However, XBRL is bounded to a taxonomy (Piechocki and Felden 2007). Considering theoretical literature and field research, a representation issue (Beerbaum, Piechocki and Weber 2017) for principles-based accounting taxonomies exists, which intelligent machines applying Artificial Intelligence (AI) (Mwilu, Prat and Comyn-Wattiau 2015) nudge to facilitate decision usefulness. This paper conceptualizes ethical questions arising from the taxonomy engineering based on machine learning systems: Should the objective of the coding rule be to support or to influence human decision making or rational artificiality? This paper therefore advocates for a democratisation of information, education and transparency about nudges and coding rules (Puaschunder 2017a, b; 2018a, b).",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3266608&type=2,page_3.html
Co-Arg: Cogent argumentation with crowd elicitation,"Boicu, M., Marcu, D., Tecuci, G., ...Uttamsingh, C., Kalale, N.",2018,"This paper presents Co-Arg, a new type of cognitive assistant to an intelligence analyst that enables the synergistic integration of analyst imagination and expertise, computer knowledge and critical reasoning, and crowd wisdom, to draw defensible and persuasive conclusions from masses of evidence of all types, in a world that is changing all the time. Co-Arg's goal is to improve the quality of the analytic results and enhance their understandability for both experts and novices. The performed analysis is based on a sound and transparent argumentation that links evidence to conclusions in a way that shows very clearly how the conclusions have been reached, what evidence was used and how, what is not known, and what assumptions have been made. The analytic results are presented in a report describes the analytic conclusion and its probability, the main favoring and disfavoring arguments, the justification of the key judgments and assumptions, and the missing information that might increase the accuracy of the solution.",https://arxiv.org/pdf/1810.01541.pdf,page_3.html
Transfer learning for estimating causal effects using neural networks,"Künzel, S.R., Stadie, B.C., Vemuri, N., ...Sekhon, J.S., Abbeel, P.",2018,"We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature. By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms. We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database. Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data.",https://arxiv.org/pdf/1808.07804.pdf,page_3.html
A Behavioral Economics Approach to Digitalisation: The Case of a Principles-Based Taxonomy,"Beerbaum, D., Puaschunder, J.M.",2018,"A growing body of academic research in the field of behavioural economics, political science and psychology demonstrate how an invisible hand can nudge people’s decisions towards a preferred option. Contrary to the assumptions of the neoclassical economics, supporters of nudging argue that people have problems coping with a complex world, because of their limited knowledge and their restricted rationality. Technological improvement in the age of information has increased the possibilities to control the innocent social media users or penalise private investors and reap the benefits of their existence in hidden persuasion and discrimination. Nudging enables nudgers to plunder the simple uneducated and uninformed citizen and investor, who is neither aware of the nudging strategies nor able to oversee the tactics used by the nudgers. The nudgers are thereby legally protected by democratically assigned positions they hold. The law of motion of the nudging societies holds an unequal concentration of power of those who have access to compiled data and coding rules, relevant for political power and influencing the investor’s decision usefulness. This paper takes as a case the “transparency technology XBRL (eXtensible Business Reporting Language)” (Sunstein 2013, 20), which should make data more accessible as well as usable for private investors. It is part of the choice architecture on regulation by governments (Sunstein 2013). However, XBRL is bounded to a taxonomy (Piechocki and Felden 2007). Considering theoretical literature and field research, a representation issue (Beerbaum, Piechocki and Weber 2017) for principles-based accounting taxonomies exists, which intelligent machines applying Artificial Intelligence (AI) (Mwilu, Prat and Comyn-Wattiau 2015) nudge to facilitate decision usefulness. This paper conceptualizes ethical questions arising from the taxonomy engineering based on machine learning systems: Should the objective of the coding rule be to support or to influence human decision making or rational artificiality? This paper therefore advocates for a democratisation of information, education and transparency about nudges and coding rules.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3210207&type=2,page_3.html
Data-efficient hierarchical reinforcement learning,"Nachum, O., Gu, S., Lee, H., Levine, S.",2018,"Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations,1 learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques2.",https://arxiv.org/pdf/1805.08296.pdf,page_3.html
Argument harvesting using chatbots,"Chalaguine, L.A., Hamilton, F.L., Hunter, A., Potts, H.W.W.",2018,"Much research in computational argumentation assumes that arguments and counterarguments can be obtained in some way. Yet, to improve and apply models of argument, we need methods for acquiring them. Current approaches include argument mining from text, hand coding of arguments by researchers, or generating arguments from knowledge bases. In this paper, we propose a new approach, which we call argument harvesting, that uses a chatbot to enter into a dialogue with a participant to get arguments and counterarguments from him or her. Because it is automated, the chatbot can be used repeatedly in many dialogues, and thereby it can generate a large corpus. We describe the architecture of the chatbot, provide methods for managing a corpus of arguments and counterarguments, and an evaluation of our approach in a case study concerning attitudes of women to participation in sport.",https://arxiv.org/pdf/1805.04253.pdf,page_3.html
Explainable recommendation: A survey and new perspectives,"Zhang, Y., Chen, X.",2018,"Explainable recommendation attempts to develop models that generate not only high-quality recommendations but also intuitive explanations. The explanations may either be post-hoc or directly come from an explainable model (also called interpretable or transparent model in some contexts). Explainable recommendation tries to address the problem of why: By providing explanations to users or system designers, it helps humans to understand why certain items are recommended by the algorithm, where the human can either be users or system designers. Explainable recommendation helps to improve the transparency, persuasiveness, effectiveness, trustworthiness, and satisfaction of recommendation systems. It also facilitates system designers for better system debugging. In recent years, a large number of explainable recommendation approaches - especially model-based methods - have been proposed and applied in real-world systems.",https://arxiv.org/pdf/1804.11192.pdf,page_3.html
Taming the Algorithm in the Room: Artificial Intelligence Regulation for the Social Good,"Schrader, D., Ghosh, D.",2018,"Artificial intelligence (AI) systems increasingly permeate human existence, influencing consumer purchases, transportation, work, family life, and social spheres of community and politics. Current news cycles evidence unfavorable effects of AI such as the Parkland conspiracy disclosures and the fake news debates, both of which demonstrate that social media apps can threaten and affect our minds (McKew, 2018; Anderson & Rainie, 2017) despite their prosocial potentialities and design. Likewise, health care apps affect mood, health, and bodies in the detection, diagnosis, treatment and prognosis evaluation of disease and wellness (Jiang, et al., 2017), and AI systems in robotics not only help but also threaten our physical safety and bodily well-being (Calo, 2015). Such ubiquitous impacts of AI, both beneficial and detrimental, necessitate policies that guide the future development of AI in a way that can protect humans from both obvious and surreptitious threats. Critically, AI is not currently apparent, transparent, nor in the conscious awareness of many people. Nonetheless AI takes various forms and serves numerous functions for millions. Social media platforms such as Facebook and YouTube, for example, were designed with pro-social motives, though use marketing to sustain the service providers, maintain user engagement, and derive economic benefit. AI thus has social utility and serves the social good. But it has also exhibited potential for nefarious purposes. For those reasons this paper analyzes potential ethical issues of AI as it is implemented in social media, particularly when the engagement and use of AI is based on the fabrication and consumption of false information, pretenses, or is contrived and intentionally dishonest for persuasive purposes, or is unintentionally deceptive. We examine the beneficial and the risky aspects of AI in selected media, the ethical dimensions of AI including functional utility, human rights such as privacy and protected speech, issues of transparency, authenticity and truth, and issues of responsibility when things ‘go wrong’ when AI systems are the primary decision-makers. Cases such as the spread of political disinformation, conspiracy theories, and online hate speech are examined to illustrate these various ethical issues. Further, we consider policy recommendations that can assure protective and beneficial AI. Based on our ethical analyses of these select cases, we suggest policies to guide the development and use of AI systems with consideration given to the type of function the AI system serves, and the format in which the system is embedded—such as in algorithmic form such as that used in marketing, in monitoring form such as health monitoring, or robotic form where humans and AI systems might be indistinguishable in appearance and function. AI ethics is essential to the protection of the personal, public and social good. Understanding the effects of emerging technologies on personal, social, economic life is critical to determining the regulations that ought to be required. AI development and related policy development should be proactive and beneficial (Baum, 2017) and consider both technological utility and protective functions to humans.",https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3142171&type=2,page_3.html
Beyond Patient Monitoring: Conversational Agents Role in Telemedicine & Healthcare Support For Home-Living Elderly Individuals,"Fadhil, A.",2018,"There is a need for systems to dynamically interact with ageing populations to gather information, monitor health condition and provide support, especially after hospital discharge or at-home settings. Several smart devices have been delivered by digital health, bundled with telemedicine systems, smartphone and other digital services. While such solutions offer personalised data and suggestions, the real disruptive step comes from the interaction of new digital ecosystem, represented by chatbots. Chatbots will play a leading role by embodying the function of a virtual assistant and bridging the gap between patients and clinicians. Powered by AI and machine learning algorithms, chatbots are forecasted to save healthcare costs when used in place of a human or assist them as a preliminary step of helping to assess a condition and providing self-care recommendations. This paper describes integrating chatbots into telemedicine systems intended for elderly patient after their hospital discharge. The paper discusses possible ways to utilise chatbots to assist healthcare providers and support patients with their condition.",https://arxiv.org/pdf/1803.06000.pdf,page_3.html
Domain modelling in computational persuasion for behaviour change in healthcare,"Chalaguine, L., Hadoux, E., Hamilton, F., ...Polberg, S., Potts, H.W.W.",2018,"The aim of behaviour change is to help people to change aspects of their behaviour for the better (e.g., to decrease calorie intake, to drink in moderation, to take more exercise, to complete a course of antibiotics once started, etc.). In current persuasion technology for behaviour change, the emphasis is on helping people to explore their issues (e.g., through questionnaires or game playing) or to remember to follow a behaviour change plan (e.g., diaries and email reminders). However, recent developments in computational persuasion are leading to an argument-centric approach to persuasion that can potentially be harnessed in behaviour change applications. In this paper, we review developments in computational persuasion, and then focus on domain modelling as a key component. We present a multi-dimensional approach to domain modelling. At the core of this proposal is an ontology which provides a representation of key factors, in particular kinds of belief, which we have identified in the behaviour change lit ENGLerature as being important in diverse behaviour change initiatives. Our proposal for domain modelling is intended to facilit ENGLate the acquisition and representation of the arguments that can be used in persuasion dialogues, together with meta-level information about them which can be used by the persuader to make strategic choices of argument to present.",https://arxiv.org/pdf/1802.10054.pdf,page_3.html
Can a Chatbot Determine My Diet?: Addressing Challenges of Chatbot Application for Meal Recommendation,"Fadhil, A.",2018,"—Poor nutrition can lead to reduced immunity, increased susceptibility to disease, impaired physical and mental development, and reduced productivity. A conversational agent can support people as a virtual coach, however building such systems still have its associated challenges and limitations. This paper describes the background and motivation for chatbot systems in the context of healthy nutrition recommendation. We discuss current challenges associated with chatbot application, we tackled technical, theoretical, behavioural, and social aspects of the challenges. We then propose a pipeline to be used as guidelines by developers to implement theoretically and technically robust chatbot systems.",https://arxiv.org/pdf/1802.09100.pdf,page_3.html
Opinion dynamics with varying susceptibility to persuasion,"Abebe, R., Kleinberg, J., Parkes, D., Tsourakakis, C.E.",2018,"A long line of work in social psychology has studied variations in people's susceptibility to persuasion - the extent to which they are willing to modify their opinions on a topic. This body of literature suggests an interesting perspective on theoretical models of opinion formation by interacting parties in a network: in addition to considering interventions that directly modify people's intrinsic opinions, it is also natural to consider interventions that modify people's susceptibility to persuasion. In this work, we adopt a popular model for social opinion dynamics, and we formalize the opinion maximization and minimization problems where interventions happen at the level of susceptibility. Specifically, we ask: Given an opinion dynamics model, and a set of agents, each of whom has an innate opinion that reflects the agent's intrinsic position on a topic and a susceptibility-to-persuasion parameter measuring the agent's propensity for changing his or her opinion, how should we modify the agents' susceptibilities in order to maximize (or minimize) the total sum of opinions at equilibrium? We study both the version when there is not a budget on the number of agents that we can target, and when there is. For the former, we provide a polynomial time algorithm for both the maximization and the minimization formulations. In contrast, we show that budgeted version is NP-hard, but unlike prior work focusing on opinion maximization, our objective function is neither sub- nor super-modular in the target set. For this version, we propose a greedy heuristic that we compare experimentally against natural baseline methods. On the experimental side, we perform an empirical study of our proposed methods on several datasets, including a Twitter network with real opinion estimates. Our experimental findings show that our proposed tools outperform natural baselines and can achieve a multi-fold effect on the total sum of opinions in the network.",https://arxiv.org/pdf/1801.07863.pdf,page_3.html
The Promise and Peril of Human Evaluation for Model Interpretability,"Herman, B.",2017,"Transparency, user trust, and human comprehension are popular ethical motivations for interpretable machine learning. In support of these goals, researchers evaluate model explanation performance using humans and real world applications. This alone presents a challenge in many areas of artificial intelligence. In this position paper, we propose a distinction between descriptive and persuasive explanations. We discuss reasoning suggesting that functional interpretability may be correlated with cognitive function and user preferences. If this is indeed the case, evaluation and optimization using functional metrics could perpetuate implicit cognitive bias in explanations that threaten transparency. Finally, we propose two potential research directions to disambiguate cognitive function and explanation models, retaining control over the tradeoff between accuracy and interpretability.",https://arxiv.org/pdf/1711.07414.pdf,page_3.html
Detection and Analysis of Human Emotions through Voice and Speech Pattern Processing,"Dasgupta, P.B.",2017,"The ability to modulate vocal sounds and generate speech is one of the features which set humans apart from other living beings. The human voice can be characterized by several attributes such as pitch, timbre, loudness, and vocal tone. It has often been observed that humans express their emotions by varying different vocal attributes during speech generation. Hence, deduction of human emotions through voice and speech analysis has a practical plausibility and could potentially be beneficial for improving human conversational and persuasion skills. This paper presents an algorithmic approach for detection and analysis of human emotions with the help of voice and speech processing. The proposed approach has been developed with the objective of incorporation with futuristic artificial intelligence systems for improving human-computer interactions.",https://arxiv.org/pdf/1710.10198.pdf,page_3.html
"""Let me convince you to buy my product ... "" A Case Study of an Automated Persuasive System for Fashion Products","Munigala, V., Tamilselvam, S., Sankaran, A.",2017,"Persuasivenes is a creative art aimed at making people believe in certain set of beliefs. Many a times, such creativity is about adapting richness of one domain into another to strike a chord with the target audience. In this research, we present PersuAIDE! - A persuasive system based on linguistic creativity to transform given sentence to generate various forms of persuading sentences. These various forms cover multiple focus of persuasion such as memorability and sentiment. For a given simple product line, the algorithm is composed of several steps including: (i) select an appropriate well-known expression for the target domain to add memorability, (ii) identify keywords and entities in the given sentence and expression and transform it to produce creative persuading sentence, and (iii) adding positive or negative sentiment for further persuasion. The persuasive conversion were manually verified using qualitative results and the effectiveness of the proposed approach is empirically discussed.",https://arxiv.org/pdf/1709.08366.pdf,page_3.html
Dynamic consistency of expected utility under non-classical (quantum) uncertainty,"Danilov, V.I., Lambert-Mogiliansky, A., Vergopoulos, V.",2017,"Quantum cognition in decision-making is a recent and rapidely growing field. In this paper we develop an expected utility theory in a context of non-classical (quantum) uncertainty. We replace the classical state space with a Hilbert space which allows introducing the concept of quantum lottery. Within that framework we formulate axioms on preferences over quantum lotteries to establish a representation theorem. We show that demanding the consistency of choice behavior conditional on new information is equivalent to the von Neuman-Lüders postulate applied to beliefs. A dynamically consistent quantum-like agent may violate dynamic recursive consistency, however. This feature suggests interesting applications in behavioral economics as we illustrate in an example of persuasion.",https://arxiv.org/pdf/1708.08244.pdf,page_3.html
Explainable Recommendation: Theory and Applications,"Yongfeng, Z.",2017,"Although personalized recommendation has been investigated for decades of years, the wide adoption of Latent Factor Models (LFM) has made the explainability of recommendations an important and critical issue to both the research community and practical application of recommender systems. For example, in many practical systems the algorithm just provide a personalized item recommendation list to the users, without persuasive personalized explanation about why such an item is recommended while another is not. Unexplainable recommendations introduce negative effects to the trustworthiness of recommender systems, and thus affect the effectiveness of recommendation engines. In this work, we investigate explainable recommendation in aspects of data explainability, model explainability, and result explainability, and the main contributions are as follows:1. Data Explainability: Data input is the first step of typical recommender systems, and user-item rating matrix is the most basic data format for most personalized recommendation algorithms, especially for Matrix Factorization (MF)-based approaches. In this work, we propose Localized Matrix Factorization (LMF) framework based Bordered Block Diagonal Form (BBDF) matrices, and further applied this technique for parallelized matrix factorization. Traditional MF algorithms treat the original rating matrix as a whole for factorization, without specific understanding of the inherent structure embedded therein. In this work, however, we propose the (recursive) BBDF structure of sparse matrices, and formally prove its equivalence with community detection on bipartite graphs, with which to explain the inherent community structures and their relationships in sparse matrices. Based on this, we further propose the LMF framework, and prove its compatibility with most of the traditional MF algorithms, which makes it a unified parallelization framework for matrix factorization, that improves both the effect and efficiency at the same time.2. Model Explainability: Based on user-item rating matrices, personalized recommendation algorithms attempt to model user preferences and make personalized recommendations. In this work, we propose Explicit Factor Models (EFM) based on phrase-level sentiment analysis, as well as dynamic user preference modeling based on time series analysis. For their prediction accuracy and scalability, Latent Factor Models (LFM) based on MF have achieved wide application in real-world systems. However, due to their inherently latent factors, it is usually difficult for LFM to provide intuitively understandable explanations to the recommendation algorithms and results, which reduces the persuasiveness of recommendations. In this work, we extract product features and user opinions towards different features from large-scale user textual reviews based on phrase-level sentiment analysis techniques, and introduce the EFM approach for explainable model learning and recommendation. Because user preference on features may change over time, we conduct dynamic user modeling based on time series analysis, so as to construct explainable dynamic recommendations.3. Economic Explainability: Based on data analysis and user preference modeling, recommender systems actually manipulate the way that items are matched with users, and eventually affect the economic benefits of the online economic system. In this work, we propose the Total Surplus Maximization (TSM) framework for personalized recommendation, as well as the model specification in different types of online applications. More and more human activities are experiencing the continuous progressing from offline to online, and many commonly used online applications can be formalized into the’producer–service–consumer’ framework. For example, in E-commerce websites online retailers (producers) provide normal goods (services), and the users (consumers) thus make choices and purchases from the vast amount of online services. Based on basic economic concepts, we provide the definitions of utility, cost, and surplus in the application scenario of Web services, and propose the general framework of web total surplus calculation and maximization. Further more, we specific the total surplus maximization framework to different types of online applications, i.e., E-commerce, P2P lending, and online freelancing services. Experimental results on real-world datasets verify that our TSM framework is able to improve the recommendation performance and at the same time benefit the social good of the Web.With the continuous growth of the Web, Personalized Recommender Systems (PRS) have been the important building blocks of many online web applications, which contribute to our daily lives in various manners. For example, the product recommendation engines in E-commerce websites recommend potentially interesting products to users, friend recommendation helps to find and connect users in social networks, video recommendation in video sharing websites help users to find favourite videos more quickly and efficiently, and news recommendation in news portals push the latest news to users according to their personalized information needs. In a way, personalized recommendation has become one of the most basic supportive techniques in the era of web intelligence.",https://arxiv.org/pdf/1708.06409.pdf,page_3.html
Beyond the technical challenges for deploying Machine Learning solutions in a software company,"Flaounas, I.",2017,"Recently software development companies started to embrace Machine Learning (ML) techniques for introducing a series of advanced functionality in their products such as personalisation of the user experience, improved search, content recommendation and automation. The technical challenges for tackling these problems are heavily researched in literature. A less studied area is a pragmatic approach to the role of humans in a complex modern industrial environment where ML based systems are developed. Key stakeholders affect the system from inception and up to operation and maintenance. Product managers want to embed “smart” experiences for their users and drive the decisions on what should be built next; software engineers are challenged to build or utilise ML software tools that require skills that are well outside of their comfort zone; legal and risk departments may influence design choices and data access; operations teams are requested to maintain ML systems which are non-stationary in their nature and change behaviour over time; and finally ML practitioners should communicate with all these stakeholders to successfully build a reliable system. This paper discusses some of the challenges we faced in Atlassian as we started investing more in the ML space.",https://arxiv.org/pdf/1708.02363.pdf,page_3.html
Preserving intermediate objectives: One simple trick to improve learning for hierarchical models,"Ravichander, A., Rijhwani, S., Kulshreshtha, R., ...Baltrušaitis, T., Morency, L.-P.",2017,"Hierarchical models are utilized in a wide variety of problems which are characterized by task hierarchies, where predictions on smaller subtasks are useful for trying to predict a final task. Typically, neural networks are first trained for the subtasks, and the predictions of these networks are subsequently used as additional features when training a model and doing inference for a final task. In this work, we focus on improving learning for such hierarchical models and demonstrate our method on the task of speaker trait prediction. Speaker trait prediction aims to computationally identify which personality traits a speaker might be perceived to have, and has been of great interest to both the Artificial Intelligence and Social Science communities. Persuasiveness prediction in particular has been of interest, as persuasive speakers have a large amount of influence on our thoughts, opinions and beliefs. In this work, we examine how leveraging the relationship between related speaker traits in a hierarchical structure can help improve our ability to predict how persuasive a speaker is. We present a novel algorithm that allows us to backpropagate through this hierarchy. This hierarchical model achieves a 25% relative error reduction in classification accuracy over current state-of-the art methods on the publicly available POM dataset.",https://arxiv.org/pdf/1706.07867.pdf,page_3.html
Abstract argumentation / persuasion / dynamics,"Arisaka, R., Satoh, K.",2017,"The act of persuasion, a key component in rhetoric argumentation, may be viewed as a dynamics modifier. We extend Dung’s frameworks with acts of persuasion among agents, and consider interactions among attack, persuasion and defence that have been largely unheeded so far. We characterise basic notions of admissibilities in this framework, and show a way of enriching them through, effectively, CTL (computation tree logic) encoding, which also permits importation of the theoretical results known to the logic into our argumentation frameworks. Our aim is to complement the growing interest in coordination of static and dynamic argumentation.",https://arxiv.org/pdf/1705.10044.pdf,page_3.html
