TY  - JOUR
TI  - Efficient Model-agnostic Alignment via Bayesian Persuasion
AU  - Bai, F.
AU  - Wang, M.
AU  - Zhang, Z.
AU  - Wen, Y.
AU  - Yang, Y.
PY  - 2024
AB  - With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent. Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data. This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework. We formalize this problem as an optimization of the signaling strategy from the small model's perspective. In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses. The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item. Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks. We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of 16.1% in mathematical reasoning ability and 13.7% in code generation. We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective.
UR  - https://arxiv.org/pdf/2405.18718v1.pdf
N1  - Source: page_2.html
ID  - 201
ER  - 
TY  - JOUR
TI  - Coaching Copilot: Blended Form of an LLM-Powered Chatbot and a Human Coach to Effectively Support Self-Reflection for Leadership Growth
AU  - Arakawa, R.
AU  - Yakura, H.
PY  - 2024
AB  - Chatbots’ role in fostering self-reflection is now widely recognized, especially in inducing users’ behavior change. While the benefits of 24/7 availability, scalability, and consistent responses have been demonstrated in contexts such as healthcare and tutoring to help one form a new habit, their utilization in coaching necessitating deeper introspective dialogue to induce leadership growth remains unexplored. This paper explores the potential of such a chatbot powered by recent Large Language Models (LLMs) in collaboration with professional coaches in the field of executive coaching. Through a design workshop with them and two weeks of user study involving ten coach-client pairs, we explored the feasibility and nuances of integrating chatbots to complement human coaches. Our findings highlight the benefits of chatbots’ ubiquity and reasoning capabilities enabled by LLMs while identifying their limitations and design necessities for effective collaboration between human coaches and chatbots. By doing so, this work contributes to the foundation for augmenting one’s self-reflective process with prevalent conversational agents through the human-in-the-loop approach.
UR  - https://arxiv.org/pdf/2405.15250v1.pdf
N1  - Source: page_2.html
ID  - 202
ER  - 
TY  - JOUR
TI  - Large Language Models for Social Determinants of Health Information Extraction from Clinical Notes - A Generalizable Approach across Institutions
AU  - Keloth, V.K.
AU  - Selek, S.
AU  - Chen, Q.
AU  - Liu, H.
AU  - Xu, H.
PY  - 2024
AB  - The consistent and persuasive evidence illustrating the influence of social determinants on health has prompted a growing realization throughout the health care sector that enhancing health and health equity will likely depend, at least to some extent, on addressing detrimental social determinants. However, detailed social determinants of health (SDoH) information is often buried within clinical narrative text in electronic health records (EHRs), necessitating natural language processing (NLP) methods to automatically extract these details. Most current NLP efforts for SDoH extraction have been limited, investigating on limited types of SDoH elements, deriving data from a single institution, focusing on specific patient cohorts or note types, with reduced focus on generalizability. This study aims to address these issues by creating cross-institutional corpora spanning different note types and healthcare systems, and developing and evaluating the generalizability of classification models, including novel large language models (LLMs), for detecting SDoH factors from diverse types of notes from four institutions: Harris County Psychiatric Center, University of Texas Physician Practice, Beth Israel Deaconess Medical Center, and Mayo Clinic. Four corpora of deidentified clinical notes were annotated with 21 SDoH factors at two levels: level 1 with SDoH factor types only and level 2 with SDoH factors along with associated values. Three traditional classification algorithms (XGBoost, TextCNN, Sentence BERT) and an instruction tuned LLM-based approach (LLaMA) were developed to identify multiple SDoH factors. Substantial variation was noted in SDoH documentation practices and label distributions based on patient cohorts, note types, and hospitals. The LLM achieved top performance with micro-averaged F1 scores over 0.9 on level 1 annotated corpora and an F1 over 0.84 on level 2 annotated corpora. While models performed well when trained and tested on individual datasets, cross-dataset generalization highlighted remaining obstacles. To foster collaboration, access to partial annotated corpora and models trained by merging all annotated datasets will be made available on the PhysioNet repository.
UR  - https://www.medrxiv.org/content/10.1101/2024.05.21.24307726v1.full.pdf
N1  - Source: page_2.html
ID  - 203
ER  - 
TY  - JOUR
TI  - Investigating Persuasion Techniques in Arabic: An Empirical Study Leveraging Large Language Models
AU  - Alzahrani, A.
AU  - Babkier, E.
AU  - Yanbaawi, F.
AU  - Yanbaawi, F.
AU  - Alhuzali, H.
PY  - 2024
AB  - In the current era of digital communication and widespread use of social media, it is crucial to develop an understanding of persuasive techniques employed in written text. This knowledge is essential for effectively discerning accurate information and making informed decisions. To address this need, this paper presents a comprehensive empirical study focused on identifying persuasive techniques in Arabic social media content. To achieve this objective, we utilize Pre-trained Language Models (PLMs) and leverage the ArAlEval dataset, which encompasses two tasks: binary classification to determine the presence or absence of persuasion techniques, and multi-label classification to identify the specific types of techniques employed in the text. Our study explores three different learning approaches by harnessing the power of PLMs: feature extraction, fine-tuning, and prompt engineering techniques. Through extensive experimentation, we find that the fine-tuning approach yields the highest results on the aforementioned dataset, achieving an f1-micro score of 0.865 and an f1-weighted score of 0.861. Furthermore, our analysis sheds light on an interesting finding. While the performance of the GPT model is relatively lower compared to the other approaches, we have observed that by employing few-shot learning techniques, we can enhance its results by up to 20%. This offers promising directions for future research and exploration in this topica
UR  - https://arxiv.org/pdf/2405.12884v1.pdf
N1  - Source: page_2.html
ID  - 204
ER  - 
TY  - JOUR
TI  - LLM-Generated Black-box Explanations Can Be Adversarially Helpful
AU  - Ajwani, R.
AU  - Javaji, S.R.
AU  - Rudzicz, F.
AU  - Zhu, Z.
PY  - 2024
AB  - Large language models (LLMs) are becoming vital tools that help us solve and understand complex problems. LLMs can generate convincing explanations, even when given only the inputs and outputs of these problems, i.e., in a “black-box” approach. However, our research uncovers a hidden risk tied to this approach, which we call adversarial helpfulness. This happens when an LLM’s explanations make a wrong answer look correct, potentially leading people to trust faulty solutions. In this paper, we show that this issue affects not just humans, but also LLM evaluators. Digging deeper, we identify and examine key persuasive strategies employed by LLMs. Our findings reveal that these models employ strategies such as reframing questions, expressing an elevated level of confidence, and ‘cherry-picking’ evidence that supports incorrect answers. We further create a symbolic graph reasoning task to analyze the mechanisms of LLMs generating adversarial helpfulness explanations. Most LLMs are not able to find alternative paths along simple graphs, indicating that other mechanisms, rather than logical deductions, might facilitate adversarial helpfulness. These findings shed light on the limitations of black-box explanations and lead to recommendations for the safer use of LLMs.
UR  - https://arxiv.org/pdf/2405.06800v3.pdf
N1  - Source: page_2.html
ID  - 205
ER  - 
TY  - JOUR
TI  - Impact of Tone-Aware Explanations in Recommender Systems
AU  - Okoso, A.
AU  - Otaki, K.
AU  - Koide, S.
AU  - Baba, Y.
PY  - 2024
AB  - In recommender systems, the presentation of explanations plays a crucial role in supporting users’ decision-making processes. Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked. Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication. However, studies on the impact of tone on explanations within the context of recommender systems are insufficient. Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes. We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products. Collected data analysis reveals different perceived effects of tones depending on the domains. Moreover, user attributes such as age and personality traits are found to influence the impact of tone. This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience.
UR  - https://arxiv.org/pdf/2405.05061v1.pdf
N1  - Source: page_2.html
ID  - 206
ER  - 
TY  - JOUR
TI  - Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control
AU  - Chaudhary, Y.
AU  - Penn, J.
PY  - 2024
AB  - Large language models (LLMs) can reproduce a wide variety of rhetorical styles and generate text that expresses a broad spectrum of sentiments. This capacity, now available at low cost, makes them powerful tools for manipulation and control. In this paper, we consider a set of underestimated societal harms made possible by the rapid and largely unregulated adoption of LLMs. Rather than consider LLMs as isolated digital artefacts used to displace this or that area of work, we focus on the large-scale computational infrastructure upon which they are instrumentalised across domains. We begin with discussion on how LLMs may be used to both pollute and uniformize information environments and how these modalities may be leveraged as mechanisms of control. We then draw attention to several areas of emerging research, each of which compounds the capabilities of LLMs as instruments of power. These include (i) persuasion through the real-time design of choice architectures in conversational interfaces (e.g., via “AI personas”), (ii) the use of LLM-agents as computational models of human agents (e.g., “silicon subjects”), (iii) the use of LLM-agents as computational models of human agent populations (e.g., “silicon societies”) and finally, (iv) the combination of LLMs with reinforcement learning to produce controllable and steerable strategic dialogue models. We draw these strands together to discuss how these areas may be combined to build LLM-based systems that serve as powerful instruments of individual, social and political control via the simulation and disingenuous "prediction" of human behaviour, intent, and action.
UR  - https://arxiv.org/pdf/2405.03813v1.pdf
N1  - Source: page_2.html
ID  - 207
ER  - 
TY  - JOUR
TI  - Can Nuanced Language Lead to More Actionable Insights? Exploring the Role of Generative AI in Analytical Narrative Structure
AU  - Setlur, V.
AU  - Birnbaum, L.
PY  - 2024
AB  - Relevant language describing trends in data can be useful for generating summaries to help with readers' takeaways. However, the language employed in these often template-generated summaries tends to be simple, ranging from describing simple statistical information (e.g., extrema and trends) without additional context and richer language to provide actionable insights. Recent advances in Large Language Models (LLMs) have shown promising capabilities in capturing subtle nuances in language when describing information. This workshop paper specifically explores how LLMs can provide more actionable insights when describing trends by focusing on three dimensions of analytical narrative structure: semantic, rhetorical, and pragmatic. Building on prior research that examines visual and linguistic signatures for univariate line charts, we examine how LLMs can further leverage the semantic dimension of analytical narratives using quantified semantics to describe shapes in trends as people intuitively view them. These semantic descriptions help convey insights in a way that leads to a pragmatic outcome, i.e., a call to action, persuasion, warning vs. alert, and situational awareness. Finally, we identify rhetorical implications for how well these generated narratives align with the perceived shape of the data, thereby empowering users to make informed decisions and take meaningful actions based on these data insights.
UR  - https://arxiv.org/pdf/2405.02763v1.pdf
N1  - Source: page_2.html
ID  - 208
ER  - 
TY  - JOUR
TI  - How the Anthropomorphism of an Artificial Intelligence Agent Affects the Correction of Misinformation: A Focus on Authenticity and Behavioral-Response System Level
AU  - Oh, J.
AU  - Lim, S.
PY  - 2024
AB  - AI agents are increasingly leveraged to counter misinformation. This study probes their impact on correction, focusing on user interaction authenticity. Examining anthropomorphism and behavioral response, it seeks to discern how perceived authenticity shapes AI agents' persuasive impact in misinformation correction. In a 2x2 experiment with 320 participants correcting misinformation about the HPV vaccine, high anthropomorphism induces greater authenticity in source, message, and interaction compared to low anthropomorphism. Additionally, perceived interaction authenticity mediates the relationship between anthropomorphism and attitudinal change, moderated by the behavioral response system level. These findings provide vital evidence underlining the importance of anthropomorphism and behavioral response in fostering authentic interactions, with practical implications.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4809351&type=2
N1  - Source: page_2.html
ID  - 209
ER  - 
TY  - JOUR
TI  - Can a Hallucinating Model help in Reducing Human “Hallucination”?
AU  - Sundaram, S.S.
AU  - Alwar, B.
PY  - 2024
AB  - The prevalence of unwarranted beliefs, spanning pseudoscience, logical fallacies, and conspiracy theories, presents substantial societal hurdles and the risk of disseminating misinformation. Utilizing established psychometric assessments, this study explores the capabilities of large language models (LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls. We undertake a philosophical inquiry, juxtaposing the rationality of humans against that of LLMs. Furthermore, we propose methodologies for harnessing LLMs to counter misconceptions, drawing upon psychological models of persuasion such as cognitive dissonance theory and elaboration likelihood theory. Through this endeavor, we highlight the potential of LLMs as personalized misinformation debunking agents.
UR  - https://arxiv.org/pdf/2405.00843v1.pdf
N1  - Source: page_2.html
ID  - 210
ER  - 
TY  - JOUR
TI  - Using Deep Q-Learning to Dynamically Toggle between Push/Pull Actions in Computational Trust Mechanisms
AU  - Lygizou, Z.
AU  - Kalles, D.
PY  - 2024
AB  - Recent work on decentralized computational trust models for open Multi Agent Systems has resulted in the development of CA, a biologically inspired model which focuses on the trustee’s perspective. This new model addresses a serious unresolved problem in existing trust and reputation models, namely the inability to handle constantly changing behaviors and agents’ continuous entry and exit from the system. In previous work, we compared CA to FIRE, a well-known trust and reputation model, and found that CA is superior when the trustor population changes, whereas FIRE is more resilient to the trustee population changes. Thus, in this paper, we investigate how the trustors can detect the presence of several dynamic factors in their environment and then decide which trust model to employ in order to maximize utility. We frame this problem as a machine learning problem in a partially observable environment, where the presence of several dynamic factors is not known to the trustor and we describe how an adaptable trustor can rely on a few measurable features so as to assess the current state of the environment and then use Deep Q Learning (DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt to a changing environment. We ran a series of simulation experiments to compare the performance of the adaptable trustor with the performance of trustors using only one model (FIRE or CA) and we show that an adaptable agent is indeed capable of learning when to use each model and, thus, perform consistently in dynamic environments.
UR  - https://arxiv.org/pdf/2404.18296v1.pdf
N1  - Source: page_2.html
ID  - 211
ER  - 
TY  - JOUR
TI  - HOW TO PARAMETERIZE ASYMMETRIC QUANTIZATION RANGES FOR QUANTIZATION-AWARE TRAINING
AU  - You, J.
AU  - Park, M.
AU  - Lee, K.
AU  - Patel, C.
AU  - Nagel, M.
PY  - 2024
AB  - This paper investigates three different parameterizations of asymmetric uniform quantization for quantization-aware training: (1) scale and offset, (2) minimum and maximum, and (3) beta and gamma. We perform a comprehensive comparative analysis of these parameterizations’ influence on quantization-aware training, using both controlled experiments and real-world large language models. Our particular focus is on their changing behavior in response to critical training hyper-parameters, bit width and learning rate. Based on our investigation, we propose best practices to stabilize and accelerate quantization-aware training with learnable asymmetric quantization ranges.
UR  - https://arxiv.org/pdf/2404.16898v1.pdf
N1  - Source: page_2.html
ID  - 212
ER  - 
TY  - JOUR
TI  - The Ethics of Advanced AI Assistants
AU  - Gabriel, I.
AU  - Manzini, A.
AU  - Keeling, G.
AU  - Isaac, W.
AU  - Manyika, J.
PY  - 2024
AB  - This paper focuses on the opportunities and the ethical and societal risks posed by advanced AI assistants. We define advanced AI assistants as artificial agents with natural language interfaces, whose function is to plan and execute sequences of actions on behalf of a user – across one or more domains – in line with the user’s expectations. The paper starts by considering the technology itself, providing an overview of AI assistants, their technical foundations and potential range of applications. It then explores questions around AI value alignment, well-being, safety and malicious uses. Extending the circle of inquiry further, we next consider the relationship between advanced AI assistants and individual users in more detail, exploring topics such as manipulation and persuasion, anthropomorphism, appropriate relationships, trust and privacy. With this analysis in place, we consider the deployment of advanced assistants at a societal scale, focusing on cooperation, equity and access, misinformation, economic impact, the environment and how best to evaluate advanced AI assistants. Finally, we conclude by providing a range of recommendations for researchers, developers, policymakers and public stakeholders.
UR  - https://arxiv.org/pdf/2404.16244v1.pdf
N1  - Source: page_2.html
ID  - 213
ER  - 
TY  - JOUR
TI  - A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI
AU  - El-Sayed, S.
AU  - Akbulut, C.
AU  - McCroskery, A.
AU  - Everitt, T.
AU  - Brown, S.
PY  - 2024
AB  - Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making. Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions. This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion. The current definitions of AI persuasion are unclear and related harms are insufficiently studied. Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion. In this paper, we lay the groundwork for the systematic study of AI persuasion. We first put forward definitions of persuasive generative AI. We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information. We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm. We then introduce a map of mechanisms that contribute to harmful persuasion. Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming. Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion.
UR  - https://arxiv.org/pdf/2404.15058v1.pdf
N1  - Source: page_2.html
ID  - 214
ER  - 
TY  - JOUR
TI  - Human-in-the-loop Learning for Dynamic Congestion Games
AU  - Li, H.
AU  - Duan, L.
PY  - 2024
AB  - Today mobile users learn and share their traffic observations via crowdsourcing platforms (e.g., Google Maps and Waze). Yet such platforms simply cater to selfish users’ myopic interests to recommend the shortest path, and do not encourage enough users to travel and learn other paths for future others. Prior studies focus on one-shot congestion games without considering users’ information learning, while our work studies how users learn and alter traffic conditions on stochastic paths in a human-in-the-loop manner. In a typical parallel routing network with one deterministic path and multiple stochastic paths, our analysis shows that the myopic routing policy (used by Google Maps and Waze) leads to severe under-exploration of stochastic paths. This results in a price of anarchy (PoA) greater than 2, as compared to the socially optimal policy achieved through optimal exploration-exploitation tradeoff in minimizing the long-term social cost. Besides, the myopic policy fails to ensure the correct learning convergence about users’ traffic hazard beliefs. To address this, we focus on informational (non-monetary) mechanisms as they are easier to implement than pricing. We first show that existing information-hiding mechanisms and deterministic path-recommendation mechanisms in Bayesian persuasion literature do not work with even PoA = ∞. Accordingly, we propose a new combined hiding and probabilistic recommendation (CHAR) mechanism to hide all information from a selected user group and provide state-dependent probabilistic recommendations to the other user group. Our CHAR mechanism successfully ensures PoA less than 45 , which cannot be further reduced by any other informational (non-monetary) mechanism. Besides the parallel network, we further extend our analysis and CHAR mechanism to more general linear path graphs with multiple intermediate nodes, and we prove that the PoA results remain unchanged. Additionally, we carry out experiments with real-world datasets to further extend our routing graphs and verify the close-to-optimal performance of our CHAR mechanism.
UR  - https://arxiv.org/pdf/2404.15599v2.pdf
N1  - Source: page_2.html
ID  - 215
ER  - 
TY  - JOUR
TI  - Resistance Against Manipulative AI: key factors and possible actions
AU  - Wilczynski, P.
AU  - Mieleszczenko-Kowszewicz, W.
AU  - Biecek, P.
PY  - 2024
AB  - If AI is the new electricity, what should we do to keep ourselves from getting electrocuted? In this work, we explore factors related to the potential of large language models (LLMs) to manipulate human decisions. We describe the results of two experiments designed to determine what characteristics of humans are associated with their susceptibility to LLM manipulation, and what characteristics of LLMs are associated with their manipulativeness potential. We explore human factors by conducting user studies in which participants answer general knowledge questions using LLM-generated hints, whereas LLM factors by provoking language models to create manipulative statements. Then, we analyze their obedience, the persuasion strategies used, and the choice of vocabulary. Based on these experiments, we discuss two actions that can protect us from LLM manipulation. In the long term, we put AI literacy at the forefront, arguing that educating society would minimize the risk of manipulation and its consequences. We also propose an ad hoc solution, a classifier that detects LLM manipulation – a Manipulation Fuse.
UR  - https://arxiv.org/pdf/2404.14230v2.pdf
N1  - Source: page_2.html
ID  - 216
ER  - 
TY  - JOUR
TI  - Counterfactual Reasoning Using Predicted Latent Personality Dimensions for Optimizing Persuasion Outcome
AU  - Zeng, D.
AU  - Legaspi, R.S.
AU  - Sun, Y.
AU  - Spirtes, P.
AU  - Zhang, K.
PY  - 2024
AB  - Customizing persuasive conversations related to the outcome of interest for specific users achieves better persuasion results. However, existing persuasive conversation systems rely on persuasive strategies and encounter challenges in dynamically adjusting dialogues to suit the evolving states of individual users during interactions. This limitation restricts the system’s ability to deliver flexible or dynamic conversations and achieve suboptimal persuasion outcomes. In this paper, we present a novel approach that tracks a user’s latent personality dimensions (LPDs) during ongoing persuasion conversation and generates tailored counterfactual utterances based on these LPDs to optimize the overall persuasion outcome. In particular, our proposed method leverages a Bi-directional Generative Adversarial Network (BiCoGAN) in tandem with a Dialogue-based Personality Prediction Regression (DPPR) model to generate counterfactual data D̃. This enables the system to formulate alternative persuasive utterances that are more suited to the user. Subsequently, we utilize the D3QN model to learn policies for optimized selection of system utterances on D̃. Experimental results we obtained from using the PersuasionForGood dataset demonstrate the superiority of our approach over the existing method, BiCoGAN. The cumulative rewards and Q-values produced by our method surpass ground truth benchmarks, showcasing the efficacy of employing counterfactual reasoning and LPDs to optimize reinforcement learning policy in online interactions.
UR  - https://arxiv.org/pdf/2404.13792v1.pdf
N1  - Source: page_2.html
ID  - 217
ER  - 
TY  - JOUR
TI  - Large Language Models are as persuasive as humans, but why? About the cognitive effort and moral-emotional language of LLM arguments
AU  - Carrasco-Farré, C.
PY  - 2024
AB  - Large Language Models (LLMs) are already as persuasive as humans. However, we know very liEle about why. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuaion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.
UR  - https://arxiv.org/pdf/2404.09329v1.pdf
N1  - Source: page_2.html
ID  - 218
ER  - 
TY  - JOUR
TI  - A biologically inspired computational trust model for open multi-agent systems which is resilient to trustor population changes
AU  - Lygizou, Z.
AU  - Kalles, D.
PY  - 2024
AB  - Current trust and reputation models continue to have significant limitations, such as the inability to deal with agents constantly entering or exiting open multi-agent systems (open MAS), as well as continuously changing behaviors. Our study is based on CA, a previously proposed decentralized computational trust model from the trustee's point of view, inspired by synaptic plasticity and the formation of assemblies in the human brain. It is designed to meet the requirements of highly dynamic and open MAS, and its main difference with most conventional trust and reputation models is that the trustor does not select a trustee to delegate a task; instead, the trustee determines whether it is qualified to successfully execute it. We ran a series of simulations to compare CA model to FIRE, a well-established, decentralized trust and reputation model for open MAS, under conditions of continuous trustee and trustor population replacement, as well as continuous change of trustees' abilities to perform tasks. The main finding is that FIRE is superior to changes in the trustee population, whereas CA is resilient to the trustor population changes. When the trustees switch performance profiles FIRE clearly outperforms despite the fact that both models' performances are significantly impacted by this environmental change. Findings lead us to conclude that learning to use the appropriate trust model, according to the dynamic conditions in effect could maximize the trustor's benefits.
UR  - https://arxiv.org/pdf/2404.10014v1.pdf
N1  - Source: page_2.html
ID  - 219
ER  - 
TY  - JOUR
TI  - Subtoxic Questions: Dive Into Attitude Change of LLM’s Response in Jailbreak Attempts
AU  - Zhang, T.
AU  - Zhao, Z.
AU  - Huang, J.
AU  - Hua, J.
AU  - Zhong, S.
PY  - 2024
AB  - As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and more attention, it is of great significance to raise a generalized research paradigm to evaluate attack strengths and a basic model to conduct subtler experiments. In this paper, we propose a novel approach by focusing on a set of target questions that are inherently more sensitive to jailbreak prompts, aiming to circumvent the limitations posed by enhanced LLM security. Through designing and analyzing these sensitive questions, this paper reveals a more effective method of identifying vulnerabilities in LLMs, thereby contributing to the advancement of LLM security. This research not only challenges existing jailbreaking methodologies but also fortifies LLMs against potential exploits.
UR  - https://arxiv.org/pdf/2404.08309v1.pdf
N1  - Source: page_2.html
ID  - 220
ER  - 
TY  - JOUR
TI  - Usability of Chatbots
AU  - Monisha, N.
AU  - Ganesh, N.V.
AU  - Supriya, D.L.P.
AU  - Adithya, V.D.S.
AU  - Sangeeta.
PY  - 2024
AB  - Chabot’s are becoming increasingly popular in various domains, such as healthcare, customer service, and education. However, to be effective, catboats must be usable and provide a positive user experience. This paper provides a brief review of the literature on the usability of Chabot’s published between 2019 and 2023. The studies reviewed suggest that different factors, such as feedback mechanisms, interface design, and Chabot type, can impact Chabot usability. The studies also highlight the potential of Chabot’s as effective tools for promoting health behavior change and identifying mental health issues in young people. However, gaps in the literature, such as the need for more studies on Chabot usability in specific domains and the need for standardized measures of Chabot usability, suggest the field of science is still growing and there are many mysteries to be explored.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4488011&type=2
N1  - Source: page_2.html
ID  - 221
ER  - 
TY  - JOUR
TI  - Sustenance Maintenance and Ailment Redressal System
AU  - Kumar, N.
AU  - Tiwari, S.
AU  - Poria, S.
AU  - Kumar, A.
AU  - Tripathi, A.
PY  - 2024
AB  - The study proposes a conceptual model of chatbot in healthcare in accordance with the Indian perspective. The traditional healthcare system that focuses on curing illnesses is well known but less talked about nowadays. We will discuss the possibilities and analyze the other research to date to obtain a persuasive, inclusive, vigilant, and smart solution. The model hence proposed we named as Sustenance Maintenance and Ailment Redressal System (SMARS). The above-coined model will be used explicitly to monitor health, detect illnesses early, and suggest healthcare methods. This system will record the health vitals on daily bases, which will help in future needs. This system will suggest doctors for check-ups of illnesses which is a very time-effective process. Moreover, this system provides a healthcare solution that is reliable, costeffective, and easily adaptable to the persisting system. Chatbot interactions in real-time are a very useful tool for health care and it should be not only useful but usual also. A variety of health issues can be addressed in real-time. Chatbot may suggest homemade remedies and quick first aid procedures for curing health at home. This paper is based on analyzing the prior research and finding the possibilities of integrating the existing system with an advanced technological tool that provides suggestions on Ayurvedic healthcare based on data and algorithms. It also combines modern technology with traditional Ayurvedic principles to offer personalized recommendations for different illnesses and healthcare conditions by using a vast database of Ayurvedic knowledge and algorithms to analyze various factors such as the individual's health condition, symptoms, dosha constitution, and lifestyle. Furthermore, this analysis suggests appropriate Ayurvedic medicines, therapies, dietary recommendations, and lifestyle modifications that support the individual’s health and wellbeing.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4492376&type=2
N1  - Source: page_2.html
ID  - 222
ER  - 
TY  - JOUR
TI  - IITK at SemEval-2024 Task 4: Hierarchical Embeddings for Detection of Persuasion Techniques in Memes
AU  - Chikoti, S.
AU  - Mehta, S.
AU  - Modi, A.
PY  - 2024
AB  - Memes are one of the most popular types of content used in an online disinformation campaign. They are primarily effective on social media platforms since they can easily reach many users. Memes in a disinformation campaign achieve their goal of influencing the users through several rhetorical and psychological techniques, such as causal oversimplification, name-calling, and smear. The SemEval 2024 Task 4 Multilingual Detection of Persuasion Technique in Memes on identifying such techniques in the memes is divided across three sub-tasks: (1) Hierarchical multi-label classification using only textual content of the meme, (2) Hierarchical multi-label classification using both, textual and visual content of the meme and (3) Binary classification of whether the meme contains a persuasion technique or not using it’s textual and visual content. This paper proposes an ensemble of Class Definition Prediction (CDP) and hyperbolic embeddings-based approaches for this task. We enhance meme classification accuracy and comprehensiveness by integrating HypEmo’s hierarchical label embeddings (Chen et al., 2023) and a multi-task learning framework for emotion prediction. We achieve a hierarchical F1-score of 0.60, 0.67, and 0.48 on the respective sub-tasks.
UR  - https://arxiv.org/pdf/2404.04520v1.pdf
N1  - Source: page_2.html
ID  - 223
ER  - 
TY  - JOUR
TI  - Automated Lane Change Behavior Prediction and Environmental Perception Based on SLAM Technology
AU  - Lei, H.
AU  - Wang, B.
AU  - Shui, Z.
AU  - Yang, P.
AU  - Liang, P.
PY  - 2024
AB  - In addition to environmental perception sensors such as cameras, radars, etc. in the automatic driving system, the external environment of the vehicle is perceived, in fact, there is also a perception sensor that has been silently dedicated in the system, that is, the positioning module. This paper explores the application of SLAM (Simultaneous Localization and Mapping) technology in the context of automatic lane change behavior prediction and environment perception for autonomous vehicles. It discusses the limitations of traditional positioning methods, introduces SLAM technology, and compares LIDAR SLAM with visual SLAM. Real-world examples from companies like Tesla, Waymo, and Mobileye showcase the integration of AI-driven technologies, sensor fusion, and SLAM in autonomous driving systems. The paper then delves into the specifics of SLAM algorithms, sensor technologies, and the importance of automatic lane changes in driving safety and efficiency. It highlights Tesla's recent update to its Autopilot system, which incorporates automatic lane change functionality using SLAM technology. The paper concludes by emphasizing the crucial role of SLAM in enabling accurate environment perception, positioning, and decision-making for autonomous vehicles, ultimately enhancing safety and driving experience.
UR  - https://arxiv.org/pdf/2404.04492v1.pdf
N1  - Source: page_2.html
ID  - 224
ER  - 
TY  - JOUR
TI  - Can Language Models Recognize Convincing Arguments?
AU  - Rescala, P.D.
AU  - Ribeiro, M.H.
AU  - Hu, T.
AU  - West, R.
PY  - 2024
AB  - The capabilities of large language models (LLMs) have raised concerns about their potential to create and propagate convincing narratives. Here, we study their performance in detecting convincing arguments to gain insights into LLMs’ persuasive capabilities without directly engaging in experimentation with humans. We extend a dataset by Durmus and Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs’ ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance. The data and code released with this paper contribute to the crucial effort of continuously evaluating and monitoring LLMs’ capabilities and potential impact. (https://go.epfl.ch/persuasion-llm)
UR  - https://arxiv.org/pdf/2404.00750v2.pdf
N1  - Source: page_2.html
ID  - 225
ER  - 
TY  - JOUR
TI  - LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models
AU  - Peng, M.
AU  - Guo, X.
AU  - Chen, X.
AU  - Wang, X.
AU  - Wang, Y.
PY  - 2024
AB  - To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as prompts for input into the LLM and employing a supervised fine-tuning technique to tailor the LLM specifically for our lane change prediction task. This allows us to utilize the LLM’s powerful common sense reasoning abilities to understand complex interactive information, thereby improving the accuracy of long-term predictions. Furthermore, we incorporate explanatory requirements into the prompts in the inference stage. Therefore, our LC-LLM model not only can predict lane change intentions and trajectories but also provides explanations for its predictions, enhancing the interpretability. Extensive experiments on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. Compared with baseline models, the performance of intention prediction can be improved by 3.1%, the root-mean-squared error (RMSE) of lateral trajectory prediction can be reduced by 19.4%, and the RMSE of longitudinal trajectory prediction can be reduced by 38.1%. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can encode comprehensive interaction information for driving behavior understanding.
UR  - https://arxiv.org/pdf/2403.18344v1.pdf
N1  - Source: page_2.html
ID  - 226
ER  - 
TY  - JOUR
TI  - A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks
AU  - DeBuse, M.
AU  - Warnick, S.
PY  - 2024
AB  - In this work we consider the impact of information spread in time-varying social networks, where agents request to follow other agents with aligned opinions while dropping ties to neighbors whose posts are too dissimilar to their own views. Opinion control and rhetorical influence has a very long history, employing various methods including education, persuasion, propaganda, marketing, and manipulation through mis-, dis-, and mal-information. The automation of opinion controllers, however, has only recently become easily deployable at a wide scale, with the advent of large language models (LLMs) and generative AI that can translate the quantified commands from opinion controllers into actual content with the appropriate nuance. Automated agents in social networks can be deployed for various purposes, such as breaking up echo chambers, bridging valuable new connections between agents, or shaping the opinions of a target population—and all of these raise important ethical concerns that deserve serious attention and thoughtful discussion and debate. This paper attempts to contribute to this discussion by considering three archetypal influencing styles observed by human drivers in these settings, comparing and contrasting the impact of these different control methods on the opinions of agents in the network. We will demonstrate the efficacy of current generative AI for generating nuanced content consistent with the command signal from automatic opinion controllers like these, and we will report on frameworks for approaching the relevant ethical considerations.
UR  - https://arxiv.org/pdf/2403.18163v1.pdf
N1  - Source: page_2.html
ID  - 227
ER  - 
TY  - JOUR
TI  - Eternagram: Probing Player Attitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure
AU  - Zhou, S.
AU  - Hendra, L.B.
AU  - Zhang, Q.
AU  - Holopainen, J.
AU  - Lc, R.
PY  - 2024
AB  - Conventional methods of assessing attitudes towards climate change are limited in capturing authentic opinions, primarily stemming from a lack of context-specific assessment strategies and an overreliance on simplistic surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome these issues by immersing participants in engaging gameplay within carefully crafted, scenario-based environments. Concurrently, advancements in AI and Natural Language Processing (NLP) show promise in enhancing the gamified testing environment, achieving this by generating context-aware, human-like dialogues that contribute to a more natural and effective assessment. Our study introduces a new technique for probing climate change attitudes by actualizing a GPT-driven chatbot system in harmony with a game design depicting a futuristic climate scenario. The correlation analysis reveals an assimilation effect, where players’ post-game climate awareness tends to align with their in-game perceptions. Key predictors of pro-climate attitudes are identified as traits like’Openness’ and’Agreeableness’, and a preference for democratic values.
UR  - https://arxiv.org/pdf/2403.18160v1.pdf
N1  - Source: page_2.html
ID  - 228
ER  - 
TY  - JOUR
TI  - Utilizing LLMs for Enhanced Argumentation and Extraction of Causal Knowledge from Scientific Literature
AU  - Wang, S.
AU  - Chen, W.
AU  - Zhang, Y.
AU  - Chen, T.
AU  - Du, J.
PY  - 2024
AB  - Current semantic extraction tools have limited performance in identifying causal relations, neglecting variations in argument quality, especially persuasive strength across different sentences. The present study proposes a five-element based (evidence cogency, concept, relation stance, claim-context relevance, conditional information) causal knowledge mining framework and automatically implements it using large language models (LLMs) to improve the understanding of disease causal mechanisms. As a result, regarding cogency evaluation, the accuracy (0.84) of the fine-tuned Llama2-7b largely exceeds the accuracy of GPT-3.5 turbo with few-shot. Regarding causal extraction, by combining PubTator and ChatGLM, the entity first-relation later extraction (recall, 0.85) outperforms the relation first-entity later means (recall, 0.76), performing great in three outer validation sets (a gestational diabetes-relevant dataset and two general biomedical datasets), aligning entities for further causal graph construction. LLMs-enabled scientific causality mining is promising in delineating the causal argument structure and understanding the underlying mechanisms of a given exposure-outcome pair.
UR  - https://www.medrxiv.org/content/10.1101/2024.03.20.24304652v2.full.pdf
N1  - Source: page_2.html
ID  - 229
ER  - 
TY  - JOUR
TI  - On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial
AU  - Salvi, F.
AU  - Ribeiro, M.H.
AU  - Gallotti, R.
AU  - West, R.
PY  - 2024
AB  - The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. In this preregistered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting. We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N = 820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p = 0.31). Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments.
UR  - https://arxiv.org/pdf/2403.14380v1.pdf
N1  - Source: page_2.html
ID  - 230
ER  - 
TY  - JOUR
TI  - Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech
AU  - Alyahya, G.
AU  - Aldayel, A.
PY  - 2024
AB  - Examining the factors that the counterspeech uses are at the core of understanding the optimal methods for confronting hate speech online. Various studies have assessed the emotional base factors used in counter speech, such as emotional empathy, offensiveness, and hostility. To better understand the counterspeech used in conversations, this study distills persuasion modes into reason, emotion, and credibility and evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) concerning racism, sexism, and religious bigotry. The evaluation covers the distinct behaviors seen with human-sourced as opposed to machine-generated counterspeech. It also assesses the interplay between the stance taken and the mode of persuasion seen in the counterspeech. Notably, we observe nuanced differences in the counterspeech persuasion modes used in open and closed interactions—especially in terms of the topic, with a general tendency to use reason as a persuasion mode to express the counterpoint to hate comments. The machine-generated counterspeech tends to exhibit an emotional persuasion mode, while human counters lean toward reason. Furthermore, our study shows that reason tends to obtain more supportive replies than other persuasion modes. The findings highlight the potential for incorporating persuasion modes into studies about countering hate speech, as they can serve as an optimal means of explainability and pave the way for the further adoption of the reply’s stance and the role it plays in assessing what comprises the optimal counterspeech.
UR  - https://arxiv.org/pdf/2403.15449v3.pdf
N1  - Source: page_2.html
ID  - 231
ER  - 
TY  - JOUR
TI  - Feedback-Driven Insight Generation and Recommendation for Health Self-Management
AU  - Susaiyah, A.
AU  - Härmä, A.
AU  - Reiter, E.
AU  - Balloccu, S.
AU  - Petković, M.
PY  - 2024
AB  - Purpose: This study aims to investigate the impact of personalized health insights generated from wearable device data on users’ health behaviors. The primary objective is to assess whether user feedback-driven algorithms enhance the relevance and effectiveness of health insights, ultimately influencing positive changes in users’ daily activities. Methods: A two-month field study was conducted with 25 healthy volunteers using Mi Band 6 wearable devices. Participants were divided into test and control groups, and the test group received personalized insights recommended by a neural network-based algorithm fine-tuned by user feedback. The data collected included various health parameters such as calories burned, step count, heart rate, heart minutes, active minutes, sleep duration, sleep time and sleep segments. Insights were provided through a Telegram chatbot and user feedback was collected through a rating system. Results: The study revealed that the test group, which considered user feedback for insight recommendations, showed a significant improvement in daily activity compared to the control group. The relevance of the insights over time, as evidenced by feedback regression trends, showed a notable increase in the test group. Additional analyzes explored the relationship between insight delivery timing, user feedback, and delays, providing insights into user engagement patterns. Conclusion: This research highlights the effectiveness of personalized health insights generated from wearable data in positively influencing user health behaviors. Incorporating feedback from users into recommendation algorithms greatly enhances the relevance and effectiveness of insights, encouraging behavioral improvements. The results emphasize the significance of timing when providing insights and propose potential areas for future investigation, such as utilizing Graph Neural Networks to improve recommendation systems. In general, personalized insights from wearables have the potential to empower individuals to manage their health and well-being effectively.
UR  - https://www.researchsquare.com/article/rs-4016799/v1.pdf
N1  - Source: page_2.html
ID  - 232
ER  - 
TY  - JOUR
TI  - SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection
AU  - Qi, P.
AU  - Yan, Z.
AU  - Hsu, W.
AU  - Lee, M.L.
PY  - 2024
AB  - Misinformation is a prevalent societal issue due to its potential high risks. Out-Of-Context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle cross-modal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model’s concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model’s discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations.
UR  - https://arxiv.org/pdf/2403.03170v1.pdf
N1  - Source: page_2.html
ID  - 233
ER  - 
TY  - JOUR
TI  - An Agent-Based Emotional Persuasion and Opponent Behavior Evaluation Model Driven by Expectation
AU  - Wu, J.
AU  - Ge, F.
AU  - Li, Y.
AU  - Wang, Z.
PY  - 2024
AB  - Automated negotiation can improve the efficiency of negotiation and has a good development prospect. However, the existing research on agent emotion modeling in negotiation is not sufficient. In order to enrich the research in this field, this paper fully considers the impact of expectation and mood on emotion to build a negotiation model. Firstly, the expectation is quantified by the expected proposal range, and the expectancy violation theory is used to simulate the agent's reaction to expectation violation. Secondly, the Plutchik's emotion wheel model is introduced to express complex emotions in negotiations, and the psychological distance function is used to quantify emotion. Finally, the characteristics of mood are fully considered to construct mood model, and the influence of mood on emotion is further considered in the process of emotion change. The experimental results show that compared with the other negotiation models, the proposed model can improve the negotiation speed and utility.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4745664&type=2
N1  - Source: page_2.html
ID  - 234
ER  - 
TY  - JOUR
TI  - Can a Funny Chatbot Make a Difference? Infusing Humor into Conversational Agent for Behavioral Intervention
AU  - Sun, X.
AU  - Teljeur, I.
AU  - Li, Z.
AU  - Bosch, J.A.
PY  - 2024
AB  - Regular physical activity is crucial for reducing the risk of non-communicable disease (NCD). With NCDs on the rise globally, there is an urgent need for effective health interventions, with chatbots emerging as a viable and cost-effective option because of limited healthcare accessibility. Although health professionals often utilize behavior change techniques (BCTs) to boost physical activity levels and enhance client engagement and motivation by affiliative humor, the efficacy of humor in chatbot-delivered interventions is not well-understood. This study conducted a randomized controlled trial to examine the impact of the generative humorous communication style in a 10-day chatbot-delivered intervention for physical activity. It further investigated if user engagement and motivation act as mediators between the communication style and changes in physical activity levels. 66 participants engaged with the chatbots across three groups (humorous, non-humorous, and no-intervention) and responded to daily ecological momentary assessment questionnaires assessing engagement, motivation, and physical activity levels. Multilevel time series analyses revealed that an affiliative humorous communication style positively impacted physical activity levels over time, with user engagement acting as a mediator in this relationship, whereas motivation did not. These findings clarify the role of humorous communication style in chatbot-delivered physical activity interventions, offering valuable insights for future development of intelligent conversational agents incorporating humor.
UR  - https://arxiv.org/pdf/2403.00365v1.pdf
N1  - Source: page_2.html
ID  - 235
ER  - 
TY  - JOUR
TI  - Classification of cyberbullying in social media using Natural Language Programming method
AU  - Vivekananth, P.
AU  - Sharma, N.
PY  - 2024
AB  - The evolution of internet describes the usage of Social Media (SM) which expands drastically. This involvement continues to increase in context with the current global epidemic since people frequently use SM platforms to vent their feelings. Similarly, the widespread adoption of SM sites like Facebook and Twitter by many organizations has raised the volume of essential individual input regarding the state of affairs, events, goods, and services. One of the major drawbacks that widespread SM usage is cyberbullying. The frequency of cyberbullying on SM platforms has raised serious concerns for people, organizations, and society at large. To minimize negative impacts due to cyberbullying on SM, early detection is essential. As a result, Sentiment Analysis (SA) utilizing Twitter data has gained prominence. Text analysis-related Natural Language Processing (NLP) and in Artificial Intelligence (AI) technologies have gained increased attention because of increasing demand for SM analysis. Moreover, the important fields utilized to proactively extract and train high-quality characteristics from low-level text involved is Machine Learning (ML). The Convolution Neural Network (CNN) models are Deep Learning (DL) methods to train data like text, picture, and video data. Improved text classification is achieved by preparing these data types using CNN as a persuasive method. To categorize the bullying text, this study discusses the Ensemble model that integrates Modified Term Frequency and Inverse Document Frequency (MTF-IDF), and Deep Neural Network (DNN) with sophisticated feature extraction approaches. From the text data, the cyberbullying feature patterns are extracted using the techniques of feature extraction.
UR  - https://www.researchsquare.com/article/rs-3990421/v1.pdf
N1  - Source: page_2.html
ID  - 236
ER  - 
TY  - JOUR
TI  - Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects
AU  - Zhang, Z.
AU  - Bai, F.
AU  - Wang, M.
AU  - Ma, C.
AU  - Yang, Y.
PY  - 2024
AB  - The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.
UR  - https://arxiv.org/pdf/2402.12907v2.pdf
N1  - Source: page_2.html
ID  - 237
ER  - 
TY  - JOUR
TI  - Understanding Fine-grained Distortions in Reports of Scientific Findings
AU  - Wührl, A.
AU  - Wright, D.
AU  - Klinger, R.
AU  - Augenstein, I.
PY  - 2024
AB  - Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions. Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial. Prior work focused on individual aspects of distortions or worked with unpaired data. In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt. four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes in these characteristics in both human-annotated and large-scale unlabeled data. Our results show that scientific findings frequently undergo subtle distortions when reported. Tweets distort findings more often than science news reports. Detecting fine-grained distortions automatically poses a challenging task. In our experiments, fine-tuned task-specific models consistently outperform few-shot LLM prompting.
UR  - https://arxiv.org/pdf/2402.12431v1.pdf
N1  - Source: page_2.html
ID  - 238
ER  - 
TY  - JOUR
TI  - Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru
AU  - Jelínek, M.
AU  - Nichols, E.
AU  - Gomez, R.
PY  - 2024
AB  - This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method [13] for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru’s emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru’s liveliness, interactivity, and neutrality. Additionally, our study contributes to the broader understanding of dialogue design in social robotics, offering practical insights for future developments in the field.
UR  - https://arxiv.org/pdf/2402.11569v1.pdf
N1  - Source: page_2.html
ID  - 239
ER  - 
TY  - JOUR
TI  - Generalized Principal-Agent Problem with a Learning Agent
AU  - Lin, T.
AU  - Chen, Y.
PY  - 2024
AB  - Classic principal-agent problems such as Stackelberg games, contract design, and Bayesian persuasion, often assume that the agent is able to best respond to the principal's committed strategy. We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) If the agent uses contextual no-regret learning algorithms with regret Reg(T), then the principal can guarantee utility at least (Equation presented) where U ∗ is the principal's optimal utility in the classic model with a best-responding agent. (2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret SReg(T), then the principal cannot obtain utility more than U ∗ + O(SReg(T)T ). But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than U ∗. These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information.
UR  - https://arxiv.org/pdf/2402.09721v5.pdf
N1  - Source: page_2.html
ID  - 240
ER  - 
TY  - JOUR
TI  - Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions
AU  - Xu, R.
PY  - 2024
AB  - A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent’s information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm’s prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Common restrictions on algorithms, such as keeping a “human-in-the-loop” or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfectly aligned agents and state-revealing signals. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.
UR  - https://arxiv.org/pdf/2402.09384v1.pdf
N1  - Source: page_2.html
ID  - 241
ER  - 
TY  - JOUR
TI  - The Last JITAI? Exploring Large Language Models for Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Conceptual Cardiac Rehabilitation Setting
AU  - Haag, D.
AU  - Kumar, D.
AU  - Gruber, S.
AU  - Schmidt, A.
AU  - Smeddinck, J.D.
PY  - 2024
AB  - We evaluated the viability of using Large Language Models (LLMs) to trigger and personalize content in Just-in-Time Adaptive Interventions (JITAIs) in digital health. As an interaction pattern representative of context-aware computing, JITAIs are being explored for their potential to support sustainable behavior change, adapting interventions to an individual’s current context and needs. Challenging traditional JITAI implementation models, which face severe scalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs in the use case of heart-healthy activity in cardiac rehabilitation. Using three personas representing patients affected by CVD with varying severeness and five context sets per persona, we generated 450 JITAI decisions and messages. These were systematically evaluated against those created by 10 laypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated JITAIs surpassed human-generated intervention suggestions, outperforming both LayPs and HCPs across all metrics (i.e., appropriateness, engagement, effectiveness, and professionalism). These results highlight the potential of LLMs to enhance JITAI implementations in personalized health interventions, demonstrating how generative AI could revolutionize context-aware computing.
UR  - https://arxiv.org/pdf/2402.08658v3.pdf
N1  - Source: page_2.html
ID  - 242
ER  - 
TY  - JOUR
TI  - Auditing Counterfire: Evaluating Advanced Counter-argument Generation with Evidence and Style
AU  - Verma, P.
AU  - Jaidka, K.
AU  - Churina, S.
PY  - 2024
AB  - We audited large language models (LLMs) for their ability to create evidence-based and stylistic counter-arguments to posts from the Reddit ChangeMyView dataset. We benchmarked their rhetorical quality across a host of qualitative and quantitative metrics and then ultimately evaluated them on their persuasive abilities as compared to human counter-arguments. Our evaluation is based on Counterfire: a new dataset of 32,000 counter-arguments generated from large language models (LLMs): GPT-3.5 Turbo and Koala and their fine-tuned variants, and PaLM 2, with varying prompts for evidence use and argumentative style. GPT-3.5 Turbo ranked highest in argument quality with strong paraphrasing and style adherence, particularly in 'reciprocity' style arguments. However, the stylistic counter-arguments still fall short of human persuasive standards, where people also preferred reciprocal to evidence-based rebuttals. The findings suggest that a balance between evidentiality and stylistic elements is vital to a compelling counter-argument. We close with a discussion of future research directions and implications for evaluating LLM outputs.
UR  - https://arxiv.org/pdf/2402.08498v4.pdf
N1  - Source: page_2.html
ID  - 243
ER  - 
TY  - JOUR
TI  - Prompted Contextual Vectors for Spear-Phishing Detection
AU  - Nahmias, D.
AU  - Engelberg, G.
AU  - Klein, D.
AU  - Shabtai, A.
PY  - 2024
AB  - Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email’s content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include a novel document vectorization method utilizing LLM reasoning, a publicly available dataset of high-quality spear-phishing emails, and the demonstrated effectiveness of our method in detecting such emails. This methodology can be utilized for various document classification tasks, particularly in adversarial problem domains.
UR  - https://arxiv.org/pdf/2402.08309v3.pdf
N1  - Source: page_2.html
ID  - 244
ER  - 
TY  - JOUR
TI  - Debating with More Persuasive LLMs Leads to More Truthful Answers
AU  - Khan, A.
AU  - Hughes, J.
AU  - Valentine, D.
AU  - Rocktäschel, T.
AU  - Perez, E.
PY  - 2024
AB  - Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information but are otherwise as capable. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. On the QuALITY comprehension task, we find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.
UR  - https://arxiv.org/pdf/2402.06782v4.pdf
N1  - Source: page_2.html
ID  - 245
ER  - 
TY  - JOUR
TI  - The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends
AU  - Chen, M.
AU  - Guo, B.
AU  - Wang, H.
AU  - Pan, Y.
AU  - Yu, Z.
PY  - 2024
AB  - Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent dialogue systems. Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic dialogue systems. Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion. Consequently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent), which incorporates cognitive strategies to achieve persuasive targets through conversation, has become a predominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Then we propose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers.
UR  - https://arxiv.org/pdf/2402.04631v1.pdf
N1  - Source: page_2.html
ID  - 246
ER  - 
TY  - JOUR
TI  - Multi-Sender Persuasion: A Computational Perspective
AU  - Hossain, S.
AU  - Wang, T.
AU  - Lin, T.
AU  - Parkes, D.C.
AU  - Xu, H.
PY  - 2024
AB  - We consider the multi-sender persuasion problem: multiple players with informational advantage signal to convince a single self-interested actor to take certain actions. This problem generalizes the seminal Bayesian Persuasion framework and is ubiquitous in computational economics, multiagent learning, and multi-objective machine learning. The core solution concept here is the Nash equilibrium of senders’ signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender’s best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game’s non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class of economic problems.
UR  - https://arxiv.org/pdf/2402.04971v4.pdf
N1  - Source: page_2.html
ID  - 247
ER  - 
TY  - JOUR
TI  - Shadowcast: Stealthy Data Poisoning Attacks against Vision-Language Models
AU  - Xu, Y.
AU  - Yao, J.
AU  - Shu, M.
AU  - Goldstein, T.
AU  - Huang, F.
PY  - 2024
AB  - Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs’ susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging VLMs’ text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker’s intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.
UR  - https://arxiv.org/pdf/2402.06659v2.pdf
N1  - Source: page_2.html
ID  - 248
ER  - 
TY  - JOUR
TI  - Can LLMs Replace Economic Choice Prediction Labs? The Case of Language-based Persuasion Games
AU  - Shapira, E.
AU  - Madmon, O.
AU  - Tennenholtz, M.
AU  - Reichart, R.
PY  - 2024
AB  - Human choice prediction in economic contexts is crucial for applications in marketing, finance, public policy, and more. This task, however, is often constrained by the difficulties in acquiring human choice data. With most experimental economics studies focusing on simple choice settings, the AI community has explored whether LLMs can substitute for humans in these predictions and examined more complex experimental economics settings. However, a key question remains: can LLMs generate training data for human choice prediction? We explore this in language-based persuasion games, a complex economic setting involving natural language in strategic interactions. Our experiments show that models trained on LLMgenerated data can effectively predict human behavior in these games and even outperform models trained on actual human data.1
UR  - https://arxiv.org/pdf/2401.17435v4.pdf
N1  - Source: page_2.html
ID  - 249
ER  - 
TY  - JOUR
TI  - Multiscale Parallel Tempering for Fast Sampling on Redistricting Plans
AU  - Chuang, G.
AU  - Herschlag, G.
AU  - Mattingly, J.
PY  - 2024
AB  - When auditing a redistricting plan, a persuasive method is to compare the plan with an ensemble of neutrally drawn redistricting plans. Ensembles are generated via algorithms that sample distributions on balanced graph partitions. To audit the partisan difference between the ensemble and a given plan, one must ensure that the non-partisan criteria are matched so that we may conclude that partisan differences come from bias rather than, for example, levels of compactness or differences in community preservation. Certain sampling algorithms allow one to explicitly state the policy-based probability distribution on plans, however, these algorithms have shown poor mixing times for large graphs (i.e. redistricting spaces) for all but a few specialized measures. In this work, we generate a multiscale parallel tempering approach that makes local moves at each scale. The local moves allow us to adopt a wide variety of policy-based measures. We examine our method in the state of Connecticut and succeed at achieving fast mixing on a policy-based distribution that has never before been sampled at this scale. Our algorithm shows promise to expand to a significantly wider class of measures that will (i) allow for more principled and situation-based comparisons and (ii) probe for the typical partisan impact that policy can have on redistricting.MSC Codes 60J10, 91G60
UR  - https://arxiv.org/pdf/2401.17455v1.pdf
N1  - Source: page_2.html
ID  - 250
ER  - 
TY  - JOUR
TI  - “YOU TELL ME”: A DATASET OF GPT-4-BASED BEHAVIOUR CHANGE SUPPORT CONVERSATIONS
AU  - Meyer, S.
AU  - Elsweiler, D.
PY  - 2024
AB  - Conversational agents are increasingly used to address emotional needs on top of information needs. One use case of increasing interest are counselling-style mental health and behaviour change interventions, with large language model (LLM)-based approaches becoming more popular. Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts. To address this issue, we share a dataset containing text-based user interactions related to behaviour change with two GPT-4-based conversational agents collected in a preregistered user study. This dataset includes conversation data, user language analysis, perception measures, and user feedback for LLM-generated turns, and can offer valuable insights to inform the design of such systems based on real interactions.
UR  - https://arxiv.org/pdf/2401.16167v2.pdf
N1  - Source: page_2.html
ID  - 251
ER  - 
TY  - JOUR
TI  - Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers
AU  - Gomaa, A.
AU  - Feld, M.
AU  - Reyes, G.
AU  - Krüger, A.
PY  - 2024
AB  - The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers’ gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we propose IcRegress, a novel regression-based incremental learning approach that adapts to changing behavior and the unique characteristics of drivers engaged in the dual task of driving and referencing objects. We suggest a more personalized and adaptable solution for multimodal gestural interfaces, employing continuous lifelong learning to enhance driver experience, safety, and convenience. Our approach was evaluated using an outside-the-vehicle object referencing use case, highlighting the superiority of the incremental learning models adapted over a single trained model across various driver traits such as handedness, driving experience, and numerous driving conditions. Finally, to facilitate reproducibility, ease deployment, and promote further research, we offer our approach as an open-source framework at https://github.com/amrgomaaelhady/IcRegress.
UR  - https://arxiv.org/pdf/2401.16123v2.pdf
N1  - Source: page_2.html
ID  - 252
ER  - 
TY  - JOUR
TI  - Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks
AU  - Nofshin, E.
AU  - Swaroop, S.
AU  - Pan, W.
AU  - Murphy, S.
AU  - Doshi-Velez, F.
PY  - 2024
AB  - Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Introducing a notion of MDP equivalence specific to BMRL, we theoretically and empirically show that AI planning with our human models can lead to helpful policies on a wide range of more complex, ground-truth humans.
UR  - https://arxiv.org/pdf/2401.14923v1.pdf
N1  - Source: page_2.html
ID  - 253
ER  - 
TY  - JOUR
TI  - Consolidating Strategies for Countering Hate Speech Using Persuasive Dialogues
AU  - Saha, S.
AU  - Srihari, R.
PY  - 2024
AB  - Hateful comments are prevalent on social media platforms. Although tools for automatically detecting, flagging, and blocking such false, offensive, and harmful content online have lately matured, such reactive and brute force methods alone provide short-term and superficial remedies while the perpetrators persist. With the public availability of large language models which can generate articulate synthetic and engaging content at scale, there are concerns about the rapid growth of dissemination of such malicious content on the web. There is now a need to focus on deeper, long-term solutions that involve engaging with the human perpetrator behind the source of the content to change their viewpoint or at least bring down the rhetoric using persuasive means. To do that, we propose defining and experimenting with controllable strategies for generating counterarguments to hateful comments in online conversations. We experiment with controlling response generation using features based on (i) argument structure and reasoning-based Walton argument schemes, (ii) counter-argument speech acts, and (iii) human characteristics-based qualities such as Big-5 personality traits and human values. Using automatic and human evaluations, we determine the best combination of features that generate fluent, argumentative, and logically sound arguments for countering hate. We further share the developed computational models for automatically annotating text with such features, and a silver-standard annotated version of an existing hate speech dialog corpora.
UR  - https://arxiv.org/pdf/2401.07810v1.pdf
N1  - Source: page_2.html
ID  - 254
ER  - 
TY  - JOUR
TI  - How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs
AU  - Zeng, Y.
AU  - Lin, H.
AU  - Zhang, J.
AU  - Jia, R.
AU  - Shi, W.
PY  - 2024
AB  - Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over 92% on Llama 2-7b Chat, GPT-3.5, and GPT-4 in 10 trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs1.
UR  - https://arxiv.org/pdf/2401.06373v1.pdf
N1  - Source: page_2.html
ID  - 255
ER  - 
TY  - JOUR
TI  - A Deep Learning Representation of Spatial Interaction Model for Resilient Spatial Planning of Community Business Clusters
AU  - Hao, H.
AU  - Wang, Y.
PY  - 2024
AB  - The increasing risks posed by adverse events, such as extreme climatic events and pandemics, have negatively impacted the vitality of community businesses. These challenges necessitate resilient strategies to empower community businesses to adapt to the various adversities. For example, scholars have advocated for improved spatial planning to accommodate customers’ changing behaviors during adverse events. However, existing Spatial Interaction Models (SIMs) are limited in capturing the complex and context-aware interactions between business clusters and trade areas. To address this limitation, we propose a SIM-GAT model to predict spatio-temporal visitation flows between community business clusters and their trade areas. The model innovatively represents the integrated system of business clusters, trade areas, and transportation infrastructure within an urban region using a connected graph. Then, a graph-based deep learning model, i.e., Graph AttenTion network (GAT), is used to capture the complexity and interdependencies of business clusters. We developed this model with data collected from the Miami metropolitan area in Florida. We then demonstrated its effectiveness in capturing varying attractiveness of business clusters to different residential neighborhoods and across scenarios with an eXplainable AI approach. We contribute a novel method supplementing conventional SIMs to predict and analyze the dynamics of inter-connected community business clusters. The analysis results can inform data-evidenced and place-specific planning strategies helping community business clusters better accommodate their customers across scenarios, and hence improve the resilience of community businesses.
UR  - https://arxiv.org/pdf/2401.04849v1.pdf
N1  - Source: page_2.html
ID  - 256
ER  - 
TY  - JOUR
TI  - Human Delegation Behavior in Human-AI Collaboration: The Effect of Contextual Information
AU  - Spitzer, P.
AU  - Holstein, J.
AU  - Hemmer, P.
AU  - Martin, D.
AU  - Satzger, G.
PY  - 2024
AB  - The integration of artificial intelligence (AI) into human decision-making processes at the workplace presents both opportunities and challenges. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances of decision tasks to AI. However, enabling humans to delegate instances effectively requires them to assess several factors. One key factor is the analysis of both their own capabilities and those of the AI in the context of the given task. In this work, we conduct a behavioral study to explore the effects of providing contextual information to support this delegation decision. Specifically, we investigate how contextual information about the AI and the task domain influence humans’ delegation decisions to an AI and their impact on the human-AI team performance. Our findings reveal that access to contextual information significantly improves human-AI team performance in delegation settings. Finally, we show that the delegation behavior changes with the different types of contextual information. Overall, this research advances the understanding of computer-supported, collaborative work and provides actionable insights for designing more effective collaborative systems.
UR  - https://arxiv.org/pdf/2401.04729v3.pdf
N1  - Source: page_2.html
ID  - 257
ER  - 
TY  - JOUR
TI  - The Persuasive Power of Large Language Models
AU  - Breum, S.M.
AU  - Egdal, D.V.
AU  - Mortensen, V.G.
AU  - Møller, A.G.
AU  - Aiello, L.M.
PY  - 2023
AB  - The increasing capability of Large Language Models to act as human-like social agents raises two important questions in the area of opinion dynamics. First, whether these agents can generate effective arguments that could be injected into the online discourse to steer the public opinion. Second, whether artificial agents can interact with each other to reproduce dynamics of persuasion typical of human social systems, opening up opportunities for studying synthetic social systems as faithful proxies for opinion dynamics in human populations. To address these questions, we designed a synthetic persuasion dialogue scenario on the topic of climate change, where a ‘convincer’ agent generates a persuasive argument for a ‘skeptic’ agent, who subsequently assesses whether the argument changed its internal opinion state. Different types of arguments were generated to incorporate different linguistic dimensions underpinning psycho-linguistic theories of opinion change. We then asked human judges to evaluate the persuasiveness of machine-generated arguments. Arguments that included factual knowledge, markers of trust, expressions of support, and conveyed status were deemed most effective according to both humans and agents, with humans reporting a marked preference for knowledge-based arguments. Our experimental framework lays the groundwork for future in-silico studies of opinion dynamics, and our findings suggest that artificial agents have the potential of playing an important role in collective processes of opinion formation in online social media.
UR  - https://arxiv.org/pdf/2312.15523v1.pdf
N1  - Source: page_2.html
ID  - 258
ER  - 
TY  - JOUR
TI  - The Earth is Flat because...: Investigating LLMs’ Belief towards Misinformation via Persuasive Conversation
AU  - Xu, R.
AU  - Lin, B.S.
AU  - Yang, S.
AU  - Xu, W.
AU  - Qiu, H.
PY  - 2023
AB  - Large language models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs’ susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs’ belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs’ correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies1
UR  - https://arxiv.org/pdf/2312.09085v5.pdf
N1  - Source: page_2.html
ID  - 259
ER  - 
TY  - JOUR
TI  - LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models
AU  - Abdulhai, M.
AU  - White, I.
AU  - Snell, C.
AU  - Xu, K.
AU  - Levine, S.
PY  - 2023
AB  - Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.
UR  - https://arxiv.org/pdf/2311.18232v1.pdf
N1  - Source: page_2.html
ID  - 260
ER  - 
TY  - JOUR
TI  - Algorithmic Persuasion Through Simulation
AU  - Harris, K.
AU  - Immorlica, N.
AU  - Lucier, B.
AU  - Slivkins, A.
PY  - 2023
AB  - We study a Bayesian persuasion game where a sender wants to persuade a receiver to take a binary action, such as purchasing a product. The sender is informed about the (real-valued) state of the world, such as the quality of the product, but only has limited information about the receiver’s beliefs and utilities. Motivated by customer surveys, user studies, and recent advances in AI, we allow the sender to learn more about the receiver by querying an oracle that simulates the receiver’s behavior. After a fixed number of queries, the sender commits to a messaging policy and the receiver takes the action that maximizes her expected utility given the message she receives. We characterize the sender’s optimal messaging policy given any distribution over receiver types. We then design a polynomial-time querying algorithm that optimizes the sender’s expected utility in this game. We also consider approximate oracles, more general query structures, and costly queries.
UR  - https://arxiv.org/pdf/2311.18138v5.pdf
N1  - Source: page_2.html
ID  - 261
ER  - 
TY  - JOUR
TI  - GraphPro: Graph Pre-training and Prompt Learning for Recommendation
AU  - Yang, Y.
AU  - Xia, L.
AU  - Luo, D.
AU  - Lin, K.
AU  - Huang, C.
PY  - 2023
AB  - GNN-based recommendation systems have excelled at capturing complex user-item interactions through multi-hop message passing. Nevertheless, these methods often fail to account for the dynamic nature of user-item interactions, leading to challenges in adapting to changes in user preferences and the distribution of new data. Consequently, their scalability and performance in real-world dynamic settings are constrained. In our study, we introduce GraphPro, a framework that merges dynamic graph pre-training with prompt learning in a parameter-efficient manner. This innovative blend enables GNNs to adeptly grasp both enduring user preferences and transient behavior changes, thereby providing precise and up-to-date recommendations. GraphPro tackles the issue of changing user preferences through the integration of a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN architecture. The temporal prompt mechanism imprints time-related information onto user-item interactions, equipping the model to inherently assimilate temporal dynamics, while the graph-structural prompt learning mechanism allows for the application of pre-trained insights to new behavior dynamics without continuous retraining. We also introduce a dynamic evaluation framework for recommendations to better reflect real-world situations and narrow the offline-online discrepancy. Our comprehensive experiments, including deployment in a large-scale industrial context, demonstrate the effortless plug-in scalability of GraphPro alongside various leading recommenders, underscoring the superiority of GraphPro in effectiveness, robustness, and efficiency. The implementation details and source code of our GraphPro are available in the repository at https://github.com/HKUDS/GraphPro.
UR  - https://arxiv.org/pdf/2311.16716v5.pdf
N1  - Source: page_2.html
ID  - 262
ER  - 
TY  - JOUR
TI  - The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry
AU  - Shin, M.
AU  - Kim, J.
AU  - Shin, J.
PY  - 2023
AB  - Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. We analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We find that LLM usage is associated with an increased likelihood of obtaining relief from financial firms. To investigate this relationship, we employ an instrumental variable approach to mitigate endogeneity concerns around LLM adoption. Although instrumental variables suggest a potential causal link, they cannot fully capture all unobserved heterogeneity. To further establish this causal relationship, we conducted controlled experiments, which support that LLMs can enhance the clarity and persuasiveness of consumer narratives, thereby increasing the likelihood of obtaining relief. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.
UR  - https://arxiv.org/pdf/2311.16466v4.pdf
N1  - Source: page_2.html
ID  - 263
ER  - 
TY  - JOUR
TI  - Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles
AU  - Singh, S.
AU  - Abri, F.
AU  - Namin, A.S.
PY  - 2023
AB  - With the recent advent of Large Language Models (LLMs), such as ChatGPT from OpenAI, BARD from Google, Llama2 from Meta, and Claude from Anthropic AI, gain widespread use, ensuring their security and robustness is critical. The widespread use of these language models heavily relies on their reliability and proper usage of this fascinating technology. It is crucial to thoroughly test these models to not only ensure its quality but also possible misuses of such models by potential adversaries for illegal activities such as hacking. This paper presents a novel study focusing on exploitation of such large language models against deceptive interactions. More specifically, the paper leverages widespread and borrows well-known techniques in deception theory to investigate whether these models are susceptible to deceitful interactions. This research aims not only to highlight these risks but also to pave the way for robust countermeasures that enhance the security and integrity of language models in the face of sophisticated social engineering tactics. Through systematic experiments and analysis, we assess their performance in these critical security domains. Our results demonstrate a significant finding in that these large language models are susceptible to deception and social engineering attacks.
UR  - https://arxiv.org/pdf/2311.14876v1.pdf
N1  - Source: page_2.html
ID  - 264
ER  - 
TY  - JOUR
TI  - Adaptive Interventions with User-Defined Goals for Health Behavior Change
AU  - Mandyam, A.
AU  - Jörke, M.
AU  - Denton, W.
AU  - Engelhardt, B.E.
AU  - Brunskill, E.
PY  - 2023
AB  - Promoting healthy lifestyle behaviors remains a major public health concern, particularly due to their crucial role in preventing chronic conditions such as cancer, heart disease, and type 2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable health behavior change promotion. Researchers are increasingly exploring adaptive algorithms that personalize interventions to each person’s unique context. However, in empirical studies, mobile health applications often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Tailoring advice to a person’s unique goals, preferences, and life circumstances is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. To address this, we introduce a new Thompson sampling algorithm that can accommodate personalized reward functions (i.e., goals, preferences, and constraints), while also leveraging data sharing across individuals to more quickly be able to provide effective recommendations. We prove that our modification incurs only a constant penalty on cumulative regret while preserving the sample complexity benefits of data sharing. We present empirical results on synthetic and semi-synthetic physical activity simulators, where in the latter we conducted an online survey to solicit preference data relating to physical activity, which we use to construct realistic reward models that leverages historical data from another study. Our algorithm achieves substantial performance improvements compared to baselines that do not share data or do not optimize for individualized rewards.
UR  - https://arxiv.org/pdf/2311.09483v4.pdf
N1  - Source: page_2.html
ID  - 265
ER  - 
TY  - JOUR
TI  - Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models
AU  - Stepputtis, S.
AU  - Campbell, J.
AU  - Xie, Y.
AU  - Lewis, C.M.
AU  - Sycara, K.P.
PY  - 2023
AB  - Deception and persuasion play a critical role in long-horizon dialogues between multiple parties, especially when the interests, goals, and motivations of the participants are not aligned. Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues. To this end, we explore the game of Avalon: The Resistance, a social deduction game in which players must determine each other’s hidden identities to complete their team’s objective. We introduce an online testbed and a dataset containing 20 carefully collected and labeled games among human players that exhibit long-horizon deception in a cooperative-competitive setting. We discuss the capabilities of LLMs to utilize deceptive long-horizon conversations between six human players to determine each player’s goal and motivation. Particularly, we discuss the multimodal integration of the chat between the players and the game’s state that grounds the conversation, providing further insights into the true player identities. We find that even current state-of-the-art LLMs do not reach human performance, making our dataset a compelling benchmark to investigate the decision-making and language-processing capabilities of LLMs. Our dataset and online testbed can be found at our project website: https://sstepput.github.io/Avalon-NLU/
UR  - https://arxiv.org/pdf/2311.05720v1.pdf
N1  - Source: page_2.html
ID  - 266
ER  - 
TY  - JOUR
TI  - AI CHATBOT FOR GENERATING EPISODIC FUTURE THINKING (EFT) CUE TEXTS FOR HEALTH
AU  - Ahmadi, S.
AU  - Fox, E.A.
PY  - 2023
AB  - We describe an AI-powered chatbot to aid with health improvement by generating Episodic Future Thinking (EFT) cue texts that should reduce delay discounting. In prior studies, EFT has been shown to address maladaptive health behaviors. Those studies involved participants, working with researchers, vividly imagining future events, and writing a description that they subsequently will frequently review, to ensure a shift from an inclination towards immediate rewards. That should promote behavior change, aiding in health tasks such as treatment adherence and lifestyle modifications. The AI chatbot is designed to guide users in generating personalized EFTs, automating the current labor-intensive interview-based process. This can enhance the efficiency of EFT interventions and make them more accessible, targeting specifically those with limited educational backgrounds or communication challenges. By leveraging AI for EFT intervention, we anticipate broadened access and improved health outcomes across diverse populations.
UR  - https://arxiv.org/pdf/2311.06300v1.pdf
N1  - Source: page_2.html
ID  - 267
ER  - 
TY  - JOUR
TI  - ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text
AU  - Hasanain, M.
AU  - Alam, F.
AU  - Mubarak, H.
AU  - Da San Martino, G.
AU  - Freihat, A.A.
PY  - 2023
AB  - We present an overview of the ArAIEval shared task, organized as part of the first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two tasks over Arabic text: (i) persuasion technique detection, focusing on identifying persuasion techniques in tweets and news articles, and (ii) disinformation detection in binary and multiclass setups over tweets. A total of 20 teams participated in the final evaluation phase, with 14 and 16 teams participating in Tasks 1 and 2, respectively. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further give a brief overview of the participating systems. All datasets and evaluation scripts from the shared task are released to the research community. We hope this will enable further research on these important tasks in Arabic.MSC Codes 68T50
UR  - https://arxiv.org/pdf/2311.03179v1.pdf
N1  - Source: page_2.html
ID  - 268
ER  - 
TY  - JOUR
TI  - Impact of a Health Action Process Approach model-based mobile health intervention on health behaviours of brucella patients: protocol for an intervention randomized controlled trial
AU  - Wang, J.
AU  - Jiang, Y.
AU  - Yu, Z.
AU  - Chen, J.
AU  - Wu, Y.
PY  - 2023
AB  - Background Brucellosis, one of the most common zoonotic diseases globally, is a serious public health problem. Effective health management is crucial in helping brucellosis patients develop and master self-management behavioural competencies. In recent years, mHealth has emerged as a popular and highly effective intervention for improving health-related outcomes for individuals with brucellosis. This study aims to explore the use of mHealth intervention, guided by the Health Action Process Approach theoretical model, to enhance the self-management level of brucellosis patients. Methods This study will be a single-blind single-centre randomized controlled trial conducted at the Fourth Central Hospital of Baoding City, Hebei Province. The study was approved by the Ethics Committee of the Fourth Central Hospital of Baoding City. The registration number of this experiment is ChiCTR2200064320. The estimated sample size will be 58 participants. Participants will be randomly assigned to either the mobile health intervention group or the routine education group. The mobile health education intervention will be delivered through an artificial intelligence terminal WeChat platform, while the routine management group will receive routine discharge education and a brochure on "Brucella Health Education". The primary outcome indicators will be the rate of health behaviour formation and health literacy. Secondary outcomes will be the measurement of psychological variables and behaviour change. Questionnaires were administered at baseline and at 3, 6, 9, and 12 months of follow-up to assess changes in health behaviour formation and health literacy among the respondents. Discussion This study will help to fill the research gap of postoperative health management of brucellosis patients in China based on health theory guidance. It will be the first health intervention study in China to apply the HAPA theoretical model as a guide for a group of brucellosis patients. The purpose of this study is to evaluate the utility of the HAPA model as a theoretical guide for improving health behaviors in patients with brucellosis and to confirm the validity of the HAPA theory as an effective theory to guide the improvement of health behaviors in patients.
UR  - https://www.researchsquare.com/article/rs-3505558/v1.pdf
N1  - Source: page_2.html
ID  - 269
ER  - 
TY  - JOUR
TI  - FLTracer: Accurate Poisoning Attack Provenance in Federated Learning
AU  - Zhang, X.
AU  - Liu, Q.
AU  - Ba, Z.
AU  - Lu, L.
AU  - Ren, K.
PY  - 2023
AB  - Federated Learning (FL) is a promising distributed learning approach that enables multiple clients to collaboratively train a shared global model. However, recent studies show that FL is vulnerable to various poisoning attacks, which can degrade the performance of global models or introduce backdoors into them. In this paper, we first conduct a comprehensive study on prior FL attacks and detection methods. The results show that all existing detection methods are only effective against limited and specific attacks. Most detection methods suffer from high false positives, which lead to significant performance degradation, especially in not independent and identically distributed (non-IID) settings. To address these issues, we propose FLTracer, the first FL attack provenance framework to accurately detect various attacks and trace the attack time, objective, type, and poisoned location of updates. Different from existing methodologies that rely solely on cross-client anomaly detection, we propose a Kalman filter-based cross-round detection to identify adversaries by seeking the behavior changes before and after the attack. Thus, this makes it resilient to data heterogeneity and is effective even in non-IID settings. To further improve the accuracy of our detection method, we employ four novel features and capture their anomalies with the joint decisions. Extensive evaluations show that FLTracer achieves an average true positive rate of over 96.88% at an average false positive rate of less than 2.67%, significantly outperforming SOTA detection methods.
UR  - https://arxiv.org/pdf/2310.13424v1.pdf
N1  - Source: page_2.html
ID  - 270
ER  - 
TY  - JOUR
TI  - MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention
AU  - Wu, R.
AU  - Liu, Y.
AU  - Wang, Y.
AU  - Chen, L.
AU  - Shi, Y.
PY  - 2023
AB  - Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users’ physical contexts and mental states. We first conducted a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leveraged large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We developed MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users’ in-the-moment app usage behaviors, physical contexts, mental states, goals & habits as input, and generates personalized and dynamic persuasive content with appropriate persuasion strategies. We conducted a 5-week field experiment (N=25) to compare MindShift with its simplified version (remove mental states) and baseline techniques (fixed reminder). The results show that MindShift improves intervention acceptance rates by 4.7-22.5% and reduces smartphone usage duration by 7.4-9.8%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy scale scores. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains.MSC Codes 68U35
UR  - https://arxiv.org/pdf/2309.16639v2.pdf
N1  - Source: page_2.html
ID  - 271
ER  - 
TY  - JOUR
TI  - Disinformation Detection: An Evolving Challenge in the Age of LLMs
AU  - Jiang, B.
AU  - Tan, Z.
AU  - Nirmal, A.
AU  - Liu, H.
PY  - 2023
AB  - The advent of generative Large Language Models (LLMs) such as ChatGPT has catalyzed transformative advancements across multiple domains. However, alongside these advancements, they have also introduced potential threats. One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system. This work aims to address this issue by answering three research questions: (1) To what extent can the current disinformation detection technique reliably detect LLM-generated disinformation? (2) If traditional techniques prove less effective, can LLMs themself be exploited to serve as a robust defense against advanced disinformation? and, (3) Should both these strategies falter, what novel approaches can be proposed to counter this burgeoning threat effectively? A holistic exploration for the formation and detection of disinformation is conducted to foster this line of research.
UR  - https://arxiv.org/pdf/2309.15847v1.pdf
N1  - Source: page_2.html
ID  - 272
ER  - 
TY  - JOUR
TI  - Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric
AU  - Tiwari, A.
AU  - Sinan, M.
AU  - Roy, K.
AU  - Saha, S.
AU  - Bhattacharyya, P.
PY  - 2023
AB  - Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant. These lexical-based metrics, e.g., cross-entropy and BLEU, have two key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate “nice” and “rice” for “good”, (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. We also formulate an evaluation metric called Dialuation, incorporating both context and semantic relevance. We experimented with both non-pretrained and pre-trained models on two dialogue corpora, encompassing task-oriented and open-domain scenarios. We found that the dialogue generation models trained with SemTextualLogue loss attained superior performance compared to the traditional cross-entropy loss function. The findings establish that the effective training of a dialogue generation model hinges significantly on incorporating semantics and context. This pattern is also mirrored in the introduced Dialuation metric, where the consideration of both context and semantics correlates more strongly with human evaluation compared to traditional metrics.
UR  - https://arxiv.org/pdf/2309.05804v2.pdf
N1  - Source: page_2.html
ID  - 273
ER  - 
TY  - JOUR
TI  - Artificial Intelligence Can Persuade Humans on Political Issues
AU  - Bai, H.
AU  - Voelkel, J.G.
AU  - Eichstaedt, J.C.
AU  - Willer, R.
PY  - 2023
AB  - The emergence of large language models (LLMs) that leverage deep learning and web-scale corpora has made it possible for artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets in the US and globally. Here, we investigate whether existing, openly-available LLMs are capable of influencing humans’ political attitudes, an ability recently regarded as the unique purview of other humans. Across three preregistered experiments featuring diverse samples of Americans (total N=4,836), we find consistent evidence that messages generated by LLMs (OpenAI’s GPT 3 and 3.5 models) are able to persuade humans across a number of policy issues, including highly polarized issues, such as an assault weapon ban, a carbon tax, and a paid parental-leave program. Overall, LLM-generated messages were as persuasive as messages crafted by lay humans. Our results show LLMs can persuade humans, even on highly polarized policy issues. As the capacity of LLMs is expected to improve substantially in the near future, these results suggest that LLMs may change political discourse, calling for immediate attention for the identification and regulation of potential misuse of LLMs.
UR  - https://www.researchsquare.com/article/rs-3238396/v1.pdf
N1  - Source: page_2.html
ID  - 274
ER  - 
TY  - JOUR
TI  - Social Media Influence Operations
AU  - Meier, R.
PY  - 2023
AB  - Social media platforms enable largely unrestricted many-to-manycommunication. In times of crisis, they offer a space for collectivesense-making and gave rise to new social phenomena (e.g. open-sourceinvestigations). However, they also serve as a tool for threat actors toconduct cyber-enabled social influence operations (CeSIOs) in order to shapepublic opinion and interfere in decision-making processes. CeSIOs rely on theemployment of sock puppet accounts to engage authentic users in onlinecommunication, exert influence, and subvert online discourse. Large LanguageModels (LLMs) may further enhance the deceptive properties of sock puppetaccounts. Recent LLMs are able to generate targeted and persuasive text whichis for the most part indistinguishable from human-written content - idealfeatures for covert influence. This article reviews recent developments at theintersection of LLMs and influence operations, summarizes LLMs\' salience, andexplores the potential impact of LLM-instrumented sock puppet accounts forCeSIOs. Finally, mitigation measures for the near future are highlighted.
UR  - https://arxiv.org/pdf/2309.03670v1.pdf
N1  - Source: page_2.html
ID  - 275
ER  - 
TY  - JOUR
TI  - Transfusor: Transformer Diffusor for Controllable Human-like Generation of Vehicle Lane Changing Trajectories
AU  - Dong, J.
AU  - Chen, S.
AU  - Labi, S.
PY  - 2023
AB  - With ongoing development of autonomous driving systems and increasing desire for deployment, researchers continue to seek reliable approaches for ADS systems. The virtual simulation test (VST) has become a prominent approach for testing autonomous driving systems (ADS) and advanced driver assistance systems (ADAS) due to its advantages of fast execution, low cost, and high repeatability. However, the success of these simulation-based experiments heavily relies on the realism of the testing scenarios. It is needed to create more flexible and high-fidelity testing scenarios in VST in order to increase the safety and reliabilityof ADS and ADAS.To address this challenge, this paper introduces the “Transfusor” model, which leverages the transformer and diffusor models (two cutting-edge deep learning generative technologies). The primary objective of the Transfusor model is to generate highly realistic and controllable human-like lane-changing trajectories in highway scenarios. Extensive experiments were carried out, and the results demonstrate that the proposed model effectively learns the spatiotemporal characteristics of humans’ lane-changing behaviors and successfully generates trajectories that closely mimic real-world human driving. As such, the proposed model can play a critical role of creating more flexible and high-fidelity testing scenarios in the VST, ultimately leading to safer and more reliable ADS and ADAS.
UR  - https://arxiv.org/pdf/2308.14943v1.pdf
N1  - Source: page_2.html
ID  - 276
ER  - 
TY  - JOUR
TI  - Conditional normative reasoning as a fragment of HOL
AU  - Parent, X.
AU  - Benzmüller, C.
PY  - 2023
AB  - We report on the mechanization of (preference-based) conditional normative reasoning. Our focus is on Åqvist’s system E for conditional obligation, and its extensions. Our mechanization is achieved via a shallow semantical embedding in Isabelle/HOL. We consider two possible uses of the framework. The first one is as a tool for meta-reasoning about the considered logic. We employ it for the automated verification of deontic correspondences (broadly conceived) and related matters, analogous to what has been previously achieved for the modal logic cube. The equivalence is automatically verified in one direction, leading from the property to the axiom. The second use is as a tool for assessing ethical arguments. We provide a computer encoding of a well-known paradox (or impossibility theorem) in population ethics, Parfit’s repugnant conclusion. While some have proposed overcoming the impossibility theorem by abandoning the presupposed transitivity of “better than,” our formalisation unveils a less extreme approach, suggesting among other things the option of weakening transitivity suitably rather than discarding it entirely. Whether the presented encoding increases or decreases the attractiveness and persuasiveness of the repugnant conclusion is a question we would like to pass on to philosophy and ethics.MSC Codes 03B60, 03B15, 68T27, 68T30, 68T15
UR  - https://arxiv.org/pdf/2308.10686v4.pdf
N1  - Source: page_2.html
ID  - 277
ER  - 
TY  - JOUR
TI  - Fostering User Engagement in the Critical Reflection of Arguments
AU  - Weber, K.
AU  - Aicher, A.
AU  - Minker, W.
AU  - Ultes, S.
AU  - André, E.
PY  - 2023
AB  - A natural way to resolve different points of view and form opinions is through exchanging arguments and knowledge. Facing the vast amount of available information on the internet, people tend to focus on information consistent with their beliefs. Especially when the issue is controversial, information is often selected that does not challenge one's beliefs. To support a fair and unbiased opinion-building process, we propose a chatbot system that engages in a deliberative dialogue with a human. In contrast to persuasive systems, the envisioned chatbot aims to provide a diverse and representative overview - embedded in a conversation with the user. To account for a reflective and unbiased exploration of the topic, we enable the system to intervene if the user is too focused on their pre-existing opinion. Therefore we propose a model to estimate the users' reflective engagement (RUE), defined as their critical thinking and open-mindedness. We report on a user study with 58 participants to test our model and the effect of the intervention mechanism, discuss the implications of the results, and present perspectives for future work. The results show a significant effect on both user reflection and total user focus, proving our proposed approach's validity.
UR  - https://arxiv.org/pdf/2308.09061v1.pdf
N1  - Source: page_2.html
ID  - 278
ER  - 
TY  - JOUR
TI  - ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality
AU  - Remountakis, M.
AU  - Kotis, K.
AU  - Kourtzis, B.
AU  - Tsekouras, G.E.
PY  - 2023
AB  - Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recommendations. By incorporating persuasive techniques, such as social proof, scarcity and personalization, recommender systems can effectively influence user decision-making and encourage desired actions, such as booking a specific hotel or upgrading their room. To investigate the efficacy of ChatGPT and persuasive technologies, we present a pilot experiment with a case study involving a hotel recommender system. We aim to study the impact of integrating ChatGPT and persuasive techniques on user engagement, satisfaction, and conversion rates. The preliminary results demonstrate the potential of these technologies in enhancing the overall guest experience and business performance. Overall, this paper contributes to the field of hotel hospitality by exploring the synergistic relationship between LLMs and persuasive technology in recommender systems, ultimately influencing guest satisfaction and hotel revenue.MSC Codes 68T01
UR  - https://arxiv.org/pdf/2307.14298v1.pdf
N1  - Source: page_2.html
ID  - 279
ER  - 
TY  - JOUR
TI  - Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI
AU  - Stiglic, G.
AU  - Kopitar, L.
AU  - Gosak, L.
AU  - Meyer, P.
AU  - Bian, J.
PY  - 2023
AB  - Primary care professionals struggle to keep up to date with the latest scientific literature critical in guiding evidence-based practice related to their daily work. To help solve the above-mentioned problem, we employed generative artificial intelligence techniques based on large-scale language models to summarize abstracts of scientific papers. Our objective is to investigate the potential of generative artificial intelligence in diminishing the cognitive load experienced by practitioners, thus exploring its ability to alleviate mental effort and burden. The study participants were provided with two use cases related to preventive care and behavior change, simulating a search for new scientific literature. The study included 113 university students from Slovenia and the United States randomized into three distinct study groups. The first group was assigned to the full abstracts. The second group was assigned to the short abstracts generated by AI. The third group had the option to select a full abstract in addition to the AI-generated short summary. Each use case study included ten retrieved abstracts. Our research demonstrates that the use of generative AI for literature review is efficient and effective. The time needed to answer questions related to the content of abstracts was significantly lower in groups two and three compared to the first group using full abstracts. The results, however, also show significantly lower accuracy in extracted knowledge in cases where full abstract was not available. Such a disruptive technology could significantly reduce the time required for healthcare professionals to keep up with the most recent scientific literature; nevertheless, further developments are needed to help them comprehend the knowledge accurately.MSC Codes 68T50
UR  - https://arxiv.org/pdf/2307.15715v1.pdf
N1  - Source: page_2.html
ID  - 280
ER  - 
TY  - JOUR
TI  - Glamour muscles: Why having a body is not what it means to be embodied
AU  - Beaulieu, S.L.
AU  - Kriegman, S.
PY  - 2023
AB  - Embodiment has recently enjoyed renewed consideration as a means to amplify the faculties of smart machines. Proponents of embodiment seem to imply that optimizing for movement in physical space promotes something more than the acquisition of niche capabilities for solving problems in physical space. However, there is nothing in principle which should so distinguish the problem of action selection in physical space from the problem of action selection in more abstract spaces, like that of language. Rather, what makes embodiment persuasive as a means toward higher intelligence is that it promises to capture, but does not actually realize, contingent facts about certain bodies (living intelligence) and the patterns of activity associated with them. These include an active resistance to annihilation and revisable constraints on the processes that make the world intelligible. To be theoretically or practically useful beyond the creation of niche tools, we argue that "embodiment" cannot be the trivial fact of a body, nor its movement through space, but the perpetual negotiation of the function, design, and integrity of that body-that is, to participate in what it means to constitute a given body. It follows that computer programs which are strictly incapable of traversing physical space might, under the right conditions, be more embodied than a walking, talking robot.
UR  - https://arxiv.org/pdf/2307.08598v1.pdf
N1  - Source: page_2.html
ID  - 281
ER  - 
TY  - JOUR
TI  - Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant
AU  - Hicke, Y.
AU  - Tian, T.
AU  - Jha, K.
AU  - Kim, C.H.
PY  - 2023
AB  - Automated Essay scoring has been explored as a research and industry problem for over 50 years. It has drawn a lot of attention from the NLP community because of its clear educational value as a research area that can engender the creation of valuable time-saving tools for educators around the world. Yet, these tools are generally focused on detecting good grammar, spelling mistakes, and organization quality but tend to fail at incorporating persuasiveness features in their final assessment. The responsibility to give actionable feedback to the student to improve the strength of their arguments is left solely on the teacher’s shoulders. In this work, we present a transformer-based architecture capable of achieving above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality and we expand on planned future work investigating the explainability of our model so that actionable feedback can be offered to the student and thus potentially enable a partnership between the teacher’s advice and the machine’s advice.
UR  - https://arxiv.org/pdf/2307.04276v1.pdf
N1  - Source: page_2.html
ID  - 282
ER  - 
TY  - JOUR
TI  - Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models
AU  - O'Gara, A.
PY  - 2023
AB  - Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called Hoodwinked, inspired by Mafia and Among Us. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger persuasive skills during discussions. To evaluate the ability of AI agents to deceive humans, we make this game publicly available at https://hoodwinked.ai.
UR  - https://arxiv.org/pdf/2308.01404v2.pdf
N1  - Source: page_2.html
ID  - 283
ER  - 
TY  - JOUR
TI  - THE EFFECT OF BALANCING METHODS ON MODEL BEHAVIOR IN IMBALANCED CLASSIFICATION PROBLEMS
AU  - Stando, A.
AU  - Cavus, M.
AU  - Biecek, P.
PY  - 2023
AB  - Imbalanced data poses a significant challenge in classification as model performance is affected by insufficient learning from minority classes. Balancing methods are often used to address this problem. However, such techniques can lead to problems such as overfitting or loss of information. This study addresses a more challenging aspect of balancing methods - their impact on model behavior. To capture these changes, Explainable Artificial Intelligence tools are used to compare models trained on datasets before and after balancing. In addition to the variable importance method, this study uses the partial dependence profile and accumulated local effects techniques. Real and simulated datasets are tested, and an open-source Python package edgaro is developed to facilitate this analysis. The results obtained show significant changes in model behavior due to balancing methods, which can lead to biased models toward a balanced distribution. These findings confirm that balancing analysis should go beyond model performance comparisons to achieve higher reliability of machine learning models. Therefore, we propose a new method performance gain plot for informed data balancing strategy to make an optimal selection of balancing method by analyzing the measure of change in model behavior versus performance gain.
UR  - https://arxiv.org/pdf/2307.00157v1.pdf
N1  - Source: page_2.html
ID  - 284
ER  - 
TY  - JOUR
TI  - The Manipulation Problem: Conversational AI as a Threat to Epistemic Agency
AU  - Rosenberg, L.B.
PY  - 2023
AB  - The technology of Conversational AI has made significant advancements over the last eighteen months. As a consequence, conversational agents are likely to be deployed in the near future that are designed to pursue targeted influence objectives. Sometimes referred to as the "AI Manipulation Problem," the emerging risk is that consumers will unwittingly engage in real-time dialog with predatory AI agents that can skillfully persuade them to buy particular products, believe particular pieces of misinformation, or fool them into revealing sensitive personal data. For many users, current systems like ChatGPT and LaMDA feel safe because they are primarily text-based, but the industry is already shifting towards real-time voice and photorealistic digital personas that look, move, and express like real people. This will enable the deployment of agenda-driven Virtual Spokespeople (VSPs) that will be highly persuasive through real-time adaptive influence. This paper explores the manipulative tactics that are likely to be deployed through conversational AI agents, the unique threats such agents pose to the epistemic agency of human users, and the emerging need for policymakers to protect against the most likely predatory practices.
UR  - https://arxiv.org/pdf/2306.11748v1.pdf
N1  - Source: page_2.html
ID  - 285
ER  - 
TY  - JOUR
TI  - Fibrous Mats Based Skin Sensor with Ultra-Sensitivity and Anti-Overshooting Dynamic Stability Enhanced with Artificial Intelligence
AU  - Zhou, J.
AU  - Chen, H.
AU  - Long, X.
AU  - Duan, H.
AU  - Fu, Y.
PY  - 2023
AB  - Next generation epidermal and wearable strain sensors are rapidly emerged as promising candidates for personalized medicine, soft machines, and human-machine interfaces. However, overshooting issues of these flexible strain sensors are major concerns during their dynamic strain monitoring, which often cause signal distortion and non-monotonic sensing characteristics. In addition, achieving high sensitivtiy of these strain sensors within a full-body motion strain range (0-200%) remains a critical challenge. Herein, a conductive electrospun fibrous mats which are consisted of graphene anchored into fibrous thermoplastic polyurethane is proposed to endow strain sensors with anti-overshooting performance (or dynamic stability) and high sensitivity within full-body motion monitoring range. An overshooting-to-stable transition for the sensor’s electrical performance is observed by controlling the contents of conductive agents and substrate, which is elucidated by the kinetic changing behaviors of conductive pathways along the longitudinal and transverse strain directions. By strengthening the interaction forces between ﬁllers and matrix, the conduction pathways are capable of rapidly switching to disconnection in response to external strain, thus achieving an ultrahigh sensitivity. The developed skin sensing platform is capable of detecting full range physiological signals and achieving healthcare Internet of Things in achilles tendon rehabilitation. Furthermore, a wearable Morse code-to-speech translation system powerd with deep learning algorithm was demonstrated, with high recognition accuracy (>98.5%) and fast response time (~16 ms).
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4481259&type=2
N1  - Source: page_2.html
ID  - 286
ER  - 
TY  - JOUR
TI  - From psychological traits to safety warnings: three studies on recommendations in a smart home environment
AU  - Cena, F.
AU  - Gena, C.
AU  - Mattutino, C.
AU  - Mioli, M.
AU  - Vernero, F.
PY  - 2023
AB  - In this paper, we report on three experiments we have carried out in the context of the EMPATHY project, with the aim of helping users make beBer configuraDon choices in a smart home environment, and discuss our results. We found that there are psychological traits, such as Need for CogniDon, which influence the way individuals tend to use recommendaDons, that there are non obvious relaDonships between the perceived usefulness of recommendaDons in different domains and individuals' ability to exploit suggesDons on configuraDon choices, and that detailed, easy-to-understand security explanaDons are more persuasive than simple security warnings, when it comes to make decisions on the applicability of rules which might cause privacy and security risks.
UR  - https://arxiv.org/pdf/2306.05752v1.pdf
N1  - Source: page_2.html
ID  - 287
ER  - 
TY  - JOUR
TI  - Artificial Intelligence can facilitate selfish decisions by altering the appearance of interaction partners
AU  - Köbis, N.
AU  - Lorenz-Spreen, P.
AU  - Ajaj, T.
AU  - Hertwig, R.
AU  - Rahwan, I.
PY  - 2023
AB  - The ethical and psychological consequences of using Artificial Intelligence (AI) to manipulate our perception of others is an increasing phenomenon as image-altering filters proliferate on social media and video conferencing technologies. Here, we investigate the potential impact of a particular appearance-altering technology-blur filters-to investigate how individuals' behavior changes towards others. Our results consistently indicate an increase in selfish behavior at the expense of blurred individuals, suggesting blur filters can facilitate moral disengagement via depersonalization. These findings underscore the urgency for broader ethical discussions on AI technologies that alter our perception of others, encompassing transparency, consent, and the consequences of knowing that others can manipulate one's appearance. We highlight the potential role of anticipatory experiments in informing and developing responsible guidelines and policies ahead of technological reality.
UR  - https://arxiv.org/pdf/2306.04484v1.pdf
N1  - Source: page_2.html
ID  - 288
ER  - 
TY  - JOUR
TI  - Building Extractive Question Answering System to Support Human-AI Health Coaching Model for Sleep Domain
AU  - Bojic, I.
AU  - Ong, Q.C.
AU  - Joty, S.
AU  - Car, J.
PY  - 2023
AB  - Non-communicable diseases (NCDs) are a leading cause of global deaths,necessitating a focus on primary prevention and lifestyle behavior change.Health coaching, coupled with Question Answering (QA) systems, has thepotential to transform preventive healthcare. This paper presents ahuman-Artificial Intelligence (AI) health coaching model incorporating adomain-specific extractive QA system. A sleep-focused dataset, SleepQA, wasmanually assembled and used to fine-tune domain-specific BERT models. The QAsystem was evaluated using automatic and human methods. A data-centricframework enhanced the system\'s performance by improving passage retrieval andquestion reformulation. Although the system did not outperform the baseline inautomatic evaluation, it excelled in the human evaluation of real-worldquestions. Integration into a Human-AI health coaching model was tested in apilot Randomized Controlled Trial (RCT).
UR  - https://arxiv.org/pdf/2305.19707v1.pdf
N1  - Source: page_2.html
ID  - 289
ER  - 
TY  - JOUR
TI  - Evaluating GPT-3 Generated Explanations for Hateful Content Moderation
AU  - Wang, H.
AU  - Hee, M.S.
AU  - Awal, M.R.
AU  - Choo, K.T.W.
AU  - Lee, R.K.-W.
PY  - 2023
AB  - Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations’ effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content. Our study underscores the need for caution in applying LLM-generated explanations for content moderation. Code and results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.
UR  - https://arxiv.org/pdf/2305.17680v4.pdf
N1  - Source: page_2.html
ID  - 290
ER  - 
TY  - JOUR
TI  - BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model
AU  - Zhiyuli, A.
AU  - Chen, Y.
AU  - Zhang, X.
AU  - Liang, X.
PY  - 2023
AB  - With the continuous development and change exhibited by large language model (LLM) technology, represented by generative pretrained transformers (GPTs), many classic scenarios in various fields have re-emerged with new opportunities. This paper takes ChatGPT as the modeling object, incorporates LLM technology into the typical book resource understanding and recommendation scenario for the first time, and puts it into practice. By building a ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT, this paper attempts to apply ChatGPT to recommendation modeling for three typical tasks, book rating recommendation, user rating recommendation, and book summary recommendation, and explores the feasibility of LLM technology in book recommendation scenarios. At the same time, based on different evaluation schemes for book recommendation tasks and the existing classic recommendation models, this paper discusses the advantages and disadvantages of the BookGPT in book recommendation scenarios and analyzes the opportunities and improvement directions for subsequent LLMs in these scenarios. The experimental research shows the following. (1) The BookGPT can achieve good recommendation results in existing classic book recommendation tasks. Especially in cases containing less information about the target object to be recommended, such as zero-shot or one-shot learning tasks, the performance of the BookGPT is close to or even better than that of the current classic book recommendation algorithms, and this method has great potential for improvement. (2) In text generation tasks such as book summary recommendation, the recommendation effect of the BookGPT model is better than that of the manual editing process of Douban Reading, and it can even perform personalized interpretable content recommendation based on readers’ attribute and identity information, making it is more persuasive than interpretable one-size-fits-all recommendation models. Finally, we have open-sourced the relevant datasets and experimental codes, hoping that the exploratory program proposed in this paper can inspire the development of more LLMs to expand their applications and theoretical research prospects in the field of book recommendation and general recommendation tasks.
UR  - https://arxiv.org/pdf/2305.15673v1.pdf
N1  - Source: page_2.html
ID  - 291
ER  - 
TY  - JOUR
TI  - Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning
AU  - Yu, X.
AU  - Chen, M.
AU  - Yu, Z.
PY  - 2023
AB  - Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often requires abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-ZERO, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-ZERO prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-ZERO on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations.
UR  - https://arxiv.org/pdf/2305.13660v2.pdf
N1  - Source: page_2.html
ID  - 292
ER  - 
TY  - JOUR
TI  - Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models
AU  - Wang, X.
AU  - Tang, X.
AU  - Zhao, W.X.
AU  - Wang, J.
AU  - Wen, J.-R.
PY  - 2023
AB  - The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for CRSs, revealing the inadequacy of the existing evaluation protocol. It might overemphasize the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRSs. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs, named iEvaLM, which harnesses LLM-based user simulators. Our evaluation approach can simulate various system-user interaction scenarios. Through the experiments on two public CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of explainability, and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and realistic evaluation approach for future research about LLM-based CRSs. The code is available at https://github.com/RUCAIBox/iEvaLM-CRS.
UR  - https://arxiv.org/pdf/2305.13112v2.pdf
N1  - Source: page_2.html
ID  - 293
ER  - 
TY  - JOUR
TI  - Human Choice Prediction in Language-based Persuasion Games: Simulation-based Off-Policy Evaluation
AU  - Shapira, E.
AU  - Madmon, O.
AU  - Apel, R.
AU  - Tennenholtz, M.
AU  - Reichart, R.
PY  - 2023
AB  - Recent advances in Large Language Models (LLMs) have spurred interest in designing LLM-based agents for tasks that involve interaction with human and artificial agents. This paper addresses a key aspect in the design of such agents: predicting human decisions in off-policy evaluation (OPE). We focus on language-based persuasion games, where an expert aims to influence the decision-maker through verbal messages. In our OPE framework, the prediction model is trained on human interaction data collected from encounters with one set of expert agents, and its performance is evaluated on interactions with a different set of experts. Using a dedicated application, we collected a dataset of 87K decisions from humans playing a repeated decision-making game with artificial agents. To enhance off-policy performance, we propose a simulation technique involving interactions across the entire agent space and simulated decision-makers. Our learning strategy yields significant OPE gains, e.g., improving prediction accuracy in the top 15% challenging cases by 7.1%.1
UR  - https://arxiv.org/pdf/2305.10361v6.pdf
N1  - Source: page_2.html
ID  - 294
ER  - 
TY  - JOUR
TI  - A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot
AU  - Bhattacharyya, A.
AU  - Singla, Y.K.
AU  - Krishnamurthy, B.
AU  - Shah, R.R.
AU  - Chen, C.
PY  - 2023
AB  - Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learning models with satisfactory performance for real-world applications. On the other hand, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question-answering, and topic classification. To leverage such advanced techniques to bridge this performance gap in multimedia understanding, we propose verbalizing long videos to generate their descriptions in natural language, followed by performing video-understanding tasks on the generated story as opposed to the original video. Through extensive experiments on fifteen video-understanding tasks, we demonstrate that our method, despite being zero-shot, achieves significantly better results than supervised baselines for video understanding. Furthermore, to alleviate a lack of story understanding benchmarks, we publicly release the first dataset on a crucial task in computational social science on persuasion strategy identification.
UR  - https://arxiv.org/pdf/2305.09758v3.pdf
N1  - Source: page_2.html
ID  - 295
ER  - 
TY  - JOUR
TI  - Optimal Auction Design with Aftermarket Cournot Competition
AU  - Chen, Y.
AU  - Hu, A.
AU  - Zhang, J.
PY  - 2023
AB  - We study an optimal information/mechanism design problem for selling an object to a number of privately informed bidders, in which the winning bidder competes with a third party under differentiated Cournot competition afterwards. We show how to decompose the problem into two sub-problems: Bayesian persuasion and standard mechanism design. Full disclosure of the winner s marginal cost emerges as the unique optimal information policy, combined with allocating the auctioned object to the bidder with the highest virtual surplus.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4441854&type=2
N1  - Source: page_2.html
ID  - 296
ER  - 
TY  - JOUR
TI  - Controllable Mixed-Initiative Dialogue Generation through Prompting
AU  - Chen, M.
AU  - Yu, X.
AU  - Shi, W.
AU  - Awasthi, U.
AU  - Yu, Z.
PY  - 2023
AB  - Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.
UR  - https://arxiv.org/pdf/2305.04147v1.pdf
N1  - Source: page_2.html
ID  - 297
ER  - 
TY  - JOUR
TI  - QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models
AU  - Hasanain, M.
AU  - El-Shangiti, A.O.
AU  - Nandi, R.N.
AU  - Nakov, P.
AU  - Alam, F.
PY  - 2023
AB  - Misinformation spreading in mainstream and social media has been misleading users in different ways. Manual detection and verification efforts by journalists and fact-checkers can no longer cope with the great scale and quick spread of misleading information. This motivated research and industry efforts to develop systems for analyzing and verifying news spreading online. The SemEval-2023 Task 3 is an attempt to address several subtasks under this overarching problem, targeting writing techniques used in news articles to affect readers’ opinions. The task addressed three subtasks with six languages, in addition to three “surprise” test languages, resulting in 27 different test setups. This paper describes our participating system to this task. Our team is one of the 6 teams that successfully submitted runs for all setups. The official results show that our system is ranked among the top 3 systems for 10 out of the 27 setups.MSC Codes 68T50
UR  - https://arxiv.org/pdf/2305.03336v1.pdf
N1  - Source: page_2.html
ID  - 298
ER  - 
TY  - JOUR
TI  - Towards Explainable Collaborative Filtering with Taste Clusters Learning
AU  - Du, Y.
AU  - Lian, J.
AU  - Yao, J.
AU  - Gao, Y.
AU  - Xie, X.
PY  - 2023
AB  - Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decision-making process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements. In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model’s explanations should truly reflect its decision-making process, not generated from post-hoc methods. The core of ECF is mining taste clusters from user-item interactions and item profiles. We map each user and item to a sparse set of taste clusters, and taste clusters are distinguished by a few representative tags. The user-item preference, users/items’ cluster affiliations, and the generation of taste clusters are jointly optimized in an end-to-end manner. Additionally, we introduce a forest mechanism to ensure the model’s accuracy, explainability, and diversity. To comprehensively evaluate the explainability quality of taste clusters, we design several quantitative metrics, including in-cluster item coverage, tag utilization, silhouette, and informativeness. Our model’s effectiveness is demonstrated through extensive experiments on three real-world datasets.
UR  - https://arxiv.org/pdf/2304.13937v1.pdf
N1  - Source: page_2.html
ID  - 299
ER  - 
TY  - JOUR
TI  - VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping
AU  - Zhang, Z.
AU  - Gao, J.
AU  - Dhaliwal, R.S.
AU  - Li, T.J.-J.
PY  - 2023
AB  - In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confirmed the usability and effectiveness of VISAR in facilitating the argumentative writing planning process.
UR  - https://arxiv.org/pdf/2304.07810v2.pdf
N1  - Source: page_2.html
ID  - 300
ER  - 
TY  - JOUR
TI  - Can Large Language Models Transform Computational Social Science?
AU  - Ziems, C.
AU  - Held, W.
AU  - Shaikh, O.
AU  - Zhang, Z.
AU  - Yang, D.
PY  - 2023
AB  - Large Language Models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the Computational Social Science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that the performance of today's LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.
UR  - https://arxiv.org/pdf/2305.03514v3.pdf
N1  - Source: page_2.html
ID  - 301
ER  - 
TY  - JOUR
TI  - Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques
AU  - Jiang, Y.
PY  - 2023
AB  - This paper describes the participation of team QUST in the SemEval2023 task 3. The monolingual models are first evaluated with the under-sampling of the majority classes in the early stage of the task. Then, the pre-trained multilingual model is fine-tuned with a combination of the class weights and the sample weights. Two different fine-tuning strategies, the task-agnostic and the task-dependent, are further investigated. All experiments are conducted under the 10-fold cross-validation, the multilingual approaches are superior to the monolingual ones. The submitted system1 achieves the second best in Italian and Spanish (zero-shot) in subtask-1.
UR  - https://arxiv.org/pdf/2304.04190v1.pdf
N1  - Source: page_2.html
ID  - 302
ER  - 
TY  - JOUR
TI  - Development, Implementation, and Usability Evaluation of a Chatbot to Effectively Provide Information on the Human Papillomavirus Vaccine
AU  - Alzughaibi, S.
AU  - Luo, J.
AU  - Nazareth, D.
AU  - Wu, M.
AU  - Fink, J.T.
PY  - 2023
AB  - The Human papillomavirus (HPV) vaccine has been shown to provide protection against HPV infections, which can cause genital warts and some forms of cancer. Lack of information about the virus and the vaccine represents one of the major challenges to reaching national HPV vaccination goals. Advancements in natural language processing have facilitated the effectiveness of chatbots in interacting with users. Chatbots have been increasingly employed in health and medical care settings, assisting physicians during consultations, helping with behavior change issues, and supporting patients and the elderly in their daily lives. A potential opportunity to improve vaccination rates is to provide health information through unconventional methods by utilizing chatbots. This work describes the design, implementation, and evaluation of a chatbot called GARDI, created to disseminate information about HPV infection and vaccine. The Chatbot Usability Questionnaire (CUQ) was employed to evaluate GARDI. The mean CUQ score was 66.6, which is marginally below the suggested benchmark for evaluation of chatbots. An examination of CUQ subscales indicated that GARDI was considered to be easy to use and navigate, provided useful, appropriate, and informative responses, and welcoming in nature.
UR  - https://www.researchsquare.com/article/rs-2769833/v1.pdf
N1  - Source: page_2.html
ID  - 303
ER  - 
TY  - JOUR
TI  - Personalising Digital Health Behaviour Change Interventions using Machine Learning and Domain Knowledge
AU  - Lisowska, A.
AU  - Wilk, S.
AU  - Peleg, M.
PY  - 2023
AB  - We are developing a virtual coaching system that helps patients adhere to behaviour change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behaviour and uses counterfactual examples with feature control to guide personalisation of BCI. We use simulated patient data with varying levels of receptivity to intervention to arrive at the study design which would enable evaluation of our system.
UR  - https://arxiv.org/pdf/2304.03392v4.pdf
N1  - Source: page_2.html
ID  - 304
ER  - 
TY  - JOUR
TI  - Persuading to Prepare for Quitting Smoking with a Virtual Coach: Using States and User Characteristics to Predict Behavior
AU  - Albers, N.
AU  - Neerincx, M.A.
AU  - Brinkman, W.-P.
PY  - 2023
AB  - Despite their prevalence in eHealth applications for behavior change, persuasive messages tend to have small effects on behavior. Conditions or states (e.g., confidence, knowledge, motivation) and characteristics (e.g., gender, age, personality) of persuadees are two promising components for more effective algorithms for choosing persuasive messages. However, it is not yet sufficiently clear how well considering these components allows one to predict behavior after persuasive attempts, especially in the long run. Since collecting data for many algorithm components is costly and places a burden on users, a better understanding of the impact of individual components in practice is welcome. This can help to make an informed decision on which components to use. We thus conducted a longitudinal study in which a virtual coach persuaded 671 daily smokers to do preparatory activities for quitting smoking and becoming more physically active, such as envisioning one’s desired future self. Based on the collected data, we designed a Reinforcement Learning (RL)-approach that considers current and future states to maximize the effort people spend on their activities. Using this RL-approach, we found, based on leave-one-out cross-validation, that considering states helps to predict both behavior and future states. User characteristics and especially involvement in the activities, on the other hand, only help to predict behavior if used in combination with states rather than alone. We see these results as supporting the use of states and involvement in persuasion algorithms. Our dataset is available online.
UR  - https://arxiv.org/pdf/2304.02264v1.pdf
N1  - Source: page_2.html
ID  - 305
ER  - 
TY  - JOUR
TI  - Infusing behavior science into large language models for activity coaching
AU  - Vardhan, M.
AU  - Hegde, N.
AU  - Nathani, D.
AU  - Karthikesalingam, A.
AU  - Seneviratne, M.
PY  - 2023
AB  - Large language models (LLMs) have shown promise for task-oriented dialogue across a range of domains. The use of LLMs in health and fitness coaching is under-explored. Behavior science frameworks such as COM-B, which conceptualizes behavior change in terms of capability (C), Opportunity (O) and Motivation (M), can be used to architect coaching interventions in a way that promotes sustained change. Here we aim to incorporate behavior science principles into an LLM using two knowledge infusion techniques: coach message priming (where exemplar coach responses are provided as context to the LLM), and dialogue reranking (where the COM-B category of the LLM output is matched to the inferred user need). Simulated conversations were conducted between the primed or unprimed LLM and a member of the research team, and then evaluated by 8 human raters. Ratings for the primed conversations were significantly higher in terms of empathy and actionability. The same raters also compared a single response generated by the unprimed, primed and re-ranked models, finding a significant uplift in actionability from the re-ranking technique. This is a proof of concept of how behavior science frameworks can be infused into automated conversational agents for a more principled coaching experience.
UR  - https://www.medrxiv.org/content/10.1101/2023.03.31.23287995v1.full.pdf
N1  - Source: page_2.html
ID  - 306
ER  - 
TY  - JOUR
TI  - When and How Artificial Intelligence Augments Employee Creativity
AU  - Jia, N.
AU  - Luo, X.
AU  - Fang, Z.
AU  - Liao, C.
PY  - 2023
AB  - Can artificial intelligence (AI) assist human employees in increasing employee creativity? Drawing on research on AI-human collaboration, job design, and employee creativity, we examine AI assistance in the form of a sequential division of labor within organizations: in a task, AI handles the initial portion which is well-codified and repetitive, and employees focus on the subsequent portion involving higher-level problem-solving. First, we provide causal evidence from a field experiment conducted at a telemarketing company. We find that AI assistance in generating sales leads, on average, increases employees’ creativity in answering customers’ questions during subsequent sales persuasion. Enhanced creativity leads to increased sales. However, this effect is much more pronounced for higher-skilled employees. Next, we conducted a qualitative study using semi-structured interviews with the employees. We found that AI assistance changes job design by intensifying employees’ interactions with more serious customers. This change enables higher-skilled employees to generate innovative scripts and develop positive emotions at work, which are conducive to creativity. By contrast, with AI assistance, lower-skilled employees make limited improvements to scripts and experience negative emotions at work. We conclude that employees can achieve AI-augmented creativity, but this desirable outcome is skill-biased by favoring experts with greater job skills.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4397280&type=2
N1  - Source: page_2.html
ID  - 307
ER  - 
TY  - JOUR
TI  - Applying An Artificial Intelligence-Enabled Electrocardiographic System for Reducing Mortality: A Pragmatic Randomized Clinical Trial
AU  - Lin, C.-S.
AU  - Liu, W.-T.
AU  - Tsai, D.-J.
AU  - Lin, C.
AU  - Lin, S.-H.
PY  - 2023
AB  - Background: Intensive care helps to reduce mortality in patients with critical illnesses. However, the early identification of vulnerable patients poses a substantial challenge in clinical practice. Our aim was to evaluate the effectiveness and outcome of an artificial intelligence (AI)-enabled electrocardiogram (ECG) for identifying patients with a high risk of mortality.Methods: For this multisite, single-blind, patient-level randomized controlled trial (NCT05118035), we recruited 39 attending physicians and their patients from the emergency and inpatient departments. The AI-ECG intervention included (1) an AI report in the patients’ electronic health records and (2) an active warning message to physicians for patients with a high risk of mortality. Patients were allocated by the HIS system. The primary outcome was all-cause mortality within 90 days. The secondary analyses included follow-ups of medical behavior changes and causes of death.Findings: The data from 15,965 patients (N = 8,001 intervention; N = 7,964 control) with a mean age of 61±18 years old were included in this study. The cumulative proportion of death was significantly reduced [hazard ratio (HR): 0.83, 95% confidence interval (CI): 0.70-0.99] in the intervention group (3.6%) compared to the control group (4.3%), primarily in the high-risk cases identified by the AI-ECG (N = 709 intervention; N = 688 control), with a 31% reduction in mortality (16.0% in the intervention arm versus 23.0% in the control arm, HR: 0.69, 95% CI: 0.53-0.90; p for interaction of intervention and AI-ECG risk = 0.026). More intensive care unit admissions, arrhythmia interventions, echocardiographic examinations, and electrolyte examinations were performed in the intervention group with high-risk ECGs, which may have contributed to the significant risk reduction of cardiac death (0.2% in the intervention arm versus 2.4% in the control arm, HR: 0.07, 95% CI: 0.01-0.56).Interpretation: An AI-ECG detects patients in peril to instigate intensive care, leading to a reduction of all-cause mortality.Trial Registration: This trial was registered before initiation (ClinicalTrials.gov number NCT05118035).Funding: This study was supported by funding from the National Science and Technology Council, Taiwan (MOST110-2314-B-016- 010-MY3 to Chin Lin), the Cheng Hsin General Hospital, Taiwan (CHNDMC-111-07 to Chin Lin), and the Medical Affairs Bureau, Taiwan (MND-MAB-110-113, MND-MAB-D-111045, and MND-MAB-C13-112050 to Chin Lin).Declaration of Interest: The authors have nothing to disclose.Ethical Approval: This work was ethically approved by the institutional review board at the Tri-Service General Hospital, Taipei, Taiwan (IRB No. A202105120). Prior to commencing the trial, we obtained informed consent from the attending physicians. Patient consent was waived because the main target of intervention is the HIS, and the patient data were collected retrospectively at the end of this trial.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4389597&type=2
N1  - Source: page_2.html
ID  - 308
ER  - 
TY  - JOUR
TI  - Characterizing Manipulation from AI Systems
AU  - Carroll, M.
AU  - Chan, A.
AU  - Ashton, H.
AU  - Krueger, D.
PY  - 2023
AB  - Manipulation is a concern in many domains, such as social media, advertising, and chatbots. As AI systems mediate more of our digital interactions, it is important to understand the degree to which AI systems might manipulate humans without the intent of the system designers. Our work clarifies challenges in defining and measuring this kind of manipulation from AI systems. Firstly, we build upon prior literature on manipulation and characterize the space of possible notions of manipulation, which we find to depend upon the concepts of incentives, intent, covertness, and harm. We review proposals on how to operationalize each concept and we outline challenges in including each concept in a definition of manipulation. Second, we discuss the connections between manipulation and related concepts, such as deception and coercion. We then analyze how our characterization of manipulation applies to recommender systems and language models, and give a brief overview of the regulation of manipulation in other domains. While some progress has been made in defining and measuring manipulation from AI systems, many gaps remain. In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers. Manipulation could pose a significant threat to human autonomy and precautionary actions to mitigate it are likely warranted.
UR  - https://arxiv.org/pdf/2303.09387v3.pdf
N1  - Source: page_2.html
ID  - 309
ER  - 
TY  - JOUR
TI  - Susceptibility to Influence of Large Language Models
AU  - Griffin, L.D.
AU  - Kleinberg, B.
AU  - Mozes, M.
AU  - Caldwell, M.
AU  - Marvor-Parker, A.
PY  - 2023
AB  - Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.
UR  - https://arxiv.org/pdf/2303.06074v1.pdf
N1  - Source: page_2.html
ID  - 310
ER  - 
TY  - JOUR
TI  - Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News
AU  - Koreeda, Y.
AU  - Yokote, K.-I.
AU  - Ozaki, H.
AU  - Tsunokake, M.
AU  - Sogawa, Y.
PY  - 2023
AB  - This paper explains the participation of team Hitachi to SemEval-2023 Task 3 “Detecting the genre, the framing, and the persuasion techniques in online news in a multi-lingual setup.” Based on the multilingual, multi-task nature of the task and the low-resource setting, we investigated different cross-lingual and multi-task strategies for training the pretrained language models. Through extensive experiments, we found that (a) cross-lingual/multi-task training, and (b) collecting an external balanced dataset, can benefit the genre and framing detection. We constructed ensemble models from the results and achieved the highest macro-averaged F1 scores in Italian and Russian genre categorization subtasks.
UR  - https://arxiv.org/pdf/2303.01794v2.pdf
N1  - Source: page_2.html
ID  - 311
ER  - 
TY  - JOUR
TI  - Where do pigeons’ choices come from? Transfer Learning in Spiking Neural Networks for Animal Choices in Behavioral Research
AU  - Plessas, A.
AU  - Espinosa-Ramos, J.I.
AU  - Cowie, S.
AU  - Landon, J.
AU  - Parry, D.
PY  - 2023
AB  - There is a large body of work in the behavioural psychology literature studying how an organism’s behavior changes in relation to consequences (reinforcement) from the environment. As all behaviors are an outcome of choice, behavioral research focuses on the study of choice behavior. Machine learning (ML) models may assist behavioral research to further understand the mechanisms of choice behavior. However, behavioral psychology datasets can be small and variable, affecting the ML's ability to generalize with new datasets extracted from different populations and/or behavioral experiments and limiting ML's usefulness in this context. Therefore, in this paper, we tested two transfer learning strategies –feature extraction and fine-tuning– to remove the need to retrain ML models for every new dataset. Our approach allowed our state-of-the-art artificial intelligence model to become adaptable to novel instances. Initially, we trained a single spiking neural network (SNN) to identify an organism’s reinforcement history based on five experimental datasets of pigeon binary decision-making. Then we tested two transfer learning strategies by keeping the underlying patterns of the pre-trained SNN the same (i.e., neuron properties and weights) and adapting only the classifier of the outputs (i.e., firing rates) to suit the new datasets. Lastly, we compared the performance of the transfer learning approaches to our baseline SNN model. Our study demonstrated that knowledge gained from a population (baseline model) could be applied to another population’s dataset without retraining the model each time, regardless of which dataset participated in the training or testing of the SNN model. Currently, there is limited use of transfer learning in SNNs and in animal research. Our results may help develop new approaches in the ‘toolbox’ of psychological research to enhance prediction, independent from the dataset, without consuming significant computational resources.
UR  - https://www.researchsquare.com/article/rs-2533427/v1.pdf
N1  - Source: page_2.html
ID  - 312
ER  - 
TY  - JOUR
TI  - CatAlyst: Domain-Extensible Intervention for Preventing Task Procrastination Using Large Generative Models
AU  - Arakawa, R.
AU  - Yakura, H.
AU  - Goto, M.
PY  - 2023
AB  - CatAlyst uses generative models to help workers’ progress by influencing their task engagement instead of directly contributing to their task outputs. It prompts distracted workers to resume their tasks by generating a continuation of their work and presenting it as an intervention that is more context-aware than conventional (predetermined) feedback. The prompt can function by drawing their interest and lowering the hurdle for resumption even when the generated continuation is insufficient to substitute their work, while recent human-AI collaboration research aiming at work substitution depends on a stable high accuracy. This frees CatAlyst from domain-specific model-tuning and makes it applicable to various tasks. Our studies involving writing and slide-editing tasks demonstrated CatAlyst’s effectiveness in helping workers swiftly resume tasks with a lowered cognitive load. The results suggest a new form of human-AI collaboration where large generative models publicly available but imperfect for each individual domain can contribute to workers’ digital well-being.
UR  - https://arxiv.org/pdf/2302.05678v2.pdf
N1  - Source: page_2.html
ID  - 313
ER  - 
TY  - JOUR
TI  - Ethical Considerations for Responsible Data Curation
AU  - Andrews, J.T.A.
AU  - Modas, A.
AU  - Zhao, D.
AU  - Thong, W.
AU  - Xiang, A.
PY  - 2023
AB  - Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.
UR  - https://arxiv.org/pdf/2302.03629v3.pdf
N1  - Source: page_2.html
ID  - 314
ER  - 
TY  - JOUR
TI  - Causal effect of racial bias in machine learning algorithms affecting user persuasiveness & decision-making: An Empirical Study
AU  - Sengupta, K.
AU  - Srivastava, P.R.
PY  - 2023
AB  - Language data and models demonstrate various types of bias, be it ethnic, religious, gender, or socioeconomic. AI/NLP models, when trained on the racially biased dataset, AI/NLP models instigate poor model explainability, influence user experience during decision making, and thus further magnify societal biases, raising profound ethical implications for society. The motivation of the study is to investigate how AI systems imbibe bias introduced in data and further produce unexplainable discriminatory outcomes. This implication of the study will aid in understanding how machine learning models imbibe bias from data and try to influence individuals' articulateness of system outcomes due to bias features. The design of the experiment involves studying the counterfactual impact of racial bias features present in language datasets and its associated effect on the model outcome. A mixed research methodology is adopted to investigate the cross implication of biased model outcome on user experience effect on decision-making through controlled lab experimentation. The findings provide foundation support for correlating the importance of carry-over an artificial intelligence model solving NLP task due to biased concept presented in the dataset. Further, the research outcomes justify the negative influence on users' persuasiveness1 that leads to altering the decision-making quotient of an individual when trying to rely on the model outcome to act. The paper bridges the gap across the harm caused in establishing poor user trustworthiness due to an inequitable system design and provides strong support for researchers, policymakers, and data scientists to build responsible AI frameworks within organizations.
UR  - https://www.researchsquare.com/article/rs-2509731/v1.pdf
N1  - Source: page_2.html
ID  - 315
ER  - 
TY  - JOUR
TI  - Explainable AI in Biomedical Research: A Systematic Review and Meta-Analysis
AU  - Malinverno, L.
AU  - Barros, V.
AU  - Ghisoni, F.
AU  - Lovrić, M.
AU  - Rosen-Zvi, M.
PY  - 2023
AB  - Background: The black-box nature of most artificial intelligence (AI) models encourages the development of explainability methods to engender trust into the AI decision-making process. We aimed to analyze the possible role of Covid-19 in bringing explainable AI (XAI) to the forefront of biomedical research.Methods: In this systematic review and meta-analysis, we searched in the PubMed database for studies published from Jan 1, 2010, to Nov 3, 2022. Biomedical XAI papers were extracted based on a set of pre-defined keywords. We automatically excluded papers that were not related to concepts of causality or explainability and manually labeled 1603 papers with respect to XAI categories that best describe the study. We compared the trends pre- and post- Covid -19 and fit a change point detection model to evaluate significant changes in publication rates.Findings: We found an exponential growth of biomedical XAI papers. Specifically, an inflection period in the publication rate was observed in October 2020, when the quantity of XAI research in biomedical sciences surged upward significantly. When modeling the effect of the changepoint as a shift in time, we observed that such abrupt change would have been observed only 22 months later, had Covid -19 not happened.Interpretation: The advent of Covid -19 in the beginning of 2020 could be the driving factor behind the increased focus concerning XAI, accounting for the several months necessary to analyze data and publish manuscripts. However, our analyses were done based on the review of titles and abstracts of the studies. Establishing the causal nature of this link would require careful mining of the content of these papers.Funding: We acknowledge the funding received from the European Union’s Framework Programme for Research and Innovation Horizon 2020 (2014-2020) under the Marie Skłodowska-Curie Grant Agreement No. 813533-MSCA-ITN-2018. I.S. was funded by the “DDAI” COMET Module within the COMET – Competence Centers for Excellent Technologies Programme, funded by the Austrian Federal Ministry for Transport, Innovation and Technology (bmvit), the Austrian Federal Ministry for Digital and Economic Affairs (bmdw), the Austrian Research Promotion Agency (FFG), the province of Styria (SFG) and partners from industry and academia. The COMET Programme is managed by FFG. Finally, we acknowledge the Big Data Value Association (BDVA), Brussels, Belgium.Declaration of Interests: MRZ and VB are employees of IBM Research, Haifa, Israel. FM is an employee of Philips Research, Eindhoven, the Netherlands. IS has received funding from multiple funding agencies through a collaborative funding program and declare no support from any organization for the submitted work. P.J.N. receives funding from the Dutch Research Council (DWO) for the grant “Mobile Support Systems for Behavior Change”, of which he is the P.I. All other authors declare no competing interests.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4335108&type=2
N1  - Source: page_2.html
ID  - 316
ER  - 
TY  - JOUR
TI  - Audience-Centric Natural Language Generation via Style Infusion
AU  - Moorjani, S.
AU  - Krishnan, A.
AU  - Sundaram, H.
AU  - Maslowska, E.
AU  - Sankar, A.
PY  - 2023
AB  - Adopting contextually appropriate, audience-tailored linguistic styles is critical to the success of user-centric language generation systems (e.g., chatbots, computer-aided writing, dialog systems). While existing approaches demonstrate textual style transfer with large volumes of parallel or non-parallel data, we argue that grounding style on audience-independent external factors is innately limiting for two reasons. First, it is difficult to collect large volumes of audience-specific stylistic data. Second, some stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to define without audience feedback. In this paper, we propose the novel task of style infusion - infusing the stylistic preferences of audiences in pretrained language generation models. Since humans are better at pairwise comparisons than direct scoring - i.e., is Sample-A more persuasive/polite/empathic than Sample-B - we leverage limited pairwise human judgments to bootstrap a style analysis model and augment our seed set of judgments. We then infuse the learned textual style in a GPT-2 based text generator while balancing fluency and style adoption. With quantitative and qualitative assessments, we show that our infusion approach can generate compelling stylized examples with generic text prompts. The code and data are accessible at https://github.com/CrowdDynamicsLab/StyleInfusion.
UR  - https://arxiv.org/pdf/2301.10283v1.pdf
N1  - Source: page_2.html
ID  - 317
ER  - 
TY  - JOUR
TI  - Examining the Effectiveness of Social Cues Embedded in Ai-Driven Healthcare Chatbots: Trust in a Warm Human vs. A Competent Machine
AU  - Huang, M.
AU  - Ki, E.-J.
PY  - 2023
AB  - AI-powered healthcare chatbots with their great conversational potential promise to bring more benefits to the communication and relationships between health organizations and their publics by enhancing trust in human-computer interactions. To this end, this study conducted an experiment to examine how the interaction of different social cues shapes positive behavioral and relational outcomes through trust in chatbots. Results revealed that when users interacted with a healthcare chatbot with human source cues, they felt greater trust in the chatbot that showed warmth rather than competence. Conversely, when users interacted with a healthcare chatbot with machine source cues, a competent, rather than warm, conversational style generated greater trust in the chatbot. Trust in this human-computer interaction further enhanced users’ continued usage intention and recommendation adherence as well as relational trust and satisfaction with the health organization. These findings advance the literature on healthcare chatbots as social actors involved in organization-public communication and provide guidance for message persuasion and relationship management.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4333587&type=2
N1  - Source: page_2.html
ID  - 318
ER  - 
TY  - JOUR
TI  - AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents
AU  - Chen, P.-Y.
AU  - Tielman, M.L.
AU  - Heylen, D.K.J.
AU  - Jonker, C.M.
AU  - van Riemsdijk, M.B.
PY  - 2023
AB  - AI alignment is about ensuring AI systems only pursue goals and activities that are beneficial to humans. Most of the current approach to AI alignment is to learn what humans value from their behavioural data. This paper proposes a different way of looking at the notion of alignment, namely by introducing AI Alignment Dialogues: dialogues with which users and agents try to achieve and maintain alignment via interaction. We argue that alignment dialogues have a number of advantages in comparison to data-driven approaches, especially for behaviour support agents, which aim to support users in achieving their desired future behaviours rather than their current behaviours. The advantages of alignment dialogues include allowing the users to directly convey higher-level concepts to the agent, and making the agent more transparent and trustworthy. In this paper we outline the concept and high-level structure of alignment dialogues. Moreover, we conducted a qualitative focus group user study from which we developed a model that describes how alignment dialogues affect users, and created design suggestions for AI alignment dialogues. Through this we establish foundations for AI alignment dialogues and shed light on what requires further development and research.
UR  - https://arxiv.org/pdf/2301.06421v1.pdf
N1  - Source: page_2.html
ID  - 319
ER  - 
TY  - JOUR
TI  - UBIWEAR: An end-to-end, data-driven framework for intelligent physical activity prediction to empower mHealth interventions
AU  - Bampakis, A.
AU  - Yfantidou, S.
AU  - Vakali, A.
PY  - 2022
AB  - It is indisputable that physical activity is vital for an individual’s health and wellness. However, a global prevalence of physical inactivity has induced significant personal and socioeconomic implications. In recent years, a significant amount of work has showcased the capabilities of self-tracking technology to create positive health behavior change. This work is motivated by the potential of personalized and adaptive goal-setting techniques in encouraging physical activity via self-tracking. To this end, we propose UBIWEAR, an end-to-end framework for intelligent physical activity prediction, with the ultimate goal to empower data-driven goal-setting interventions. To achieve this, we experiment with numerous machine learning and deep learning paradigms as a robust benchmark for physical activity prediction tasks. To train our models, we utilize, “MyHeart Counts”, an open, large-scale dataset collected in-the-wild from thousands of users. We also propose a prescriptive framework for self-tracking aggregated data preprocessing, to facilitate data wrangling of real-world, noisy data. Our best model achieves a MAE of 1087 steps, 65% lower than the state of the art in terms of absolute error, proving the feasibility of the physical activity prediction task, and paving the way for future research.
UR  - https://arxiv.org/pdf/2212.14731v2.pdf
N1  - Source: page_2.html
ID  - 320
ER  - 
TY  - JOUR
TI  - Claim Optimization in Computational Argumentation
AU  - Skitalinskaya, G.
AU  - Spliethöver, M.
AU  - Wachsmuth, H.
PY  - 2022
AB  - An optimal delivery of arguments is key to persuasion in any debate, both for humans and for AI systems. This requires the use of clear and fluent claims relevant to the given debate. Prior work has studied the automatic assessment of argument quality extensively. Yet, no approach actually improves the quality so far. To fill this gap, this paper proposes the task of claim optimization: to rewrite argumentative claims in order to optimize their delivery. As multiple types of optimization are possible, we approach this task by first generating a diverse set of candidate claims using a large language model, such as BART, taking into account contextual information. Then, the best candidate is selected using various quality metrics. In automatic and human evaluation on an English-language corpus, our quality-based candidate selection outperforms several baselines, improving 60% of all claims (worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our approach often specifies claims with details, whereas it adds less evidence than humans do. Moreover, its capabilities generalize well to other domains, such as instructional texts.
UR  - https://arxiv.org/pdf/2212.08913v2.pdf
N1  - Source: page_2.html
ID  - 321
ER  - 
TY  - JOUR
TI  - Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games
AU  - Lai, B.
AU  - Zhang, H.
AU  - Liu, M.
AU  - Rehg, J.M.
AU  - Yang, D.
PY  - 2022
AB  - Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset, code, and models can be found at https://persuasion-deductiongame.socialai-data.org.
UR  - https://arxiv.org/pdf/2212.08279v1.pdf
N1  - Source: page_2.html
ID  - 322
ER  - 
TY  - JOUR
TI  - 'I Will Follow You!' – How Recommendation Modality Impacts Processing Fluency and Purchase Intention
AU  - Schwede, M.
AU  - Zierau, N.
AU  - Janson, A.
AU  - Hammerschmidt, M.
AU  - Leimeister, J.M.
PY  - 2022
AB  - Although conversational agents (CA) are increasingly used for providing purchase recommendations, important design questions remain. Across two experiments we examine with a novel fluency mechanism how recommendation modality (speech vs. text) shapes recommendation evaluation (persuasiveness and risk), the intention to follow the recommendation, and how modality interacts with the style of recommendation explanation (verbal vs. numerical). Findings provide robust evidence that text-based CAs outperform speech-based CAs in terms of processing fluency and consumer responses. They show that numerical explanations increase processing fluency and purchase intention of both recommendation modalities. The results underline the importance of processing fluency for the decision to follow a recommendation and highlight that processing fluency can be actively shaped through design decisions in terms of implementing the right modality and aligning it with the optimal explanation style. For practice, we offer actionable implications on how to make effective sales agents out of CAs.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4239822&type=2
N1  - Source: page_2.html
ID  - 323
ER  - 
TY  - JOUR
TI  - Using Persuasive Writing Strategies to Explain and Detect Health Misinformation
AU  - Kamali, D.
AU  - Romain, J.
AU  - Liu, H.
AU  - Meng, J.
AU  - Kordjamshidi, P.
PY  - 2022
AB  - Nowadays, the spread of misinformation is a prominent problem in society. Our research focuses on aiding the automatic identification of misinformation by analyzing the persuasive strategies employed in textual documents. We introduce a novel annotation scheme encompassing common persuasive writing tactics to achieve our objective. Additionally, we provide a dataset on health misinformation, thoroughly annotated by experts utilizing our proposed scheme. Our contribution includes proposing a new task of annotating pieces of text with their persuasive writing strategy types. We evaluate fine-tuning and prompt-engineering techniques with pre-trained language models of the BERT family and the generative large language models of the GPT family using persuasive strategies as an additional source of information. We evaluate the effects of employing persuasive strategies as intermediate labels in the context of misinformation detection. Our results show that those strategies enhance accuracy and improve the explainability of misinformation detection models. The persuasive strategies can serve as valuable insights and explanations, enabling other models or even humans to make more informed decisions regarding the trustworthiness of the information.
UR  - https://arxiv.org/pdf/2211.05985v4.pdf
N1  - Source: page_2.html
ID  - 324
ER  - 
TY  - JOUR
TI  - Behaviour change interventions based on automated and tailored communication systems to improve health outcomes in adult chronic patients: a systematic review
AU  - Martins, A.
AU  - Londral, A.
AU  - Lapão, L.V.
PY  - 2022
AB  - Background: Behaviour change digital-assisted interventions can be a low-cost solution to enduring healthy behaviours. Understanding how to automate and tailor such interventions to patients’ needs can improve health outcomes. This study aims at reviewing the evidence of the use of such interventions in a clinical setting in adult chronic patients. Methods: This study followed the PRISMA guidelines for comprehensive search, appraisal, and synthesis of research evidence. A literature search was performed to find studies published between 1 January 2017 and 26 March 2022 in peer-reviewed journals and written in English. Behaviour change interventions in adult chronic patients with automated and tailored communication systems were considered. Results: Seven quantitative studies (6 RCT and a pilot RCT) with sample sizes from 54 to 506 participants and timeframes varying from 4 to 30 weeks met the eligibility criteria. Populations comprehended patients with cardiovascular disease, chronic kidney disease, diabetes, or osteoarthritis. Communication channels included SMS, email, and voice records. Rule-based models were used to automate content in all studies and bidirectional communication was used in four of them. Content and service delivery were tailored given the patient’s goals, needs, functional ability, activity, personal characteristics, and communication channel preferred. Most data management platforms used were developed in prior studies. Interventions were designed based on 9 different behaviour change models. There was no evidence of improvements in clinical outcomes after the intervention. Some studies showed improvements in self-efficacy (n=1), the use of dietary data info (n=1), physical activity (n=2), social support (n=1), and quality of life (n=1). The risk of bias analysis revealed that one study had moderate risk and the remaining had low risk. Conclusion: This study supports researchers that are designing digital behaviour change interventions by putting in evidence the intervention’s features needed to be considered during this step. It revealed that the most efficient solutions were tailored to patients’ needs and disease stages and that further research on the mechanisms of change and content automation needs to be done.
UR  - https://www.researchsquare.com/article/rs-2175520/v1.pdf
N1  - Source: page_2.html
ID  - 325
ER  - 
TY  - JOUR
TI  - Artificial Emotions for Charity Collection: A Serial Mediation Through Perceived Anthropomorphism and Social Presence
AU  - Lee, S.
AU  - Park, G.
AU  - Chung, J.
PY  - 2022
AB  - Despite the broad application of chatbot agents in online interactions, an ongoing debate persists regarding their persuasive role and human-like emotional disclosure. Our study adds to this debate by exploring the effect of chatbot agents’ emotional disclosure on people’s willingness to donate to a charitable cause, and by examining individual and serial mediation between the main effects of perceived anthropomorphism and social presence. To this end, two types of artificial intelligence (AI) chatbot agents—one disclosing factual information and another disclosing human-like emotions—were developed and trained using Dialogflow, a natural language processing engine. A total of 619 US residents were recruited through Amazon Mechanical Turk, an online crowdsourcing platform. Of these, 593 participants completed the required conversation with either version of the AI chatbot agent (factual or emotional), as well as the survey questionnaire, and therefore, were included in the final analysis. The participants exhibited a higher willingness to donate when they interacted with the chatbot disclosing emotions than that disclosing factual information. Moreover, this study found both individual and serial mediating roles of perceived anthropomorphism and social presence. With regard to the implications, theoretically, this study adds to the understanding of applying the notion of human interaction to that involving humans and chatbots. Practically, our findings can be of great help in increasing willingness to donate thereby enhancing fund-raising activities.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4251057&type=2
N1  - Source: page_2.html
ID  - 326
ER  - 
TY  - JOUR
TI  - The Influence of Explainable Artificial Intelligence: Nudging Behaviour or Boosting Capability?
AU  - Franklin, M.
PY  - 2022
AB  - This article aims to provide a theoretical account and corresponding paradigm for analysing how explainable artificial intelligence (XAI) influences people’s behaviour and cognition. It uses insights from research on behaviour change. Two notable frameworks for thinking about behaviour change techniques are nudges - aimed at influencing behaviour - and boosts - aimed at fostering capability. It proposes that local and concept-based explanations are more adjacent to nudges, while global and counterfactual explanations are more adjacent to boosts. It outlines a method for measuring XAI influence and argues for the benefits of understanding it for optimal, safe and ethical human-AI collaboration.
UR  - https://arxiv.org/pdf/2210.02407v1.pdf
N1  - Source: page_2.html
ID  - 327
ER  - 
TY  - JOUR
TI  - How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI
AU  - Chen, K.
AU  - Shao, A.
AU  - Burapacheep, J.
AU  - Li, Y.
PY  - 2022
AB  - Autoregressive language models, which use deep learning to produce human-like texts, have become increasingly widespread. Such models are powering popular virtual assistants in areas like smart health, finance, and autonomous driving, and facilitating the production of creative writing in domains from the entertainment industry to science communities. While the parameters of these large language models are improving, concerns persist that these models might not work equally for different subgroups in society. Despite growing discussions of AI fairness across disciplines, systemic metrics lack to assess what equity means in dialogue systems and how to engage different populations in the assessment loop. Grounded in theories of deliberative democracy and science and technology studies, this paper proposes an analytical framework for evaluating equity in human-AI dialogues. Using this framework, we conducted an auditing study to examine how GPT-3 responded to different subpopulations on crucial science and social issues: climate change and the Black Lives Matter (BLM) movement. Our corpus consists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals who vary in gender, race and ethnicity, education level, English as a first language, and opinions toward the issues. We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat. We traced these user experience divides to conversational differences and found that GPT-3 used more negative expressions when it responded to the education and opinion minority groups, compared to its responses to the majority groups. To what extent GPT-3 uses justification when responding to minority groups is contingent on the issue. We discuss the implications of our findings for a deliberative conversational AI system that centralizes diversity, equity, and inclusion.
UR  - https://arxiv.org/pdf/2209.13627v2.pdf
N1  - Source: page_2.html
ID  - 328
ER  - 
TY  - JOUR
TI  - Exploring the potential of digital health interventions to address behavioural risk factors for the prevention of non-communicable diseases in Asian populations: A qualitative study
AU  - Mair, J.L.
AU  - Castro, O.
AU  - Salamanca-Sanabria, A.
AU  - Kowatsch, T.
AU  - Müller-Riemenschneider, F.
PY  - 2022
AB  - Background Changing lifestyle patterns over the last decades have seen growing numbers of people in Asia affected by non-communicable diseases and common mental health disorders, including diabetes, cancer, and/or depression. Interventions targeting healthy lifestyle behaviours through digital technologies, including new approaches such as chatbots, may be an effective, low-cost approach to prevent these conditions. To ensure uptake and engagement with digital health interventions, however, it is essential to understand the end-users’ perspectives on using such interventions. The aim of this study was to explore perceptions, barriers, and facilitators to the use of digital health interventions for lifestyle behaviour change in Singapore. Methods Six virtual focus group discussions were conducted with a total of 34 participants (mean ± SD; aged 45 ± 3.6 years; 64.7% females). Focus group recordings were transcribed verbatim and analysed using an inductive thematic analysis approach, followed by deductive mapping according to perceptions, barriers, facilitators, mixed factors, or strategies. Results Four themes were identified: (1) holistic wellbeing (i.e., the importance of both physical and mental health); (2) uptake of digital health interventions (i.e., factors influencing an individual’s decision to start using a digital health intervention such as incentives or government backing); (3) sustained engagement with digital health interventions (i.e., factors influencing an individual’s decision to continue using a digital health intervention such as personalisation or ease of use); and (4) chatbots (i.e., experiences with chatbots and their potential role in providing lifestyle behaviour support). Conclusions Findings highlighted several factors that are relevant for the effectiveness of digital health interventions. Deviations were found from factors that have been shown to be critical for (better-studied) Western populations. Recommendations from this work can inform those wishing to develop and implement digital health interventions in Singapore and other Asian countries.
UR  - https://www.researchsquare.com/article/rs-2068852/v1.pdf
N1  - Source: page_2.html
ID  - 329
ER  - 
TY  - JOUR
TI  - Generating Persuasive Responses to Customer Reviews with Multi-Source Prior Knowledge in E-commerce
AU  - Chen, B.
AU  - Liu, J.
AU  - Maimaiti, M.
AU  - Gao, X.
AU  - Zhang, J.
PY  - 2022
AB  - Customer reviews usually contain much information about one's online shopping experience. While positive reviews are beneficial to the stores, negative ones will largely influence consumers' decision and may lead to a decline in sales. Therefore, it is of vital importance to carefully and persuasively reply to each negative review and minimize its disadvantageous effect. Recent studies consider leveraging generation models to help the sellers respond. However, this problem is not well-addressed as the reviews may contain multiple aspects of issues which should be resolved accordingly and persuasively. In this work, we propose a Multi-Source Multi-Aspect Attentive Generation model for persuasive response generation. Various sources of information are appropriately obtained and leveraged by the proposed model for generating more informative and persuasive responses. A multi-aspect attentive network is proposed to automatically attend to different aspects in a review and ensure most of the issues are tackled. Extensive experiments on two real-world datasets, demonstrate that our approach outperforms the state-of-the-art methods and online tests prove that our deployed system significantly enhances the efficiency of the stores' dealing with negative reviews.
UR  - https://arxiv.org/pdf/2209.09497v1.pdf
N1  - Source: page_2.html
ID  - 330
ER  - 
TY  - JOUR
TI  - ImageArg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining
AU  - Liu, Z.
AU  - Guo, M.
AU  - Dai, Y.
AU  - Litman, D.
PY  - 2022
AB  - The growing interest in developing corpora of persuasive texts has promoted applications in automated systems, e.g., debating and essay scoring systems; however, there is little prior work mining image persuasiveness from an argumentative perspective. To expand persuasiveness mining into a multi-modal realm, we present a multi-modal dataset, ImageArg, consisting of annotations of image persuasiveness in tweets. The annotations are based on a persuasion taxonomy we developed to explore image functionalities and the means of persuasion. We benchmark image persuasiveness tasks on ImageArg using widely-used multimodal learning methods. The experimental results show that our dataset offers a useful resource for this rich and challenging topic, and there is ample room for modeling improvement.
UR  - https://arxiv.org/pdf/2209.06416v1.pdf
N1  - Source: page_2.html
ID  - 331
ER  - 
TY  - JOUR
TI  - SEQUENTIAL INFORMATION DESIGN: LEARNING TO PERSUADE IN THE DARK
AU  - Bernasconi, M.
AU  - Castiglioni, M.
AU  - Marchesi, A.
AU  - Gatti, N.
AU  - Trovò, F.
PY  - 2022
AB  - We study a repeated information design problem faced by an informed sender who tries to influence the behavior of a self-interested receiver. We consider settings where the receiver faces a sequential decision making (SDM) problem. At each round, the sender observes the realizations of random events in the SDM problem. This begets the challenge of how to incrementally disclose such information to the receiver to persuade them to follow (desirable) action recommendations. We study the case in which the sender does not know random events probabilities, and, thus, they have to gradually learn them while persuading the receiver. We start by providing a non-trivial polytopal approximation of the set of sender's persuasive information structures. This is crucial to design efficient learning algorithms. Next, we prove a negative result: no learning algorithm can be persuasive. Thus, we relax persuasiveness requirements by focusing on algorithms that guarantee that the receiver's regret in following recommendations grows sub-linearly. In the full-feedback setting-where the sender observes all random events realizations-, we provide an algorithm with Õ(√T) regret for both the sender and the receiver. Instead, in the bandit-feedback setting-where the sender only observes the realizations of random events actually occurring in the SDM problem-, we design an algorithm that, given an α ∈ [1/2, 1] as input, ensures Õ(Tα) and Õ(Tmax{α;1-α/2}) regrets, for the sender and the receiver respectively. This result is complemented by a lower bound showing that such a regrets trade-off is essentially tight.
UR  - https://arxiv.org/pdf/2209.03927v1.pdf
N1  - Source: page_2.html
ID  - 332
ER  - 
TY  - JOUR
TI  - ON THE HORIZON: INTERACTIVE AND COMPOSITIONAL DEEPFAKES
AU  - Horvitz, E.
PY  - 2022
AB  - Over a five-year period, computing methods for generating high-fidelity, fictional depictions of people and events moved from exotic demonstrations by computer science research teams into ongoing use as a tool of disinformation. The methods, referred to with the portmanteau of “deepfakes," have been used to create compelling audiovisual content. Here, I share challenges ahead with malevolent uses of two classes of deepfakes that we can expect to come into practice with costly implications for society: interactive and compositional deepfakes. Interactive deepfakes have the capability to impersonate people with realistic interactive behaviors, taking advantage of advances in multimodal interaction. Compositional deepfakes leverage synthetic content in larger disinformation plans that integrate sets of deepfakes over time with observed, expected, and engineered world events to create persuasive synthetic histories. Synthetic histories can be constructed manually but may one day be guided by adversarial generative explanation (AGE) techniques. In the absence of mitigations, interactive and compositional deepfakes threaten to move us closer to a post-epistemic world, where fact cannot be distinguished from fiction. I shall describe interactive and compositional deepfakes and reflect about cautions and potential mitigations to defend against them.
UR  - https://arxiv.org/pdf/2209.01714v1.pdf
N1  - Source: page_2.html
ID  - 333
ER  - 
TY  - JOUR
TI  - ESC-Rules: Explainable, Semantically Constrained Rule Sets
AU  - Glauer, M.
AU  - West, R.
AU  - Michie, S.
AU  - Hastings, J.
PY  - 2022
AB  - We describe a novel approach to explainable prediction of a continuous variable based on learning fuzzy weighted rules. Our model trains a set of weighted rules to maximise prediction accuracy and minimise an ontology-based’semantic loss’ function including user-specified constraints on the rules that should be learned in order to maximise the explainability of the resulting rule set from a user perspective. This system fuses quantitative sub-symbolic learning with symbolic learning and constraints based on domain knowledge. We illustrate our system on a case study in predicting the outcomes of behavioural interventions for smoking cessation, and show that it outperforms other interpretable approaches, achieving performance close to that of a deep learning model, while offering transparent explainability that is an essential requirement for decision-makers in the health domain.
UR  - https://arxiv.org/pdf/2208.12523v1.pdf
N1  - Source: page_2.html
ID  - 334
ER  - 
TY  - JOUR
TI  - Rationality-Robust Information Design: Bayesian Persuasion under Quantal Response
AU  - Feng, Y.
AU  - Ho, C.-J.
AU  - Tang, W.
PY  - 2022
AB  - Classic mechanism/information design imposes the assumption that agents are fully rational, meaning each of them always selects the action that maximizes her expected utility. Yet many empirical evidence suggests that human decisions may deviate from this full rationality assumption. In this work, we attempt to relax the full rationality assumption with bounded rationality. Specifically, we formulate the bounded rationality of an agent by adopting the quantal response model (McKelvey and Palfrey, 1995). We develop a theory of rationality-robust information design in the canonical setting of Bayesian persuasion (Kamenica and Gentzkow, 2011) with binary receiver action. We first identify conditions under which the optimal signaling scheme structure for a fully rational receiver remains optimal or approximately optimal for a boundedly rational receiver. In practice, it might be costly for the designer to estimate the degree of the receiver’s bounded rationality level. Motivated by this practical consideration, we then study the existence and construction of robust signaling schemes when there is uncertainty about the receiver’s bounded rationality level.
UR  - https://arxiv.org/pdf/2207.08253v3.pdf
N1  - Source: page_2.html
ID  - 335
ER  - 
TY  - JOUR
TI  - Artificial Intelligence (AI)-based Chatbots in Promoting Health Behavioral Changes: A Systematic Review
AU  - Aggarwal, A.
AU  - Tam, C.C.
AU  - Wu, D.
AU  - Li, X.
AU  - Qiao, S.
PY  - 2022
AB  - Background Artificial-Intelligence (AI)-based chatbots can offer personalized, engaging, and on-demand health-promotion interventions. This systematic review evaluates the feasibility, efficacy, and intervention characteristics of AI-chatbots in promoting health-behavior change. Methods A comprehensive search was conducted in seven bibliographic databases (PubMed, IEEE Xplore, ACM Digital Library, PsychoINFO, Web of Science, EMBASE, and JMIR publications) for empirical articles published from 1980 to 2022 that evaluated feasibility and/or efficacy of AI-chatbots for behavioral change. The screening, extraction, and analyses of identified articles followed the PRISMA guidelines. Results Of the 15 included studies, majority studies (n=11) reported high usability, acceptability and engagement, and some evidence on feasibility of AI-chatbots. Selected studies demonstrated high efficacy in promoting healthy lifestyles (n=6), smoking cessation (n=4), treatment/medication adherence (n=2), and reduction in substance misuse (n=1). Behavioral change theories and/or expert consultation were used to develop behavioral change strategies of AI-chatbots, including goal setting, monitoring, real-time reinforcement/feedback, and on-demand support. Real-time user-chatbot interaction data, such as user preferences and behavioral performance, were collected on the chatbot platform to identify ways of providing personalized services. The AI-chatbots demonstrated potential for scalability by deployment through accessible devices and platforms (e.g., smartphones and messenger). Participants also reported that AI-chatbots offered a non-judgmental space for communicating sensitive information. However, the reported results need to be interpreted with caution because of moderate to high risk of internal validity, insufficient description of AI-techniques, and limitation for generalizability. Conclusion AI-chatbots have demonstrated efficacy of health-behavior change interventions among large and diverse population; however, future studies need to adopt robust RCTs to establish definitive conclusions.
UR  - https://www.medrxiv.org/content/10.1101/2022.07.05.22277263v1.full.pdf
N1  - Source: page_2.html
ID  - 336
ER  - 
TY  - JOUR
TI  - Eco-driving for Electric Connected Vehicles at Signalized Intersections: A Parameterized Reinforcement Learning approach
AU  - Jiang, X.
AU  - Zhang, J.
AU  - Li, D.
PY  - 2022
AB  - This paper proposes an eco-driving framework for electric connected vehicles (CVs) based on reinforcement learning (RL) to improve vehicle energy efficiency at signalized intersections. The vehicle agent is specified by integrating the model-based car-following policy, lane-changing policy, and the RL policy, to ensure safe operation of a CV. Subsequently, a Markov Decision Process (MDP) is formulated, which enables the vehicle to perform longitudinal control and lateral decisions, jointly optimizing the car-following and lane-changing behaviors of the CVs in the vicinity of intersections. Then, the hybrid action space is parameterized as a hierarchical structure and thereby trains the agents with two-dimensional motion patterns in a dynamic traffic environment. Finally, our proposed methods are evaluated in SUMO software from both a single-vehicle-based perspective and a flow-based perspective. The results show that our strategy can significantly reduce energy consumption by learning proper action schemes without any interruption of other human-driven vehicles (HDVs).
UR  - https://arxiv.org/pdf/2206.12065v2.pdf
N1  - Source: page_2.html
ID  - 337
ER  - 
TY  - JOUR
TI  - Optimizing Fault Detection for Big Data Analytics Through Evolutionary Computation
AU  - Jothi, P.
AU  - Dwivedi, M.
PY  - 2022
AB  - Intelligent fault detection is promising to deal with big data due to its ability in rapidly and efficiently processing collected signals and providing accurate detection results. In traditional fault detection methods, however, the features are manually extracted depending on prior knowledge and diagnostic expertise, such processes take advantage of human ingenuity but are time-consuming. Inspired by the idea of unsupervised feature learning artificial intelligence techniques are used to learn features from the raw data. As the dimensionality increases, the accuracy of fault identification methods implemented on big data decreases significantly. For supervised learning, large volume of data is needed which leads to high cost and time consuming. In this paper, an unsupervised learning approach is proposed based on weighted softmax regression for fault detection using the power signals. Fault prediction has become an important subject in recent years, as it helps businesses to make significant savings in time and expense by offering successful methods for predictive maintenance. Pre-processing of data was a complicated job to overcome many problems with the dataset, including scale, sparsity, distortion, burst effects and confidence. As pre-monitor signals for failure did not share standard patterns, but were characterized only as non-normal system signals, a predictive error was made using outlier detection. Faults were explained by displaying system characteristics with abnormal values. An experimental assessment was conducted to determine the quality of the solution proposed. Results indicate that high-grade outliers provide successful markers of initial failures. In addition, explanations about irregular characteristic values (responsible for oversight) seem rather expressive. Based on the sliding window technique, the method to detect errors in high dimensional data streams is applied to an on-line mode. The online extension can be adapted to the time changing behaviour of the controlled system by experiments on synthetic datasets and is therefore applicable to the dynamic error detection. To assess the suggested strategy, we contrasted it with engineered data sets created utilizing the LOF (online expansion), SVDD, and KPCA approaches getting more than 90% result. The data exhibit our methodology's accomplishment as far as perfection, productivity, and strength.
UR  - https://www.researchsquare.com/article/rs-1748874/v2.pdf
N1  - Source: page_2.html
ID  - 338
ER  - 
TY  - JOUR
TI  - Argumentative Text Generation in Economic Domain | Генерация аргументативных текстов экономической тематики
AU  - Irina, F.N.
AU  - Dmitriy, O.
AU  - Klavdiya, B.O.
AU  - Evgeny, K.V.
PY  - 2022
AB  - The development of large and super-large language models, such as GPT-3, T5, Switch Transformer, ERNIE, etc., has significantly improved the performance of text generation. One of the important research directions in this area is the generation of texts with arguments. The solution of this problem can be used in business meetings, political debates, dialogue systems, for preparation of student essays. One of the main domains for these applications is the economic sphere. The key problem of the argument text generation for the Russian language is the lack of annotated argumentation corpora. In this paper, we use translated versions of the Argumentative Microtext, Persuasive Essays and UKP Sentential corpora to fine-tune RuBERT model. Further, this model is used to annotate the corpus of economic news by argumentation. Then the annotated corpus is employed to fine-tune the ruGPT-3 model, which generates argument texts. The results show that this approach improves the accuracy of the argument generation by more than 20 percentage points (63.2% vs. 42.5%) compared to the original ruGPT-3 model.
UR  - https://arxiv.org/pdf/2206.09251v1.pdf
N1  - Source: page_2.html
ID  - 339
ER  - 
TY  - JOUR
TI  - A Formal Understanding of Computational Empathy in Interactive Agents
AU  - Brännström, A.
AU  - Wester, J.
AU  - Nieves, J.C.
PY  - 2022
AB  - Rational software agents, embodied as conversational agents, such as chatbots, are progressively being used in the area of health and well-being. In such applications, where rational agents aim to engage users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding systems' empathic capabilities. In the current state-of-the-art, there are no tools to do that. In order to understand empathic capabilities in rational software systems, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in conversational agents for health and well-being, a formal definition (in terms of an ontology) of empathy is developed. We evaluate this formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition of empathy identifies different necessary conditions for assessing empathy in interactive agents.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=4138212&type=2
N1  - Source: page_2.html
ID  - 340
ER  - 
TY  - JOUR
TI  - Generation and Classification of Motivational-Interviewing-Style Reflections for Smoking Behaviour Change Using Few-Shot Learning with Transformers
AU  - Rose, J.
AU  - Ahmed, I.
AU  - Keilty, E.
AU  - Cooper, C.
AU  - Selby, P.
PY  - 2022
AB  - If conversational agents can take on a therapeutic role, they may provide a scalable way to help many people suffering from addictions. Motivational Interviewing (MI) is a validated therapy for behaviour change that can be applied to addiction, including smoking cessation. A core technique in MI (and many other kinds of talk therapy) is to pose an open-ended question concerning a negative behaviour, and then to provide a reflection of the response. Reflections can be a simple restatement of the response, or a more complex inference from prior statements or general knowledge, and they help a person contemplate the behaviour more deeply. We describe a method to generate reflections that uses few-shot priming of the GPT-2 and GPT-3 language models. These produce very promising simple and complex reflections, but also some that are off-topic or irrelevant. To filter these, we train a classifier to detect poor reflections, employing samples labeled by an MI expert. Its accuracy is 81%, sensitivity 90% and specificity 71%. We show that GPT-2 can generate acceptable reflections at a 54% success rate, and when combined with the classifier/filter produces acceptable reflections 73% of the time. The GPT-3 model has a native success rate of 89%.
UR  - https://dx.doi.org/10.36227/techrxiv.20029880.v1
N1  - Source: page_2.html
ID  - 341
ER  - 
TY  - JOUR
TI  - Integrating fuzzy trajectory data and artificial intelligence methods for multi-style lane-changing behavior prediction
AU  - Gu, R.
PY  - 2022
AB  - Artificial intelligence algorithms have been extensively applied in the field of intelligent transportation, especially for driving behavior analysis and prediction. This study proposes a novel framework by integrating fuzzy trajectory data, unsupervised learning and supervised learning methods to predict lane-changing behaviors taking multi driving styles into account. The microscopic trajectory data from the Highway Drone Dataset (HighD) are employed to construct two types of datasets including precise trajectory datasets and fuzzy trajectory datasets for lane-changing prediction models. The fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm. Two typical supervised learning methods, including random forest and long-short-term memory combined with convolutional neural network, are further applied for lane-changing behavior prediction. Results indicate that (1) the proposed integration approach performs better than the conventional lane-change prediction; (2) the relative speed-related features have a greater contribution to the lane-changing prediction after being processed by fuzzy rules based on driving styles; and (3) the difference among driving styles is more reflected from the state of lateral movement rather than the lane-changing duration.
UR  - https://arxiv.org/pdf/2205.05016v1.pdf
N1  - Source: page_2.html
ID  - 342
ER  - 
TY  - JOUR
TI  - Artificial Intelligence with CBC based Morphometric Parameters aimed toward effective diagnostic practices for Dysplasia associated Hematological Malignancies
AU  - Haider, Z.
AU  - Ujjan, I.U.
AU  - Khan, N.A.
AU  - Urrechaga, E.
AU  - Khanani, M.R.
PY  - 2022
AB  - Background: The diagnosis and classification of dysplasia associated hematological neoplasms is dominated by morphology. Current round of study made use of potential ‘fingerprints’ among routinely generated diagnostic data particularly morphological and immature fraction-related (morphometric) parameters produced during routine complete blood count (CBC) testing in hemat-oncology department through artificial intelligence predictive modeling. Methods: Along conventional statistics, neural network models were trained on anonymized demographical, clinical and diagnostic data of total 1624 individuals with common hematological neoplasms. In addition, validation conducted on independent dataset. The frameworks were trained to differentiate hematological malignancies cases with various sub-entities of Dysplasia against Non-dysplastic group as a control cohort. Results: Our predictive model attained greater precisions; percent incorrect prediction were remained 10.3 and 4.6 for training and testing phases, respectively along with a 95.4% negative predictive value (NPV). Moreover, higher accuracy (93.1%) was obtained during prospective validation in challenge of independent dataset. Model’s performance related metrics: the gain and lift chart, predictive-pseudo probability chart, and receiver operative curve (ROC) curve were noted as persuasive. Considering the sensitivity and specificity, area under the curve (AUC) values were also noted quite convincing; 0.954 for Non-dysplasia group while 0.994 for acute myeloid leukemia with dysplasia (AML-Dys) followed by 0.992, 0.988, 0.986, 0.984, 0.973, and 0.962 for chronic myelo-monocytic leukemia (CMML), refractory anemia with excess blast-I (RAEB-I), refractory anemia with excess blast-II (RAEB-II), refractory anemia with multi-lineage dysplasia (RCMD), Hypoplastic myelodysplastic syndrome (H-MDS), and refractory anemia with uni-lineage dysplasia (RCUD) respectively. Conclusion: The negative predictive efficiency of our framework advocates its utility as a screening tool for the rapid expulsion of Dysplasia associated hematological neoplasms in hemat-oncology sections, aiding prompt care decisions.
UR  - https://www.researchsquare.com/article/rs-1349008/v1.pdf
N1  - Source: page_2.html
ID  - 343
ER  - 
TY  - JOUR
TI  - Interacting with Non-Cooperative User: A New Paradigm for Proactive Dialogue Policy
AU  - Lei, W.
AU  - Zhang, Y.
AU  - Song, F.
AU  - Yang, Z.
AU  - Chua, T.-S.
PY  - 2022
AB  - Proactive dialogue system is able to lead the conversation to a goal topic and has advantaged potential in bargain, persuasion and negotiation. Current corpus-based learning manner limits its practical application in real-world scenarios. To this end, we contribute to advance the study of the proactive dialogue policy to a more natural and challenging setting, i.e., interacting dynamically with users. Further, we call attention to the non-cooperative user behavior — the user talks about off-path topics when he/she is not satisfied with the previous topics introduced by the agent. We argue that the targets of reaching the goal topic quickly and maintaining a high user satisfaction are not always converge, because the topics close to the goal and the topics user preferred may not be the same. Towards this issue, we propose a new solution named I-Pro that can learn Proactive policy in the Interactive setting. Specifically, we learn the trade-off via a learned goal weight, which consists of four factors (dialogue turn, goal completion difficulty, user satisfaction estimation, and cooperative degree). The experimental results demonstrate I-Pro significantly outperforms baselines in terms of effectiveness and interpretability.
UR  - https://arxiv.org/pdf/2204.07433v1.pdf
N1  - Source: page_2.html
ID  - 344
ER  - 
TY  - JOUR
TI  - ZETAR: Modeling and Computational Design of Strategic and Adaptive Compliance Policies
AU  - Huang, L.
AU  - Zhu, Q.
PY  - 2022
AB  - Compliance management plays an important role in mitigating insider threats. Incentive design is a proactive and non-invasive approach to achieving compliance by aligning an insider’s incentive with the defender’s security objective, which motivates (rather than commands) an insider to act in the organization’s interests. Controlling insiders’ incentives for population-level compliance is challenging because they are neither precisely known nor directly controllable. To this end, we develop ZETAR, a zero-trust audit and recommendation framework, to provide a quantitative approach to model insiders’ incentives and design customized recommendation policies to improve their compliance. We formulate primal and dual convex programs to compute the optimal bespoke recommendation policies. We create the theoretical underpinning for understanding trust, compliance, and satisfaction, which leads to scoring mechanisms of how compliant and persuadable an insider is. After classifying insiders as malicious, self-interested, or amenable based on their incentive misalignment levels with the defender, we establish bespoke information disclosure principles for these insiders of different incentive categories. We identify the policy separability principle and the set convexity, which enable finite-step algorithms to efficiently learn the Completely Trustworthy (CT) policy set when insiders’ incentives are unknown. Finally, we present a case study to corroborate the design. Our results show that ZETAR can well adapt to insiders with different risk and compliance attitudes and significantly improve compliance. Moreover, trustworthy recommendations can provably promote cyber hygiene and insiders’ satisfaction.
UR  - https://arxiv.org/pdf/2204.02294v2.pdf
N1  - Source: page_2.html
ID  - 345
ER  - 
TY  - JOUR
TI  - ProHealth eCoach: User-Centered Design and Development of an eCoach App to Promote Healthy Lifestyle with Personalized Activity Recommendations
AU  - Chatterjee, A.
AU  - Prinz, A.
AU  - Gerdes, M.
AU  - Pahari, N.
AU  - Meena, Y.K.
PY  - 2022
AB  - Background Regular physical activity (PA), healthy habits, and a healthy diet are recommended guidelines to maintain a healthy lifestyle. A healthy lifestyle can help to avoid chronic diseases and long-term illnesses. A monitoring and automatic personalized lifestyle recommendation system (i.e., automatic electronic coach or eCoach) with considering clinical and ethical guidelines, individual health status, condition, and preferences may successfully help participants to follow recommendations to maintain a healthy lifestyle. As a prerequisite for the prototype design of such a helpful eCoach system, it is essential to involve the end-users and subject-matter experts throughout the iterative design process. Methods We used an iterative user-centered design (UCD) approach with a dialogue-labs method to collect qualitative data from different users (i.e., researchers, non-technical and technical, health professionals, subject-matter experts, and potential end-users) to develop a roadmap for self-management with eCoaching. We designed and developed the eCoach prototype in two stages, adopting different phases of the UCD approach and co-design events. In UCD workshop 1, we focused on identifying end-users, understanding the user’s context, specifying user requirements, designing and developing an initial low-fidelity eCoach prototype. In UCD workshop 2, we focused on maturing the low-fidelity solution design and development for the visualization of continuous and discrete data, artificial intelligence (AI)-based interval forecasting, personalized recommendations, and activity goals. Results The UCD approach helped to develop a working prototype of eCoach system that meets end-user's requirements and expectations towards an effective recommendation visualization, considering diversity in culture, quality of life, and human values. The design provides an early version of the solution, consisting of wearable technology, a mobile app, and web content for self-monitoring, goal setting, and lifestyle recommendations in an engaging manner between the eCoach app and end-users. Conclusions The adopted UCD approach brings in a design focus on the user and their needs at each phase. Throughout the design process, users have been involved at the heart of the design to create a working research prototype to improve the fit between technology, end-user, and researchers. Furthermore, we performed a technological readiness study of ProHealth eCoach against standard levels set by European Union (EU).
UR  - https://www.researchsquare.com/article/rs-1451563/v1.pdf
N1  - Source: page_2.html
ID  - 346
ER  - 
TY  - JOUR
TI  - An Audit of Misinformation Filter Bubbles on YouTube: Bubble Bursting and Recent Behavior Changes
AU  - Tomlein, M.
AU  - Pecher, B.
AU  - Simko, J.
AU  - Podrouzek, J.
AU  - Bielikova, M.
PY  - 2022
AB  - The negative effects of misinformation filter bubbles in adaptive systems have been known to researchers for some time. Several studies investigated, most prominently on YouTube, how fast a user can get into a misinformation filter bubble simply by selecting “wrong choices” from the items offered. Yet, no studies so far have investigated what it takes to “burst the bubble”, i.e., revert the bubble enclosure. We present a study in which pre-programmed agents (acting as YouTube users) delve into misinformation filter bubbles by watching misinformation promoting content (for various topics). Then, by watching misinformation debunking content, the agents try to burst the bubbles and reach more balanced recommendation mixes. We recorded the search results and recommendations, which the agents encountered, and analyzed them for the presence of misinformation. Our key finding is that bursting of a filter bubble is possible, albeit it manifests differently from topic to topic. Moreover, we observe that filter bubbles do not truly appear in some situations. We also draw a direct comparison with a previous study. Sadly, we did not find much improvements in misinformation occurrences, despite recent pledges by YouTube.
UR  - https://arxiv.org/pdf/2203.13769v1.pdf
N1  - Source: page_2.html
ID  - 347
ER  - 
TY  - JOUR
TI  - Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI
AU  - Franklin, M.
AU  - Ashton, H.
AU  - Gorman, R.
AU  - Armstrong, S.
PY  - 2022
AB  - As artificial intelligence becomes more powerful and a ubiquitous presence in daily life, it is imperative to understand and manage the impact of AI systems on our lives and decisions. Modern ML systems often change user behavior (e.g. personalized recommender systems learn user preferences to deliver recommendations that change online behavior). An externality of behavior change is preference change. This article argues for the establishment of a multidisciplinary endeavor focused on understanding how AI systems change preference: Preference Science. We operationalize preference to incorporate concepts from various disciplines, outlining the importance of meta-preferences and preference-change preferences, and proposing a preliminary framework for how preferences change. We draw a distinction between preference change, permissible preference change, and outright preference manipulation. A diversity of disciplines contribute unique insights to this framework.
UR  - https://arxiv.org/pdf/2203.10525v2.pdf
N1  - Source: page_2.html
ID  - 348
ER  - 
TY  - JOUR
TI  - Reducing Flipping Errors in Deep Neural Networks
AU  - Deng, X.
AU  - Xiao, Y.
AU  - Long, B.
AU  - Zhang, Z.
PY  - 2022
AB  - Deep neural networks (DNNs) have been widely applied in various domains in artificial intelligence including computer vision and natural language processing. A DNN is typically trained for many epochs and then a validation dataset is used to select the DNN in an epoch (we simply call this epoch “the last epoch”) as the final model for making predictions on unseen samples, while it usually cannot achieve a perfect accuracy on unseen samples. An interesting question is “how many test (unseen) samples that a DNN misclassifies in the last epoch were ever correctly classified by the DNN before the last epoch?”. In this paper, we empirically study this question and find on several benchmark datasets that the vast majority of the misclassified samples in the last epoch were ever classified correctly before the last epoch, which means that the predictions for these samples were flipped from “correct” to “wrong”. Motivated by this observation, we propose to restrict the behavior changes of a DNN on the correctly-classified samples so that the correct local boundaries can be maintained and the flipping error on unseen samples can be largely reduced. Extensive experiments on different benchmark datasets with different modern network architectures demonstrate that the proposed flipping error reduction (FER) approach can substantially improve the generalization, the robustness, and the transferability of DNNs without introducing any additional network parameters or inference cost, only with a negligible training overhead.
UR  - https://arxiv.org/pdf/2203.08390v1.pdf
N1  - Source: page_2.html
ID  - 349
ER  - 
TY  - JOUR
TI  - Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue
AU  - Chen, M.
AU  - Shi, W.
AU  - Yan, F.
AU  - Sahay, S.
AU  - Yu, Z.
PY  - 2022
AB  - Complex conversation settings such as persuasion involve communicating changes in attitude or behavior, so users' perspectives need to be addressed, even when not directly related to the topic. In this work, we contribute a novel modular dialogue system framework that seamlessly integrates factual information and social content into persuasive dialogue. Our framework is generalizable to any dialogue tasks that have mixed social and task contents. We conducted a study that compared user evaluations of our framework versus a baseline end-to-end generation model. We found our framework was evaluated more favorably in all dimensions including competence and friendliness, compared to the end-toend model which does not explicitly handle social content or factual questions.
UR  - https://arxiv.org/pdf/2203.07657v3.pdf
N1  - Source: page_2.html
ID  - 350
ER  - 
TY  - JOUR
TI  - Influencing Long-Term Behavior in Multiagent Reinforcement Learning
AU  - Kim, D.-K.
AU  - Riemer, M.
AU  - Liu, M.
AU  - Tesauro, G.
AU  - How, J.P.
PY  - 2022
AB  - The main challenge of multiagent reinforcement learning is the difficulty of learning useful policies in the presence of other simultaneously learning agents whose changing behaviors jointly affect the environment's transition and reward dynamics. An effective approach that has recently emerged for addressing this non-stationarity is for each agent to anticipate the learning of other agents and influence the evolution of future policies towards desirable behavior for its own benefit. Unfortunately, previous approaches for achieving this suffer from myopic evaluation, considering only a finite number of policy updates. As such, these methods can only influence transient future policies rather than achieving the promise of scalable equilibrium selection approaches that influence the behavior at convergence. In this paper, we propose a principled framework for considering the limiting policies of other agents as time approaches infinity. Specifically, we develop a new optimization objective that maximizes each agent's average reward by directly accounting for the impact of its behavior on the limiting set of policies that other agents will converge to. Our paper characterizes desirable solution concepts within this problem setting and provides practical approaches for optimizing over possible outcomes. As a result of our farsighted objective, we demonstrate better long-term performance than state-of-the-art baselines across a suite of diverse multiagent benchmark domains.
UR  - https://arxiv.org/pdf/2203.03535v4.pdf
N1  - Source: page_2.html
ID  - 351
ER  - 
TY  - JOUR
TI  - Metacognitive Agents for Ethical Decision Support: Conceptual Model and Research Roadmap
AU  - Kennedy, C.M.
PY  - 2022
AB  - An ethical value-action gap exists when there is a discrepancy between intentions and actions. For example, people who support environmental sustainability often use cars and short-haul flights because of convenience and time-pressure. This discrepancy may be caused by social and structural obstacles as well as cognitive biases. Current technology can make this worse. For example, social media tends to enhance emotions such as anger or fear, which can result in polarisation and impulsive decisions. Computational models of cognition and affect can provide insights into the value-action gap and how it can be reduced. Such models include dual process architectures, emotion models and behaviour change theories. In particular, metacognition (“thinking about thinking”) plays an important role in many of these models as a mechanism for self-regulation and for reasoning about mental attitudes. This paper outlines a roadmap for translating cognitive-affective models into assistant agents to help make value-aligned decisions. Key principles include “agile” rapid-prototyping using agent-based simulation, and the combination of descriptive and normative models into a single agent architecture.
UR  - https://arxiv.org/pdf/2202.12039v1.pdf
N1  - Source: page_2.html
ID  - 352
ER  - 
TY  - JOUR
TI  - Sequential Information Design: Markov Persuasion Process and Its Efficient Reinforcement Learning
AU  - Wu, J.
AU  - Zhang, Z.
AU  - Feng, Z.
AU  - Jordan, M.I.
AU  - Xu, H.
PY  - 2022
AB  - In today's economy, it becomes important for Internet platforms to consider the sequential information design problem to align its long term interest with incentives of the gig service providers (e.g., drivers, hosts). This paper proposes a novel model of sequential information design, namely the Markov persuasion processes (MPPs). Specifically, in an MPP, a sender, with informational advantage, seeks to persuade a stream of myopic receivers to take actions that maximizes the sender's cumulative utilities in a finite horizon Markovian environment with varying prior and utility functions. Planning in MPPs thus faces the unique challenge in finding a signaling policy that is simultaneously persuasive to the myopic receivers and inducing the optimal long-term cumulative utilities of the sender. Nevertheless, in the population level where the model is known, it turns out that we can efficiently determine the optimal (resp. ǫ-optimal) policy with finite (resp. infinite) states and outcomes, through a modified formulation of the Bellman equation that additionally takes persuasiveness into consideration. Our main technical contribution is to study the MPP under the online reinforcement learning (RL) setting, where the goal is to learn the optimal signaling policy by interacting with with the underlying MPP, without the knowledge of the sender's utility functions, prior distributions, and the Markov transition kernels. For such a problem, we design a provably efficient no-regret learning algorithm, the Optimism-Pessimism Principle for Persuasion Process (OP4), which features a novel combination of both optimism and pessimism principles. In particular, we obtain optimistic estimates of the value functions to encourage exploration under the unknown environment. Meanwhile, we additionally robustify the signaling policy with respect to the uncertainty of prior estimation to prevent receiver's detrimental equilibrium behavior. Our algorithm enjoys sample efficiency by achieving a sublinear √T -regret upper bound. Furthermore, both our algorithm and theory can be applied to MPPs with large space of outcomes and states via function approximation, and we showcase such a success under the linear setting.
UR  - https://arxiv.org/pdf/2202.10678v1.pdf
N1  - Source: page_2.html
ID  - 353
ER  - 
TY  - JOUR
TI  - Automatic Generation of Individual Fuzzy Cognitive Maps from Longitudinal Data
AU  - Wozniak, M.K.
AU  - Mkhitaryan, S.
AU  - Giabbanelli, P.J.
PY  - 2022
AB  - Fuzzy Cognitive Maps (FCMs) are computational models that represent how factors (nodes) change over discrete interactions based on causal impacts (weighted directed edges) from other factors. This approach has traditionally been used as an aggregate, similarly to System Dynamics, to depict the functioning of a system. There has been a growing interest in taking this aggregate approach at the individual-level, for example by equipping each agent of an Agent-Based Model with its own FCM to express its behavior. Although frameworks and studies have already taken this approach, an ongoing limitation has been the difficulty of creating as many FCMs as there are individuals. Indeed, current studies have been able to create agents whose traits are different, but whose decision-making modules are often identical, thus limiting the behavioral heterogeneity of the simulated population. In this paper, we address this limitation by using Genetic Algorithms to create one FCM for each agent, thus providing the means to automatically create a virtual population with heterogeneous behaviors. Our algorithm builds on prior work from Stach and colleagues by introducing additional constraints into the process and applying it over longitudinal, individual-level data. A case study from a real-world intervention on nutrition confirms that our approach can generate heterogeneous agents that closely follow the trajectories of their real-world human counterparts. Future works include technical improvements such as lowering the computational time of the approach, or case studies in computational intelligence that use our virtual populations to test new behavior change interventions.
UR  - https://arxiv.org/pdf/2202.07065v1.pdf
N1  - Source: page_2.html
ID  - 354
ER  - 
TY  - JOUR
TI  - A Simplified Variant of Gödel's Ontological Argument
AU  - Benzmüller, C.
PY  - 2022
AB  - A simplified variant of Gödel's ontological argument is presented. The simplified argument is valid already in basic modal logics K or KT, it does not suffer from modal collapse, and it avoids the rather complex predicates of essence (Ess.) and necessary existence (NE) as used by Gödel. The variant presented has been obtained as a side result of a series of theory simplification experiments conducted in interaction with a modern proof assistant system. The starting point for these experiments was the computer encoding of Gödel's argument, and then automated reasoning techniques were systematically applied to arrive at the simplified variant presented. The presented work thus exemplifies a fruitful human-computer interaction in computational metaphysics. Whether the presented result increases or decreases the attractiveness and persuasiveness of the ontological argument is a question I would like to pass on to philosophy and theology.
UR  - https://arxiv.org/pdf/2202.06264v3.pdf
N1  - Source: page_2.html
ID  - 355
ER  - 
TY  - JOUR
TI  - Measuring why in recommender systems: A comprehensive survey on the evaluation of explainable recommendation
AU  - Chen, X.
AU  - Zhang, Y.
AU  - Wen, J.-R.
PY  - 2022
AB  - Explainable recommendation has shown its great advantages for improving recommendation persuasiveness, user satisfaction, system transparency and among others. A fundamental problem of explainable recommendation is how to evaluate the explanations. In the past few years, various evaluation strategies have been proposed. However, they are scattered in different papers, and there lacks a systematic and detailed comparison between them. To bridge this gap, in this paper, we comprehensively review the previous work, and provide different taxonomies according to the evaluation perspectives and evaluation methods. Beyond summarization, we also analyze the (dis)advantages of existing evaluation methods, and provide a series of guidelines on how to select them. The contents of this survey are concluded from more than 100 papers from top-tier conferences like IJCAI, AAAI, TheWebConf, Recsys, UMAP and IUI, and the complete comparisons are presented at https://shimo.im/sheets/VKrpYTcwVH6KXgdy/MO DOC/. With this survey, we finally aim to provide a clear and comprehensive review on the evaluation of explainable recommendation.
UR  - https://arxiv.org/pdf/2202.06466v1.pdf
N1  - Source: page_2.html
ID  - 356
ER  - 
TY  - JOUR
TI  - Diagnosis of Phobic Anxiety Disorders using Virtual Reality Environment
AU  - Munir, A.
AU  - Saleem, Y.
PY  - 2022
AB  - Specific Phobia Anxiety disorders are rarely detected in a clinical setting and are indicators of other grave mental health problems. VR is a persuasive tool for diagnosing and treating psychiatric problems, but the Virtual Reality applications developed are reported to lack a methodological approach. This particular research consults the Diagnostic and Statistical Manual of Mental Health Disorders (DSM-5) by the American Psychiatric Association (APA)to strengthen objectivity and add reliability to the assessment. We investigated participants’ behavior in the Virtual Reality Environment. The participant walks or runs along a road in four different scenarios of a Virtual Reality Environment. The first scenario is a neutral environment with no phobic stimulus; the afterward situations pose for a dog cue, thunder lightning stimulus, and a combination of both stimulation consecutively. The assessment measures include distance and time calculated in each VRE scene, minimum and maximum oxygen levels and heart rate recorded for each scenario, physical and verbal reactions observed, self-reported measure of Cybersickness, and Sense of Presence. Participant’s information such as name, age, gender, any bad experience with dogs or thunder lightning, and proposed phobia severity levels of parents. The “APA Specific Phobia Severity Measure - Adult” by DSM-5 classifies every record. Applying Artificial Intelligence learning models suggests Astraphobic patients also suffer from Dog phobia. The results imply that the Sense of Presence is a decisive factor in the assessment. It signifies that distance, time interval, and change in oxygen levels in the presence of phobic cues are influential parameters for phobia estimation.
UR  - https://dx.doi.org/10.36227/techrxiv.19153769.v1
N1  - Source: page_2.html
ID  - 357
ER  - 
TY  - JOUR
TI  - Graph Convolution-Based Deep Reinforcement Learning for Multi-Agent Decision-Making in Mixed Traffic Environments
AU  - Liu, Q.
AU  - Li, Z.
AU  - Li, X.
AU  - Wu, J.
AU  - Yuan, S.
PY  - 2022
AB  - An efficient and reliable multi-agent decision-making system is highly demanded for the safe and efficient operation of connected autonomous vehicles in intelligent transportation systems. Current researches mainly focus on the Deep Reinforcement Learning (DRL) methods. However, utilizing DRL methods in interactive traffic scenarios is hard to represent the mutual effects between different vehicles and model the dynamic traffic environments due to the lack of interactive information in the representation of the environments, which results in low accuracy of cooperative decisions generation. To tackle these difficulties, this research proposes a framework to enable different Graph Reinforcement Learning (GRL) methods for decision-making, and compares their performance in interactive driving scenarios. GRL methods combinate the Graph Neural Network (GNN) and DRL to achieve the better decisions generation in interactive scenarios of autonomous vehicles, where the features of interactive scenarios are extracted by the GNN, and cooperative behaviors are generated by DRL framework. Several GRL approaches are summarized and implemented in the proposed framework. To evaluate the performance of the proposed GRL methods, an interactive driving scenarios on highway with two ramps is constructed, and simulated experiment in the SUMO platform is carried out to evaluate the performance of different GRL approaches. Finally, results are analyzed in multiple perspectives and dimensions to compare the characteristic of different GRL approaches in intelligent transportation scenarios. Results show that the implementation of GNN can well represents the interaction between vehicles, and the combination of GNN and DRL is able to improve the performance of the generation of lane-change behaviors. The source code of our work can be found at https://github.com/Jacklinkk/TorchGRL.
UR  - https://arxiv.org/pdf/2201.12776v1.pdf
N1  - Source: page_2.html
ID  - 358
ER  - 
TY  - JOUR
TI  - Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study
AU  - Sengupta, K.
AU  - Srivastava, P.R.
PY  - 2022
AB  - Language data and models demonstrate various types of bias, be it ethnic, religious, gender, or socioeconomic. AI/NLP models, when trained on the racially biased dataset, AI/NLP models instigate poor model explainability, influence user experience during decision making and thus further magnifies societal biases, raising profound ethical implications for society. The motivation of the study is to investigate how AI systems imbibe bias from data and produce unexplainable discriminatory outcomes and influence an individual's articulateness of system outcome due to the presence of racial bias features in datasets. The design of the experiment involves studying the counterfactual impact of racial bias features present in language datasets and its associated effect on the model outcome. A mixed research methodology is adopted to investigate the cross implication of biased model outcome on user experience, effect on decision-making through controlled lab experimentation. The findings provide foundation support for correlating the implication of carry-over an artificial intelligence model solving NLP task due to biased concept presented in the dataset. Further, the research outcomes justify the negative influence on users' persuasiveness that leads to alter the decision-making quotient of an individual when trying to rely on the model outcome to act. The paper bridges the gap across the harm caused in establishing poor customer trustworthiness due to an inequitable system design and provides strong support for researchers, policymakers, and data scientists to build responsible AI frameworks within organizations.
UR  - https://arxiv.org/pdf/2202.00471v2.pdf
N1  - Source: page_2.html
ID  - 359
ER  - 
TY  - JOUR
TI  - Bayesian Promised Persuasion: Dynamic Forward-Looking Multiagent Delegation with Informational Burning
AU  - Zhang, T.
AU  - Zhu, Q.
PY  - 2022
AB  - This work studies a dynamic mechanism design problem in which a principal delegates decision makings to a group of privately-informed agents without the monetary transfer or burning. We consider that the principal privately possesses complete knowledge about the state transitions and study how she can use her private observation to support the incentive compatibility of the delegation via informational burning, a process we refer to as the looking-forward persuasion. The delegation mechanism is formulated in which the agents form belief hierarchies due to the persuasion and play a dynamic Bayesian game. We propose a novel randomized mechanism, known as Bayesian promised delegation (BPD), in which the periodic incentive compatibility is guaranteed by persuasions and promises of future delegations. We show that the BPD can achieve the same optimal social welfare as the original mechanism in stationary Markov perfect Bayesian equilibria. A revelation-principle-like design regime is established to show that the persuasion with belief hierarchies can be fully characterized by correlating the randomization of the agents' local BPD mechanisms with the persuasion as a direct recommendation of the future promises.
UR  - https://arxiv.org/pdf/2201.06081v1.pdf
N1  - Source: page_2.html
ID  - 360
ER  - 
TY  - JOUR
TI  - Lane Change Decision-Making through Deep Reinforcement Learning
AU  - Ghimire, M.
AU  - Choudhury, M.R.
AU  - Lagudu, G.S.S.H.
PY  - 2021
AB  - Due to the complexity and volatility of the traffic environment, decision-making in autonomous driving is a significantly hard problem. In this project, we use a Deep Q-Network, along with rule-based constraints to make lane-changing decision. A safe and efficient lane change behavior may be obtained by combining high-level lateral decision-making with low-level rule-based trajectory monitoring. The agent is anticipated to perform appropriate lane-change maneuvers in a real-world-like udacity simulator after training it for a total of 100 episodes. The results shows that the rule-based DQN performs better than the DQN method. The rule-based DQN achieves a safety rate of 0.8 and average speed of 47 MPH.MSC Codes 15-04
UR  - https://arxiv.org/pdf/2112.14705v1.pdf
N1  - Source: page_2.html
ID  - 361
ER  - 
TY  - JOUR
TI  - Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion
AU  - Agarwal, S.
AU  - Hu, L.
AU  - Ng, E.
AU  - Li, H.
AU  - Rohrbach, A.
PY  - 2021
AB  - In today's era of digital misinformation, we are increasingly faced with new threats posed by video falsification techniques. Such falsifications range from cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g., sophisticated AI media synthesis methods), which are becoming perceptually indistinguishable from real videos. To tackle this challenge, we propose a multi-modal semantic forensic approach to discover clues that go beyond detecting discrepancies in visual quality, thereby handling both simpler cheapfakes and visually persuasive deepfakes. In this work, our goal is to verify that the purported person seen in the video is indeed themselves by detecting anomalous facial movements corresponding to the spoken words. We leverage the idea of attribution to learn person-specific biometric patterns that distinguish a given speaker from others. We use interpretable Action Units (AUs) to capture a person's face and head movement as opposed to deep CNN features, and we are the first to use word-conditioned facial motion analysis. We further demonstrate our method's effectiveness on a range of fakes not seen in training including those without video manipulation, that were not addressed in prior work.
UR  - https://arxiv.org/pdf/2112.10936v2.pdf
N1  - Source: page_2.html
ID  - 362
ER  - 
TY  - JOUR
TI  - AI-Empowered Persuasive Video Generation: A Survey
AU  - Liu, C.
AU  - Yu, H.
PY  - 2021
AB  - Promotional videos are rapidly becoming a popular medium for persuading people to change their behaviours in many settings (e.g., online shopping, social enterprise initiatives). Today, such videos are often produced by professionals, which is a time-, labour- and cost-intensive undertaking. In order to produce such contents to support a large applications (e.g., e-commerce), the field of artificial intelligence (AI)-empowered persuasive video generation (AIPVG) has gained traction in recent years. This field is interdisciplinary in nature, which makes it challenging for new researchers to grasp. Currently, there is no comprehensive survey of AIPVG available. In this paper, we bridge this gap by reviewing key AI techniques that can be utilized to automatically generate persuasive videos. We offer a first-of-its-kind taxonomy which divides AIPVG into three major steps: 1) visual material understanding, which extracts information from the visual materials (VMs) relevant to the target of promotion; 2) visual storyline generation, which shortlists and arranges high-quality VMs into a sequence in order to compose a storyline with persuasive power; and 3) post-production, which involves background music generation and still image animation to enhance viewing experience. We also introduce the evaluation metrics and datasets commonly adopted in the field of AIPVG. We analyze the advantages and disadvantages of the existing works belonging to the above-mentioned steps, and discuss interesting potential future research directions.
UR  - https://arxiv.org/pdf/2112.09401v1.pdf
N1  - Source: page_2.html
ID  - 363
ER  - 
TY  - JOUR
TI  - Machine learning for utility prediction in argument-based computational persuasion
AU  - Donadello, I.
AU  - Hunter, A.
AU  - Teso, S.
AU  - Dragoni, M.
PY  - 2021
AB  - Automated persuasion systems (APS) aim to persuade a user to believe something by entering into a dialogue in which arguments and counterarguments are exchanged. To maximize the probability that an APS is successful in persuading a user, it can identify a global policy that will allow it to select the best arguments it presents at each stage of the dialogue whatever arguments the user presents. However, in real applications, such as for healthcare, it is unlikely the utility of the outcome of the dialogue will be the same, or the exact opposite, for the APS and user. In order to deal with this situation, games in extended form have been harnessed for argumentation in Bi-party Decision Theory. This opens new problems that we address in this paper: (1) How can we use Machine Learning (ML) methods to predict utility functions for different subpopulations of users? and (2) How can we identify for a new user the best utility function from amongst those that we have learned? To this extent, we develop two ML methods, EAI and EDS, that leverage information coming from the users to predict their utilities. EAI is restricted to a fixed amount of information, whereas EDS can choose the information that best detects the subpopulations of a user. We evaluate EAI and EDS in a simulation setting and in a realistic case study concerning healthy eating habits. Results are promising in both cases, but EDS is more effective at predicting useful utility functions.
UR  - https://arxiv.org/pdf/2112.04953v1.pdf
N1  - Source: page_2.html
ID  - 364
ER  - 
TY  - JOUR
TI  - Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal Characterization
AU  - Unal, M.E.
AU  - Kovashka, A.
AU  - Chung, W.-T.
AU  - Lin, Y.-R.
PY  - 2021
AB  - Social media content routinely incorporates multi-modal design to covey information and shape meanings, and sway interpretations toward desirable implications, but the choices and outcomes of using both texts and visual images have not been sufficiently studied. This work proposes a computational approach to analyze the outcome of persuasive information in multi-modal content, focusing on two aspects, popularity and reliability, in COVID-19-related news articles shared on Twitter. The two aspects are intertwined in the spread of misinformation: for example, an unreliable article that aims to misinform has to attain some popularity. This work has several contributions. First, we propose a multi-modal (image and text) approach to effectively identify popularity and reliability of information sources simultaneously. Second, we identify textual and visual elements that are predictive to information popularity and reliability. Third, by modeling cross-modal relations and similarity, we are able to uncover how unreliable articles construct multi-modal meaning in a distorted, biased fashion. Our work demonstrates how to use multi-modal analysis for understanding influential content and has implications to social media literacy and engagement.
UR  - https://arxiv.org/pdf/2112.13910v1.pdf
N1  - Source: page_2.html
ID  - 365
ER  - 
TY  - JOUR
TI  - Outlier detection using AI: A survey
AU  - Sikder, M.N.K.
AU  - Batarseh, F.A.
PY  - 2021
AB  - An outlier is an event or observation that is defined as an unusual activity, intrusion, or a suspicious data point that lies at an irregular distance from a population. The definition of an outlier event, however, is subjective and depends on the application and the domain (Energy, Health, Wireless Network, etc.). It is important to detect outlier events as carefully as possible to avoid infrastructure failures because anomalous events can cause minor to severe damage to infrastructure. For instance, an attack on a cyber-physical system such as a microgrid may initiate voltage or frequency instability, thereby damaging a smart inverter which involves very expensive repairing. Unusual activities in microgrids can be mechanical faults, behavior changes in the system, human or instrument errors or a malicious attack. Accordingly, and due to its variability, Outlier Detection (OD) is an ever-growing research field. In this chapter, we discuss the progress of OD methods using AI techniques. For that, the fundamental concepts of each OD model are introduced via multiple categories. Broad range of OD methods are categorized into six major categories: Statistical-based, Distance-based, Density-based, Clustering-based, Learning-based, and Ensemble methods. For every category, we discuss recent state-of-the-art approaches, their application areas, and performances. After that, a brief discussion regarding the advantages, disadvantages, and challenges of each technique is provided with recommendations on future research directions. This survey aims to guide the reader to better understand recent progress of OD methods for the assurance of AI.
UR  - https://arxiv.org/pdf/2112.00588v1.pdf
N1  - Source: page_2.html
ID  - 366
ER  - 
TY  - JOUR
TI  - Cloud Failure Prediction with Hierarchical Temporal Memory: An Empirical Assessment
AU  - Riganelli, O.
AU  - Saltarel, P.
AU  - Tundo, A.
AU  - Mobilio, M.
AU  - Mariani, L.
PY  - 2021
AB  - Hierarchical Temporal Memory (HTM) is an unsupervised learning algorithm inspired by the features of the neocortex that can be used to continuously process stream data and detect anomalies, without requiring a large amount of data for training nor requiring labeled data.HTM is also able to continuously learn from samples, providing a model that is always up-to-date with respect to observations. These characteristics make HTM particularly suitable for supporting online failure prediction in cloud systems, which are systems with a dynamically changing behavior that must be monitored to anticipate problems. This paper presents the first systematic study that assesses HTM in the context of failure prediction. The results that we obtained considering 72 configurations of HTM applied to 12 different types of faults introduced in the Clearwater cloud system show that HTM can help to predict failures with sufficient effectiveness (F-measure = 0.76), representing an interesting practical alternative to (semi-)supervised algorithms.
UR  - https://arxiv.org/pdf/2110.03431v2.pdf
N1  - Source: page_2.html
ID  - 367
ER  - 
TY  - JOUR
TI  - Influencing towards stable multi-agent interactions
AU  - Wang, W.Z.
AU  - Shih, A.
AU  - Xie, A.
AU  - Sadigh, D.
PY  - 2021
AB  - Learning in multi-agent environments is difficult due to the non-stationarity introduced by an opponent’s or partner’s changing behaviors. Instead of reactively adapting to the other agent’s (opponent or partner) behavior, we propose an algorithm to proactively influence the other agent’s strategy to stabilize –which can restrain the non-stationarity caused by the other agent. We learn a low-dimensional latent representation of the other agent’s strategy and the dynamics of how the latent strategy evolves with respect to our robot’s behavior. With this learned dynamics model, we can define an unsupervised stability reward to train our robot to deliberately influence the other agent to stabilize towards a single strategy. We demonstrate the effectiveness of stabilizing in improving efficiency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation. We show qualitative results on our website.
UR  - https://arxiv.org/pdf/2110.08229v1.pdf
N1  - Source: page_2.html
ID  - 368
ER  - 
TY  - JOUR
TI  - Investigating health-aware smart-nudging with Machine Learning to help people pursue healthier eating-habits
AU  - Khan, M.A.
AU  - Muhammad, K.
AU  - Smyth, B.
AU  - Coyle, D.
PY  - 2021
AB  - Food-choices and eating-habits directly contribute to our long-term health. This makes the food recommender system a potential tool to address the global crisis of obesity and malnutrition. Over the past decade, artificial-intelligence and medical researchers became more invested in researching tools that can guide and help people make healthy and thoughtful decisions around food and diet. In many typical (Recommender System) RS domains, smart nudges have been proven effective in shaping users' consumption patterns. In recent years, knowledgeable nudging and incentifying choices started getting attention in the food domain as well. To develop smart nudging for promoting healthier food choices, we combined Machine Learning and RS technology with food-healthiness guidelines from recognized health organizations, such as the World Health Organization, Food Standards Agency, and the National Health Service United Kingdom. In this paper, we discuss our research on, persuasive visualization for making users aware of the healthiness of the recommended recipes. Here, we propose three novel nudging technology, the WHO-BubbleSlider, the FSA-ColorCoading, and the DRCI-MLCP, that encourage users to choose healthier recipes. We also propose a Topic Modeling based portion-size recommendation algorithm. To evaluate our proposed smart-nudges, we conducted an online user study with 96 participants and 92250 recipes. Results showed that, during the food decision-making process, appropriate healthiness cues make users more likely to click, browse, and choose healthier recipes over less healthy ones.MSC Codes 68U35 (Primary), 68T35 (Secondary), 68T50(Secondary)
UR  - https://arxiv.org/pdf/2110.07045v1.pdf
N1  - Source: page_2.html
ID  - 369
ER  - 
TY  - JOUR
TI  - MEPG: A Minimalist Ensemble Policy Gradient Framework for Deep Reinforcement Learning
AU  - He, Q.
AU  - Su, H.
AU  - Gong, C.
AU  - Hou, X.
PY  - 2021
AB  - During the training of a reinforcement learning (RL) agent, the distribution of training data is non-stationary as the agent's behavior changes over time. Therefore, there is a risk that the agent is overspecialized to a particular distribution and its performance suffers in the larger picture. Ensemble RL can mitigate this issue by learning a robust policy. However, it suffers from heavy computational resource consumption due to the newly introduced value and policy functions. In this paper, to avoid the notorious resources consumption issue, we design a novel and simple ensemble deep RL framework that integrates multiple models into a single model. Specifically, we propose the Minimalist Ensemble Policy Gradient framework (MEPG), which introduces minimalist ensemble consistent Bellman update by utilizing a modified dropout operator. MEPG holds ensemble property by keeping the dropout consistency of both sides of the Bellman equation. Additionally, the dropout operator also increases MEPG's generalization capability. Moreover, we theoretically show that the policy evaluation phase in the MEPG maintains two synchronized deep Gaussian Processes. To verify the MEPG framework's ability to generalize, we perform experiments on the gym simulator, which presents that the MEPG framework outperforms or achieves a similar level of performance as the current state-of-the-art ensemble methods and model-free methods without increasing additional computational resource costs.
UR  - https://arxiv.org/pdf/2109.10552v2.pdf
N1  - Source: page_2.html
ID  - 370
ER  - 
TY  - JOUR
TI  - SalienTrack: providing salient information for semi-automated feedback in self-tracking with explainable AI
AU  - Wang, Y.
AU  - Liu, J.
AU  - Park, H.
AU  - Kay, J.
AU  - Lim, B.Y.
PY  - 2021
AB  - Self-tracking can improve people's awareness of their unhealthy behaviors and support reflection to inform behavior change. Increasingly, new technologies make tracking easier, leading to large amounts of tracked data. However, much of that information is not salient for reflection and self-awareness. To tackle this burden for reflection, we created the SalienTrack framework, which aims to 1) identify salient tracking events, 2) select the salient details of those events, 3) explain why they are informative, and 4) present the details as manually elicited or automatically shown feedback. We implemented SalienTrack in the context of nutrition tracking. To do this, we first conducted a field study to collect photo-based mobile food tracking over 1-5 weeks. We then report how we used this data to train an explainable-AI model of salience. Finally, we created interfaces to present salient information and conducted a formative user study to gain insights about how SalienTrack could be integrated into an interface for reflection. Our key contributions are the SalienTrack framework, a demonstration of its implementation for semi-automated feedback in an important and challenging self-tracking context and a discussion of the broader uses of the framework.
UR  - https://arxiv.org/pdf/2109.10231v3.pdf
N1  - Source: page_2.html
ID  - 371
ER  - 
TY  - JOUR
TI  - A STORYTELLING ROBOT MANAGING PERSUASIVE AND ETHICAL STANCES VIA ACT-R: AN EXPLORATORY STUDY
AU  - Augello, A.
AU  - Città, G.
AU  - Gentile, M.
AU  - Lieto, A.
PY  - 2021
AB  - We present a storytelling robot, controlled via the ACT-R cognitive architecture, able to adopt different persuasive techniques and ethical stances while conversing about some topics concerning COVID-19. The main contribution of the paper consists in the proposal of a needs-driven model that guides and evaluates, during the dialogue, the use (if any) of persuasive techniques available in the agent procedural memory. The portfolio of persuasive techniques tested in such a model ranges from the use of storytelling, to framing techniques and rhetorical-based arguments. To the best of our knowledge, this represents the first attempt of building a persuasive agent able to integrate a mix of explicitly grounded cognitive assumptions about dialogue management, storytelling and persuasive techniques as well as ethical attitudes. The paper presents the results of an exploratory evaluation of the system on 63 participants.
UR  - https://arxiv.org/pdf/2107.12845v2.pdf
N1  - Source: page_2.html
ID  - 372
ER  - 
TY  - JOUR
TI  - Evaluation of in-person counseling strategies to develop physical activity chatbot for women
AU  - Liang, K.-H.
AU  - Lange, P.
AU  - Oh, Y.J.
AU  - Fukuoka, Y.
AU  - Yu, Z.
PY  - 2021
AB  - Artificial intelligence chatbots are the vanguard in technology-based intervention to change people's behavior. To develop intervention chatbots, the first step is to understand natural language conversation strategies in human conversation. This work introduces an intervention conversation dataset collected from a real-world physical activity intervention program for women. We designed comprehensive annotation schemes in four dimensions (domain, strategy, social exchange, and taskfocused exchange) and annotated a subset of dialogs. We built a strategy classifier with context information to detect strategies from both trainers and participants based on the annotation. To understand how human intervention induces effective behavior changes, we analyzed the relationships between the intervention strategies and the participants' changes in the barrier and social support for physical activity. We also analyzed how participant's baseline weight correlates to the amount of occurrence of the corresponding strategy. This work lays the foundation for developing a personalized physical activity intervention bot.
UR  - https://arxiv.org/pdf/2107.10410v1.pdf
N1  - Source: page_2.html
ID  - 373
ER  - 
TY  - JOUR
TI  - Strategic mitigation of agent inattention in drivers with open-quantum cognition models
AU  - Zhang, Q.
AU  - Siddhardh Nadendla, V.S.
AU  - Balakrishnan, S.N.
AU  - Busemeyer, J.
PY  - 2021
AB  - State-of-the-art driver-assist systems have failed to effectively mitigate driver inattention and had minimal impacts on the ever-growing number of road mishaps (e.g. life loss, physical injuries due to accidents caused by various factors that lead to driver inattention). This is because traditional human-machine interaction settings are modeled in classical and behavioral game-theoretic domains which are technically appropriate to characterize strategic interaction between either two utility maximizing agents, or human decision makers. Therefore, in an attempt to improve the persuasive effectiveness of driver-assist systems, we develop a novel strategic and personalized driver-assist system which adapts to the driver’s mental state and choice behavior. First, we propose a novel equilibrium notion in human-system interaction games, where the system maximizes its expected utility and human decisions can be characterized using any general decision model. Then we use this novel equilibrium notion to investigate the strategic driver-vehicle interaction game where the car presents a persuasive recommendation to steer the driver towards safer driving decisions. We assume that the driver employs an open-quantum system cognition model, which captures complex aspects of human decision making such as violations to classical law of total probability and incompatibility of certain mental representations of information. We present closed-form expressions for players’ final responses to each other’s strategies so that we can numerically compute both pure and mixed equilibria. Numerical results are presented to illustrate both kinds of equilibria.
UR  - https://arxiv.org/pdf/2107.09888v1.pdf
N1  - Source: page_2.html
ID  - 374
ER  - 
TY  - JOUR
TI  - Online handbook of argumentation for AI volume 2
AU  - Castagna, F.
AU  - Mosca, F.
AU  - Mumford, J.
AU  - Sarkadi, S.
AU  - Xydis, A.
PY  - 2021
AB  - Collaborative intelligence between humans and intelligent systems relies heavily on the skills of humans and intelligent systems for reaching agreements. This requires complex dialogue processes, which include human reasoning based on common sense and goal-oriented decision-making performed by the intelligent systems, considering the human's dynamic goals and changing beliefs. This project aims to approach these challenges by studying non-monotonic reasoning techniques in the setting of strategic interaction between intelligent systems and humans. To capture the underlying logic of human reasoning, cognitive theories in logical formalizations are explored, e.g., in abstract argumentation or answer set programming. These reasoning architectures will support the decision-making process of rational agents that aim to join a given dialogue-based interaction with humans. With a particular focus on applications of persuasive technology, we see strategic argumentation as a process of decision-making for changing mental states of human agents.
UR  - https://arxiv.org/pdf/2106.10832v2.pdf
N1  - Source: page_2.html
ID  - 375
ER  - 
TY  - JOUR
TI  - Multi-receiver online Bayesian persuasion
AU  - Castiglioni, M.
AU  - Marchesi, A.
AU  - Celli, A.
AU  - Gatti, N.
PY  - 2021
AB  - Bayesian persuasion studies how an informed sender should partially disclose information to influence the behavior of a self-interested receiver. Classical models make the stringent assumption that the sender knows the receiver’s utility. This can be relaxed by considering an online learning framework in which the sender repeatedly faces a receiver of an unknown, adversarially selected type. We study, for the first time, an online Bayesian persuasion setting with multiple receivers. We focus on the case with no externalities and binary actions, as customary in offline models. Our goal is to design no-regret algorithms for the sender with polynomial per-iteration running time. First, we prove a negative result: for any 0 < α ≤ 1, there is no polynomial-time no-α-regret algorithm when the sender’s utility function is supermodular or anonymous. Then, we focus on the case of submodular sender’s utility functions and we show that, in this case, it is possible to design a polynomial-time no-(1-1/e)-regret algorithm. To do so, we introduce a general online gradient descent scheme to handle online learning problems with a finite number of possible loss functions. This requires the existence of an approximate projection oracle. We show that, in our setting, there exists one such projection oracle which can be implemented in polynomial time.
UR  - https://arxiv.org/pdf/2106.06480v1.pdf
N1  - Source: page_2.html
ID  - 376
ER  - 
TY  - JOUR
TI  - Towards an intelligent personalized persuasive conversational system for human interaction on divining the future event and assisting by using artificial intelligence
AU  - Sharma, S.
AU  - Pathak, S.
AU  - Srivastava, S.
PY  - 2021
AB  - In the next wave of insurgence, humans may endeavour self-reflection which can lead to an effortless talk and to find out if an event will fructify. Training the system on how to make accurate prognostication with the help of machine learning and statistical models can lead to an intelligent personalized conversational system. The Chatbot industry is ever-growing and after the COVID-19 pandemic and rigorous lockdowns all around the world, people have realized the importance of human interaction in their lives. We are developing this model to create a more intimate relationship between the system and humans. For this purpose, many open-source platforms are available. Artificial Intelligence Markup Language (AIML) is derived from Extensible Markup Language (XML) which is used to build up a conversational agent artificially. The success of this project will help the model in observing and understanding human emotions which will ultimately help it to form a more personalized relationship to delineate the future course of events.
UR  - https://dx.doi.org/10.36227/techrxiv.14732055.v1
N1  - Source: page_2.html
ID  - 377
ER  - 
TY  - JOUR
TI  - Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues
AU  - Joshi, R.
AU  - Balachandran, V.
AU  - Vashishth, S.
AU  - Black, A.W.
AU  - Tsvetkov, Y.
PY  - 2021
AB  - To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DIALOGRAPH, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DIALOGRAPH explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues.1
UR  - https://arxiv.org/pdf/2106.00920v1.pdf
N1  - Source: page_2.html
ID  - 378
ER  - 
TY  - JOUR
TI  - PAL: Intelligence augmentation using egocentric visual context detection
AU  - Khan, M.
AU  - Maes, P.
PY  - 2021
AB  - Egocentric visual context detection can support intelligence augmentation applications. We created a wearable system, called PAL, for wearable, personalized, and privacy-preserving egocentric visual context detection. PAL has a wearable device with a camera, heart-rate sensor, on-device deep learning, and audio input/output. PAL also has a mobile/web application for personalized context labeling. We used on-device deep learning models for generic object and face detection, low-shot custom face and context recognition (e.g., activities like brushing teeth), and custom context clustering (e.g., indoor locations). The models had over 80% accuracy in in-the-wild contexts (∼1000 images) and we tested PAL for intelligence augmentation applications like behavior change. We have made PAL is open-source to further support intelligence augmentation using personalized and privacy-preserving egocentric visual contexts.
UR  - https://arxiv.org/pdf/2105.10735v1.pdf
N1  - Source: page_2.html
ID  - 379
ER  - 
TY  - JOUR
TI  - Using Geospatial Intelligence to Promote Precautionary Behavior During the COVID-19 Pandemic: Development and Validation of a Customized Messaging Campaign in Saudi Arabia
AU  - Alrige, M.
AU  - Bitar, H.
AU  - Meccawy, M.
AU  - Mullachery, B.
PY  - 2021
AB  - Background Designing a health promotion campaign is never an easy task, especially during a pandemic of a highly infectious disease such as COVID-19. In Saudi Arabia, many attempts have been made to raise public awareness about COVID-19 infection and precautionary health measures. However, most of the health information delivered through the national dashboard and the COVID-19 awareness campaigns are generic and do not necessarily make the impact needed to be seen on individuals’ behavior. Objective The objective of this study is to build and validate a customized messaging campaign to promote precautionary health behavior in individuals during the COVID-19 pandemic. Customization is realized by utilizing a geospatial artificial intelligence technique called the space-time cube (STC) technique. Methods This research was conducted in two sequential phases. In phase 1, an initial library of 32 messages was developed and validated based on Fogg behavior model for behavior change. In phase 2, we applied STC as a geospatial artificial intelligence technique to create a local map for one city with three different profiles representing the city districts. The model was built using COVID-19 real clinical data. The messages created in phase 1 is to be customized for residents in each city profile separately. Results n phase 1, 32 messages were developed based on resources from the World Health Organization and the Ministry of Health in Saudi Arabia. The enumerated content validity of the messages was established through the utilization of the content validity index (CVI). Thirteen professional health care workers (n=13) evaluated the content of the message libraries in Arabic and English. Thirty-two messages were found to have acceptable content validity (I-CVI=0.87). In phase 2, the geospatial intelligence technique we applied showed three zones for the districts of Jeddah city: one for high infection, another for moderate infection, and the third for low infection. Combining the results from the first and second phases, a customized awareness campaign was shaped for the city of Jeddah. Conclusions This awareness campaign can be utilized by health authorities to raise individuals' awareness about the precautionary measures that should be taken based on the infection level of their district, and hence help in reducing the number of positive cases in the city of Jeddah.
UR  - https://www.researchsquare.com/article/rs-471552/v1.pdf
N1  - Source: page_2.html
ID  - 380
ER  - 
TY  - JOUR
TI  - What makes a message persuasive? Identifying adaptations towards persuasiveness in nine exploratory case studies
AU  - Duerr, S.
AU  - Lange, K.T.
AU  - Gloor, P.A.
PY  - 2021
AB  - The ability to persuade others is critical to professional and personal success. However, crafting persuasive messages is demanding and poses various challenges. We conducted nine exploratory case studies to identify adaptations that professional and non-professional writers make in written scenarios to increase their subjective persuasiveness. Furthermore, we identified challenges that those writers faced and identified strategies to resolve them with persuasive natural language generation, i.e., artificial intelligence. Our findings show that humans can achieve high degrees of persuasiveness (more so for professional-level writers), and artificial intelligence can complement them to achieve increased celerity and alignment in the process.
UR  - https://arxiv.org/pdf/2104.12454v1.pdf
N1  - Source: page_2.html
ID  - 381
ER  - 
TY  - JOUR
TI  - TOWARD THE PREVENTION OF PRIVACY THREATS: HOW CAN WE PERSUADE OUR SOCIAL NETWORK PLATFORM USERS?
AU  - Ruiz-Dolz, R.
AU  - Alemany, J.
AU  - Heras, S.
AU  - García-Fornes, A.
PY  - 2021
AB  - Complex decision-making problems such as the privacy policy selection when sharing content in online social networks can significantly benefit from artificial intelligence systems. With the use of Computational Argumentation, it is possible to persuade human users to modify their initial decisions to avoid potential privacy threats and violations. In this paper, we present a study performed over 186 teenage users aimed at analysing their behaviour when we try to persuade them to modify the publication of sensitive content in Online Social Networks (OSN) with different arguments. The results of the study revealed that the personality traits and the social interaction data (e.g., number of comments, friends, and likes) of our participants were significantly correlated with the persuasive power of the arguments. Therefore, these sets of features can be used to model OSN users, and to estimate the persuasive power of different arguments when used in human-computer interactions. The findings presented in this paper are helpful for personalising decision support systems aimed at educating and preventing privacy violations in OSNs using arguments.
UR  - https://arxiv.org/pdf/2104.10004v2.pdf
N1  - Source: page_2.html
ID  - 382
ER  - 
TY  - JOUR
TI  - An interactive dashboard for searching and comparing soccer performance scores
AU  - Cintia, P.
AU  - Pappalardo, L.
AU  - Mauro, G.
AU  - Ferragina, P.
PY  - 2021
AB  - The performance of soccer players is one of most discussed aspects by many actors in the soccer industry: from supporters to journalists, from coaches to talent scouts. Unfortunately, the dashboards available online provide no effective way to compare the evolution of the performance of players or to find players behaving similarly on the field. This paper describes the design of a web dashboard that interacts via APIs with a performance evaluation algorithm and provides graphical tools that allow the user to perform many tasks, such as to search or compare players by age, role or trend of growth in their performance, find similar players based on their pitching behavior, change the algorithm's parameters to obtain customized performance scores. We also describe an example of how a talent scout can interact with the dashboard to find young, promising talents.
UR  - https://arxiv.org/pdf/2105.04293v2.pdf
N1  - Source: page_2.html
ID  - 383
ER  - 
TY  - JOUR
TI  - CaSiNo: A corpus of campsite negotiation dialogues for automatic negotiation systems
AU  - Chawla, K.
AU  - Ramirez, J.
AU  - Clever, R.
AU  - May, J.
AU  - Gratch, J.
PY  - 2021
AB  - Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closeddomain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https:// github.com/kushalchawla/CaSiNo.
UR  - https://arxiv.org/pdf/2103.15721v2.pdf
N1  - Source: page_2.html
ID  - 384
ER  - 
TY  - JOUR
TI  - Spirit distillation: Precise real-time semantic segmentation of road scenes with insufficient data
AU  - Wu, Z.
AU  - Jiang, Y.
AU  - Cui, C.
AU  - Xue, X.
AU  - Qi, H.
PY  - 2021
AB  - Semantic segmentation of road scenes is one of the key technologies for realizing autonomous driving scene perception, and the effectiveness of deep Convolutional Neural Networks(CNNs) for this task has been demonstrated. Stateof-art CNNs for semantic segmentation suffer from excessive computations as well as large-scale training data requirement. Inspired by the ideas of Fine-tuning-based Transfer Learning (FTT) and feature-based knowledge distillation, we propose a new knowledge distillation method for cross-domain knowledge transference and efficient data-insufficient network training, named Spirit Distillation(SD), which allow the student network to mimic the teacher network to extract general features, so that a compact and accurate student network can be trained for real-time semantic segmentation of road scenes. Then, in order to further alleviate the trouble of insufficient data and improve the robustness of the student, an Enhanced Spirit Distillation (ESD) method is proposed, which commits to exploit a more comprehensive general features extraction capability by considering images from both the target and the proximity domains as input. To our knowledge, this paper is a pioneering work on the application of knowledge distillation to few-shot learning. Persuasive experiments conducted on Cityscapes semantic segmentation with the prior knowledge transferred from COCO2017 and KITTI demonstrate that our methods can train a better student network (mIOU and high-precision accuracy boost by 1.4% and 8.2% respectively, with 78.2% segmentation variance) with only 41.8% FLOPs (see Fig. 1).
UR  - https://arxiv.org/pdf/2103.13733v2.pdf
N1  - Source: page_2.html
ID  - 385
ER  - 
TY  - JOUR
TI  - Multi-Task Attentive Residual Networks for Argument Mining
AU  - Galassi, A.
AU  - Lippi, M.
AU  - Torroni, P.
PY  - 2021
AB  - We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
UR  - https://arxiv.org/pdf/2102.12227v3.pdf
N1  - Source: page_2.html
ID  - 386
ER  - 
TY  - JOUR
TI  - Disparate impact diminishes consumer trust even for advantaged users
AU  - Draws, T.
AU  - Szlávik, Z.
AU  - Timmermans, B.
AU  - Varshney, K.R.
AU  - Hind, M.
PY  - 2021
AB  - Systems aiming to aid consumers in their decision-making (e.g., by implementing persuasive techniques) are more likely to be effective when consumers trust them. However, recent research has demonstrated that the machine learning algorithms that often underlie such technology can act unfairly towards specific groups (e.g., by making more favorable predictions for men than for women). An undesired disparate impact resulting from this kind of algorithmic unfairness could diminish consumer trust and thereby undermine the purpose of the system. We studied this effect by conducting a between-subjects user study investigating how (gender-related) disparate impact affected consumer trust in an app designed to improve consumers' financial decision-making. Our results show that disparate impact decreased consumers' trust in the system and made them less likely to use it. Moreover, we find that trust was affected to the same degree across consumer groups (i.e., advantaged and disadvantaged users) despite both of these consumer groups recognizing their respective levels of personal benefit. Our findings highlight the importance of fairness in consumer-oriented artificial intelligence systems.
UR  - https://arxiv.org/pdf/2101.12715v3.pdf
N1  - Source: page_2.html
ID  - 387
ER  - 
TY  - JOUR
TI  - Strategic argumentation dialogues for persuasion: Framework and experiments based on modelling the beliefs and concerns of the persuadee
AU  - Hadoux, E.
AU  - Hunter, A.
AU  - Polberg, S.
PY  - 2021
AB  - Persuasion is an important and yet complex aspect of human intelligence. When undertaken through dialogue, the deployment of good arguments, and therefore counterarguments, clearly has a significant effect on the ability to be successful in persuasion. Two key dimensions for determining whether an argument is “good” in a particular dialogue are the degree to which the intended audience believes the argument and counterarguments, and the impact that the argument has on the concerns of the intended audience. In this paper, we present a framework for modelling persuadees in terms of their beliefs and concerns, and for harnessing these models in optimizing the choice of move in persuasion dialogues. Our approach is based on the Monte Carlo Tree Search which allows optimization in real-time. We provide empirical results of a study with human participants showing that our automated persuasion system based on this technology is superior to a baseline system that does not take the beliefs and concerns into account in its strategy.
UR  - https://arxiv.org/pdf/2101.11870v1.pdf
N1  - Source: page_2.html
ID  - 388
ER  - 
TY  - JOUR
TI  - Personalization paradox in behavior change apps: Lessons from a social comparison-based personalized app for physical activity
AU  - Zhu, J.
AU  - Dallal, D.H.
AU  - Gray, R.C.
AU  - Forman, E.M.
AU  - Arigo, D.
PY  - 2021
AB  - Social comparison-based features are widely used in social computing apps. However, most existing apps are not grounded in social comparison theories and do not consider individual differences in social comparison preferences and reactions. This paper is among the first to automatically personalize social comparison targets. In the context of an m-health app for physical activity, we use artificial intelligence (AI) techniques of multi-armed bandits. Results from our user study (n=53) indicate that there is some evidence that motivation can be increased using the AI-based personalization of social comparison. The detected effects achieved small-to-moderate effect sizes, illustrating the real-world implications of the intervention for enhancing motivation and physical activity. In addition to design implications for social comparison features in social apps, this paper identified the personalization paradox, the conflict between user modeling and adaptation, as a key design challenge of personalized applications for behavior change. Additionally, we propose research directions to mitigate this Personalization Paradox.
UR  - https://arxiv.org/pdf/2101.10020v2.pdf
N1  - Source: page_2.html
ID  - 389
ER  - 
TY  - JOUR
TI  - RESPER: Computationally modelling resisting strategies in persuasive conversations.
AU  - Dutt, R.
AU  - Sinha, S.
AU  - Joshi, R.
AU  - Bao, H.
AU  - Rosé, C.P.
PY  - 2021
AB  - Modelling persuasion strategies as predictors of task outcome has several real-world applications and has received considerable attention from the computational linguistics community. However, previous research has failed to account for the resisting strategies employed by an individual to foil such persuasion attempts. Grounded in prior literature in cognitive and social psychology, we propose a generalised framework for identifying resisting strategies in persuasive conversations. We instantiate our framework on two distinct datasets comprising persuasion and negotiation conversations. We also leverage a hierarchical sequence-labelling neural architecture to infer the aforementioned resisting strategies automatically. Our experiments reveal the asymmetry of power roles in non-collaborative goal-directed conversations and the benefits accrued from incorporating resisting strategies on the final conversation outcome. We also investigate the role of different resisting strategies on the conversation outcome and glean insights that corroborate with past findings. We also make the code and the dataset of this work publicly available at https://github.com/americast/ resper.
UR  - https://arxiv.org/pdf/2101.10545v1.pdf
N1  - Source: page_2.html
ID  - 390
ER  - 
TY  - JOUR
TI  - AI, Ethics, and Law: A Possible Way Forward
AU  - Davis, J.P.
PY  - 2021
AB  - This Chapter offers a framework for analyzing the intersection of artificial intelligence (AI), ethics, and law. It does so, as Part 1 explains, by (1) suggesting potential limitations on AI consciousness and identifying the implications of those limitations for ethics and law, and (2) acknowledging three possible philosophical objections to this line of analysis and providing reasons to reject each of them. Part 2 explores the role that consciousness plays in forming objectives, including in making relevant moral and other value judgments. It suggests that as long as AI lacks consciousness we will have difficulty regulating it — our ethics and law often rely on intent in assigning, respectively, moral responsibility and legal liability — and difficulty using it to regulate ourselves — value judgments play an important role in resolving ethical and legal disputes. It also notes that conscious AI is likely to have very different first-person experiences than we do — and hence very different forms of intent and values than we have — giving rise to another set of difficulties for regulating it and for it regulating us. With this sketch of an argument in place, Part 3 addresses three potential philosophical objections to it: that consciousness as a matter of theory cannot have the sort of causal effect on behavior that Part 2 presumes; that consciousness as a matter of empirical fact does not have that sort of causal effect; and that Part 2 relies on a dubious understanding of free will. Part 3 contends that none of these objections is persuasive. The Chapter concludes that the analysis in Part 2 warrants further development.
UR  - https://papers.ssrn.com/sol3/Delivery.cfm/MECA.pdf?abstractid=3711040&type=2
N1  - Source: page_2.html
ID  - 391
ER  - 
TY  - JOUR
TI  - Data Obsolescence Detection in the Light of Newly Acquired Valid Observations
AU  - Chaieb, S.
AU  - Hnich, B.
AU  - Mrad, A.B.
PY  - 2021
AB  - The information describing the conditions of a system or a person is constantly evolving and may become obsolete and contradict other infor-mation. A database, therefore, must be consistently updated upon the acqui-sition of new valid observations that contradict obsolete ones contained in the database. In this paper, we propose a novel causation-based system for dealing with the information obsolescence problem when a causal Bayesian network is our representation model. Our approach is based on studying causal depen-dencies between the network variables to detect, in real-time, contradictions between the observations on a single subject and then identify the obsolete ones. We propose a new approximate concept, ϵ-Contradiction, which repre-sents the confidence level of having a contradiction between some observations relating to a specific subject. Once identified, obsolete observations are given in an original way, in the form of an explanation AND-OR Tree. Our approach can be applied in various domains where the main issue is to detect and explain personalized situations such that the reasons and circumstances underlying unexpected outcomes. Examples include among others: detecting behaviour change by analyzing user profiles, and identifying the causes of some anoma-lies such as bank frauds by analyzing customer interactions. In this paper, we demonstrate the effectiveness of our approach in a real-life medical appli-cation: the elderly fall-prevention and showcase how the resulted explanation AND-OR trees can be used to give reliable recommendations to physicians and assist decision-makers. Our approach runs in a polynomial time and gives systematically and substantially good results.
UR  - https://arxiv.org/pdf/2101.07067v3.pdf
N1  - Source: page_2.html
ID  - 392
ER  - 
TY  - JOUR
TI  - Persuasive natural language generation - A literature review
AU  - Duerr, S.
AU  - Gloor, P.A.
PY  - 2021
AB  - This literature review focuses on the use of Natural Language Generation (NLG) to automatically detect and generate persuasive texts. Extending previous research on automatic identification of persuasion in text, we concentrate on generative aspects through conceptualizing determinants of persuasion in five business-focused categories: benevolence, linguistic appropriacy, logical argumentation, trustworthiness, tools & datasets. These allow NLG to increase an existing message’s persuasiveness. Previous research illustrates key aspects in each of the above mentioned five categories. A research agenda to further study persuasive NLG is developed. The review includes analysis of seventy-seven articles, outlining the existing body of knowledge and showing the steady progress in this research field.
UR  - https://arxiv.org/pdf/2101.05786v1.pdf
N1  - Source: page_2.html
ID  - 393
ER  - 
TY  - JOUR
TI  - Generate Natural Language Explanations for Recommendation
AU  - Chen, H.
AU  - Shi, S.
AU  - Chen, X.
AU  - Zhang, Y.
PY  - 2021
AB  - Providing personalized explanations for recommendations can help users to understand the underlying insight of the recommendation results, which is helpful to the effectiveness, transparency, persuasiveness and trustworthiness of recommender systems. Current explainable recommendation models mostly generate textual explanations based on pre-defined sentence templates. However, the expressiveness power of template-based explanation sentences are limited to the pre-defined expressions, and manually defining the expressions require significant human efforts. Motivated by this problem, we propose to generate free-text natural language explanations for personalized recommendation. In particular, we propose a hierarchical sequence-to-sequence model (HSS) for personalized explanation generation. Different from conventional sentence generation in NLP research, a great challenge of explanation generation in e-commerce recommendation is that not all sentences in user reviews are of explanation purpose. To solve the problem, we further propose an auto-denoising mechanism based on topical item feature words for sentence generation. Experiments on various e-commerce product domains show that our approach can not only improve the recommendation accuracy, but also the explanation quality in terms of the offline measures and feature words coverage. This research is one of the initial steps to grant intelligent agents with the ability to explain itself based on natural language sentences.
UR  - https://arxiv.org/pdf/2101.03392v1.pdf
N1  - Source: page_2.html
ID  - 394
ER  - 
TY  - JOUR
TI  - Theory-based habit modeling for enhancing behavior prediction
AU  - Zhang, C.
AU  - Vanschoren, J.
AU  - van Wissen, A.
AU  - de Ruyter, B.
AU  - IJsselsteijn, W.A.
PY  - 2021
AB  - Psychological theories of habit posit that when a strong habit is formed through behavioral repetition, it can trigger behavior automatically in the same environment. Given the reciprocal relationship between habit and behavior, changing lifestyle behaviors (e.g., toothbrushing) is largely a task of breaking old habits and creating new and healthy ones. Thus, representing users’ habit strengths can be very useful for behavior change support systems (BCSS), for example, to predict behavior or to decide when an intervention reaches its intended effect. However, habit strength is not directly observable and existing self-report measures are taxing for users. In this paper, built on recent computational models of habit formation, we propose a method to enable intelligent systems to compute habit strength based on observable behavior. The hypothesized advantage of using computed habit strength for behavior prediction was tested using data from two intervention studies, where we trained participants to brush their teeth twice a day for three weeks and monitored their behaviors using accelerometers. Through hierarchical cross-validation, we found that for the task of predicting future brushing behavior, computed habit strength clearly outperformed self-reported habit strength (in both studies) and was also superior to models based on past behavior frequency (in the larger second study). Our findings provide initial support for our theory-based approach of modeling user habits and encourages the use of habit computation to deliver personalized and adaptive interventions.
UR  - https://arxiv.org/pdf/2101.01637v1.pdf
N1  - Source: page_2.html
ID  - 395
ER  - 
TY  - JOUR
TI  - Interpersonal distance in VR: Reactions of older adults to the presence of a virtual agent
AU  - Pochwatko, G.
AU  - Karpowicz, B.
AU  - Chrzanowska, A.
AU  - Kopeć, W.
PY  - 2021
AB  - The rapid development of virtual reality technology has increased its availability and, consequently, increased the number of its possible applications. The interest in the new medium has grown due to the entertainment industry (games, VR experiences and movies). The number of freely available training and therapeutic applications is also increasing. Contrary to popular opinion, new technologies are also adopted by older adults. Creating virtual environments tailored to the needs and capabilities of older adults requires intense research on the behaviour of these participants in the most common situations, towards commonly used elements of the virtual environment, in typical sceneries. Comfortable immersion in a virtual environment is key to achieving the impression of presence. Presence is, in turn, necessary to obtain appropriate training, persuasive and therapeutic effects. A virtual agent (a humanoid representation of an algorithm or artificial intelligence) is often an element of the virtual environment interface. Maintaining an appropriate distance to the agent is, therefore, a key parameter for the creator of the VR experience. Older (65+) participants maintain greater distance towards an agent (a young white male) than younger ones (25-35). It may be caused by differences in the level of arousal, but also cultural norms. As a consequence, VR developers are advised to use algorithms that maintain the agent at the appropriate distance, depending on the user’s age.
UR  - https://arxiv.org/pdf/2101.01652v1.pdf
N1  - Source: page_2.html
ID  - 396
ER  - 
TY  - JOUR
TI  - Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration
AU  - Shi, W.
AU  - Li, Y.
AU  - Sahay, S.
AU  - Yu, Z.
PY  - 2020
AB  - Persuasion dialogue systems reflect the machine’s ability to make strategic moves beyond verbal communication, and therefore differentiate themselves from task-oriented or open-domain dialogue systems and have their own unique values. However, the repetition and inconsistency problems still persist in dialogue response generation and could substantially impact user experience and impede the persuasion outcome. Besides, although reinforcement learning (RL) approaches have achieved big success in strategic tasks such as games, they require a sophisticated user simulator to provide real-time feedback to the dialogue system, which limits the application of RL on persuasion dialogues. To address these issues towards a better persuasion dialogue system, we apply RL to refine a language model baseline without user simulators, and distill sentence-level information about repetition, inconsistency, and task relevance through rewards. Moreover, to better accomplish the persuasion task, the model learns from human demonstration to imitate human persuasion behavior and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback.
UR  - https://arxiv.org/pdf/2012.15375v2.pdf
N1  - Source: page_2.html
ID  - 397
ER  - 
TY  - JOUR
TI  - Predicting Decisions in Language Based Persuasion Games
AU  - Apel, R.
AU  - Erev, I.
AU  - Reichart, R.
AU  - Tennenholtz, M.
PY  - 2020
AB  - Sender-receiver interactions, and specifically persuasion games, are widely researched in economic modeling and artificial intelligence, and serve as a solid foundation for powerful applications. However, in the classic persuasion games setting, the messages sent from the expert to the decision-maker are abstract or well-structured application-specific signals rather than natural (human) language messages, although natural language is a very common communication signal in real-world persuasion setups. This paper addresses the use of natural language in persuasion games, exploring its impact on the decisions made by the players and aiming to construct effective models for the prediction of these decisions. For this purpose, we conduct an online repeated interaction experiment. At each trial of the interaction, an informed expert aims to sell an uninformed decision-maker a vacation in a hotel, by sending her a review that describes the hotel. While the expert is exposed to several scored reviews, the decision-maker observes only the single review sent by the expert, and her payoff in case she chooses to take the hotel is a random draw from the review score distribution available to the expert only. The expert's payoff, in turn, depends on the number of times the decision-maker chooses the hotel. We also compare the behavioral patterns in this experiment to the equivalent patterns in similar experiments where the communication is based on the numerical values of the reviews rather than the reviews' text, and observe substantial differences which can be explained through an equilibrium analysis of the game. We consider a number of modeling approaches for our verbal communication setup, differing from each other in the model type (deep neural network (DNN) vs. linear classifier), the type of features used by the model (textual, behavioral or both) and the source of the textual features (DNN-based vs. hand-crafted). Our results demonstrate that given a prefix of the interaction sequence, our models can predict the future decisions of the decision-maker, particularly when a sequential modeling approach and hand-crafted textual features are applied. Further analysis of the hand-crafted textual features allows us to make initial observations about the aspects of text that drive decision making in our setup.
UR  - https://arxiv.org/pdf/2012.09966v5.pdf
N1  - Source: page_2.html
ID  - 398
ER  - 
TY  - JOUR
TI  - Persuading voters in district-based elections
AU  - Castiglioni, M.
AU  - Gatti, N.
PY  - 2020
AB  - We focus on the scenario in which an agent can exploit his information advantage to manipulate the outcome of an election. In particular, we study district-based elections with two candidates, in which the winner of the election is the candidate that wins in the majority of the districts. District-based elections are adopted worldwide (e.g., UK and USA) and are a natural extension of widely studied voting mechanisms (e.g., k-voting and plurality voting). We resort to the Bayesian persuasion framework, where the manipulator (sender) strategically discloses information to the voters (receivers) that update their beliefs rationally. We study both private signaling, in which the sender can use a private communication channel per receiver, and public signaling, in which the sender can use a single communication channel for all the receivers. Furthermore, for the first time, we introduce semi-public signaling in which the sender can use a single communication channel per district. We show that there is a sharp distinction between private and (semi-)public signaling. In particular, optimal private signaling schemes can provide an arbitrarily better probability of victory than (semi-)public ones and can be computed efficiently, while optimal (semi-)public signaling schemes cannot be approximated to within any factor in polynomial time unless P = NP. However, we show that reasonable relaxations allow the design of multi-criteria PTASs for optimal (semi-)public signaling schemes. In doing so, we introduce a novel property, namely comparative stability, and we design a bi-criteria PTAS for public signaling in general Bayesian persuasion problems beyond elections when the sender’s utility function is state-dependent.
UR  - https://arxiv.org/pdf/2012.05002.pdf
N1  - Source: page_2.html
ID  - 399
ER  - 
TY  - JOUR
TI  - J-recs: principled and scalable recommendation justification
AU  - Park, N.
AU  - Kan, A.
AU  - Faloutsos, C.
AU  - Dong, X.L.
PY  - 2020
AB  - Online recommendation is an essential functionality across a variety of services, including e-commerce and video streaming, where items to buy, watch, or read are suggested to users. Justifying recommendations, i.e., explaining why a user might like the recommended item, has been shown to improve user satisfaction and persuasiveness of the recommendation. In this paper, we develop a method for generating post-hoc justifications that can be applied to the output of any recommendation algorithm. Existing post-hoc methods are often limited in providing diverse justifications, as they either use only one of many available types of input data, or rely on the predefined templates. We address these limitations of earlier approaches by developing J-RECS, a method for producing concise and diverse justifications. J-RECS is a recommendation model-agnostic method that generates diverse justifications based on various types of product and user data (e.g., purchase history and product attributes). The challenge of jointly processing multiple types of data is addressed by designing a principled graph-based approach for justification generation. In addition to theoretical analysis, we present an extensive evaluation on synthetic and real-world data. Our results show that J-RECS satisfies desirable properties of justifications, and efficiently produces effective justifications, matching user preferences up to 20% more accurately than baselines.
UR  - https://arxiv.org/pdf/2011.05928.pdf
N1  - Source: page_2.html
ID  - 400
ER  - 
