Title,Authors,Abstract,Published Year,Published Month,Journal,Volume,Issue,Pages,Accession Number,DOI,Ref,Covidence #,Study,Notes,Tags,Sofiyas_decision
An agent-based emotional persuasion model driven by integrated trust assessment,"Wu, J.; Zhang, Y.; Cao, R.; Li, Y.","Recent research on automated negotiation has primarily focused on improving the artificial intelligence of agents and equipping them with more flexible internal mechanisms to facilitate high-quality negotiations. However, the study on the systematic modeling of human-like psychological and behavioral activities and their role in the negotiation process is still in its early stages. In light of this, this paper proposes an emotional persuasion model that takes into account the effect of negotiators' integrated trust assessments on negotiation. Firstly, the paper presents a negotiation agent with both cognitive and emotional functions, detailing its internal system and operating mechanism. Secondly, the integrated trust of a negotiator is obtained by evaluating multiple single trusts, and the mapping of the integrated trust to the negotiation round parameter is modeled. Integrated trust is also parameterized into the agent's cognitive processes. Finally, the paper introduces a new framework for the generation of emotional persuasive behavior to assist agents in making new proposals. A series of experiments were conducted, yielding the following results: Compared with the non-emotional model, the performance of negotiation rounds and utility differences improved by 7.97 % and 4.81 %, respectively. Furthermore, the trust-driven emotional persuasion model outperformed the several existing competing models by at least 31.1 % in utility difference and 81.0 % in negotiation rounds. Additionally, a case study of human-computer negotiation demonstrated that the agent designed using the proposed method has negotiating capabilities comparable to those of a real human, which further showcases the application effect of the artificial intelligence agent in practice. © 2025 Elsevier Ltd",2025,,Engineering Applications of Artificial Intelligence,149,,,,10.1016/j.engappai.2025.110567,,#4535,Wu 2025,,,1
Wearable Meets LLM for Stress Management: A Duoethnographic Study Integrating Wearable-Triggered Stressors and LLM Chatbots for Personalized Interventions,"Neupane, S.; Dongre, P.; Gracanin, D.; Kumar, S.","We use a duoethnographic approach to study how wearable-integrated LLM chatbots can assist with personalized stress management, addressing the growing need for immediacy and tailored interventions. Two researchers interacted with custom chatbots over 22 days, responding to wearable-detected physiological prompts, recording stressor phrases, and using them to seek tailored interventions from their LLM-powered chatbots. They recorded their experiences in autoethnographic diaries and analyzed them during weekly discussions, focusing on the relevance, clarity, and impact of chatbot-generated interventions. Results showed that even though most events triggered by the wearable were meaningful, only one in five warranted an intervention. It also showed that interventions tailored with brief event descriptions were more effective than generic ones. By examining the intersection of wearables and LLM, this research contributes to developing more effective, user-centric mental health tools for real-time stress relief and behavior change. © 2025 Copyright held by the owner/author(s).",2025,,Conference on Human Factors in Computing Systems - Proceedings ,,,,,10.1145/3706599.3720197,,#4538,Neupane 2025,,,1
What Dialogue Content Leads to a Trust Relationship and Behavior Change? Dialogue and Questionnaire Analysis,"Sato, T.; Sano, T.; Kumakawa, E.; Fujimura, K.; Yamashita, N.","Recently, there has been increasing research on designing chatbots for behavior change. While trust between individuals and their supporters is recognized as a crucial factor in fostering behavior change, it remains unclear what types of dialogue contribute to building such trust. In this study, we investigated health guidance interviews to address two key questions: 1. What kind of trust relationship facilitates behavior change? and 2. What type of dialogue contributes to fostering that trust? Our findings indicate that individuals were more motivated to pursue behavioral goals when they perceived the interviewer as having integrity. Furthermore, an analysis of interviewer speech using four dialogue categories revealed that perceptions of integrity were stronger when interviewers spent more time on “Providing Tailored Insights” rather than “Building a Trust Relationship.” These insights contribute to designing chatbots that effectively support behavior change by fostering trust through dialogue strategies. © 2025 Copyright held by the owner/author(s).",2025,,Conference on Human Factors in Computing Systems - Proceedings ,,,,,10.1145/3706599.3720143,,#4541,Sato 2025,,,1
"Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation","Cima, L.; Miaschi, A.; Trujillo, A.; Avvenuti, M.; Dell’Orletta, F.; Cresci, S.","AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct a LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation. © 2025 Copyright held by the owner/author(s).",2025,,WWW 2025 - Proceedings of the ACM Web Conference,,,5022-5033,,10.1145/3696410.3714507,,#4547,Cima 2025,,,1
How AI sources can increase openness to opposing views,"Lu, L.; Tormala, Z.L.; Duhachek, A.","Exposure to counterattitudinal information has been shown to yield mixed effects on attitude polarization. The current research explores the differential impact of such information when generated by artificial intelligence (AI) versus human sources. While prior work highlights a general aversion to AI for decision-making, our research reveals a consistent openness to AI in the context of counterattitudinal messages. Across four pre-registered studies (N = 2061), we find that when people receive counterattitudinal messages on potentially polarizing issues, AI sources are perceived as less biased, more informative, and having less persuasive intent than human sources. This leads to greater receptiveness to counterattitudinal messages when those messages come from AI rather than human sources. In addition, we find preliminary evidence that receiving counterattitudinal messages from an AI (versus human) source can diminish outgroup animosity and facilitate attitude change. © The Author(s) 2025.",2025,,Scientific Reports,15,1,,,10.1038/s41598-025-00791-z,,#4549,Lu 2025,,,1
Combining the Strengths of LLMs and Persuasive Technology to Combat Cyberhate,"Almaliki, M.; Almars, A.M.; Aljuhani, K.O.; Atlam, E.-S.","Cyberhate presents a multifaceted, context-sensitive challenge that existing detection methods often struggle to tackle effectively. Large language models (LLMs) exhibit considerable potential for improving cyberhate detection due to their advanced contextual understanding. However, detection alone is insufficient; it is crucial for software to also promote healthier user behaviors and empower individuals to actively confront the spread of cyberhate. This study investigates whether integrating large language models (LLMs) with persuasive technology (PT) can effectively detect cyberhate and encourage prosocial user behavior in digital spaces. Through an empirical study, we examine users’ perceptions of a self-monitoring persuasive strategy designed to reduce cyberhate. Specifically, the study introduces the Comment Analysis Feature to limit cyberhate spread, utilizing a prompt-based fine-tuning approach combined with LLMs. By framing users’ comments within the relevant context of cyberhate, the feature classifies input as either cyberhate or non-cyberhate and generates context-aware alternative statements when necessary to encourage more positive communication. A case study evaluated its real-world performance, examining user comments, detection accuracy, and the impact of alternative statements on user engagement and perception. The findings indicate that while most of the users (83%) found the suggestions clear and helpful, some resisted them, either because they felt the changes were irrelevant or misaligned with their intended expression (15%) or because they perceived them as a form of censorship (36%). However, a substantial number of users (40%) believed the interventions enhanced their language and overall commenting tone, with 68% suggesting they could have a positive long-term impact on reducing cyberhate. These insights highlight the potential of combining LLMs and PT to promote healthier online discourse while underscoring the need to address user concerns regarding relevance, intent, and freedom of expression. © 2025 by the authors.",2025,,Computers,14,5,,,10.3390/computers14050173,,#4552,Almaliki 2025,,,1
Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations,"Danry, V.; Pataranutaporn, P.; Groh, M.; Epstein, Z.","Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and discredit true information. We examined the impact of deceptive AI generated explanations on individuals' beliefs in a pre-registered online experiment with 11,780 observations from 589 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that logically invalid explanations are deemed less credible - diminishing the effects of deception. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation. © 2025 Copyright held by the owner/author(s).",2025,,Conference on Human Factors in Computing Systems - Proceedings ,,,,,10.1145/3706598.3713408,,#4553,Danry 2025,,,1
"Hyper-DREAM, a Multimodal Digital Transformation Hypertension Management Platform Integrating Large Language Model and Digital Phenotyping: Multicenter Development and Initial Validation Study","Wang, Y.; Zhu, T.; Zhou, T.; Wu, B.; Tan, W.; Ma, K.; Yao, Z.; Wang, J.; Li, S.; Qin, F.; Xu, Y.; Tan, L.; Liu, J.","Within the mHealth framework, systematic research that collects and analyzes patient data to establish comprehensive digital health archives for hypertensive patients, and leverages large language models (LLMs) to assist clinicians in health management and Blood Pressure (BP) control remains limited. In this study, our aims to describe the design, development and usability evaluation process of a management platform (Hyper-DREAM) for hypertension. Our multidisciplinary team employed an iterative design approach over the course of a year to develop the Hyper-DREAM platform. This platform's primary functionalities encompass multimodal data collection (personal hypertensive digital phenotype archive), multimodal interventions (BP measurement, medication assistance, behavior modification, and hypertension education) and multimodal interactions (clinician-patient engagement and BP Coach component). In August 2024, the mHealth App Usability Questionnaire (MAUQ) was conducted involving 51 hypertensive patients recruited from three distinct centers. In parallel, six clinicians engaged in management activities and contributed feedback via the Doctor’s Software Satisfaction Questionnaire (DSSQ). Concurrently, a real-world comparative experiment was conducted to evaluate the usability of the BP Coach, ChatGPT-4o Mini, ChatGPT-4o and clinicians. The comparative experiment demonstrated that the BP Coach achieved significantly higher scores in utility (mean scores 4.05, SD 0.87) and completeness (mean scores 4.12, SD 0.78) when compared to ChatGPT-4o Mini, ChatGPT-4o, and clinicians. In terms of clarity, the BP Coach was slightly lower than clinicians (mean scores 4.03, SD 0.88). In addition, the BP Coach exhibited lower performance in conciseness (mean scores 3.00, SD 0.96). Clinicians reported a marked improvement in work efficiency (2.67 vs. 4.17, P <.001) and experienced faster and more effective patient interactions (3.0 vs. 4.17, P =.004). Furthermore, the Hyper-DREAM platform significantly decreased work intensity (2.5 vs. 3.5, P =.01) and minimized disruptions to daily routines (2.33 vs. 3.55, P =.004). The Hyper-DREAM platform demonstrated significantly greater overall satisfaction compared to the WeChat-based standard management (3.33 vs. 4.17, P =.01). Additionally, clinicians exhibited a markedly higher willingness to integrate the Hyper-DREAM platform into clinical practice (2.67 vs. 4.17, P <.001). Furthermore, patient management time decreased from 11.5 min (SD 1.87) with Wechat-based standard management to 7.5 min (SD 1.84, P =.01) with Hyper-DREAM. Hypertensive patients reported high satisfaction with the Hyper-DREAM platform, including ease of use (mean scores 1.60, SD 0.69), system information arrangement (mean scores 1.69, SD 0.71), and usefulness (mean scores 1.57, SD 0.58). In conclusion, our study presents Hyper-DREAM, a novel artificial intelligence-driven platform for hypertension management, designed to alleviate clinician workload and exhibiting significant promise for clinical application. The Hyper-DREAM platform is distinguished by its user-friendliness, high satisfaction rates, utility, and effective organization of information. Furthermore, the BP Coach component underscores the potential of LLMs in advancing mHealth approaches to hypertension management. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.",2025,,Journal of Medical Systems,49,1,,,10.1007/s10916-025-02176-1,,#4557,Wang 2025,,,1
Promoting mindful consumption through a chatbot with an experiential mind,"Seo, J.K.; Yoon, H.J.","Purpose: To promote long-term sustainability and improve consumers’ quality of life, marketers can use artificial intelligence (AI) chatbots to initiate conversations about mindful consumption. Although anthropomorphic designs are integral for successful persuasion, there is scant research on how anthropomorphizing chatbots’ internal traits influences consumers. Integrating the Uncanny Valley Effect (UVE) and the Arousal-Biased Competition (ABC) theory, this study aims to investigate how a chatbot with a higher experiential mind shapes attitudes toward mindful consumption messages by examining people’s emotional responses (i.e. eeriness and amazement) and cognitive engagement. Design/methodology/approach: Incorporating real-time interactions with a chatbot, this research adopted an online experiment with a one-factor, two-condition (a higher vs a lower experiential mind) design with eeriness and amazement as parallel mediators, leading to cognitive absorption and, consequently, attitudes toward mindful consumption messages in a serial manner. Findings: This study found that a chatbot with a higher (vs lower) experiential mind simultaneously triggers higher levels of eeriness and amazement, leading to higher cognitive absorption and a more positive message attitude. Originality/value: This study expands the current anthropomorphism literature by examining the effects of imbuing nonhuman agents with an experiential mind and emphasizing a nuanced view of emotional responses to anthropomorphized chatbots. The findings contribute to establishing a theoretical link between a chatbot’s experiential mind level and persuasion outcomes, offering strategic and ethical insights for anthropomorphized AI use in sustainability marketing. © 2025, Emerald Publishing Limited.",2025,,Journal of Consumer Marketing,42,4,498-511,,10.1108/JCM-05-2024-6844,,#4568,Seo 2025,,,1
Persuasion in Pixels and Prose: The Effects of Emotional Language and Visuals in Agent Conversations on Decision-Making,"Genç, H.U.; Chandrasegaran, S.; Dingler, T.; Verma, H.","The growing sophistication of Large Language Models allows conversational agents (CAs) to engage users in increasingly personalized and targeted conversations. While users may vary in their receptiveness to CA persuasion, stylistic elements and agent personalities can be adjusted on the fly. Combined with image generation models that create context-specific realistic visuals, CAs have the potential to influence user behavior and decision making. We investigate the effects of linguistic and visual elements used by CAs on user perception and decision making in a charitable donation context with an online experiment (n=344). We find that while CA attitude influenced trust, it did not affect donation behavior. Visual primes played no role in shaping trust, though their absence resulted in higher donations and situational empathy. Perceptions of competence and situational empathy were potential predictors of donation amounts. We discuss the complex interplay of user and CA characteristics and the fine line between benign behavior signaling and manipulation. © 2025 Copyright held by the owner/author(s).",2025,,Conference on Human Factors in Computing Systems - Proceedings ,,,,,10.1145/3706598.3713579,,#4569,Genç 2025,,,1
Fact-based Counter Narrative Generation to Combat Hate Speech,"Wilk, B.; Shomee, H.H.; Maity, S.K.; Medya, S.","Online hatred has become an increasingly pervasive issue, affecting individuals and communities across various digital platforms. To combat hate speech in such platforms, counter narratives (CNs) are regarded as an effective method. In recent years, there has been growing interest in using generative AI tools to construct CNs. However, most of the generative models produce generic responses to hate speech and can hallucinate, reducing their effectiveness. To address the above limitations, we propose a counter narrative generation method that enhances CNs by providing non-aggressive, fact-based narratives with relevant background knowledge from two distinct sources, including a web search module. Furthermore, we conduct a comprehensive evaluation using multiple metrics, including LLM-based measures for persuasion, factuality, and informativeness, along with human and traditional NLP evaluations. Our method significantly outperforms baselines, achieving an average factuality score of 0.915, compared to 0.741, 0.701, and 0.69 for competitive baselines, and performs well in human evaluations. © 2025 Copyright held by the owner/author(s).",2025,,WWW 2025 - Proceedings of the ACM Web Conference,,,3354-3365,,10.1145/3696410.3714718,,#4571,Wilk 2025,,,1
AI-determined similarity increases likability and trustworthiness of human voices,"Jaggy, O.; Schwan, S.; Meyerhoff, H.S.","Modern artificial intelligence (AI) technology is capable of generating human sounding voices that could be used to deceive recipients in various contexts (e.g., deep fakes). Given the increasing accessibility of this technology and its potential societal implications, the present study conducted online experiments using original data to investigate the validity of AI-based voice similarity measures and their impact on trustworthiness and likability. Correlation analyses revealed that voiceprints – numerical representations of voices derived from a speaker verification system – can be used to approximate human (dis)similarity ratings. With regard to cognitive evaluations, we observed that voices similar to one’s own voice increased trustworthiness and likability, whereas average voices did not elicit such effects. These findings suggest a preference for self-similar voices and underscore the risks associated with the misuse of AI in generating persuasive artificial voices from brief voice samples. © 2025 Jaggy et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",2025,,PLoS ONE,20,03-Mar,,,10.1371/journal.pone.0318890,,#4572,Jaggy 2025,,,1
Generative propaganda: Evidence of AI’s impact from a state-backed disinformation campaign,"Wack, M.; Ehrett, C.; Linvill, D.; Warren, P.","Can AI bolster state-backed propaganda campaigns, in practice? Growing use of AI and large language models has drawn attention to the potential for accompanying tools to be used by malevolent actors. Though recent laboratory and experimental evidence has substantiated these concerns in principle, the usefulness of AI tools in the production of propaganda campaigns has remained difficult to ascertain. Drawing on the adoption of generative-AI techniques by a state-affiliated propaganda site with ties to Russia, we test whether AI adoption enabled the website to amplify and enhance its production of disinformation. First, we find that the use of generative-AI tools facilitated the outlet’s generation of larger quantities of disinformation. Second, we find that use of generative-AI coincided with shifts in the volume and breadth of published content. Finally, drawing on a survey experiment comparing perceptions of articles produced prior to and following the adoption of AI tools, we show that the AI-assisted articles maintained their persuasiveness in the postadoption period. Our results illustrate how generative-AI tools have already begun to alter the size and scope of state-backed propaganda campaigns. © The Author(s) 2025. Published by Oxford University Press on behalf of National Academy of Sciences.",2025,,PNAS Nexus,4,4,,,10.1093/pnasnexus/pgaf083,,#4576,Wack 2025,,,1
Fact-checking in the age of AI: Reducing biases with non-human information sources,"Moon, W.-K.; Kahlor, L.A.","This study examines the obstacles to the effectiveness of fact-checking, focusing primarily on the pervasive impact of entrenched biases. Fact-checking efforts often face resistance when linked to mistrusted sources, leading to cognitive dissonance and the rejection of messages in favor of pre-existing beliefs, a phenomenon known as motivated reasoning. This resistance hinders organizations’ ability to correct misconceptions surrounding social issues and entities. The research delves into whether non-human entities such as AI can facilitate less biased information processing due to their perceived impartiality. Applying a moderated mediation model in experimental settings, we found that labeling a source as artificial intelligence is pivotal in evaluating fact-checking. AI labels moderate the impact of partisan biases on the persuasive outcomes of fact-checks, such as message credibility and acceptance, compared to the human source. This study offers valuable insights for enhancing the effectiveness of fact-checking in the context of cognitive and psychological biases by highlighting the critical influence of information sources in reducing polarization in public perceptions of scientific issues. © 2024 Elsevier Ltd",2025,,Technology in Society,80,,,,10.1016/j.techsoc.2024.102760,,#4577,Moon 2025,,,1
"Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health","Wang, X.; Griffith, J.; Adler, D.A.; Castillo, J.; Choudhury, T.; Wang, F.","Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HealthGuru, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HealthGuru's multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HealthGuru to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HealthGuru. We also identify challenges and design considerations for personalization and user engagement in health chatbots. © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.",2025,,Conference on Human Factors in Computing Systems - Proceedings ,,,,,10.1145/3706598.3713852,,#4579,Wang 2025,,,1
"A Learning Agent for Stress Multi-Level Diagnostics, Personalised Stress Profiles and Interventions in the Work Context","Jung, O.; Schmoigl-Tonis, M.; Schranz, C.; Kunas, B.J.; Laireiter, A.-R.; Mehlis, J.; Beer, M.","Background: Work-related stress affects 39% of Austrians, contributing to mental health issues like depression and burnout, driven by factors such as workload and lack of control. Objectives: The Relax project aims to develop a holistic stress management framework using continuous stress assessment and personalized interventions based on physiological, behavioral, emotional, and cognitive indicators. Methods: The study combines wearable sensors (e.g., Polar Verity Sense), psychological methods, and technical strategies, with a longitudinal design to assess the app’s usability and effectiveness. Results: Usability was hindered by technical issues, but stress data visualization was well-received. The Micro-Model aligned well with stress estimates, while the Macro-Model faced data limitations. Methodological constraints suggested further refinement. Conclusion: The study highlights the need for individualized, multimodal stress management, with future research focusing on system improvements, technical refinement, and validation through an anonymized dataset. © 2025 The Authors.",2025,,Studies in Health Technology and Informatics,324,,246-251,,10.3233/SHTI250196,,#4581,Jung 2025,,,1
Stereotypical bias amplification and reversal in an experimental model of human interaction with generative artificial intelligence,"Allan, K.; Azcona, J.; Sripada, S.; Leontidis, G.; Sutherland, C.A.M.; Phillips, L.H.; Martin, D.","Stereotypical biases are readily acquired and expressed by generative artificial intelligence (AI), causing growing societal concern about these systems amplifying existing human bias. This concern rests on reasonable psychological assumptions, but stereotypical bias amplification during human-AI interaction relative to pre-existing baseline levels has not been demonstrated. Here, we use previous psychological work on gendered character traits to capture and control gender stereotypes expressed in character descriptions generated by Open AI's GPT3.5. In four experiments (N = 782) with a first impressions task, we find that unexplained ('black-box') character recommendations using stereotypical traits already convey a potent persuasive influence significantly amplifying baseline stereotyping within first impressions. Recommendations that are counter-stereotypical eliminate and effectively reverse human baseline bias, but these stereotype-challenging influences propagate less well than reinforcing influences from stereotypical recommendations. Critically, the bias amplification and reversal phenomena occur when GPT3.5 elaborates on the core stereotypical content, although GPT3.5's explanations propagate counter-stereotypical influence more effectively and persuasively than black-box recommendations. Our findings strongly imply that without robust safeguards, generative AI will amplify existing bias. But with safeguards, existing bias can be eliminated and even reversed. Our novel approach safely allows such effects to be studied in various contexts where gender and other bias-inducing social stereotypes operate.  © 2025 The Author(s).",2025,,Royal Society Open Science,12,4,,,10.1098/rsos.241472,,#4582,Allan 2025,,,1
Testing theories of political persuasion using AI,"Argyle, L.P.; Busby, E.C.; Gubler, J.R.; Lyman, A.; Olcott, J.; Pond, J.; Wingate, D.","Despite its importance to society and many decades of research, key questions about the social and psychological processes of political persuasion remain unanswered, often due to data limitations. We propose that AI tools, specifically generative large language models (LLMs), can be used to address these limitations, offering important advantages in the study of political persuasion. In two preregistered online survey experiments, we demonstrate the potential of generative AI as a tool to study persuasion and provide important insights about the psychological and communicative processes that lead to increased persuasion. Specifically, we test the effects of four AI-generated counterattitudinal persuasive strategies, designed to test the effectiveness of messages that include customization (writing messages based on a receiver’s personal traits and beliefs), and elaboration (increased psychological engagement with the argument through interaction). We find that all four types of persuasive AI produce significant attitude change relative to the control and shift vote support for candidates espousing views consistent with the treatments. However, we do not find evidence that message customization via microtargeting or cognitive elaboration through interaction with the AI have much more persuasive effect than a single generic message. These findings have implications for different theories of persuasion, which we discuss. Finally, we find that although persuasive messages are able to moderate some people’s attitudes, they have inconsistent and weaker effects on the democratic reciprocity people grant to their political opponents. This suggests that attitude moderation (ideological depolarization) does not necessarily lead to increased democratic tolerance or decreased affective polarization. Copyright © 2025 the Author(s).",2025,,Proceedings of the National Academy of Sciences of the United States of America,122,18,,,10.1073/pnas.2412815122,,#4585,Argyle 2025,,,1
THE SOCIOLOGICAL AND HEALTHCARE INTERVENTIONS FOR EXERCISE ADDICTION USING GENERATIVE ARTIFICIAL INTELLIGENCE,"Xie, Q.; Lipowski, M.; Liu, T.","This work aims to design and validate an innovative hybrid framework that combines the Support Vector Machine (SVM) for the precise identification of exercise addiction with personalized interventions based on Reinforcement Learning (RL). This approach effectively addresses the growing issue of exercise addiction and fosters the development of healthy exercise habits in individuals. 100 participants are recruited, and high-quality data, including physical activity levels and heart rate, are obtained through collaboration with a fitness tracker manufacturer. Additionally, a self-report questionnaire is designed to gather information on participants’ exercise habits and psychological states, and medical records are integrated to enhance the data comprehensiveness. The SVM model is adopted to classify and identify exercise addiction behaviors, achieving high-precision individual state assessments. Later, based on the identification results, this work develops an intelligent recommendation system based on RL to dynamically adjust and personalize intervention strategies. The results reveal that the intervention led to a significant 35.9% reduction in weekly excessive exercise hours, demonstrating the effectiveness of the measures in curbing excessive exercise behavior. Additionally, the number of self-reported rest days increases from 0.8 to 2.1 days, marking a 162.5% rise. This demonstrates that individuals begin to prioritize rest and recovery, which is crucial for preventing exercise addiction. This work not only provides a new technical means for the early identification of exercise addiction, but also highlights the substantial potential of artificial intelligence in fostering healthy behavior changes. © 2025 World Scientific Publishing Company.",2025,,Journal of Mechanics in Medicine and Biology,25,5,,,10.1142/S0219519425400354,,#4589,Xie 2025,,,1
"Psychological, economic, and ethical factors in human feedback for a chatbot-based smoking cessation intervention","Albers, N.; Melo, F.S.; Neerincx, M.A.; Kudina, O.; Brinkman, W.-P.","Integrating human support with chatbot-based behavior change interventions raises three challenges: (1) attuning the support to an individual’s state (e.g., motivation) for enhanced engagement, (2) limiting the use of the concerning human resources for enhanced efficiency, and (3) optimizing outcomes on ethical aspects (e.g., fairness). Therefore, we conducted a study in which 679 smokers and vapers had a 20% chance of receiving human feedback between five chatbot sessions. We find that having received feedback increases retention and effort spent on preparatory activities. However, analyzing a reinforcement learning (RL) model fit on the data shows there are also states where not providing feedback is better. Even this “standard” benefit-maximizing RL model is value-laden. It not only prioritizes people who would benefit most, but also those who are already doing well and want feedback. We show how four other ethical principles can be incorporated to favor other smoker subgroups, yet, interdependencies exist. © The Author(s) 2025.",2025,,npj Digital Medicine,8,1,,,10.1038/s41746-025-01701-3,,#4592,Albers 2025,,,1
Artificial Intelligence and Political Deepfakes: Shaping Citizen Perceptions Through Misinformation,"Momeni, M.","In the post-truth age, political conspiracies circulate rapidly on social media, cultivating false narratives, while challenging the public’s ability to distinguish truth from fiction. ‘Deepfakes’ represent the most recent type of misinformation. They display deceitful representations of events to lead audiences to believe in fabricated realities. There has been limited research on deepfakes in political communications. As this technology progresses, deepfakes look deceptively authentic; thus, it is necessary to explore their effects on public perceptions. This study examines viewers’ comments on an Instagram-published deepfake video of Hillary Clinton to understand the impact of this technology. The results demonstrate that individuals struggle to identify deepfake videos and that their opinions are affected by this persuasive type of misinformation. This study also explores different ethical concerns posed by political deepfakes. By offering insights into public reactions to manipulated content, this study contributes to our understanding of the political effects of AI-fabricated content. © 2024 The Author(s).",2025,,Journal of Creative Communications,20,1,41-56,,10.1177/09732586241277335,,#4603,Momeni 2025,,,1
ZzzMate: A Self-Conscious Emotion-Aware Chatbot for Sleep Intervention,"Tang, X.; Li, Z.; Sun, X.; Xu, X.; Zhang, M.-L.","Empathy-driven design is increasingly recognized as a key element in behavior change interventions. However, self-conscious emotions such as guilt, shame, embarrassment, and pride are often overlooked. We introduce ZzzMate, an empathetic chatbot designed to detect and address these emotions to support healthy sleep routines. ZzzMate incorporates a novel emotion detection model that identifies self-conscious emotions from user interactions, which then informs a large language model (LLM) to generate empathetic responses. The system combines detected emotions with users’ sleep goals and empathy strategies to provide personalized interventions. In a comparative pilot study against a standard GPT, ZzzMate demonstrated better performance in emotional intelligence, self-efficacy and sleep adherence. This late-breaking-work presents our initial findings and system design of ZzzMate, laying the groundwork for future research in self-conscious emotion-aware health intervention technologies. © 2025 Copyright held by the owner/author(s).",2025,,Conference on Human Factors in Computing Systems - Proceedings ,,,,,10.1145/3706599.3720224,,#4612,Tang 2025,,,1
A virtual assistant can persuade you to get vaccinated against the flu. Online dialogue as a tool of social influence in promoting vaccinations,"Kozłowska, A.; Grzyb, T.; Doliński, D.","Declining acceptance of vaccinations poses a severe challenge to public health. This study delves into the effectiveness of online dialogue in shaping vaccination attitudes amidst this concerning trend. In today's digital age, utilizing the Internet for public health interventions seems crucial. We developed the FLORA application to engage users in health discussions through a conversational agent and tested it in two experiments. The first (N = 903) assessed the impact of online dialogue on vaccine willingness, while the second (N = 870) included a direct vaccination request. The results revealed a noticeable increase in vaccination intention among participants engaged in the interactive dialogue ending with a request. These findings highlight the potential of online dialogue to convey information about vaccines and support healthcare efforts effectively. © 2025 Elsevier Ltd",2025,,Social Science and Medicine,369,,,,10.1016/j.socscimed.2025.117825,,#4613,Kozłowska 2025,,,1
A large language model-based platform for real-time building monitoring and occupant interaction,"Xu, Y.; Zhu, S.; Cai, J.; Chen, J.; Li, S.","Effective management of indoor environments requires a comprehensive evaluation of health, energy consumption, and thermal comfort. However, real-time assessment of these factors is challenging due to the lack of integrated applications that combine IoT technology, real-time simulation, and user-friendly interfaces for communication. To address these challenges, this research introduces a novel platform specifically designed to manage health, energy consumption, and thermal comfort in smart buildings, leveraging IoT-based building information modeling (BIM), cloud computing, and an AI-powered conversational suggestion system based on the large language model (GPT). The platform integrates real-time monitoring, simulation, alerting, and persuasion capabilities to manage health, energy consumption, and thermal comfort, enabling responsive building environment controls by assessing tradeoffs among these dimensions and providing timely recommendations. Additionally, it employs persuasive techniques to encourage occupants to adopt environmentally-friendly practices. A case study in a university building demonstrated the platform's functionality and visualization capability. A survey assessing the persuasive system revealed high adoption rates—95.59 % for switching rooms to improve indoor air quality and health, and 79.90 % for adjusting clothing to enhance thermal comfort—indicating strong participant willingness to adopt sustainable practices through the platform's strategies. The key contribution of this research is the development of a comprehensive, real-time platform that enhances indoor environmental quality and sustainability through advanced monitoring, analysis, and social interaction. © 2024 Elsevier Ltd",2025,,Journal of Building Engineering,100,,,,10.1016/j.jobe.2024.111488,,#4617,Xu 2025,,,1
The persuasive role of generic-you in online interactions,"Niu, M.; Mower Provost, E.; Jurgens, D.; Gelman, S.A.; Kross, E.; Orvell, A.","Persuasion plays a crucial role in human communication. Yet, convincing someone to change their mind is often challenging. Here, we demonstrate that a subtle linguistic device, generic-you (i.e., “you” that refers to people in general, e.g., “You win some, you lose some”), is associated with successfully shifting people’s pre-existing views in a naturalistic context. Leveraging Large Language Models, we conducted a preregistered study using a large (= 204,120) online debate dataset. Every use of generic-you in an argument was associated with an up to 14% percent increase in the odds of successful persuasion. These findings underscore the need to distinguish between the specific and generic uses of “you” in large-scale linguistic analyses, an aspect that has been overlooked in the literature. The robust association between generic-you and persuasion persisted with the inclusion of various covariates, and above and beyond other pronouns (i.e., specific-you, I or we). However, these findings do not imply causality. In Supplementary Experiment 2, arguments with generic-you (vs. first-person singular pronouns, e.g., I) were rated as more persuasive by open-minded individuals. In Supplementary Experiment 3, generic-you (vs. specific-you) arguments did not differentially predict attitude change. We discuss explanations for these results, including differential mechanisms, boundary conditions, and the possibility that people intuitively draw on generic-you when expressing more persuasive ideas. Together, these findings add to a growing literature on the interpersonal implications of broadening one’s perspective via a subtle shift in language, while motivating future research on contextual and individual differences that may moderate these effects. © The Author(s) 2025.",2025,,Scientific Reports,15,1,,,10.1038/s41598-024-83440-1,,#4620,Niu 2025,,,1
Enhancing Self-Efficacy in Health Self-Examination through Conversational Agent's Encouragement,"Kollerup, N.K.; Bahodi, M.-T.; Cox, S.R.; Van Berkel, N.","Health self-examination, such as checking for changes to skin moles, is key to identifying potential negative changes to one's body. A major barrier to initiating a self-examination is a perceived lack of confidence or knowledge. In this study, we use a 2 × 2 between-subjects design to evaluate the effect of an AI conversational agent (CA) on participant self-efficacy and trust. We manipulated both participants' perceived skill in self-examination (based on prior perceived Success vs. Failure) and the CA's verbal persuasions (Encouraging vs. Neutral), with participants asked to complete a series of skin self-assessment tasks. Our findings show that participants' self-efficacy increased when exposed to encouraging CA persuasion. Additionally, we observed that an encouraging CA significantly increased participants' trust scores in perceived benevolence compared to a neutral-sounding CA. Our results inform the design of CAs to support users' independent self-examination. © 2025 Copyright held by the owner/author(s).",2025,,Conference on Human Factors in Computing Systems - Proceedings ,,,,,10.1145/3706598.3713142,,#4622,Kollerup 2025,,,1
Amplifying Minority Voices: AI-Mediated Devil’s Advocate System for Inclusive Group Decision-Making,"Lee, S.; Kim, M.; Hwang, S.; Kim, D.; Lee, K.","Group decision-making often benefits from diverse perspectives, yet power imbalances and social influence can stifle minority opinions and compromise outcomes. This prequel introduces an AI-mediated communication system that leverages the Large Language Model to serve as a devil’s advocate, representing underrepresented viewpoints without exposing minority members’ identities. Rooted in persuasive communication strategies and anonymity, the system aims to improve psychological safety and foster more inclusive decision-making. Our multi-agent architecture, which consists of a summary agent, conversation agent, AI duplicate checker, and paraphrase agent, encourages the group’s critical thinking while reducing repetitive outputs. We acknowledge that reliance on text-based communication and fixed intervention timings may limit adaptability, indicating pathways for refinement. By focusing on the representation of minority viewpoints anonymously in power-imbalanced settings, this approach highlights how AI-driven methods can evolve to support more divergent and inclusive group decision-making. © 2025 Copyright held by the owner/author(s).",2025,,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,17-21,,10.1145/3708557.3716334,,#4624,Lee 2025,,,1
"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour","Meyer, S.; Elsweiler, D.","This study examines the use of Motivational Interviewing (MI) principles in a GPT-4-based chatbot, MIcha, to promote behaviour change. We conducted a pre-registered randomised controlled trial to assess the integration of MI techniques in conversational agents, aiming to support users’ behaviour change through guided self-reflection and identify how users interact with large language model (LLM)-based systems in this context. Results indicate that short conversations with LLM-based chatbots are successful at increasing users’ readiness to change and usage of MI principles during text generation can effectively mitigate potential harms. Additionally, we identified distinct user behaviour types — cooperative, reflective, and pre-informed—that significantly influenced the outcomes of interactions. These findings demonstrate the potential of MI principles in enhancing the efficacy of conversational agents for behaviour change and highlight the importance of user behaviour in shaping interaction dynamics. © 2025",2025,,International Journal of Human Computer Studies,200,,,,10.1016/j.ijhcs.2025.103514,,#4626,Meyer 2025,,,1
Scaling language model size yields diminishing returns for single-message political persuasion,"Hackenburg, K.; Tappin, B.M.; Röttger, P.; Hale, S.A.; Bright, J.; Margetts, H.","Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 US political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25,982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence that model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are only slightly more persuasive than models smaller in size by an order of magnitude or more. Second, we find that the association between language model size and persuasiveness shrinks toward zero and is no longer statistically significant once we adjust for mere task completion (coherence, staying on topic), a pattern that highlights task completion as a potential mediator of larger models’ persuasive advantage. Given that current frontier models are already at ceiling on this task completion metric in our setting, taken together, our results suggest that further scaling model size may not much increase the persuasiveness of static LLM-generated political messages. Copyright © 2025 the Author(s).",2025,,Proceedings of the National Academy of Sciences of the United States of America,122,10,,,10.1073/pnas.2413443122,,#4627,Hackenburg 2025,,,1
Changes in attitudes toward meat consumption after chatting with a large language model,"Karakaş, N.; Jaeger, B.","Researchers have started to explore the persuasive power of large language models (LLMs) and initial results suggest that LLMs can be as persuasive as humans, even for controversial and moralized topics. We conducted a preregistered proof-of-concept study to test how a brief conversation with ChatGPT-4o influences attitudes and beliefs about meat consumption. We found that participants (n = 101) reported weaker commitment to eating meat and weaker beliefs that eating meat is necessary and natural, but not weaker beliefs that eating meat is normal or nice, after their conversation (vs. before). Our study provides preliminary evidence and can act as a blueprint for future explorations of how conversations with LLMs can change attitudes on meat consumption and related topics. © 2025 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",2025,,Social Influence,20,1,,,10.1080/15534510.2025.2475802,,#4643,Karakaş 2025,,,1
AI is more persuasive than people in online debates,"Simms, C.","When given information about its human opponents, the large language model GPT-4 was able to make particularly convincing arguments. © Springer Nature Limited 2025.",2025,,Nature,,,,,10.1038/d41586-025-01599-7,,#4655,Simms 2025,,,1
"Intelligent Cognitive System for Computational Psychotherapy with a Conversational Agent for Attitude and Behavior Change in Stress, Anxiety, and Depression","Kolenik, T.","The paper summarizes a Doctoral Thesis presenting a computational psychotherapy system for stress, anxiety, and depression (SAD) using a conversational agent. The first contribution is a novel panel dataset combining quantitative diagnostic-level questionnaires and qualitative daily diaries. The second contribution is the system itself, built upon a cognitive architecture simulating Theory of Mind through an ensemble of user, machine learning, and knowledge models for SAD prediction, forecasting, and personalized intervention. © 2025 Slovene Society Informatika. All rights reserved.",2025,,Informatica (Slovenia),49,2,451-454,,10.31449/inf.v49i2.8738,,#4661,Kolenik 2025,,,1
Assessment of Compliance and Validity of Digital Application-based Diet Diary with Suggestive Local Food Habits for Prevention of Dental Caries in Children,"Agrawal, P.V.; Rathi, N.; Sinnarkar, S.E.; Tasgaonkar, A.; Kotnis, R.; Joshi, P.R.","Aim: The purpose of the study is to use a community-based participatory research strategy to refine and test the usability of an artificial intelligence (AI)-powered smartphone app, Sugar Smart, to be used by children’s parents/caregivers for the maintenance of dietary records and oral hygiene habits of children in Maharashtra, India, through a single-arm parallel randomized control trial. Materials and methods: The app, developed based on validated research on local dietary habits, was utilized by 100 children. To evaluate the app, two validated questionnaires and a 7-day diet diary format were employed. The first questionnaire assessed ease of use and compliance using a Chi-squared test, while the second examined improvements in dietary awareness before and after app usage through a paired t-test. Results: The findings indicate that a significant majority of users positively perceived the mobile app. The first questionnaire showed high ease of use and positive feedback (87% found it easy to use, 99% reported it helped regulate sugar intake). The Chi-squared test confirmed the statistical significance (p < 0.05). The second questionnaire revealed significant improvements in oral health awareness and diet (correct brushing: 64–98%, reduced sugar usage: 28–91%). The paired t-test confirmed these improvements (p < 0.05), validating the app’s effectiveness in enhancing dietary habits and oral health awareness. Conclusion: The study provides strong evidence for the effectiveness of the digital app in improving parental awareness and management of children’s sugar intake. The significant improvements observed underscore the app’s potential as a valuable tool in pediatric dentistry and public health. Clinical significance: This app demonstrates the potential to revolutionize dietary monitoring and behavior change in pediatric populations, promoting healthier dietary behaviors and thus being an effective digital tool in the reduction of caries burden. © The Author(s).",2025,,International Journal of Clinical Pediatric Dentistry,18,2,191-197,,10.5005/jp-journals-10005-3047,,#4663,Agrawal 2025,,,1
Identifying Characteristics of Dietary Guidance Methods Through Qualitative Analysis,"Fujimura, K.; Sato, T.; Kumakawa, E.; Takahashi, M.; Katagiri, Y.; Kasahara, A.; Yamamoto, Y.; Seyama, M.; Yamada, S.","Continuous support for improving lifestyle habits is effective in preventing metabolic syndrome. However, providing sustained support by healthcare professionals imposes a significant burden on them. With the increasing use of large language models (LLMs) capable of efficiently generating messages, incorporating the characteristics of dietary guidance by healthcare professionals into LLM prompts may enable the generation of messages with the same motivational effect as professional guidance. To identify these key characteristics, we conducted a dietary-improvement experiment involving the LOCABO Challenge dietary-guidance program and collected data on dietary advice. Through qualitative analysis, we elucidated the process by which healthcare professionals review participants’ dietary records, formulate advice, and use motivational strategies in dietary guidance. We also explored potential approaches for incorporating our findings into the LLM design. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Communications in Computer and Information Science,2522 CCIS,,260-268,,10.1007/978-3-031-94150-4_27,,#4665,Fujimura 2025,,,1
Toward Scalable Content Generation for Gamified mHealth Interventions: The Evaluation of LLM-Generated Goals on User Engagement,"James, L.J.; Genga, L.; Montagne, B.; Hagenaars, M.A.; Van Gorp, P.M.E.","Gamified mHealth applications are increasingly used to promote healthy behavior change, yet low participant engagement remains a challenge. Personalized content has shown promise in increasing engagement, however, creating personalized content is a time-consuming task. Generative Artificial Intelligence models, particularly Large Language Models, offer a potential solution with their ability to quickly generate relevant content. In this study, we aim to evaluate the impact of LLM-generated content on participant engagement in health interventions using gamified mHealth applications. A total of 73 students and staff members of a university participated in a health intervention and were assigned into groups receiving either LLM-generated goals or predetermined goals. Engagement and perceived intrinsic motivation were measured and compared between the two groups. The results show no significant difference in engagement or perceived intrinsic motivation between participants receiving LLM-generated goals and those receiving predetermined goals. LLM-generated goals did not significantly negatively impact participant engagement and could therefore possibly offer a time-efficient approach to scalable content generation for mHealth applications. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) ,15192 LNCS,,94-107,,10.1007/978-3-031-74353-5_7,,#4670,James 2025,,,1
Using Hexad Archetypes to Motivate Students in a Chatbot-Enhanced Web-Based-Training,"Schlimbach, R.; Behne, J.; Lange, T.; Robra-Bissantz, S.","This study explores the challenge of maintaining motivation in further education for working students, who face the double burden of work and learning. To address this issue, we investigate the design and implementation of a pedagogical conversational agent (PCA) within a web-based training (WBT) platform. Drawing on literature, interviews with 11 experts, and a creative workshop with 14 working students, we use the Hexad user type framework to tailor the WBT to each user’s motivational archetype. We prioritize design features for each of the six archetypes and instantiate these in a prototype. In a field experiment with 17 working students using the WBT prototype for exam preparation, we observe a significant increase in intrinsic and extrinsic motivation. This study contributes to the emerging field of PCA-enhanced digital learning, highlighting the potential of personalized motivation in persuasive dialogue systems. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Lecture Notes in Information Systems and Organisation,76,,449-465,,10.1007/978-3-031-80125-9_26,,#4674,Schlimbach 2025,,,1
Influence of Floor Type on Social Navigation with Small Free-Standing Groups in Virtual Reality,"Zojaji, S.; Schiött, J.; Ivegren, W.; Matviienko, A.; Peters, C.","Human footsteps play a significant role in everyday life, allowing individuals to discern the emotions, gender, and intentions of others solely from the sound of their footsteps. However, the influence of footstep sounds made when walking on different floor types in virtual reality (VR) environments when joining conversational groups remains unclear. In this paper, we present a controlled study (N=50) to assess the impact of five different floor types, associated with specific footstep sounds and visuals, on the persuasiveness of Embodied Conversational Agents (ECAs) when inviting participants to join a free-standing conversational group. We analyze routes taken by participants and the positions at which they join the group, which may be compliant or not with the agent’s request when approaching the group while walking on different virtual floor types. Our findings reveal that the type of floor being walked upon, defined by footstep sounds and visual appearance, significantly impacts the persuasiveness of ECAs and the trajectories taken by participants to join the group. Participants took longer paths and joined the group in the presence of more pleasant footstep sounds. Further, they tended to adhere to social norms by avoiding walking through the group’s center. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Lecture Notes in Computer Science,15789 LNCS,,280-298,,10.1007/978-3-031-93712-5_17,,#4675,Zojaji 2025,,,1
Deepfakes as a Democratic Threat: Experimental Evidence Shows Noxious Effects That Are Reducible Through Journalistic Fact Checks,"Dan, V.","Concerns have been raised over AI-generated deepfakes and their impact on democracy. Unlike earlier forms of disinformation relying on text or traditional video-editing techniques (cheapfakes), deepfakes employ artificial intelligence, provoking speculations that they may be even more persuasive and harder to debunk. Using an experiment with a multiple-message design (N = 2,085), we found that fake videos suggesting a sex, corruption, or prejudice scandal—but not text-only fakes—elicited substantial reputational damage for an innocent politician, regardless of whether the underlying technique was “cheap” or “deep.” This was visible in altered attitudes, emotions, and voting intentions. However, exposure to a journalistic fact-check substantially reduced and even eliminated the detrimental effects. These findings have important implications for our theoretical understanding related to the effects of and mitigation strategies for deepfakes. While clearly highlighting the significant persuasive potential of deepfakes (and visual disinformation in general), the present study paints a more nuanced picture than was previously possible. © The Author(s) 2025.",2025,,International Journal of Press/Politics,,,,,10.1177/19401612251317766,,#4682,Dan 2025,,,1
Designing for Trust: Integrating Self-referencing in Large Language Model-Based Health Coaching,"Meywirth, S.; Janson, A.; Söllner, M.","This research applies the Design Science Research (DSR) methodology to investigate how self-referencing in Large Language Model (LLM)-based health coaching influences user trust and perceptions of anthropomorphism. We synthesized theory-driven design principles to guide the integration of self-referencing and demonstrated them in a vignette-based prototype. Through a single-factorial between-subjects experiment, analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM) and qualitative feedback, we identified a dual effect of self-referencing: while professional self-referencing enhances trust via increased anthropomorphism, overly personal references can directly undermine trust. Based on these findings, we refined our design principles to optimize trust-building in LLM-based coaching. Our contributions provide actionable design guidelines for creating more effective and trustworthy AI-driven health interventions, advancing the understanding of anthropomorphic design in digital coaching contexts. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Lecture Notes in Computer Science,15703 LNCS,,296-309,,10.1007/978-3-031-93976-1_19,,#4686,Meywirth 2025,,,1
Effects of Consumers’ Belief in a Just World on Artificial Intelligence Recommendations: Mediating Effects of Perceived Benevolence and Selfishness,"Ahn, J.; Kim, E.","This study investigated the effect of users’ “belief in a just world” (BJW) on the persuasiveness of artificial intelligence (AI) agents’ recommendations. Our prediction that users’ preferences for human or AI agents vary according to their BJW levels was tested experimentally. The results revealed that individuals with high BJW rated human agents’ recommendations more favorably than those of AI agents, whereas those with low BJW preferred AI agents’ ones. This interaction was mediated by the perceptions of the agents’ benevolence and selfishness, which varied depending on the BJW levels and agent type. High-BJW individuals perceived human agents as more benevolent and less selfish, whereas low-BJW individuals showed the opposite pattern. In contrast, AI agents’ benevolence and selfishness perceptions were not influenced by BJW levels. This study provides theoretical insights by identifying BJW as a key factor affecting AI agents’ persuasive effects and suggests that perceived benevolence and selfishness are the psychological mechanisms behind these effects. These findings also offer practical guidance for designing more effective AI agent strategies tailored to consumer BJW levels. Copyright 2025, Mary Ann Liebert, Inc., publishers.",2025,,"Cyberpsychology, Behavior, and Social Networking",,,,,10.1089/cyber.2024.0513,,#4687,Ahn 2025,,,1
"Development of Chatbot-Based Oral Health Care for Young Children and Evaluation of its Effectiveness, Usability, and Acceptability: Mixed Methods Study","Pupong, K.; Hunsrisakhun, J.; Pithpornchaiyakul, S.; Naorungroj, S.","Background: Chatbots are increasingly accepted in public health for their ability to replicate human-like communication and provide scalable, 24/7 services. The high prevalence of dental caries in children underscores the need for early and effective intervention. Objective: This study aimed to develop the 30-Day FunDee chatbot and evaluate its effectiveness, usability, and acceptability in delivering oral health education to caregivers of children aged 6 to 36 months. Methods: The chatbot was created using the artificial intelligence (AI) chatbot behavior change model, integrating behavioral change theories into content designed for 3‐5 minutes of daily use over 30 days. A pre-post experimental study was conducted from December 2021 to February 2022 in Hat Yai District, Songkhla Province, and Maelan District, Pattani Province, Thailand. Fifty-eight caregivers completed a web-based structured questionnaire at baseline and 2 months post baseline to evaluate knowledge, protection motivation theory-based perceptions, and tooth-brushing practices. Usability was assessed via chatbot logfiles and a web-based questionnaire at 2 months post baseline. Acceptability was evaluated through three methods: (1) open-ended chatbot interactions on day 30, (2) a web-based structured questionnaire at 2 months post baseline, and (3) semistructured telephone interviews with 15 participants 2 weeks post intervention. Participants for interviews were stratified by adherence levels and randomly selected from Hatyai and Maelan districts. All self-reported variables were measured on a 5-point Likert scale (1=lowest, 5=highest). Results: The chatbot was successfully developed based on the 4 components of the AI chatbot behavior change model. Participants had a mean age of 34.5 (SD 8.6) years. The frequency of tooth brushing among caregivers significantly improved, increasing from 72.4% at baseline to 93.1% two months post baseline (P=.006). Protection motivation theory-based perceptions also showed significant improvement, with mean scores rising from 4.0 (SD 0.6) at baseline to 4.5 (SD 0.6) two months post baseline (P<.001). The chatbot received high ratings for satisfaction (4.7/5, SD 0.6) and usability (4.7/5, SD 0.5). Participants engaged with the chatbot for an average of 24.7 (SD 7.2) days out of 30. Caregivers praised the chatbot’s content quality, empathetic communication, and multimedia design, but noted the intervention’s lengthy duration and messaging system as limitations. Conclusions: The 30-Day FunDee chatbot effectively enhanced caregivers’ perceptions of oral health care and improved tooth-brushing practices for children aged 6‐36 months. High user satisfaction and engagement demonstrate its potential as an innovative tool for oral health education. These findings warrant further validation through large-scale, randomized controlled trials. © Kittiwara Pupong, Jaranya Hunsrisakhun, Samerchit Pithpornchaiyakul, Supawadee Naorungroj.",2025,,JMIR Pediatrics and Parenting,8,,,,10.2196/62738,,#4693,Pupong 2025,,,1
Which AI doctor would you like to see? Emulating healthcare provider–patient communication models with GPT-4: proof-of-concept and ethical exploration,"Zohny, H.; Allen, J.W.; Wilkinson, D.; Savulescu, J.","Large language models (LLMs) have demonstrated potential in enhancing various aspects of healthcare, including health provider–patient communication. However, some have raised the concern that such communication may adopt implicit communication norms that deviate from what patients want or need from talking with their healthcare provider. This paper explores the possibility of using LLMs to enable patients to choose their preferred communication style when discussing their medical cases. By providing a proof-of-concept demonstration using ChatGPT-4, we suggest LLMs can emulate different healthcare provider–patient communication approaches (building on Emanuel and Emanuel’s four models: paternalistic, informative, interpretive and deliberative). This allows patients to engage in a communication style that aligns with their individual needs and preferences. We also highlight potential risks associated with using LLMs in healthcare communication, such as reinforcing patients’ biases and the persuasive capabilities of LLMs that may lead to unintended manipulation. © Author(s) (or their employer(s)) 2025.",2025,,Journal of Medical Ethics,,,,,10.1136/jme-2024-110256,,#4697,Zohny 2025,,,1
"Exploring Client Perceptions on Gaining Infant Feeding Information Through the Texas Women, Infants, and Children (WIC) Chatbot","Baez, K.; Biediger-Friedman, L.; Johnson, C.M.; Stubblefield, E.; Escalera, L.; Markides, B.R.","The modernization of the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) program is a priority. The Texas WIC chatbot, Maya, streamlines client interactions through dialog-based responses. This qualitative study explored client capabilities, motivations, and opportunities for seeking nutrition information about breastfeeding, formula feeding, infant feeding safety, adequacy of infant feeding, and complementary feeding via a chatbot. A team conducted in-depth semi-structured interviews with Texas WIC clients (n = 19 women). All interviews were transcribed and subjected to a two-coder, four-phase process utilizing a theory-based codebook. Codes were compiled and thematically categorized. Identified themes included (1) motivations through necessity or resource availability, (2) client capabilities and Maya usability, and (3) opportunities for connection, support, and encouragement. Texas WIC clients that participated in this study expressed motivations, capabilities, and opportunities to engage with nutrition information through Maya. They described Maya as a favorable resource for behavior changes, and a trusted source of nutrition information, citing the credibility of WIC and reliability of the chatbot. The findings may inform future research and development of public health chatbots. Additional research is required to explore how different factors such as language and technology usage may impact client capabilities, motivations, and opportunities to seek nutrition information with regard to infant feeding. © 2025 by the authors.",2025,,International Journal of Environmental Research and Public Health,22,2,,,10.3390/ijerph22020193,,#4698,Baez 2025,,,1
AI-Based Health Assistant for Young Adults,"Chen, X.H.; Yu, B.; Fan, H.; Bin Azizan, M.F.; Ong, C.A.; Merritt, T.; Fernando, O.N.N.","This paper introduces an AI-based health assistant designed to foster healthier eating habits among young adults. Leveraging the Transtheoretical Model (TTM) of behavior change, the system integrates personalized dietary interventions with advanced AI technologies, including a graph-based database and dynamic user interactions powered by ChatGPT. Key features include meal logging, nutrient visualization, and stage-specific recommendations tailored to each user’s progress. A structured evaluation framework lays the foundation for assessing the system’s effectiveness in promoting sustained behavioral improvements. While the current work focuses on short-term outcomes, future enhancements aim to incorporate wearable integrations, gamification, and expanded personalization to address a diverse user base. This research represents a significant step toward harnessing AI to address complex dietary challenges through ethical, user-centric health interventions. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Lecture Notes in Computer Science,15821 LNAI,,263-280,,10.1007/978-3-031-93418-6_18,,#4702,Chen 2025,,,1
Enhancing physical activity through a relational artificial intelligence chatbot: A feasibility and usability study,"Oh, Y.J.; Liang, K.-H.; Kim, D.D.; Zhang, X.; Yu, Z.; Fukuoka, Y.; Zhang, J.","Objective: This study presents a pilot randomized controlled trial to assess the usability, feasibility, and initial efficacy of a mobile app-based relational artificial intelligence (AI) chatbot (Exerbot) intervention for increasing physical activity behavior. Methods: The study was conducted over a 1-week period, during which participants were randomized to either converse with a baseline chatbot without relational capacity (control group) or a relational chatbot using social relational communication strategies. Objectively measured physical activity data were collected using smartphone pedometers. Results: The study was feasible in enrolling a sample of 36 participants and with a 94% retention rate after 1 week. Daily engagement rate with the AI chatbot reached over 88% across the groups. Findings revealed that the control group experienced a significant decrease in steps on the final day, whereas the group interacting with the relational chatbot maintained their step counts throughout the study period. Importantly, individuals who engaged with the relational chatbot reported a stronger social bond with the chatbot compared to those in the control group. Conclusions: Leveraging AI chatbot and the relationship-building capabilities of AI holds promise in the development of cost-effective, accessible, and sustainable behavior change interventions. This approach may benefit individuals with limited access to conventional in-person behavior interventions. Clinical trial registrations: ClinicalTrials.gov; NCT05794308; https://clinicaltrials.gov/ct2/show/NCT05794308. © The Author(s) 2025.",2025,,Digital Health,11,,,,10.1177/20552076251324445,,#4703,Oh 2025,,,1
On the conversational persuasiveness of GPT-4,"Salvi, F.; Horta Ribeiro, M.; Gallotti, R.; West, R.","Early work has found that large language models (LLMs) can generate persuasive content. However, evidence on whether they can also personalize arguments to individual attributes remains limited, despite being crucial for assessing misuse. This preregistered study examines AI-driven persuasion in a controlled setting, where participants engaged in short multiround debates. Participants were randomly assigned to 1 of 12 conditions in a 2 × 2 × 3 design: (1) human or GPT-4 debate opponent; (2) opponent with or without access to sociodemographic participant data; (3) debate topic of low, medium or high opinion strength. In debate pairs where AI and humans were not equally persuasive, GPT-4 with personalization was more persuasive 64.4% of the time (81.2% relative increase in odds of higher post-debate agreement; 95% confidence interval [+26.0%, +160.7%], P < 0.01; N = 900). Our findings highlight the power of LLM-based persuasion and have implications for the governance and design of online platforms. © The Author(s) 2025.",2025,,Nature Human Behaviour,,,,,10.1038/s41562-025-02194-6,,#4708,Salvi 2025,,,1
From Destination to Daily Life: A Longitudinal Study on the Effects of Flashback Nudging on Pro-environmental Behavior Spillover,"Majid, G.M.; Tussyadiah, I.; Kim, Y.R.; Chen, J.L.","This study examines how flashback nudging—a reminder of past pro-environmental actions—delivered through a chatbot survey over 6 weeks in a quasi-experimental design, can facilitate pro-environmental behavior spillover among tourists visiting nature-based destinations. Integrating conversational artificial intelligence allows for personalized, timely, and scalable interventions, making the delivery of flashback nudges more efficient and engaging than traditional methods. Flashback nudging effectively strengthens the impact of environmental self-identity on behavioral change. Although nudges take time to work, the multigroup analysis revealed significant differences in self-reported past pro-environmental behaviors between the treatment and control groups. The findings demonstrate the importance of deliberate, technology-enabled interventions in enhancing meaning-making processes and promoting transformative tourism experiences. Artificial intelligence advances behavior change efforts by enabling innovative, lasting environmental engagement. This study emphasizes the potential of conversational artificial intelligence to drive tourism’s contributions to a sustainable future, advancing the discourse on leveraging technology for behavioral interventions in tourism. © The Author(s) 2025.",2025,,Journal of Travel Research,,,,,10.1177/00472875251337777,,#4710,Majid 2025,,,1
"Design, development, and evaluation of an mHealth app to reduce stress and promote happiness through smiling","Orji, J.; Chan, G.; Orji, R.","The field of mental health application research is growing, yet comprehensive, long-term studies validating claims of stress reduction and mood enhancement are limited, with many apps lacking empirical evidence. The purpose of this study was to evaluate an mHealth application called SmileApp to promote positive mood as a means of reducing stress. The design of SmileApp is grounded in psychological theories and integrates artificial intelligence (AI) and persuasive technology (PT). To evaluate SmileApp, we conducted a two-week in-the-wild study involving 72 participants. This was followed by an optional semi-structured interview with 23 participants. Quantitative results suggest that SmileApp is usable, useful, and encourages users to smile more frequently. Furthermore, qualitative results suggest that SmileApp was a unique design to help users alleviate stress. These results offer valuable insights into innovative approaches for designing mHealth applications that promote positive mood. Moreover, the findings underscore the importance of utilising technology to support emotional well-being. We present a novel approach to promote desired behaviours by motivating users to read supportive messages and playing mobile games through the act of smiling. © 2025 Informa UK Limited, trading as Taylor & Francis Group.",2025,,Behaviour and Information Technology,,,,,10.1080/0144929X.2025.2494278,,#4714,Orji 2025,,,1
Why don't you do what you said you would? Conversational strategies for agents to understand users' reasons in supporting behavior,"Chen, P.-Y.; Birna van Riemsdijk, M.; Heylen, D.K.J.; Jonker, C.M.; Tielman, M.L.","Effective support from personal assistive technologies relies on accurate user models that capture user values, preferences, and context. Knowledge-based techniques model these relationships, enabling support agents to align their actions with user values. However, understanding values in a single context is insufficient due to the dynamic nature of behaviour. This study explores the use of dialogue strategies to update user models. Participants were randomly assigned to different strategies and they discussed one randomly chosen non-adherence situation with the agent. Then, their emotions, acquired information accuracy, completeness, and dialogue experience were rated. Our findings suggest that multiple-choice dialogues may limit response depth, reducing the perceived completeness of behaviour reasons. In contrast, open-ended questions allow more detailed input but require more time and effort, potentially worsening the dialogue experience. Through inductive coding, we identified key topics, such as individual challenges, priorities, tangible outcomes, and values, essential for constructing personalised user models. We also analyzed conversation paths to improve dialogue-based user model updates in support agents. Further research is needed to refine the relationship between dialogue strategies and self-conscious emotions, considering diverse backgrounds and health goals, while enhancing dialogue design. © 2025 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",2025,,Behaviour and Information Technology,,,,,10.1080/0144929X.2025.2483793,,#4716,Chen 2025,,,1
Laypeople’s Use of and Attitudes Toward Large Language Models and Search Engines for Health Queries: Survey Study,"Mendel, T.; Singh, N.; Mann, D.M.; Wiesenfeld, B.; Nov, O.","Background: Laypeople have easy access to health information through large language models (LLMs), such as ChatGPT, and search engines, such as Google. Search engines transformed health information access, and LLMs offer a new avenue for answering laypeople’s questions. Objective: We aimed to compare the frequency of use and attitudes toward LLMs and search engines as well as their comparative relevance, usefulness, ease of use, and trustworthiness in responding to health queries. Methods: We conducted a screening survey to compare the demographics of LLM users and nonusers seeking health information, analyzing results with logistic regression. LLM users from the screening survey were invited to a follow-up survey to report the types of health information they sought. We compared the frequency of use of LLMs and search engines using ANOVA and Tukey post hoc tests. Lastly, paired-sample Wilcoxon tests compared LLMs and search engines on perceived usefulness, ease of use, trustworthiness, feelings, bias, and anthropomorphism. Results: In total, 2002 US participants recruited on Prolific participated in the screening survey about the use of LLMs and search engines. Of them, 52% (n=1045) of the participants were female, with a mean age of 39 (SD 13) years. Participants were 9.7% (n=194) Asian, 12.1% (n=242) Black, 73.3% (n=1467) White, 1.1% (n=22) Hispanic, and 3.8% (n=77) were of other races and ethnicities. Further, 1913 (95.6%) used search engines to look up health queries versus 642 (32.6%) for LLMs. Men had higher odds (odds ratio [OR] 1.63, 95% CI 1.34-1.99; P<.001) of using LLMs for health questions than women. Black (OR 1.90, 95% CI 1.42-2.54; P<.001) and Asian (OR 1.66, 95% CI 1.19-2.30; P<.01) individuals had higher odds than White individuals. Those with excellent perceived health (OR 1.46, 95% CI 1.1-1.93; P=.01) were more likely to use LLMs than those with good health. Higher technical proficiency increased the likelihood of LLM use (OR 1.26, 95% CI 1.14-1.39; P<.001). In a follow-up survey of 281 LLM users for health, most participants used search engines first (n=174, 62%) to answer health questions, but the second most common first source consulted was LLMs (n=39, 14%). LLMs were perceived as less useful (P<.01) and less relevant (P=.07), but elicited fewer negative feelings (P<.001), appeared more human (LLM: n=160, vs search: n=32), and were seen as less biased (P<.001). Trust (P=.56) and ease of use (P=.27) showed no differences. Conclusions: Search engines are the primary source of health information; yet, positive perceptions of LLMs suggest growing use. Future work could explore whether LLM trust and usefulness are enhanced by supplementing answers with external references and limiting persuasive language to curb overreliance. Collaboration with health organizations can help improve the quality of LLMs’ health output. ©Tamir Mendel, Nina Singh, Devin M Mann, Batia Wiesenfeld, Oded Nov.",2025,,Journal of Medical Internet Research,27,,,,10.2196/64290,,#4719,Mendel 2025,,,1
Towards a Proof-of-Principle of an LLM-Powered Low Resource Social Engineering Attack Coach,"Müller, L.; Sütterlin, S.; Morgenstern, H.","This article investigates the potential misuse of large language models (LLM) for low-resource, highly personalised social engineering attacks. The study explores how ChatGPT can infer personality traits during natural conversations by leveraging publicly available personal information, such as social media data, as an entry point. Utilising the social engineering personality framework (SEPF), the research endeavours to optimise attack vectors based on the Big Five personality traits, with the objective of enhancing the persuasiveness of social engineering strategies. The approach is divided into four phases: verifying conversational capabilities, conducting personality analyses, applying the SEPF for attack optimisation, and evaluating the persuasiveness of personalised attacks. The present paper offers a proof-of-principle for the initial phase, demonstrating ChatGPT’s capacity to engage in natural conversations while conducting personality analyses in a discreet manner. The findings indicate that while ChatGPT exhibits the capacity to simulate human-like interactions, limitations in conversational variance and the reliability of personality assessment were observed. The study identifies challenges such as generalisations, lack of score differentiation, and confirmation bias, and proposes refinements like increasing interaction depth, adjusting scoring scales, and using tailored personas. Subsequent research will investigate enhanced personality inference techniques, personalisation of attack vectors, and their impact on susceptibility to social engineering attacks. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Lecture Notes in Computer Science,15778 LNAI,,205-217,,10.1007/978-3-031-93724-8_16,,#4721,Müller 2025,,,1
Digital Home-Based Self-Monitoring System for People with Heart Failure: Protocol for Development of SmartHeart and Evaluation of Feasibility and Acceptability,"Maddison, R.; Nourse, R.; Daryabeygikhotbehsara, R.; Tegegne, T.K.; Jansons, P.; Rawstorn, J.C.; Atherton, J.; Driscoll, A.; Oldenburg, B.; Vasa, R.; Kostakos, V.; Dingler, T.; Abbott, G.; Scuffham, P.; Manski-Nankervis, J.-A.E.; Kwasnicka, D.; Kensing, F.; Islam, S.M.S.; Maeder, A.; Zhang, Y.","Background: Heart failure (HF) is a chronic, progressive condition where the heart cannot pump enough blood to meet the body’s needs. In addition to the daily challenges that HF poses, acute exacerbations can lead to costly hospitalizations and increased mortality. High health care costs and the burden of HF have led to the emerging application of new technologies to support people living with HF to stay well while living in the community. However, many digital solutions have not involved consumers and health care professionals in their design, leading to poor adoption. The SmartHeart project aimed to codevelop a smart health ecosystem to support the early detection of HF deterioration and encourage self-care, potentially preventing hospitalizations. Objective: This study aims to provide an overview of the SmartHeart project by describing our approach to designing the SmartHeart system, outlining its features, and describing the planned pilot study to determine the feasibility of the system. Methods: We used the Integrate, Design, Assess, and Share (IDEAS) framework to guide the development of the SmartHeart system, involving users (people with HF and their caregivers) and stakeholders (health care providers involved in the management of HF) in its design. SmartHeart is a complete remote heart health monitoring and automated feedback delivery system. It includes 2 user interfaces for patients: an Amazon Alexa conversational agent and a smartphone app. The system collects physiological, symptom, and behavioral data through wireless sensors and self-reports from users. These data are processed and analyzed to provide personalized health insights, self-care support, and alerts in case of health deterioration. The system also includes a web-based user interface for health care professionals, allowing them to access data, send messages to users, and receive notifications about potential health deterioration. A single-arm, multicenter pilot trial (N=20) is planned to determine the feasibility and acceptability of SmartHeart before evaluation through a randomized controlled trial. The primary outcome will be a description of the study's feasibility (recruitment, attrition, engagement, and changes in self-care). Results: The SmartHeart study started in January 2021 on procurement of funding. Recruitment for the pilot trial started in August 2024 and will be completed by March 2025. We have currently enrolled 12 participants. Follow-up of all participants will be completed by the end of May 2025. Conclusions: We have co-designed and developed a complete remote heart health monitoring and automated feedback delivery system for the early detection of HF deterioration and prevention of HF-related hospitalizations. The next step is a pilot study, which will provide valuable information on feasibility and preliminary effects to inform a larger evaluation trial. SmartHeart has the potential to augment existing health services and help people with HF stay well while living in the community. ©Ralph Maddison, Rebecca Nourse, Reza Daryabeygikhotbehsara, Teketo Kassaw Tegegne, Paul Jansons, Jonathan Charles Rawstorn, John Atherton, Andrea Driscoll, Brian Oldenburg, Rajesh Vasa, Vassilis Kostakos, Tilman Dingler, Gavin Abbott, Paul Scuffham, Jo-Anne Elizabeth Manski-Nankervis, Dominika Kwasnicka, Finn Kensing, Sheikh Mohammed Shariful Islam, Anthony Maeder, Yuxin Zhang.",2025,,JMIR Research Protocols,14,,,,10.2196/62964,,#4724,Maddison 2025,,,1
PIRsuader: A Persuasive Chatbot for Mitigating Psychological Insulin Resistance in Type-2 Diabetic Patients,"Gollapalli, S.D.; Ng, S.-K.","Psychological Insulin Resistance (PIR) is described as the reluctance towards initiation and adherence of insulin-based treatments due to psychological barriers in diabetic patients. Though studies have shown that timely initiation with lifestyle changes is crucial in sugar control and prevention of chronic conditions in Type 2 Diabetes (T2D) patients, many patients often have deep-rooted fears and misgivings related to insulin which hinder them from adapting to an insulin-based treatment regimen when recommended by healthcare specialists. Therefore, it is vitally important to address and allay these fallacious beliefs in T2D patients and persuade them to consider insulin as a treatment option. In this paper, we describe the design of PIRsuader, a persuasive chatbot for mitigating PIR in T2D patients. In PIRsuader, we effectively harness the conversation generation capabilities of state-of-the-art Large Language Models via a context-specific persuasive dialog act schema. We design reward functions that capture dialog act preferences for persuading reluctant patients and apply reinforcement learning to learn a dialog act prediction model. Our experiments using a collection of real doctor-diabetic patient conversations indicate that PIRsuader is able to improve the willingness in patients to try insulin as well as address specific concerns they have in an empathetic manner. © 2025 Association for Computational Linguistics.",2025,,"Proceedings - International Conference on Computational Linguistics, COLING",Part F206484-1,,5997-6013,,,,#4729,Gollapalli 2025,,,1
Debate-to-Write: A Persona-Driven Multi-Agent Framework for Diverse Argument Generation,"Hu, Z.; Chan, H.P.; Li, J.; Yin, Y.","Writing arguments is a challenging task for both humans and machines. It entails incorporating high-level beliefs from various perspectives on the topic, along with deliberate reasoning and planning to construct a coherent narrative. Current language models often generate outputs autoregressively, lacking explicit integration of these underlying controls, resulting in limited output diversity and coherence. In this work, we propose a persona-based multi-agent framework for argument writing. Inspired by the human debate, we first assign each agent a persona representing its high-level beliefs from a unique perspective, and then design an agent interaction process so that the agents can collaboratively debate and discuss the idea to form an overall plan for argument writing. Such debate process enables fluid and nonlinear development of ideas. We evaluate our framework on argumentative essay writing. The results show that our framework generates more diverse and persuasive arguments by both automatic and human evaluations. © 2025 Association for Computational Linguistics.",2025,,"Proceedings - International Conference on Computational Linguistics, COLING",Part F206484-1,,4689-4703,,,,#4733,Hu 2025,,,1
Reviewer Experience Detecting and Judging Human Versus Artificial Intelligence Content: The Stroke Journal Essay Contest,"Silva, G.S.; Khera, R.; Schwamm, L.H.; Acampa, M.; Adelman, E.E.; Boltze, J.; Broderick, J.P.; Brodtmann, A.; Christensen, H.; Dalli, L.; Duncan, K.R.; Elgendy, I.Y.; Ergul, A.; Goldstein, L.B.; Hinkle, J.L.; Johansen, M.C.; Jood, K.; Kasner, S.E.; Levine, S.R.; Li, Z.; Lip, G.; Marsh, E.B.; Muir, K.W.; Ospel, J.M.; Pera, J.; Quinn, T.J.; Räty, S.; Ranta, A.; Richards, L.G.; Romero, J.R.; Willey, J.Z.; Hillis, A.E.; Veerbeek, J.M.","Artificial intelligence (AI) large language models (LLMs) now produce human-like general text and images. LLMs' ability to generate persuasive scientific essays that undergo evaluation under traditional peer review has not been systematically studied. To measure perceptions of quality and the nature of authorship, we conducted a competitive essay contest in 2024 with both human and AI participants. Human authors and 4 distinct LLMs generated essays on controversial topics in stroke care and outcomes research. A panel of Stroke Editorial Board members (mostly vascular neurologists), blinded to author identity and with varying levels of AI expertise, rated the essays for quality, persuasiveness, best in topic, and author type. Among 34 submissions (22 human and 12 LLM) scored by 38 reviewers, human and AI essays received mostly similar ratings, though AI essays were rated higher for composition quality. Author type was accurately identified only 50% of the time, with prior LLM experience associated with improved accuracy. In multivariable analyses adjusted for author attributes and essay quality, only persuasiveness was independently associated with odds of a reviewer assigning AI as author type (adjusted odds ratio, 1.53 [95% CI, 1.09-2.16]; P=0.01). In conclusion, a group of experienced editorial board members struggled to distinguish human versus AI authorship, with a bias against best in topic for essays judged to be AI generated. Scientific journals may benefit from educating reviewers on the types and uses of AI in scientific writing and developing thoughtful policies on the appropriate use of AI in authoring manuscripts.  © 2024 American Heart Association, Inc.",2024,,Stroke,55,10,2573-2578,,10.1161/STROKEAHA.124.045012,,#4745,Silva 2024,,,1
Technological Influence on Digital Banking Adoption: A Framework and Empirical Study of the Influence of Social Robots and IVAs in a Small Island Context,"Gittens, C.L.; Gittens, M.; Jiang, Y.; Hung, P.C.K.; Wood, T.; Zhong, H.-X.","Digital banking adoption in Small Island Developing States (SIDS) faces unique cultural and technological challenges, such as limited digital literacy, infrastructure deficits, and socioeconomic disparities. This paper introduces the Technological Influence on Digital Banking Adoption (TIDiBanD) Framework, a novel approach to contextualizing how backend technologies, data assurance practices, and frontend innovations interact with cultural barriers in digital banking adoption. The framework provides researchers with a structured method to examine the influence of technologies such as social robots, intelligent virtual assistants (IVAs), and generative AI tools in fostering digital banking acceptance. To demonstrate the utility of TIDiBanD, we conducted an empirical study evaluating the persuasiveness of social robots and IVAs in promoting digital banking services in a SIDS population. Results indicate that while IVAs were perceived as more competent and easier to interact with than social robots, social robots evoked stronger positive emotions. The study also finds that consumers in that state may prefer familiar technologies over novel ones for financial interactions, highlighting the need for culturally aligned digital banking strategies. These findings provide actionable insights for banks and policymakers aiming to enhance digital financial inclusion through persuasive technologies. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Lecture Notes in Computer Science,15805 LNCS,,32-51,,10.1007/978-3-031-92826-0_3,,#4757,Gittens 2025,,,1
A holistic lifestyle mobile health intervention for the prevention of type 2 diabetes and common mental disorders in Asian women with a history of gestational diabetes: a randomised control trial with 3-year follow-up protocol,"Salamanca-Sanabria, A.; Liew, S.J.; Mair, J.; De Iorio, M.; Ling, Y.D.Y.; Tint, M.T.; Wei, Y.T.; Lim, K.; Ong, D.; Chooi, Y.C.; Tay, V.; Eriksson, J.G.","Background: Women with a history of gestational diabetes mellitus (GDM) are 12-fold more likely to develop type 2 diabetes (T2D) 4–6 years after delivery than women without GDM. Similarly, GDM is associated with the development of common mental disorders (CMDs) (e.g. anxiety and depression). Evidence shows that holistic lifestyle interventions focusing on physical activity (PA), dietary intake, sleep, and mental well-being strategies can prevent T2D and CMDs. This study aims to assess the effectiveness of a holistic lifestyle mobile health intervention (mHealth) with post-GDM women in preventing T2D and CMDs in a community setting in Singapore. Methods: The study consists of a 1-year randomised controlled trial (RCT) with a 3-year follow-up period. Post-GDM women with no current diabetes diagnosis and not planning to become pregnant will be eligible for the study. In addition, participants will complete mental well-being questionnaires (e.g. depression, anxiety, sleep) and their child’s socio-emotional and cognitive development. The participants will be randomised to either Group 1 (Intervention) or Group 2 (comparison). The intervention group will receive the “LVL UP App”, a smartphone-based, conversational agent-delivered holistic lifestyle intervention focused on three pillars: Move More (PA), Eat Well (Diet), and Stress Less (mental wellbeing). The intervention consists of health literacy and psychoeducational coaching sessions, daily “Life Hacks” (healthy activity suggestions), slow-paced breathing exercises, a step tracker (including brisk steps), a low-burden food diary, and a journaling tool. Women from both groups will be provided with an Oura ring for tracking physical activity, sleep, and heart rate variability (a proxy for stress), and the “HAPPY App”, a mHealth app which provides health promotion information about PA, diet, sleep, and mental wellbeing, as well as display body mass index, blood pressure, and results from the oral glucose tolerance tests. Short-term aggregate effects will be assessed at 26/27 weeks (midpoint) and a 1-year visit, followed by a 2, 3, and 4-year follow-up period. Discussion: High rates of progression of T2D and CMDs in women with post-GDM suggest an urgent need to promote a healthy lifestyle, including diet, PA, sleep, and mental well-being. Preventive interventions through a holistic, healthy lifestyle may be the solution, considering the inextricable relationship between physical and psychological health. We expect that holistic lifestyle mHealth may effectively support behavioural changes among women with a history of GDM to prevent T2D and CMDs. Trial status: The protocol study was approved by the National Healthcare Group in Singapore, Domain Specific Review Board (DSRB) [2023/00178]; June 2023. Recruitment began on October 18, 2023. Trial registration: ClinicalTrials.gov NCT05949957. The first submission date is June 08, 2023. © The Author(s) 2024.",2024,,Trials,25,1,,,10.1186/s13063-024-08247-x,,#4762,Salamanca-Sanabria 2024,,,1
Leveraging human-centered design and causal pathway diagramming toward enhanced specification and development of innovative implementation strategies: a case example of an outreach tool to address racial inequities in breast cancer screening,"Marcotte, L.M.; Langevin, R.; Hempstead, B.R.; Ganguly, A.; Lyon, A.R.; Weiner, B.J.; Akinsoto, N.; Houston, P.L.; Fang, V.; Hsieh, G.","Background: Implementation strategies are strategies to improve uptake of evidence-based practices or interventions and are essential to implementation science. Developing or tailoring implementation strategies may benefit from integrating approaches from other disciplines; yet current guidance on how to effectively incorporate methods from other disciplines to develop and refine innovative implementation strategies is limited. We describe an approach that combines community-engaged methods, human-centered design (HCD) methods, and causal pathway diagramming (CPD)—an implementation science tool to map an implementation strategy as it is intended to work—to develop innovative implementation strategies. Methods: We use a case example of developing a conversational agent or chatbot to address racial inequities in breast cancer screening via mammography. With an interdisciplinary team including community members and operational leaders, we conducted a rapid evidence review and elicited qualitative data through interviews and focus groups using HCD methods to identify and prioritize key determinants (facilitators and barriers) of the evidence-based intervention (breast cancer screening) and the implementation strategy (chatbot). We developed a CPD using key determinants and proposed strategy mechanisms and proximal outcomes based in conceptual frameworks. Results: We identified key determinants for breast cancer screening and for the chatbot implementation strategy. Mistrust was a key barrier to both completing breast cancer screening and using the chatbot. We focused design for the initial chatbot interaction to engender trust and developed a CPD to guide chatbot development. We used the persuasive health message framework and conceptual frameworks about trust from marketing and artificial intelligence disciplines. We developed a CPD for the initial interaction with the chatbot with engagement as a mechanism to use and trust as a proximal outcome leading to further engagement with the chatbot. Conclusions: The use of interdisciplinary methods is core to implementation science. HCD is a particularly synergistic discipline with multiple existing applications of HCD to implementation research. We present an extension of this work and an example of the potential value in an integrated community-engaged approach of HCD and implementation science researchers and methods to combine strengths of both disciplines and develop human-centered implementation strategies rooted in causal perspective and healthcare equity. © The Author(s) 2024.",2024,,Implementation Science Communications,5,1,,,10.1186/s43058-024-00569-w,,#4764,Marcotte 2024,,,1
Evaluating AI-Personalized Learning Interventions in Distance Education,"Panwale, S.B.; Vijayakumar, S.","This study aimed to evaluate the utility of artificial intelligence (AI) in improving the persuasive communication skills of online Master of Business Administration (MBA) students. In particular, this study investigated the influence of personalization through AI using the Google Gemini platform on conventional and online instructional approaches. This quasi-experimental study used a pretest and posttest design to compare two groups of MBA students pursuing persuasive online communication. The experimental group (n = 32) interacted with the AI-based personalized learning materials, whereas the control group (n = 32) used standard instructor-designed online modules. During the 12-week intervention period, the experimental group was provided with customized practice activities. Conversely, the control group was offered conventional online learning material. The effectiveness of both approaches was evaluated using pretests and posttests. The results of Tukey’s Honestly Significant Difference (HSD) test provided insight into the areas where AI-based personalized learning had a statistically significant impact. These results support the conclusions derived from an analysis of variance and further validate the study’s research hypotheses. This study demonstrates the advantages of incorporating AI into language development for remote learners and offers valuable insights for integrating AI-driven technologies into distance education. © 2025, Athabasca University. All rights reserved.",2025,,International Review of Research in Open and Distributed Learning,26,1,157-174,,10.19173/irrodl.v26i1.7813,,#4765,Panwale 2025,,,1
Facing LLMs: Robot Communication Styles in Mediating Health Information between Parents and Young Adults,"Wester, J.; Moghe, B.; Winkle, K.; Van Berkel, N.","Young adults may feel embarrassed when disclosing sensitive information to their parents, while parents might similarly avoid sharing sensitive aspects of their lives with their children. How to design interactive interventions that are sensitive to the needs of both younger and older family members in mediating sensitive information remains an open question. In this paper, we explore the integration of large language models (LLMs) with social robots. Specifically, we use GPT-4 to adapt different Robot Communication Styles (RCS) for a social robot mediator designed to elicit self-disclosure and mediate health information between parents and young adults living apart. We design and compare four literature-informed RCS: three LLM-adapted (Humorous, Self-deprecating, and Persuasive) and one manually created (Human-scripted), and assess participant perceptions of Likeability, Usefulness, Helpfulness, Relatedness, and Interpersonal Closeness. Through an online experiment with 183 participants, we assess the RCS across two groups: adults with children (Parents) and young adults without children (Young Adults). Our results indicate that both Parents and Young Adults favoured the Human-scripted and Self-deprecating RCS as compared to the other two RCS. The Self-deprecating RCS furthermore led to increased relatedness as compared to the Humorous RCS. Our qualitative findings reveal challenges people have in disclosing health information to family members, and who normally assumes the role of family facilitator-two areas in which social robots can play a key role. The findings offer insights for integrating LLMs with social robots in health-mediation and other contexts involving the sharing of sensitive information.  © 2024 Owner/Author.",2024,,Proceedings of the ACM on Human-Computer Interaction,8,CSCW2,,,10.1145/3687036,,#4767,Wester 2024,,,1
The critical need for expert oversight of ChatGPT: Prompt engineering for safeguarding child healthcare information,"Leslie-Miller, C.J.; Simon, S.L.; Dean, K.; Mokhallati, N.; Cushing, C.C.","Objective: ChatGPT and other large language models have the potential to transform the health information landscape online. However, lack of domain-specific expertise and known errors in large language models raise concerns about the widespread adoption of content generated by these tools for parents making healthcare decisions for their children. The aim of this study is to determine if health-related text generated by ChatGPT under the supervision of an expert is comparable to that generated by an expert regarding persuasiveness and credibility from the perspective of a parent. Methods: In a cross-sectional study 116 parents aged 18-65 years (M = 45.02, SD = 10.92) were asked to complete a baseline assessment of their behavioral intentions regarding pediatric healthcare topics. Subsequently, participants were asked to rate text generated by either an expert or by ChatGPT under supervision of an expert. Results: Results indicate that prompt engineered ChatGPT is capable of impacting behavioral intentions for medication, sleep, and diet decision-making. Additionally, there was little distinction between prompt engineered ChatGPT and content experts on perceived morality, trustworthiness, expertise, accuracy, and reliance. Notably, when differences were present, prompt engineered ChatGPT was rated as higher in trustworthiness and accuracy, and participants indicated they would be more likely to rely on the information presented by prompt engineered ChatGPT compared to the expert. Discussion: Given that parents will trust and rely on information generated by ChatGPT, it is critically important that human domain-specific expertise be applied to healthcare information that will ultimately be presented to consumers (e.g., parents). © 2024 The Author(s).",2024,,Journal of Pediatric Psychology,49,11,812-817,,10.1093/jpepsy/jsae075,,#4768,Leslie-Miller 2024,,,1
Effectiveness of artificial intelligence vs. human coaching in diabetes prevention: a study protocol for a randomized controlled trial,"Abusamaan, M.S.; Ballreich, J.; Dobs, A.; Kane, B.; Maruthur, N.; McGready, J.; Riekert, K.; Wanigatunga, A.A.; Alderfer, M.; Alver, D.; Lalani, B.; Ringham, B.; Vandi, F.; Zade, D.; Mathioudakis, N.N.","Background: Prediabetes is a highly prevalent condition that heralds an increased risk of progression to type 2 diabetes, along with associated microvascular and macrovascular complications. The Diabetes Prevention Program (DPP) is an established effective intervention for diabetes prevention. However, participation in this 12-month lifestyle change program has historically been low. Digital DPPs have emerged as a scalable alternative, accessible asynchronously and recognized by the Centers for Disease Control and Prevention (CDC). Yet, most digital programs still incorporate human coaching, potentially limiting scalability. Furthermore, existing effectiveness results of digital DPPs are primarily derived from per protocol, longitudinal non-randomized studies, or comparisons to control groups that do not represent the standard of care DPP. The potential of an AI-powered DPP as an alternative to the DPP is yet to be investigated. We propose a randomized controlled trial (RCT) to directly compare these two approaches. Methods: This open-label, multicenter, non-inferiority RCT will compare the effectiveness of a fully automated AI-powered digital DPP (ai-DPP) with a standard of care human coach-based DPP (h-DPP). A total of 368 participants with elevated body mass index (BMI) and prediabetes will be randomized equally to the ai-DPP (smartphone app and Bluetooth-enabled body weight scale) or h-DPP (referral to a CDC recognized DPP). The primary endpoint, assessed at 12 months, is the achievement of the CDC’s benchmark for type 2 diabetes risk reduction, defined as any of the following: at least 5% weight loss, at least 4% weight loss and at least 150 min per week on average of physical activity, or at least a 0.2-point reduction in hemoglobin A1C. Physical activity will be objectively measured using serial actigraphy at baseline and at 1-month intervals throughout the trial. Secondary endpoints, evaluated at 6 and 12 months, will include changes in A1C, weight, physical activity measures, program engagement, and cost-effectiveness. Participants include adults aged 18–75 years with laboratory confirmed prediabetes, a BMI of ≥ 25 kg/m2 (≥ 23 kg/m2 for Asians), English proficiency, and smartphone users. This U.S. study is conducted at Johns Hopkins Medicine in Baltimore, MD, and Reading Hospital (Tower Health) in Reading, PA. Discussion: Prediabetes is a significant public health issue, necessitating scalable interventions for the millions affected. Our pragmatic clinical trial is unique in directly comparing a fully automated AI-powered approach without direct human coach interaction. If proven effective, it could be a scalable, cost-effective strategy. This trial will offer vital insights into both AI and human coach-based behavioral change strategies in real-world clinical settings. Trial registration: ClinicalTrials.gov NCT05056376. Registered on September 24, 2021, https://clinicaltrials.gov/study/NCT05056376 © The Author(s) 2024.",2024,,Trials,25,1,,,10.1186/s13063-024-08177-8,,#4776,Abusamaan 2024,,,1
Pharmacogenomic augmented machine learning in electronic health record alerts: A health system-wide usability survey of clinicians,"Grant, C.W.; Marrero-Polanco, J.; Joyce, J.B.; Barry, B.; Stillwell, A.; Kruger, K.; Anderson, T.; Talley, H.; Hedges, M.; Valery, J.; White, R.; Sharp, R.R.; Croarkin, P.E.; Dyrbye, L.N.; Bobo, W.V.; Athreya, A.P.","Pharmacogenomic (PGx) biomarkers integrated using machine learning can be embedded within the electronic health record (EHR) to provide clinicians with individualized predictions of drug treatment outcomes. Currently, however, drug alerts in the EHR are largely generic (not patient-specific) and contribute to increased clinician stress and burnout. Improving the usability of PGx alerts is an urgent need. Therefore, this work aimed to identify principles for optimal PGx alert design through a health-system-wide, mixed-methods study. Clinicians representing multiple practices and care settings (N = 1062) in urban, rural, and underserved regions were invited to complete an electronic survey comparing the usability of three drug alerts for citalopram, as a case study. Alert 1 contained a generic warning of pharmacogenomic effects on citalopram metabolism. Alerts 2 and 3 provided patient-specific predictions of citalopram efficacy with varying depth of information. Primary outcomes included the System's Usability Scale score (0–100 points) of each alert, the perceived impact of each alert on stress and decision-making, and clinicians' suggestions for alert improvement. Secondary outcomes included the assessment of alert preference by clinician age, practice type, and geographic setting. Qualitative information was captured to provide context to quantitative information. The final cohort comprised 305 geographically and clinically diverse clinicians. A simplified, individualized alert (Alert 2) was perceived as beneficial for decision-making and stress compared with a more detailed version (Alert 3) and the generic alert (Alert 1) regardless of age, practice type, or geographic setting. Findings emphasize the need for clinician-guided design of PGx alerts in the era of digital medicine. © 2024 The Author(s). Clinical and Translational Science published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics.",2024,,Clinical and Translational Science,17,10,,,10.1111/cts.70044,,#4778,Grant 2024,,,1
"""Hey Genie, You Got Me Thinking about My Menu Choices!"" Impact of Proactive Feedback on User Perception and Reflection in Decision-making Tasks","Dubiel, M.; Leiva, L.A.; Bongard-Blanchy, K.; Sergeeva, A.","Conversational agents (CAs) that deliver proactive interventions can benefit users by reducing their cognitive workload and improving performance. However, little is known regarding how such interventions would impact users' reflection on choices in voice-only decision-making tasks. We conducted a within-subjects experiment to evaluate the effect of CA's feedback delivery strategy at three levels (no feedback, unsolicited and solicited feedback) and the impact on users' likelihood of changing their choices in an interactive food ordering scenario. We discovered that in both feedback conditions the CA was perceived to be significantly more persuasive than in the baseline condition, while being perceived as significantly less confident. Interestingly, while unsolicited feedback was perceived as less appropriate than the baseline, both types of proactive feedback led participants to relisten and reconsider menu options significantly more often. Our results provide insights regarding the impact of proactive feedback on CA perception and user's reflection in decision-making tasks, thereby paving a new way for designing proactive CAs.  © 2024 Copyright held by the owner/author(s).",2024,,ACM Transactions on Computer-Human Interaction,31,5,,,10.1145/3685274,,#4786,Dubiel 2024,,,1
Digital nudge persuasiveness of avatars in restaurants toward healthy choices and happy diners,"Aman, A.M.; Ng, W.; Hao, F.; Zhang, C.; Chon, K.K.S.","Purpose: Amid rising concerns over unhealthy dietary habits and their impact on public health, this study aims to explore the role of avatars in promoting healthier eating and enhancing customer satisfaction in restaurants. By leveraging the theory of planned behavior (TPB) and nudge theory, this research sought to gain insights into consumer behavior and assess how digital innovations can encourage healthier food choices. Design/methodology/approach: This study recruited 672 participants in the USA. Participants viewed a video featuring an avatar that informed them about available healthy food options, simulating a restaurant ordering scenario. Following the video, participants completed comprehensive online surveys. The collected data was analyzed using partial least squares structural equation modeling to assess the effectiveness and implications of the intervention. Findings: The findings revealed that health consciousness, environmental awareness, social norms and perceived behavioral control significantly influence dietary habits. Restaurant health initiatives and avatar persuasiveness were found to encourage healthier food choices, improve customer satisfaction and loyalty and enhance electronic word-of-mouth. The study confirmed that avatars equipped with attributes such as competence, warmth, trustworthiness and credibility can be an effective digital nudge for consumers toward healthier dietary decisions. Originality/value: This study’s originality lies in its integration of TPB and nudge theory, bridging the gap between individual psychological factors and external cues. This comprehensive framework provides valuable insights for restaurant managers, artificial intelligence developers and policymakers, offering practical strategies to promote healthier eating and enhance customer experiences through digital innovation. © 2025, Emerald Publishing Limited.",2025,,Tourism Review,,,,,10.1108/TR-07-2024-0567,,#4791,Aman 2025,,,1
Cardiac Self-Efficacy Improvement in a Digital Heart Health Program: Secondary Analysis From a Feasibility and Acceptability Pilot Study,"Lockwood, K.G.; Kulkarni, P.R.; Branch, O.H.; Graham, S.A.","Background: Lifestyle modification programs play a critical role in preventing and managing cardiovascular disease (CVD). A key aim of many programs is improving patients’ self-efficacy. In-person lifestyle modification programs can enhance self-efficacy in managing CVD risk, also known as cardiac self-efficacy (CSE). However, such programs are typically staffing and resource intensive. Digital lifestyle modification programs may offer a scalable and accessible way to improve CSE, but this has not been shown in prior research. Objective: This study examined changes in CSE among individuals using a digital lifestyle modification program for cardiovascular health. Evaluation of improvement in CSE was a secondary goal of a feasibility and acceptability pilot study of a digital program for Heart Health. Methods: Participants were individuals with elevated risk for CVD who enrolled in a 90-day pilot study that involved mobile app–based, artificial intelligence–powered health coaching and educational lessons focused on behaviors that promote cardiovascular health. Participants completed the 9-item CSE Scale at baseline and in month 2. Changes in confidence in participants’ ability to manage their cardiovascular health were assessed. Results: The sample included 273 (n=207, 61.2% female; mean age 59.3, SD 10.1 years) participants who submitted a complete CSE Scale at baseline and in month 2. The total CSE Scale score increased by 12.9% (P<.001) from baseline to month 2. Additionally, there were significant increases in mean score on each of the 9 individual CSE Scale items (all P<.001), with the largest increases in confidence “in knowing when to call or visit the doctor for your heart disease” (17% increase; P<.001), “in knowing how much physical activity is good for you” (16.3% increase; P<.001), and “that you can get regular aerobic exercise” (19% increase; P<.001). Conclusions: The present analyses indicate that participants in a digital lifestyle modification program for cardiovascular health showed significant improvements in CSE within 2 months. This work adds to the growing literature examining ways to improve health-related self-efficacy and scalable access to programs for prevention and management of CVD. © Kimberly G Lockwood, Priya R Kulkarni, OraLee H Branch, Sarah A Graham.",2025,,JMIR Formative Research,9,,,,10.2196/60676,,#4793,Lockwood 2025,,,1
Feasibility and Preliminary Behavioral and Clinical Efficacy of a Diabetes Education Chatbot Pilot Among Adults With Type 2 Diabetes,"Nassar, C.M.; Dunlea, R.; Montero, A.; Tweedt, A.; Magee, M.F.","Background: Diabetes self-management education and support (DSMES) improves diabetes outcomes yet remains consistently underutilized. Chatbot technology offers the potential to increase access to and engagement in DSMES. Evidence supporting the case for chatbot uptake and efficacy in people with diabetes (PWD) is needed. Method: A diabetes education and support chatbot was deployed in a regional health care system. Adults with type 2 diabetes with an A1C of 8.0% to 8.9% and/or having recently completed a 12-week diabetes care management program were enrolled in a pilot program. Weekly chats included three elements: knowledge assessment, limited self-reporting of blood glucose data and medication taking behaviors, and education content (short videos and printable materials). A clinician facing dashboard identified need for escalation via flags based on participant responses. Data were collected to assess satisfaction, engagement, and preliminary glycemic outcomes. Results: Over 16 months, 150 PWD (majority above 50 years of age, female, and African American) were enrolled. The unenrollment rate was 5%. Most escalation flags (N = 128) were for hypoglycemia (41%), hyperglycemia (32%), and medication issues (11%). Overall satisfaction was high for chat content, length, and frequency, and 87% reported increased self-care confidence. Enrollees completing more than one chat had a mean drop in A1C of −1.04%, whereas those completing one chat or less had a mean increase in A1C of +0.09% (P =.008). Conclusion: This diabetes education chatbot pilot demonstrated PWD acceptability, satisfaction, and engagement plus preliminary evidence of self-care confidence and A1C improvement. Further efforts are needed to validate these promising early findings. © 2023 Diabetes Technology Society.",2025,,Journal of Diabetes Science and Technology,19,1,54-62,,10.1177/19322968231178020,,#4795,Nassar 2025,,,1
Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered Motivational Interviewing,"Steenstra, I.; Nouraei, F.; Arjmand, M.; Bickmore, T.W.","We introduce a novel application of large language models (LLMs) in developing a virtual counselor capable of conducting motivational interviewing (MI) for alcohol use counseling. Access to effective counseling remains limited, particularly for substance abuse, and virtual agents offer a promising solution by leveraging LLM capabilities to simulate nuanced communication techniques inherent in MI. Our approach combines prompt engineering and integration into a user-friendly virtual platform to facilitate realistic, empathetic interactions. We evaluate the effectiveness of our virtual agent through a series of studies focusing on replicating MI techniques and human counselor dialog. Initial findings suggest that our LLM-powered virtual agent matches human counselors’ empathetic and adaptive conversational skills, presenting a significant step forward in virtual health counseling and providing insights into the design and implementation of LLM-based therapeutic interactions. © 2024 Copyright held by the owner/author(s).",2024,,"Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents, IVA 2024",,,,,10.1145/3652988.3673932,,#4799,Steenstra 2024,,,1
A mobile healthy lifestyle intervention to promote mental health in adolescence: a mixed-methods evaluation,"Peuters, C.; Maenhout, L.; Cardon, G.; De Paepe, A.; DeSmet, A.; Lauwerier, E.; Leta, K.; Crombez, G.","Background: A healthy lifestyle may improve mental health. It is yet not known whether and how a mobile intervention can be of help in achieving this in adolescents. This study investigated the effectiveness and perceived underlying mechanisms of the mobile health (mHealth) intervention #LIFEGOALS to promote healthy lifestyles and mental health. #LIFEGOALS is an evidence-based app with activity tracker, including self-regulation techniques, gamification elements, a support chatbot, and health narrative videos. Methods: A quasi-randomized controlled trial (N = 279) with 12-week intervention period and process evaluation interviews (n = 13) took place during the COVID-19 pandemic. Adolescents (12-15y) from the general population were allocated at school-level to the intervention (n = 184) or to a no-intervention group (n = 95). Health-related quality of life (HRQoL), psychological well-being, mood, self-perception, peer support, resilience, depressed feelings, sleep quality and breakfast frequency were assessed via a web-based survey; physical activity, sedentary time, and sleep routine via Axivity accelerometers. Multilevel generalized linear models were fitted to investigate intervention effects and moderation by pandemic-related measures. Interviews were coded using thematic analysis. Results: Non-usage attrition was high: 18% of the participants in the intervention group never used the app. An additional 30% stopped usage by the second week. Beneficial intervention effects were found for physical activity (χ 21 = 4.36, P =.04), sedentary behavior (χ 21 = 6.44, P =.01), sleep quality (χ 21 = 6.11, P =.01), and mood (χ 21 = 2.30, P =.02). However, effects on activity-related behavior were only present for adolescents having normal sports access, and effects on mood only for adolescents with full in-school education. HRQoL (χ 22 = 14.72, P <.001), mood (χ 21 = 6.03, P =.01), and peer support (χ 21 = 13.69, P <.001) worsened in adolescents with pandemic-induced remote-education. Interviewees reported that the reward system, self-regulation guidance, and increased health awareness had contributed to their behavior change. They also pointed to the importance of social factors, quality of technology and autonomy for mHealth effectiveness. Conclusions: #LIFEGOALS showed mixed results on health behaviors and mental health. The findings highlight the role of contextual factors for mHealth promotion in adolescence, and provide suggestions to optimize support by a chatbot and narrative episodes. Trial registration: ClinicalTrials.gov [NCT04719858], registered on 22/01/2021. © 2023, The Author(s).",2024,,BMC Public Health,24,1,,,10.1186/s12889-023-17260-9,,#4801,Peuters 2024,,,1
Advancing health coaching: A comparative study of large language model and health coaches,"Ong, Q.C.; Ang, C.-S.; Chee, D.Z.Y.; Lawate, A.; Sundram, F.; Dalakoti, M.; Pasalic, L.; To, D.; Erlikh Fox, T.; Bojic, I.; Car, J.","Objective: Recent advances in large language models (LLM) offer opportunities to automate health coaching. With zero-shot learning ability, LLMs could revolutionize health coaching by providing better accessibility, scalability, and customization. The aim of this study is to compare the quality of responses to clients' sleep-related questions provided by health coaches and an LLM. Design, setting, and participants: From a de-identified dataset of coaching conversations from a pilot randomized controlled trial, we extracted 100 question-answer pairs comprising client questions and corresponding health coach responses. These questions were entered into a retrieval-augmented generation (RAG)-enabled open-source LLM (LLaMa-2-7b-chat) to generate LLM responses. Out of 100 question-answer pairs, 90 were taken out and assigned to three groups of evaluators: experts, lay-users, and GPT-4. Each group conducted two evaluation tasks: (Task 1) a single-response quality assessment spanning five criteria—accuracy, readability, helpfulness, empathy, and likelihood of harm—rated on a five-point Likert scale, and (Task 2) a pairwise comparison to choose the superior response between pairs. A suite of inferential statistical methods, including the paired and independent sample t-tests, Pearson correlation, and chi-square tests, were utilized to answer the study objective. Recognizing potential biases in human judgment, the remaining 10 question-answer pairs were used to assess inter-evaluator reliability among the human evaluators, quantified using the interclass correlation coefficient and percentage agreement metrics. Results: Upon exclusion of incomplete data, the analysis included 178 single-response evaluations (Task 1) and 83 pairwise comparisons (Task 2). Expert and GPT-4 assessments revealed no discernible disparities in health coach and LLM responses across the five metrics. Contrarily, lay-users deemed LLM responses significantly more helpful than that of human coaches (p < 0.05). LLM responses were preferred in the majority (62.25 %, n = 155) of the aggregate 249 assessments, with all three evaluator groups favoring LLM over health coach inputs. While GPT-4 rated both health coach and LLM responses significantly higher than experts in terms of readability, helpfulness, and empathy, its ratings on accuracy and likelihood of harm aligned with those of experts. Response length positively correlated with accuracy and empathy scores, but negatively affected readability across all evaluator groups. Expert and lay-user evaluators demonstrated moderate to high inter-evaluator reliability. Conclusion: Our study showed encouraging findings by demonstrating that RAG-enabled LLM has comparable performance to health coaches in the domain tested. Serving as an initial step towards the creation of more sophisticated, adaptive, round-the-clock automated health coaching systems, our findings call for more extensive evaluation which could assist in the development of the model that could in the future lead to potential clinical implementation. © 2024 Elsevier B.V.",2024,,Artificial Intelligence in Medicine,157,,,,10.1016/j.artmed.2024.103004,,#4804,Ong 2024,,,1
A Gaze into Argumentative Chatbots: Exploring the Influence of Challenger Arguments on Reflection and Attention,"Weber, K.; Hogh, N.; Conati, C.; André, E.","A natural way to resolve different points of view and form opinions is through exchanging arguments and knowledge. Facing the vast amount of available information on the internet, people tend to focus on information consistent with their beliefs. To support a fair and unbiased opinion-building process, we propose an intelligent agent in the form of a chatbot that engages in a deliberative dialogue with a human. In contrast to persuasive systems, the chatbot aims to provide a diverse and representative overview - embedded in a conversation with the user. To account for a reflective and unbiased exploration of the topic, we enable the system to intervene if the user is too focused on their pre-existing opinion. To achieve that, the agent employs a metric to assess the user’s focus on challenger arguments. We report on a user study with 45 participants showing that the intervention significantly affects the user’s focus on challenger arguments. We further report significant main effects of intervention on the user’s attention to crucial areas of the system, and report on multiple interaction effects of user characteristics. © 2024 Copyright held by the owner/author(s).",2024,,"Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents, IVA 2024",,,,,10.1145/3652988.3673935,,#4806,Weber 2024,,,1
The potential of generative AI for personalized persuasion at scale,"Matz, S.C.; Teeny, J.D.; Vaid, S.S.; Peters, H.; Harari, G.M.; Cerf, M.","Matching the language or content of a message to the psychological profile of its recipient (known as “personalized persuasion”) is widely considered to be one of the most effective messaging strategies. We demonstrate that the rapid advances in large language models (LLMs), like ChatGPT, could accelerate this influence by making personalized persuasion scalable. Across four studies (consisting of seven sub-studies; total N = 1788), we show that personalized messages crafted by ChatGPT exhibit significantly more influence than non-personalized messages. This was true across different domains of persuasion (e.g., marketing of consumer products, political appeals for climate action), psychological profiles (e.g., personality traits, political ideology, moral foundations), and when only providing the LLM with a single, short prompt naming or describing the targeted psychological dimension. Thus, our findings are among the first to demonstrate the potential for LLMs to automate, and thereby scale, the use of personalized persuasion in ways that enhance its effectiveness and efficiency. We discuss the implications for researchers, practitioners, and the general public. © The Author(s) 2024.",2024,,Scientific Reports,14,1,,,10.1038/s41598-024-53755-0,,#4807,Matz 2024,,,1
Effects of Chatbot Components to Facilitate Mental Health Services Use in Individuals With Eating Disorders Following Online Screening: An Optimization Randomized Controlled Trial,"Fitzsimmons-Craft, E.E.; Rackoff, G.N.; Shah, J.; Strayhorn, J.C.; D'Adamo, L.; DePietro, B.; Howe, C.P.; Firebaugh, M.-L.; Newman, M.G.; Collins, L.M.; Taylor, C.B.; Wilfley, D.E.","Objective: Few individuals with eating disorders (EDs) receive treatment. Innovations are needed to identify individuals with EDs and address care barriers. We developed a chatbot for promoting services uptake that could be paired with online screening. However, it is not yet known which components drive effects. This study estimated individual and combined contributions of four chatbot components on mental health services use (primary), chatbot helpfulness, and attitudes toward changing eating/shape/weight concerns (“change attitudes,” with higher scores indicating greater importance/readiness). Methods: Two hundred five individuals screening with an ED but not in treatment were randomized in an optimization randomized controlled trial to receive up to four chatbot components: psychoeducation, motivational interviewing, personalized service recommendations, and repeated administration (follow-up check-ins/reminders). Assessments were at baseline and 2, 6, and 14 weeks. Results: Participants who received repeated administration were more likely to report mental health services use, with no significant effects of other components on services use. Repeated administration slowed the decline in change attitudes participants experienced over time. Participants who received motivational interviewing found the chatbot more helpful, but this component was also associated with larger declines in change attitudes. Participants who received personalized recommendations found the chatbot more helpful, and receiving this component on its own was associated with the most favorable change attitude time trend. Psychoeducation showed no effects. Discussion: Results indicated important effects of components on outcomes; findings will be used to finalize decision making about the optimized intervention package. The chatbot shows high potential for addressing the treatment gap for EDs. © 2024 Wiley Periodicals LLC.",2024,,International Journal of Eating Disorders,57,11,2204-2216,,10.1002/eat.24260,,#4813,Fitzsimmons-Craft 2024,,,1
Can AI tell good stories? Narrative transportation and persuasion with ChatGPT,"Chu, H.; Liu, S.","Storytelling is a human universal. The ubiquity of stories and the rapid development in Artificial Intelligence (AI) pose important questions: can AI like ChatGPT tell engaging and persuasive stories? If so, what makes a narrative engaging and persuasive? Three pre-registered experiments comparing human-generated narratives from existing research and the ChatGPT-generated versions using descriptions and materials from these studies show that labeling AI as a narrative source led to lower transportation, higher counterarguing, and lower story-consistent beliefs. However, AI-generated narratives led to lower (Study 1 and 3) or similar levels (Study 2) of counterarguing than the human-generated version. Readers showed lower (Study 2) or similar levels of transportation (Study 1 and 3) when reading the AI- than the human-generated stories. We suggest the AI model's linguistic competence and logical coherence contribute to its stories' verisimilitude. However, AI's lack of lived experience and creativity may limit its storytelling ability.  © 2024 The Author(s). Published by Oxford University Press on behalf of International Communication Association. All rights reserved.",2024,,Journal of Communication,74,5,347-358,,10.1093/joc/jqae029,,#4816,Chu 2024,,,1
Conversational AI and equity through assessing GPT-3’s communication with diverse social groups on contentious topics,"Chen, K.; Shao, A.; Burapacheep, J.; Li, Y.","Autoregressive language models, which use deep learning to produce human-like texts, have surged in prevalence. Despite advances in these models, concerns arise about their equity across diverse populations. While AI fairness is discussed widely, metrics to measure equity in dialogue systems are lacking. This paper presents a framework, rooted in deliberative democracy and science communication studies, to evaluate equity in human–AI communication. Using it, we conducted an algorithm auditing study to examine how GPT-3 responded to different populations who vary in sociodemographic backgrounds and viewpoints on crucial science and social issues: climate change and the Black Lives Matter (BLM) movement. We analyzed 20,000 dialogues with 3290 participants differing in gender, race, education, and opinions. We found a substantively worse user experience among the opinion minority groups (e.g., climate deniers, racists) and the education minority groups; however, these groups changed attitudes toward supporting BLM and climate change efforts much more compared to other social groups after the chat. GPT-3 used more negative expressions when responding to the education and opinion minority groups. We discuss the social-technological implications of our findings for a conversational AI system that centralizes diversity, equity, and inclusion. © 2024, The Author(s).",2024,,Scientific Reports,14,1,,,10.1038/s41598-024-51969-w,,#4819,Chen 2024,,,1
Communicating the benefits and risks of AI technology in hiring: implications for organization–public relationships,"Kim, J.K.; Xiong, Y.; Hunt, D.S.","Purpose: As artificial intelligence (AI) has become increasingly accessible, a growing number of organizations have begun to adopt AI in hiring. Despite the increasing use of AI in hiring, little is known about how organizations can effectively communicate with their stakeholders about their AI use. Using gain and loss message frames and organization–public relationships (OPRs) as theoretical frameworks, this study tests the impact of exposure to job advertisements that address AI use on individuals’ attitudes and behaviors toward organizations using AI during hiring. Design/methodology/approach: We conducted a 2 (message frames: gain vs loss) × 2 (benefits of using AI in hiring: faster hiring process vs reduced unconscious hiring bias) between-subjects experiment using a Prolific online panel (N = 224). Participants were randomly assigned to one of four conditions and viewed a fictitious company’s job advertisement that varied in message frames and benefits of using AI. After viewing the experimental stimuli, participants answered questions about OPRs, attitudes toward the company and positive word-of-mouth (WOM) intentions. Findings: Loss-framed messages appeared as more effective in increasing OPRs, favorable attitudes toward the company and positive WOM intentions. OPRs positively mediated the impact of exposure to loss-framed job advertisements on attitudes and positive WOM intentions. The benefits of using AI in hiring moderated the impact of loss-framed messages on OPRs, attitudes and positive WOM intentions. Practical implications: This study offers important practical implications for organizations that use AI technology in their hiring practices or are interested in incorporating AI into their hiring processes. The significant impact of loss-framed messages suggests that organizations should highlight the anticipated negative outcomes of not using AI rather than addressing the positive outcomes from AI use. The findings align with established negativity bias research in framing literature, demonstrating individuals’ greater susceptibility to negative information and adverse outcomes in persuasive contexts. The moderating effect of the benefits of using AI in hiring indicates that the impact of message frames on individuals’ attitudinal and behavioral responses to organizations’ AI use varies depending on organizations’ rationale for using AI. Originality/value: This study is one of the early studies to examine individuals’ attitudinal and behavioral responses to organizations’ use of AI in hiring. Prior research suggests that OPRs mediate the relationship between individuals’ exposure to organizational messages and their attitudes toward organizations. Extending this line of inquiry, the current study explores OPRs’ mediating role in the context of AI-driven hiring processes and investigates how OPRs mediate the relationships between organizations’ message framing strategies and individuals’ attitudes toward the organization as well as their WOM intentions. By examining the effectiveness of gain- and loss-framed job advertisements, this study offers important insights for organizations on how to more effectively and ethically communicate about AI use with their stakeholders. © 2025, Emerald Publishing Limited.",2025,,Corporate Communications,,,,,10.1108/CCIJ-11-2024-0204,,#4820,Kim 2025,,,1
Overview and confirmatory and exploratory factor analysis of AI literacy scale,"Koch, M.J.; Wienrich, C.; Straka, S.; Latoschik, M.E.; Carolus, A.","Comprehensive concepts of AI literacy (AIL) and valid measures are essential for research (e.g., intervention studies) and practice (e.g., personnel selection/development) alike. To date, several scales have been published, sharing standard features but differing in some aspects. We first aim to briefly overview instruments identified from unsystematic literature research in February 2023. We identified four scales and one collection of items. We describe the instruments and compare them. We identified common themes and overlaps in the instruments and developmental procedure. We also found differences regarding scale development procedures and latent dimensions. Following this literature research, we came to the conclusion that the literature on AI literacy measurement was fragmented, and little effort was undertaken to integrate different AI literacy conceptualizations. The second focus of this study is to test the factorial structures of existing AIL measurement instruments and identify latent dimensions of AIL across all instruments. We used robust maximum-likelihood confirmatory factor analysis to test factorial structures in a joint survey of all AIL items in an English-speaking online sample (N=219). We found general support for all instruments' factorial structures with minor deviations from the original factorial structures for some of the instruments. In a second analysis step, to address the issue of fragmented research on AI literacy conceptualization and measurement, we used principal axis exploratory factor analysis with oblique rotation to identify latent dimensions across all items. We found four correlated latent dimensions of AIL, which were mostly interpretable as the abilities to use and interact with AI, to design/program AI (incl. in-depth technical knowledge), to perform complex cognitive operations regarding AI (e.g., ethical considerations), and a common factor for the abilities to detect AI/differentiate between AI and humans and manage persuasive influences of AI (i.e., persuasion literacy). Our findings sort the multitude of AIL instruments and reveal four latent core dimensions of AIL. Thus, they contribute importantly to the conceptual understanding of AIL that has been fragmented so far. © 2024 The Author(s)",2024,,Computers and Education: Artificial Intelligence,7,,,,10.1016/j.caeai.2024.100310,,#4821,Koch 2024,,,1
The potential and limitations of large language models in identification of the states of motivations for facilitating health behavior change,"Bak, M.; Chin, J.","Importance: The study highlights the potential and limitations of the Large Language Models (LLMs) in recognizing different states of motivation to provide appropriate information for behavior change. Following the Transtheoretical Model (TTM), we identified the major gap of LLMs in responding to certain states of motivation through validated scenario studies, suggesting future directions of LLMs research for health promotion. Objectives: The LLMs-based generative conversational agents (GAs) have shown success in identifying user intents semantically. Little is known about its capabilities to identify motivation states and provide appropriate information to facilitate behavior change progression. Materials and Methods: We evaluated 3 GAs, ChatGPT, Google Bard, and Llama 2 in identifying motivation states following the TTM stages of change. GAs were evaluated using 25 validated scenarios with 5 health topics across 5 TTM stages. The relevance and completeness of the responses to cover the TTM processes to proceed to the next stage of change were assessed. Results: 3 GAs identified the motivation states in the preparation stage providing sufficient information to proceed to the action stage. The responses to the motivation states in the action and maintenance stages were good enough covering partial processes for individuals to initiate and maintain their changes in behavior. However, the GAs were not able to identify users' motivation states in the precontemplation and contemplation stages providing irrelevant information, covering about 20%-30% of the processes. Discussion: GAs are able to identify users' motivation states and provide relevant information when individuals have established goals and commitments to take and maintain an action. However, individuals who are hesitant or ambivalent about behavior change are unlikely to receive sufficient and relevant guidance to proceed to the next stage of change. Conclusion: The current GAs effectively identify motivation states of individuals with established goals but may lack support for those ambivalent towards behavior change. © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.",2024,,Journal of the American Medical Informatics Association,31,9,2047-2053,,10.1093/jamia/ocae057,,#4822,Bak 2024,,,1
Uncreative Writing: Teaching Rhetoric in the Age of AI,"Pascoal, S.","Rhetoric, Aristotle's art of persuasion, holds contemporary relevance in diverse realms such as politics, economics, and daily communication. This paper explores the enduring influence of rhetoric, from its historical roots to its modern applications in marketing, public relations, and digital contexts. Specifically, it investigates an experimental teaching strategy within a course unit of Rhetoric and Argumentation, included in the context of an undergraduate Business Communication curriculum, emphasizing the fusion of classical rhetoric principles with the demands of the digital age. The study involves students crafting Press Releases, juxtaposed with AI-generated content from tools like ChatGPT, WriteSonic or CopyAI. Through a comparative peer and blind analysis, the research assesses factors like structure, reliability, style, and persuasion. Findings reveal distinctions between human and AI-generated content, shedding light on potential applications and challenges in integrating AI tools for teaching writing skills. This exploration contributes insights into the dynamic relationship between traditional and digital rhetoric in contemporary communication education, stressing and remind us that we should adopt a new paradigm in the way we teach and manage language in the digital age. Writing has become more mechanical because of technology. Consequently, authors are investigating and engaging in forms of writing that were formerly believed to be outside the scope of literary practice: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",2025,,Lecture Notes in Networks and Systems,858 LNNS,,301-310,,10.1007/978-3-031-74751-9_28,,#4823,Pascoal 2025,,,1
"'Age Isn't Just a Number': Effects of Virtual Human Age and Gender on Persuasion, Social Presence and Influence in Interpersonal Social Encounters in VR","Schlesener, E.A.; Shivakumar, V.; Breeze, D.; Rennison, B.; Soehmelioglu, B.; Babu, S.V.","As virtual reality (VR) technology continues to gain traction as a powerful tool for persuasion and marketing, its application in immersive sales environments has become increasingly relevant. VR's ability to simulate realistic interactions allows for novel approaches to influencing consumer behavior, particularly through the use of interactive Virtual Humans (VHs). In this study, we investigate the effects of user and virtual human age and gender on persuasion and social influence, via various social psychological factors such as brand attitude, interpersonal attraction, source credibility, and presence. In the process, we conducted a 3 (virtual agent age) × 2 (virtual agent gender) × 3 (module topic) multi-factorial design (N=45). The study included two agent gender conditions, namely gender-matched and gender-mismatched with the user, assigned in a between-subjects manner, and three agent age conditions, namely (1) young adult (20-39), (2) middle-aged adult (40-59), and (3) older adult (60-79). Our findings indicate that the age and gender of virtual humans significantly influence their persuasiveness and social impact, which varies based on the user's own age and gender. Specifically, virtual humans of a different gender from the user may exhibit higher levels of persuasion and social influence. Furthermore, female and older users appear to be more susceptible to influence compared to male and younger users. Our findings provide novel, critical insights for designing more effective virtual sales strategies and tailoring persuasive technologies to diverse user profiles, thus filling a vital gap in understanding the interaction between user and agent characteristics in virtual environments.1 © 2025 IEEE.",2025,,"Proceedings - 2025 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2025",,,82-92,,10.1109/VR59515.2025.00033,,#4824,Schlesener 2025,,,1
On LLM Wizards: Identifying Large Language Models’ Behaviors for Wizard of Oz Experiments,"Fang, J.; Arechiga, N.; Namikoshi, K.; Bravo, N.; Hogan, C.; Shamma, D.A.","The Wizard of Oz (WoZ) method is a widely adopted research approach where a human Wizard “role-plays” a not readily available technology and interacts with participants to elicit user behaviors and probe the design space. With the growing ability for modern large language models (LLMs) to role-play, one can apply LLMs as Wizards in WoZ experiments with better scalability and lower cost than the traditional approach. However, methodological guidance on responsibly applying LLMs in WoZ experiments and a systematic evaluation of LLMs’ role-playing ability are lacking. Through two LLM-powered WoZ studies, we take the first step towards identifying an experiment lifecycle for researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings that involve Wizards role-played by LLMs. We also contribute a heuristic-based evaluation framework that allows the estimation of LLMs’ role-playing ability in WoZ experiments and reveals LLMs’ behavior patterns at scale. © 2024 Copyright held by the owner/author(s).",2024,,"Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents, IVA 2024",,,,,10.1145/3652988.3673967,,#4825,Fang 2024,,,1
Resistance Against Manipulative AI: Key Factors and Possible Actions,"Wilczyński, P.; Mieleszczenko-Kowszewicz, W.; Biecek, P.","If AI is the new electricity, what should we do to keep ourselves from getting electrocuted? In this work, we explore factors related to the potential of large language models (LLMs) to manipulate human decisions. We describe the results of two experiments designed to determine what characteristics of humans are associated with their susceptibility to LLM manipulation, and what characteristics of LLMs are associated with their manipulativeness potential. We explore human factors by conducting user studies in which participants answer general knowledge questions using LLM-generated hints, whereas LLM factors by provoking language models to create manipulative statements. Then, we analyze their obedience, the persuasion strategies used, and the choice of vocabulary. Based on these experiments, we discuss two actions that can protect us from LLM manipulation. In the long term, we put AI literacy at the forefront, arguing that educating society would minimize the risk of manipulation and its consequences. We also propose an ad hoc solution, a classifier that detects LLM manipulation - a Manipulation Fuse. © 2024 The Authors.",2024,,Frontiers in Artificial Intelligence and Applications,392,,802-809,,10.3233/FAIA240565,,#4830,Wilczyński 2024,,,1
Empowering Calibrated (Dis-)Trust in Conversational Agents: A User Study on the Persuasive Power of Limitation Disclaimers vs. Authoritative Style,"Metzger, L.; Miller, L.; Baumann, M.; Kraus, J.","While conversational agents based on Large Language Models (LLMs) can drive progress in many domains, they are prone to generating faulty information. To ensure an efficient, safe, and satisfactory user experience maximizing benefits of these systems, users must be empowered to judge the reliability of system outputs. In this, both disclaimers and agents' communicative style are pivotal design instances. In an online study with 594 participants, we investigated how these affect users' trust and a mock-up agent's persuasiveness, based on an established framework from social psychology. While prior information on potential inaccuracies or faulty information did not affect trust, an authoritative communicative style elicited more trust. Also, a trusted agent was more persuasive resulting in more positive attitudes regarding the subject of the conversation. Results imply that disclaimers on agents' limitations fail to effectively alter users' trust but can be supported by appropriate communicative style during interaction. © 2024 Copyright held by the owner/author(s)",2024,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3613904.3642122,,#4834,Metzger 2024,,,1
Leveraging ChatGPT for Automated Human-centered Explanations in Recommender Systems,"Silva, Í.; Marinho, L.; Said, A.; Willemsen, M.C.","The adoption of recommender systems (RSs) in various domains has become increasingly popular, but concerns have been raised about their lack of transparency and interpretability. While significant advancements have been made in creating explainable RSs, there is still a shortage of automated approaches that can deliver meaningful and contextual human-centered explanations. Numerous researchers have evaluated explanations based on human-generated recommendations and explanations to address this gap. However, such approaches do not scale for real-world systems. Building on recent research that exploits Large Language Models (LLMs) for RSs, we propose leveraging the conversational capabilities of ChatGPT to provide users with personalized, human-like, and meaningful explanations for recommended items. Our paper presents one of the first user studies that measure users' perceptions of ChatGPT-generated explanations while acting as an RS. Regarding recommendations, we assess whether users prefer ChatGPT over random (but popular) recommendations. Concerning explanations, we assess users' perceptions of personalization, effectiveness, and persuasiveness. Our findings reveal that users tend to prefer ChatGPT-generated recommendations over popular ones. Additionally, personalized rather than generic explanations prove to be more effective when the recommended item is unfamiliar. © 2024 Owner/Author.",2024,,ACM International Conference Proceeding Series,,,597-608,,10.1145/3640543.3645171,,#4842,Silva 2024,,,1
Can a Funny Chatbot Make a Difference? Infusing Humor into Conversational Agent for Behavioral Intervention,"Sun, X.; Teljeur, I.; Li, Z.; Bosch, J.A.","Regular physical activity is crucial for reducing the risk of non-communicable disease (NCD). With NCDs on the rise globally, there is an urgent need for effective health interventions, with chatbots emerging as a viable and cost-effective option because of limited healthcare accessibility. Although health professionals often utilize behavior change techniques (BCTs) to boost physical activity levels and enhance client engagement and motivation by affiliative humor, the efficacy of humor in chatbot-delivered interventions is not well-understood. This study conducted a randomized controlled trial to examine the impact of the generative humorous communication style in a 10-day chatbot-delivered intervention for physical activity. It further investigated whether user engagement and motivation act as mediators between the communication style and changes in physical activity levels. 66 participants engaged with the chatbots across three groups (humorous, non-humorous, and no-intervention) and responded to daily ecological momentary assessment questionnaires assessing engagement, motivation, and physical activity levels. Multilevel time series analyses revealed that an affiliative humorous communication style positively impacted physical activity levels over time, with user engagement acting as a mediator in this relationship, whereas motivation did not. These findings clarify the role of humorous communication style in chatbot-delivered interventions for physical activity, offering valuable insights for future development of intelligent conversational agents incorporating humor. © 2024 Owner/Author.",2024,,"Proceedings of the 6th Conference on ACM Conversational User Interfaces, CUI 2024",,,,,10.1145/3640794.3665555,,#4843,Sun 2024,,,1
Dual-process theory-driven transparent approach for seniors to accept health misinformation detection results,"Liu, F.; Zhou, J.; Zuo, M.; Li, Y.","Given seniors’ concerns about the reliability of black-box models in health misinformation detection (HMID), there is a pressing need for explainable HMID methods that provide transparency and instill trust. Explainable artificial intelligence (AI) aims to foster understanding and trust in AI models by implementing transparency that embodies explainability and interpretability. However, most existing explainable HMID methods solely provide post-hoc explanations and neglect the interpretation of the models’ internal logic, which prevents seniors from satisfactorily accepting the HMID results. Therefore, this study proposes a transparent Knowledge Graph-aware Two-Stage approach (KG2S) driven by the dual-process theory. KG2S combines explainability and interpretability by utilizing knowledge graphs (KGs) in two stages: Knowledge Breadth Retrieval (KBR) and Knowledge Depth Reasoning (KDR). These stages correspond with the heuristic and analytic processes of the dual-process theory, which encompasses human information processing. In the KBR stage, we leverage rich facts in KGs to replicate heuristic distillation behaviors of humans through a novel similarity-diversity twofold filter. In the KDR stage, we employ a hierarchical attention network to emulate humans’ coarse-to-fine knowledge analyses during decision-making. Extensive experiments were conducted on two real-world datasets, along with user testing, to assess the effectiveness of the proposed model. Results demonstrated that the proposed model not only outperformed competing methods in terms of HMID accuracy but also provided persuasive detection processes and reasons. Moreover, the model showed high adaptability to new topics in HMID. Our research offers valuable insights into integrating humanistic into AI algorithms and promoting the trustworthiness of AI systems. © 2024 Elsevier Ltd",2024,,Information Processing and Management,61,4,,,10.1016/j.ipm.2024.103751,,#4850,Liu 2024,,,1
A Chatbot-Delivered Stress Management Coaching for Students (MISHA App): Pilot Randomized Controlled Trial,"Ulrich, S.; Lienhard, N.; Künzli, H.; Kowatsch, T.","BACKGROUND: Globally, students face increasing mental health challenges, including elevated stress levels and declining well-being, leading to academic performance issues and mental health disorders. However, due to stigma and symptom underestimation, students rarely seek effective stress management solutions. Conversational agents in the health sector have shown promise in reducing stress, depression, and anxiety. Nevertheless, research on their effectiveness for students with stress remains limited. OBJECTIVE: This study aims to develop a conversational agent-delivered stress management coaching intervention for students called MISHA and to evaluate its effectiveness, engagement, and acceptance. METHODS: In an unblinded randomized controlled trial, Swiss students experiencing stress were recruited on the web. Using a 1:1 randomization ratio, participants (N=140) were allocated to either the intervention or waitlist control group. Treatment effectiveness on changes in the primary outcome, that is, perceived stress, and secondary outcomes, including depression, anxiety, psychosomatic symptoms, and active coping, were self-assessed and evaluated using ANOVA for repeated measure and general estimating equations. RESULTS: The per-protocol analysis revealed evidence for improvement of stress, depression, and somatic symptoms with medium effect sizes (Cohen d=-0.36 to Cohen d=-0.60), while anxiety and active coping did not change (Cohen d=-0.29 and Cohen d=0.13). In the intention-to-treat analysis, similar results were found, indicating reduced stress (β estimate=-0.13, 95% CI -0.20 to -0.05; P<.001), depressive symptoms (β estimate=-0.23, 95% CI -0.38 to -0.08; P=.003), and psychosomatic symptoms (β estimate=-0.16, 95% CI -0.27 to -0.06; P=.003), while anxiety and active coping did not change. Overall, 60% (42/70) of the participants in the intervention group completed the coaching by completing the postintervention survey. They particularly appreciated the quality, quantity, credibility, and visual representation of information. While individual customization was rated the lowest, the target group fitting was perceived as high. CONCLUSIONS: Findings indicate that MISHA is feasible, acceptable, and effective in reducing perceived stress among students in Switzerland. Future research is needed with different populations, for example, in students with high stress levels or compared to active controls. TRIAL REGISTRATION: German Clinical Trials Register DRKS 00030004; https://drks.de/search/en/trial/DRKS00030004. ©Sandra Ulrich, Natascha Lienhard, Hansjörg Künzli, Tobias Kowatsch. Originally published in JMIR mHealth and uHealth (https://mhealth.jmir.org), 26.06.2024.",2024,,JMIR mHealth and uHealth,12,,e54945,,10.2196/54945,,#4851,Ulrich 2024,,,1
AI-Driven Mediation Strategies for Audience Depolarisation in Online Debates,"Govers, J.; Velloso, E.; Kostakos, V.; Goncalves, J.","Online polarisation can tear the fabric of civility through reinforcing social media's perceptions of division and discord. Social media platforms often rely on content-moderation to combat polarisation, contingent on the reactive removal or fagging of content. However, this approach often remains agnostic of the underlying debate's ideas and stifes open discourse. In this study, we use prompt-tuned language models to mediate social media debates, applying the strategies of the Thomas-Kilmann Confict Mode Instrument (TKI). We evaluate multiple mediation strategies in providing targeted responses to the debates, as shown to a debate audience. Our fndings show that high-cooperativeness TKI strategies ofered more persuasive arguments, while an accommodating argument strategy was the most successful at depolarising the audience's opinion. Furthermore, high-cooperativeness strategies also increased the perception that the debaters will reach a consensus. Our work paves the way for scalable and personalised tools that mediate social media debates to encourage depolarisation. © 2024 Copyright held by the owner/author(s)",2024,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3613904.3642322,,#4854,Govers 2024,,,1
Basic Study on Generation of Dopamine-Secretion-Promoting Intrinsic Motivational Messages Using ChatGPT,"Aruga, R.; Katagiri, Y.; Watanabe, J.","People sometimes put off an action because they do not perceive it as worth doing immediately, despite recognizing they should do it. Procrastination, especially in medical and healthcare fields, can lead to serious consequences, and it is an important social issue to promote cognitive and behavioral change for those reluctant to practice health behaviors. One important factor that can positively influence the psychology of procrastination is intrinsic motivation. Dopamine is known as the primary neurotransmitter associated with motivation and has been shown to relate to positive emotions and cognition. In this study, we defined personal experiences related to emotions and cognitions occurring during dopamine secretion as the person's original favorite activities (FAs). We then attempted to generate intrinsic motivational messages expected to produce dopamine secretion by partially replacing or modifying the activities the subject should do but were disinclined to do (reluctant activities (RAs)) with FAs. Preliminary experiments using questionnaire evaluations showed that dopamine-secretion-promoting intrinsic motivational messages may induce more excited feelings than general motivational messages.  © 2024 ACM.",2024,,"BTIW 2024 - Proceedings of the 2024 Behavior Transformation by IoT International Workshop, Part of: ACM MobiSys 2024",,,01-Jun,,10.1145/3662008.3662011,,#4855,Aruga 2024,,,1
Comparative analysis of communication strategies impact on the effectiveness of interaction with an AI chatbot,"Voronin, A.N.; Palenova, V.V.","Introduction. The integration of artificial intelligence (AI) into everyday human interactions, particularly through chatbots, has significantly increased in recent years. AI-based chatbots successfully mimic human conversations by understanding user requests and responding appropriately. Improving the effectiveness of educational and professional activities related to text processing in these conditions seems extremely relevant and predetermines the goal of our study: comparing the effectiveness of interaction with ChatGPT when formulating queries in various communication strategies. Procedure and methods. The study on the effectiveness of interacting with ChatGPT was conducted using a natural experiment scheme with preliminary training. The subjects of the study were 34 employees (15 men, 19 women, aged 25-36; M = 31.6; SD = 1.8) from the marketing department of the company ""Technologies of Trust"". These employees performed rewriting tasks by interacting with ChatGPT within the framework of specially adapted communication strategies (persuasive and cooperative) and evaluated the chatbot's usability and the quality of the text generated by ChatGPT. The Wilcoxon signed-rank test for dependent samples was used to identify the effects of communication strategies on interaction efficiency, and exploratory factor analysis with subsequent varimax rotation was employed to identify latent variables of interaction under different communication strategies. Research Results. Comparison of the results of the ChatGPT interaction performance evaluation when using 2 strategies (cooperative and persuasive) showed that for all 11 usability indicators, there is a significant advantage of the cooperative communicative strategy at the significance level of p<0.05. However, on 6 indicators of text quality obtained through rewriting, the persuasive strategy shows advantages and on 4 of them at a significance level of p<0.05. In the course of the study it was found that the factor solutions for the studied communicative strategies differ significantly: the key factor in using the persuasive strategy is speed and accuracy in achieving goals, while in the cooperative strategy the adequate response of chatbots to objections is critical. Conclusion. The results obtained in the study contribute significantly to the understanding of communication with AI chatbots, highlighting the nuances of interacting through linguistic means typical for human communication. The findings demonstrate the complexity of choosing the optimal strategy for interacting with ChatGPT and uncover the factors that determine the features of subjective evaluation of the interaction. © 2024 LLC Ecological Help. All rights reserved.",2024,,Perspektivy Nauki i Obrazovania,71,5,680-693,,10.32744/pse.2024.5.40,,#4857,Voronin 2024,,,1
"Towards AI-Driven Healthcare: Systematic Optimization, Linguistic Analysis, and Clinicians' Evaluation of Large Language Models for Smoking Cessation Interventions","Calle, P.; Shao, R.; Liu, Y.; Hébert, E.T.; Kendzor, D.; Neil, J.; Businelle, M.; Pan, C.","Creating intervention messages for smoking cessation is a labor-intensive process. Advances in Large Language Models (LLMs) offer a promising alternative for automated message generation. Two critical questions remain: 1) How to optimize LLMs to mimic human expert writing, and 2) Do LLM-generated messages meet clinical standards? We systematically examined the message generation and evaluation processes through three studies investigating prompt engineering (Study 1), decoding optimization (Study 2), and expert review (Study 3). We employed computational linguistic analysis in LLM assessment and established a comprehensive evaluation framework, incorporating automated metrics, linguistic attributes, and expert evaluations. Certified tobacco treatment specialists assessed the quality, accuracy, credibility, and persuasiveness of LLM-generated messages, using expert-written messages as the benchmark. Results indicate that larger LLMs, including ChatGPT, OPT-13B, and OPT-30B, can effectively emulate expert writing to generate well-written, accurate, and persuasive messages, thereby demonstrating the capability of LLMs in augmenting clinical practices of smoking cessation interventions. © 2024 Copyright held by the owner/author(s)",2024,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3613904.3641965,,#4858,Calle 2024,,,1
Eternagram: Probing Player Atitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure,"Zhou, S.; Hendra, L.B.; Zhang, Q.; Holopainen, J.; Lc, R.","Conventional methods of assessing attitudes towards climate change are limited in capturing authentic opinions, primarily stemming from a lack of context-specific assessment strategies and an overreliance on simplistic surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome these issues by immersing participants in engaging gameplay within carefully crafted, scenario-based environments. Concurrently, advancements in AI and Natural Language Processing (NLP) show promise in enhancing the gamified testing environment, achieving this by generating context-aware, human-like dialogues that contribute to a more natural and effective assessment. Our study introduces a new technique for probing climate change attitudes by actualizing a GPT-driven chatbot system in harmony with a game design depicting a futuristic climate scenario. The correlation analysis reveals an assimilation effect, where players' post-game climate awareness tends to align with their in-game perceptions. Key predictors of pro-climate attitudes are identified as traits like'Openness' and'Agreeableness', and a preference for democratic values. © 2024 Copyright held by the owner/author(s)",2024,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3613904.3642850,,#4860,Zhou 2024,,,1
Can You Spot the AI? Incorporating GenAI into Technical Writing Assignments,"Rajabi, P.; Kerslake, C.","In an effort to foster critical reflection on the usage of generative AI (genAI) during computer science writing assignments, this three-part assignment challenges students to predict whether their peers can detect which essays are generated using AI. Implemented as part of a third-year professional responsibility and technical writing course for N=200 students during Spring 2024, students individually generated two short persuasive essays, one using genAI and the other without. They then combined the two essays into a single document and submitted it for peer-review. Additionally, they formulated a guess on whether their peers would be able to detect which essay was generated as well as a rationale for their guess. Following the peer-review process, students reflected on their own experience trying to detect which essays were generated as well as the outcome of their guess about their peers abilities as well. Feedback indicates its effectiveness in engaging students in their understanding of the potentials and limitations of genAI. Recommended prerequisites include a clear course AI-usage policy and a brief overview of genAI prompt engineering. © 2024 Owner/Author(s).",2024,,ACM International Conference Proceeding Series,,,,,10.1145/3660650.3660673,,#4861,Rajabi 2024,,,1
"Systematic Development of an AI Chatbot for Physical Activity Habit Formation in Prehypertension Individual Integrating Needs, Theories, and Evidence","Ma, H.; Pei, R.; Li, S.; Wang, A.; Tang, X.; Piao, M.","Prehypertension, an early stage in the development of hypertension, impacts a substantial segment of the adult population worldwide. Addressing this issue, our study introduces HabitBot, an AI-driven chatbot tailored to encourage physical activity (PA) habits among individuals with prehypertension. HabitBot combines natural language processing with multidisciplinary approaches, drawing from both theoretical frameworks and empirical studies. The chatbot development followed a systematic, five-phase process: comprehensive needs assessment, literature review on behavior change theories, analysis for selecting effective behavior change techniques (BCTs), prototype design through intervention mapping, and refining the intervention based on user feedback. The outcome includes a prototype that integrates the Health Action Process Approach and Habit Formation Theory, utilizing twelve identified BCTs effective in fostering PA habits. User feedback further refined the chatbot across multiple dimensions such as user interface, content accessibility, and privacy. HabitBot exemplifies an innovative integration of behavior change strategies with advanced language model technology, paving the way for digital health interventions in chronic disease prevention. Future studies should assess its long-term efficacy in habit formation and explore its applicability to various demographic groups  © 2024 The Authors.",2024,,Studies in Health Technology and Informatics,315,,581-582,,10.3233/SHTI240226,,#4865,Ma 2024,,,1
"""You tell me"": A Dataset of GPT-4-Based Behaviour Change Support Conversations","Meyer, S.; Elsweiler, D.","Conversational agents are increasingly used to address emotional needs on top of information needs. One use case of increasing interest are counselling-style mental health and behaviour change interventions, with large language model (LLM)-based approaches becoming more popular. Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts. To address this issue, we share a dataset containing text-based user interactions related to behaviour change with two GPT-4-based conversational agents collected in a preregistered user study. This dataset includes conversation data, user language analysis, perception measures, and user feedback for LLM-generated turns, and can offer valuable insights to inform the design of such systems based on real interactions. © 2024 Owner/Author.",2024,,CHIIR 2024 - Proceedings of the 2024 Conference on Human Information Interaction and Retrieval,,,411-416,,10.1145/3627508.3638330,,#4875,Meyer 2024,,,1
"The End of the Policy Analyst? Testing the Capability of Artificial Intelligence to Generate Plausible, Persuasive, and Useful Policy Analysis","Safaei, M.; Longo, J.","Policy advising in government centers on the analysis of public problems and the developing of recommendations for deal- ing with them. In carrying out this work, policy analysts consult a variety of sources and work to synthesize that body of evidence into useful decision support documents commonly called briefing notes. Advances in natural language process- ing (NLP) have led to the continuing development of tools that can undertake a similar task. Given a brief prompt, a large language model (LLM) can synthesize information in content databases. This article documents the findings from an exper- iment that tested whether contemporary NLP technology is capable of producing public policy relevant briefing notes that expert evaluators judge to be useful. The research involved two stages. First, briefing notes were created using three mod- els: NLP generated; human generated; and NLP generated/human edited. Next, two panels of retired senior public servants (with only one panel informed of the use of NLP in the experiment) were asked to judge the briefing notes using a heuristic evaluation rubric. The findings indicate that contemporary NLP tools were not able to, on their own, generate useful policy briefings. However, the feedback from the expert evaluators indicates that automatically generated briefing notes might serve as a useful supplement to the work of human policy analysts. And the speed with which the capabilities of NLP tools are developing, supplemented with access to a larger corpus of previously prepared policy briefings and other policy-relevant material, suggests that the quality of automatically generated briefings may improve significantly in the coming years. The article concludes with reflections on what such improvements might mean for the future practice of policy analysis.  Copyright © 2024 held by the owner/author(s).",2024,,Digital Government: Research and Practice,5,1,,,10.1145/3604570,,#4876,Safaei 2024,,,1
Social Robots vs. Chatbots: Evaluating the Effect as a Persuasive Technology for Children in the Healthy Eating Domain,"De Carolis, B.N.; Palestra, G.; Oranger, E.","Childhood obesity and overweight are concerning issues worldwide with significant health and social implications, making it necessary to implement prevention and intervention policies. Engaging children through creative and playful methods is an effective approach to motivating behavior change. Social robots have emerged as promising tools in education and persuasive technology, due to their interactive and engaging nature. This paper presents a study examining the potential of a social robot acting as a nutrition coach for children, by comparing interactions with a social robot versus a chatbot. Results suggest that, in the short term, the social robot was perceived as more persuasive, increasing participants' intention to adopt healthier eating behavior. These findings offer insights for the development of personalized nutrition coaching for children. © 2024 Copyright held by the owner/author(s).",2024,,ACM International Conference Proceeding Series,,,,,10.1145/3656650.3656765,,#4878,DeCarolis 2024,,,1
The impact of Chatbot-Assisted Self Assessment (CASA) on intentions for sexual health screening in people from minoritised ethnic groups at risk of sexually transmitted infections,"Nadarzynski, T.; Knights, N.; Husbands, D.; Graham, C.A.; Llewellyn, C.D.; Buchanan, T.; Montgomery, I.; Khlafa, N.; Tichackova, J.; Odeyemi, R.; Johnson, S.; Jesuthas, N.; Tahia, S.; Ridge, D.","Background: Sexually transmitted infections (STIs) present a significant global public health issue, with disparities in STI rates often observed across ethnic groups. The study investigates the impact of Chatbot-Assisted Self Assessment (CASA) on the intentions for sexual health screening within minoritised ethnic groups (MEGs) at risk of STIs as well as the subsequent use of a chatbot for booking STI screening. Methods: A simulation within-subject design was utilised to evaluate the effect of CASA on intentions for STI/HIV screening, concern about STIs, and attitudes towards STI screening. Screening intentions served as the dependent variable, while demographic and behavioural factors related to STI/HIV risk were the independent variables. ANCOVA tests were conducted to measure the impact of CASA on these perceptions. Results: Involving 548 participants (54% women, 66% black, average age = 30 years), the study found that CASA positively influenced screening intentions t(547) = -10.3, P < 0.001], concerns about STIs t(544) = -4.96, P < 0.001, and attitudes towards sexual health screening [t(543) = -4.36, P < 0.001. Positive attitudes towards CASA were observed (mean, 13.30; s.d., 6.73; range, -17 to 21). About 72% of users who booked STI screening appointments via chatbot were from MEGs. Conclusion: CASA increased motivations for STI screening intentions among ethnically diverse communities. The intervention's non-judgemental nature and the chatbot's ability to emulate sexual history-taking were critical in fostering an environment conducive to behavioural intention change. The study's high acceptability indicates the potential for broader application in digital health interventions. However, the limitation of not tracking actual post-intervention behaviour warrants further investigation into CASA's real-world efficacy.  © 2024 The Author(s) (or their employer(s)). Published by CSIRO Publishing.",2024,,Sexual Health,21,4,,,10.1071/SH24058,,#4885,Nadarzynski 2024,,,1
Designing Behavior Change Support Systems Targeting Blood Donation Behavior,"Müller, H.M.; Reuter-Oppermann, M.","While blood is crucial for many surgeries and patient treatments worldwide, it cannot be produced artificially. Fulfilling the demand for blood products on average days is already a major challenge in countries like South Africa and Ghana. In these countries, less than 1 % of the population donates blood and most of the donations come from first-time donors who do not return. Sufficient new, first-time and even lapsed donors must be motivated to donate regularly. This study argues that blood donation behavior change support systems (BDBCSS) can be beneficially applied to support blood donor management in African countries. In this study, the design science research (DSR) approach is applied in order to derive generic design principles for BDBCSS and instantiate the design knowledge in prototypes for a blood donation app and a chatbot. The design principles were evaluated in a field study in South Africa. The results demonstrate the positive effects of BDBCSS on users’ intentional and developmental blood donation behavior. This study contributes to research and practice by proposing a new conceptualization of blood donation information systems support and a nascent design theory for BDBCSS that builds on behavioral theories as well as related work on blood donation information systems. Thus, the study provides valuable implications for designing preventive health BCSS by stating three design principles for a concrete application context in healthcare. © The Author(s) 2024.",2024,,Business and Information Systems Engineering,66,3,299-319,,10.1007/s12599-024-00878-3,,#4893,Müller 2024,,,1
Coaching Copilot: Blended Form of an LLM-Powered Chatbot and a Human Coach to Effectively Support Self-Reflection for Leadership Growth,"Arakawa, R.; Yakura, H.","Chatbots' role in fostering self-reflection is now widely recognized, especially in inducing users' behavior change. While the benefits of 24/7 availability, scalability, and consistent responses have been demonstrated in contexts such as healthcare and tutoring to help one form a new habit, their utilization in coaching necessitating deeper introspective dialogue to induce leadership growth remains unexplored. This paper explores the potential of such a chatbot powered by recent Large Language Models (LLMs) in collaboration with professional coaches in the field of executive coaching. Through a design workshop with them and two weeks of user study involving ten coach-client pairs, we explored the feasibility and nuances of integrating chatbots to complement human coaches. Our findings highlight the benefits of chatbots' ubiquity and reasoning capabilities enabled by LLMs while identifying their limitations and design necessities for effective collaboration between human coaches and chatbots. By doing so, this work contributes to the foundation for augmenting one's self-reflective process with prevalent conversational agents through the human-in-the-loop approach. © 2024 Owner/Author.",2024,,"Proceedings of the 6th Conference on ACM Conversational User Interfaces, CUI 2024",,,,,10.1145/3640794.3665549,,#4896,Arakawa 2024,,,1
Care-Based Eco-Feedback Augmented with Generative AI: Fostering Pro-Environmental Behavior through Emotional Atachment,"Berney, M.; Ouaazki, A.; Macko, V.; Kocher, B.; Holzer, A.","Lights out! With the escalating climate crisis, eco-feedback has gained prominence over the last decade. However, traditional approaches could be underperforming as they often use data-driven strategies and assume that people only need additional information about their consumption to change behavior. A proposed path to overcome this issue is to design eco-feedback to foster emotional connections with users. However, not much is known about the effectiveness of such designs. In this paper, we propose a novel care-based eco-feedback system. Central to the system is a Tamagotchi-inspired digital character named Infi who gets its life force from the user's energy savings. Additionally, we harness the latest advancements in generative artificial intelligence to enhance emotional attachment through conversational interactions that users can have with Infi. The results of a randomized controlled experiment (N=420) convey the fact that this design increases emotional attachment, which in turn increases energy-saving behavior. © 2024 Copyright held by the owner/author(s)",2024,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3613904.3642296,,#4898,Berney 2024,,,1
Evaluating the persuasive influence of political microtargeting with large language models,"Hackenburg, K.; Margetts, H.","Recent advancements in large language models (LLMs) have raised the prospect of scalable, automated, and fine-grained political microtargeting on a scale previously unseen; however, the persuasive influence of microtargeting with LLMs remains unclear. Here, we build a custom web application capable of integrating self-reported demographic and political data into GPT-4 prompts in real-time, facilitating the live creation of unique messages tailored to persuade individual users on four political issues. We then deploy this application in a preregistered randomized control experiment (n = 8,587) to investigate the extent to which access to individual-level data increases the persuasive influence of GPT-4. Our approach yields two key findings. First, messages generated by GPT-4 were broadly persuasive, in some cases increasing support for an issue stance by up to 12 percentage points. Second, in aggregate, the persuasive impact of microtargeted messages was not statistically different from that of non-microtargeted messages (4.83 vs. 6.20 percentage points, respectively, P = 0.226). These trends hold even when manipulating the type and number of attributes used to tailor the message. These findings suggest—contrary to widespread speculation—that the influence of current LLMs may reside not in their ability to tailor messages to individuals but rather in the persuasiveness of their generic, nontargeted messages. We release our experimental dataset, GPTarget2024, as an empirical baseline for future research. © 2024 the Author(s). Published by PNAS.",2024,,Proceedings of the National Academy of Sciences of the United States of America,121,24,,,10.1073/pnas.2403116121,,#4899,Hackenburg 2024,,,1
StayFocused: Examining the Efects of Reflective Prompts and Chatbot Support on Compulsive Smartphone Use,"Li, Z.; Liang, M.; Lc, R.; Luo, Y.","Amidst the increasingly prevalent smartphone addiction, we introduce StayFocused, a mobile app to help people focus on their tasks at hand by reducing compulsive smartphone use. Besides guiding people to set focus sessions for non-screen time, we incorporated reflective prompts probing individuals' phone-checking intentions whenever they check their phones and a chatbot to deliver these prompts. To examine the effects of the reflective prompts and the chatbot support, we designed three versions of StayFocused: baseline, reflection, and reflection-chatbot, and conducted a stage-based between-subjects study with 36 college students over five weeks. We found that participants who received the reflective prompts were able to focus longer and resist distractions, and those with chatbot support seemed to better maintain their smartphone use reduction. By highlighting how participants reflected on their focus session activities and their preferences for the chatbot, we discuss the implications of designing persuasive conversational interfaces to reduce unintended behaviors. © 2024 Copyright held by the owner/author(s)",2024,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3613904.3642479,,#4902,Li 2024,,,1
Debate Chatbots to Facilitate Critical Thinking on YouTube: Social Identity and Conversational Style Make A Difference,"Tanprasert, T.; Fels, S.; Sinnamon, L.; Yoon, D.","Exposure to diverse perspectives is helpful for bursting the filter bubble in online public video platforms. The recent advancement of Large Language Models (LLMs) illuminates the potential of creating a debate chatbot that prompts users to critically examine their stances on a topic formed by watching videos. However, whether the viewer is influenced by the chatbot may depend on its persona. In this paper, we investigated the effect of two relevant persona attributes - social identity and rhetorical styles - on critical thinking. In a mixed-methods study (n=36), we found that chatbots with outgroup (vs. ingroup) identity (t(33)=-2.33, p=0.03) and persuasive (vs. eristic) rhetoric (t(44)=1.98, p=0.05) induced critical thinking most effectively, making participants re-examine their arguments. However, participants' stances remain largely unaffected, likely due to the chatbot's lack of contextual knowledge and human touch. Our paper provides empirical groundwork for designing chatbot persona for remedying filter bubbles in online communities. © 2024 Copyright held by the owner/author(s)",2024,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3613904.3642513,,#4909,Tanprasert 2024,,,1
Do We Dare Use Generative AI for Mental Health?,"Sackett, C.; Harper, D.; Pavez, A.","The mental-health app Woebot launched in 2017, back when 'chatbot' wasn't a familiar term and someone seeking a therapist could only imagine talking to a human being. Woebot was something exciting and new: a way for people to get on-demand mental-health support in the form of a responsive, empathic, AI-powered chatbot. Users found that the friendly robot avatar checked in on them every day, kept track of their progress, and was always available to talk something through. ¶ Today, the situation is vastly different. Demand for mental-health services has surged while the supply of clinicians has stagnated. There are thousands of apps that offer automated support for mental health and wellness. And ChatGPT has helped millions of people experiment with conversational AI. ¶ But even as the world has become fascinated with generative AI, people have also seen its downsides. As a company that relies on conversation, Woebot Health had to decide whether generative AI could make Woebot a better tool, or whether the technology was too dangerous to incorporate into our product. ¶ Woebot is designed to have structured conversations through which it delivers evidence-based tools inspired by cognitive behavioral therapy (CBT), a technique that aims to change behaviors and feelings. Throughout its history, Woebot Health has used technology from a subdiscipline of AI known as natural-language processing (NLP). The company has used AI artfully and by design-Woebot uses NLP only in the service of better understanding a user's written texts so it can respond in the most appropriate way, thus encouraging users to engage more deeply with the process.  © 1964-2012 IEEE.",2024,,IEEE Spectrum,61,6,42-47,,10.1109/MSPEC.2024.10551790,,#4912,Sackett 2024,,,1
Testing behaviour change with an artificial intelligence chatbot in a randomized controlled study,"van Baal, S.T.; Le, S.T.T.; Fatehi, F.; Verdejo-Garcia, A.; Hohwy, J.","Chatbots can effect large-scale behaviour change because they are accessible through social media, flexible, scalable, and gather data automatically. Yet research on the feasibility and effectiveness of chatbot-administered behaviour change interventions is sparse. The effectiveness of established behaviour change interventions when implemented in chatbots is not guaranteed, given the unique human–machine interaction dynamics. We pilot-tested chatbot-based behaviour change through information provision and embedded animations. We evaluated whether the chatbot could increase understanding and intentions to adopt protective behaviours during the pandemic. Fifty-nine culturally and linguistically diverse participants received a compassion intervention, an exponential growth intervention, or no intervention. We measured participants’ COVID-19 testing intentions and measured their staying-home attitudes before and after their chatbot interaction. We found reduced uncertainty about protective behaviours. The exponential growth intervention increased participants’ testing intentions. This study provides preliminary evidence that chatbots can spark behaviour change, with applications in diverse and underrepresented groups. © The Author(s) 2024.",2024,,Journal of Public Health Policy,45,3,506-522,,10.1057/s41271-024-00500-6,,#4913,vanBaal 2024,,,1
Dialoging Resonance in Human-Chatbot Conversation: How Users Perceive and Reciprocate Recommendation Chatbot's Self-Disclosure Strategy,"Liang, K.-H.; Shi, W.; Oh, Y.J.; Wang, H.-C.; Zhang, J.; Yu, Z.","Using chatbots to make recommendations is increasingly popular. The design of recommendation chatbots has mainly been taking an information-centric approach by focusing on the recommended content per se. Limited attention is on how social connection and relational strategies, such as self-disclosure from a chatbot, may influence users' perception and acceptance of the recommendation. In this work, we designed, implemented, and evaluated a social chatbot capable of performing three different levels of self-disclosure: factual information (low), cognitive opinions (medium), and emotions (high). In the evaluation, we recruited 372 participants to converse with the chatbot on two topics: movies and COVID-19 experiences. In each topic, the chatbot conducted small talks and made relevant recommendations to the topic. Participants were randomly assigned to four experimental conditions where the chatbot used factual, cognitive, emotional, and adaptive strategies to perform self-disclosures. By training a text classifier to identify users' level of self-disclosure in real-time, the adaptive chatbot can dynamically match its self-disclosure language to the level of disclosure exhibited by the users. Our results show that users reciprocate with higher-level self-disclosure when a recommendation chatbot displays emotions throughout the conversation. The utilization of emotional disclosure by the chatbot resulted in enhanced enjoyment during interactions and a more favorable perception of the bot. This, in turn, led to greater effectiveness in making recommendations, including a higher likelihood of accepting the recommendation. We discuss the understandings obtained and implications to future design. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",2024,,Proceedings of the ACM on Human-Computer Interaction,8,CSCW1,,,10.1145/3653691,,#4921,Liang 2024,,,1
MindShif: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention,"Wu, R.; Yu, C.; Pan, X.; Liu, Y.; Zhang, N.; Fu, Y.; Wang, Y.; Zheng, Z.; Chen, L.; Jiang, Q.; Xu, X.; Shi, Y.","Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conducted a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leveraged large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We developed MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment app usage behaviors, physical contexts, mental states, goals & habits as input, and generates personalized and dynamic persuasive content with appropriate persuasion strategies. We conducted a 5-week field experiment (N=25) to compare MindShift with its simplified version (remove mental states) and baseline techniques (fixed reminder). The results show that MindShift improves intervention acceptance rates by 4.7-22.5% and reduces smartphone usage duration by 7.4-9.8%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy scale scores. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains. © 2024 Copyright held by the owner/author(s)",2024,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3613904.3642790,,#4926,Wu 2024,,,1
From web forms to chatbots: The roles of consistency and reciprocity for user information disclosure,"Adam, M.; Benlian, A.","Interactive decision aids (IDAs) on websites often require users to disclose relevant information (e.g., preferences, contact information) to help users in making decisions (e.g., product choice). With technological advances in IDAs, websites increasingly switch from static, non-conversational IDAs (e.g., web forms) to conversational ones (e.g., chatbots) to boost user information disclosure that nurtures the websites' economic viability. While this novel form of IDAs is already widely employed in practice, information systems research has yet to examine the defining dialogue design features of conversational IDAs and their effects on eliciting user information. Drawing on persuasion theory and particularly on consistency and reciprocity as influence techniques, we develop a research model around two crucial dialogue design features of conversational IDAs. Specifically, we investigate the distinct and joint effects of conversational style (i.e., absence vs. presence of a conversational presentation of requests) and reciprocation triggers (i.e., absence vs. presence of reciprocity-inducing information) on user information disclosure (i.e., email addresses). By combining the complementary properties of a randomised field experiment (N = 386) and a follow-up online experiment (N = 182), we empirically provide evidence in support of the distinct and joint effects of conversational style and reciprocation triggers of IDAs on user information disclosure. Moreover, we demonstrate that these dialogue design features have indirect effects on information disclosure via perceptions of social presence and privacy concerns. Thus, our paper provides theoretical and practical insights into whether, how, and why critical IDA dialogue design features can better elicit user information for website services. © 2023 The Authors. Information Systems Journal published by John Wiley & Sons Ltd.",2024,,Information Systems Journal,34,4,1175-1216,,10.1111/isj.12490,,#4927,Adam 2024,,,1
Exploring VUI-Supported Mindfulness Techniques for Smoking Cessation,"Kjaerulff, S.B.; Pedersen, S.B.; Sigvardsen, T.J.; Van Berkel, N.; Papachristos, E.","This study investigates the effectiveness of Voice User Interfaces (VUIs) in supporting mindfulness techniques for smoking cessation. We conducted a month-long between-subject study involving nine participants, comparing a VUI on smart speakers against an augmented VUI (a blend of VUI and Graphical User Interface) on mobile devices. Specifically, we evaluated how these interfaces support individuals in quitting smoking through mindfulness practices. Our results include qualitative insights on participants' experiences with mindfulness, their smoking cessation motivation, and engagement with the VUI prototypes, alongside quantitative data on their usage patterns. Our findings offer insights into the potential application of VUIs in smoking cessation and suggest design guidelines for future health-oriented applications. The study underscores the importance of device context in designing effective health interventions and sets the direction for future work in HCI and mindfulness applications. © 2024 Owner/Author.",2024,,"Proceedings of the 6th Conference on ACM Conversational User Interfaces, CUI 2024",,,,,10.1145/3640794.3665546,,#4928,Kjaerulff 2024,,,1
Emotional Reframing of Economic News using a Large Language Model,"Jeng, J.H.; Kasangu, G.; Starke, A.; Trattner, C.","News media framing can shape public perception and potentially polarize views. Emotional language can exacerbate these framing effects, as a user's emotional state can be an important contextual factor to use in news recommendation. Our research explores the relation between emotional framing techniques and the emotional states of readers, as well as readers' perceived trust in specific news articles. Users (N = 200) had to read three economic news articles from the Washington Post. We used ChatGPT-4 to reframe news articles with specific emotional languages (Anger, Fear, Hope), compared to a neutral baseline reframed by a human journalist. Our results revealed that negative framing (Anger, Fear) elicited stronger negative emotional states among users than the neutral baseline, while Hope led to little changes overall. In contrast, perceived trust levels varied little across the different conditions. We discuss the implications of our findings and how emotional framing could affect societal polarization issues. © 2024 Owner/Author.",2024,,"UMAP 2024 - Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization",,,231-235,,10.1145/3631700.3665191,,#4929,Jeng 2024,,,1
MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate,"Amayuelas, A.; Yang, X.; Antoniades, A.; Hua, W.; Pan, L.; Wang, W.Y.","Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary's effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model's persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy. © 2024 Association for Computational Linguistics.",2024,,"EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024",,,6929-6948,,10.18653/v1/2024.findings-emnlp.407,,#4939,Amayuelas 2024,,,1
Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval,"Furumai, K.; Legaspi, R.; Vizcarra, J.; Yamazaki, Y.; Nishimura, Y.; Semnani, S.J.; Ikeda, K.; Shi, W.; Lam, M.S.","Persuasion plays a pivotal role in a wide range of applications from health intervention to the promotion of social good. Persuasive chatbots employed responsibly for social good can be an enabler of positive individual and social change. Existing methods rely on fine-tuning persuasive chatbots with task-specific training data which is costly, if not infeasible, to collect. Furthermore, they employ only a handful of pre-defined persuasion strategies. We propose PersuaBot, a zero-shot chatbot based on Large Language Models (LLMs) that is factual and more persuasive by leveraging many more nuanced strategies. PersuaBot uses an LLM to first generate natural responses, from which the strategies used are extracted. To combat hallucination of LLMs, Persuabot replace any unsubstantiated claims in the response with retrieved facts supporting the extracted strategies. We applied our chatbot, PersuaBot, to three significantly different domains needing persuasion skills: donation solicitation, recommendations, and health intervention. Our experiments on simulated and human conversations show that our zero-shot approach is more persuasive than prior work, while achieving factual accuracy surpassing state-of-the-art knowledge-oriented chatbots. © 2024 Association for Computational Linguistics.",2024,,"EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024",,,11224-11249,,10.18653/v1/2024.findings-emnlp.656,,#4940,Furumai 2024,,,1
How persuasive is AI-generated propaganda?,"Goldstein, J.A.; Chao, J.; Grossman, S.; Stamos, A.; Tomz, M.","Can large language models, a form of artificial intelligence (AI), generate persuasive propaganda? We conducted a preregistered survey experiment of US respondents to investigate the persuasiveness of news articles written by foreign propagandists compared to content generated by GPT-3 davinci (a large language model). We found that GPT-3 can create highly persuasive text as measured by participants’ agreement with propaganda theses. We further investigated whether a person fluent in English could improve propaganda persuasiveness. Editing the prompt fed to GPT-3 and/or curating GPT-3’s output made GPT-3 even more persuasive, and, under certain conditions, as persuasive as the original propaganda. Our findings suggest that propagandists could use AI to create convincing content with limited effort. © The Author(s) 2024. Published by Oxford University Press on behalf of National Academy of Sciences.",2024,,PNAS Nexus,3,2,,,10.1093/pnasnexus/pgae034,,#4944,Goldstein 2024,,,1
Parents and teenagers mental health suggestions chatbot using cosine similarity,"Infant, A.; Indirapriyadharshini, A.; Narmatha, P.","Parents today have a particularly hard time empathising with their children' worries, and teenagers themselves are having trouble finding solutions. To become a better version of themselves, everyone needs counsel. The most important part of being human is having mental support, which is especially important during adolescence when a person starts to feel a variety of emotions and his or her parents are itself new to that and have to deal with it. So, we are talking about this issue in this paper to discover answers and give advice to both parents and children using a chatbot. The dataset is gathered by examining teen behavior changes, and the system is trained using NLP and cosine similarity.  © 2024 E3S Web of Conferences",2024,,E3S Web of Conferences,491,,,,10.1051/e3sconf/202449103017,,#4948,Infant 2024,,,1
Perceiving AI intervention does not compromise the persuasive effect of fact-checking,"Chae, J.H.; Tewksbury, D.","Efforts to scale up fact-checking through technology, such as artificial intelligence (AI), are increasingly being suggested and tested. This study examines whether previously observed effects of reading fact-checks remain constant when readers are aware of AI’s involvement in the fact-checking process. We conducted three online experiments (N = 3,978), exposing participants to fact-checks identified as either human-generated or AI-assisted, simulating cases where AI fully generates the fact-check or automatically retrieves human fact-checks. Our findings indicate that the persuasive effect of fact-checking, specifically in increasing truth discernment, persists even among participants without a positive prior attitude toward AI. Additionally, in some cases, awareness of AI’s role reduced perceived political bias in fact-checks among Republicans. Finally, neither AI-generated nor human fact-checks significantly affected participants’ feelings toward or their perceptions of the competence of the targeted politicians. © The Author(s) 2024.",2024,,New Media and Society,,,,,10.1177/14614448241286881,,#4954,Chae 2024,,,1
Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies,"Qi, Z.; Inaba, M.","Recent advancements in natural language processing, particularly with large language models (LLMs) like GPT-4, have significantly enhanced dialogue systems, enabling them to generate more natural and fluent conversations. Despite these improvements, challenges persist, such as managing continuous dialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024 addresses these challenges by employing the Werewolf Game, an incomplete information game, to test the capabilities of LLMs in complex interactive environments. This paper introduces a LLM-based Werewolf Game AI, where each role is supported by situation analysis to aid response generation. Additionally, for the werewolf role, various persuasion strategies, including logical appeal, credibility appeal, and emotional appeal, are employed to effectively persuade other players to align with its actions. © 2024 Association for Computational Linguistics.",2024,,"AIWolfDial 2024 - 2nd International AIWolfDial Workshop, held in conjunction with the 17th International Natural Language Generation Conference - Proceedings of the Workshop",,,30-39,,,,#4958,Qi 2024,,,1
The persuasive effects of political microtargeting in the age of generative artificial intelligence,"Simchon, A.; Edwards, M.; Lewandowsky, S.","The increasing availability of microtargeted advertising and the accessibility of generative artificial intelligence (AI) tools, such as ChatGPT, have raised concerns about the potential misuse of large language models in scaling microtargeting efforts for political purposes. Recent technological advancements, involving generative AI and personality inference from consumed text, can potentially create a highly scalable “manipulation machine” that targets individuals based on their unique vulnerabilities without requiring human input. This paper presents four studies examining the effectiveness of this putative “manipulation machine.” The results demonstrate that personalized political ads tailored to individuals’ personalities are more effective than nonpersonalized ads (studies 1a and 1b). Additionally, we showcase the feasibility of automatically generating and validating these personalized ads on a large scale (studies 2a and 2b). These findings highlight the potential risks of utilizing AI and microtargeting to craft political messages that resonate with individuals based on their personality traits. This should be an area of concern to ethicists and policy makers. © The Author(s) 2024.",2024,,PNAS Nexus,3,2,,,10.1093/pnasnexus/pgae035,,#4959,Simchon 2024,,,1
Food Assistant for Consumer Behaviour Change through Citizen Science and AI,"Casado-Mansilla, D.; Gómez-Carmona, O.; Fernández-De-Retana, M.; Muzzioli, L.; Kušar, A.; Vandevijvere, S.; López-De-Ipiña, D.","The contemporary food system within the European Union is characterised by significant sustainability and health challenges. This paper presents an innovative approach to address these issues by leveraging Citizen Science (CS) and Artificial Intelligence (AI). The study involved thirteen participants from Spain, France, and the USA who shared pictures of their food receipts over a month through WhatsApp. The receipts were analysed using optical character recognition (OCR) technology, and the data obtained was used to generate tailored nutritional and sustainable living recommendations through a Large Language Model (ChatGPT v4.0). The key objectives were to foster healthier and more environmentally friendly shopping habits and to engage consumers in a scientific process by making them active participants in data collection. The findings indicate that the integration of AI-generated advice can positively influence consumer behaviour towards more sustainable and healthy food choices. The study also assessed participant satisfaction with the AI recommendations and their self-perception in assigned consumer clusters, such as Health-Focused and Sustainability-Conscious. The majority of participants reported a high level of satisfaction with the intervention and agreed with their cluster assignment, indicating the potential of AI tools in consumer health and environmental education. This paper provides open access to the collected data and the methodologies used, to benefit the public and facilitate further research.  © 2024 University of Split, FESB.",2024,,"2024 9th International Conference on Smart and Sustainable Technologies, SpliTech 2024",,,,,10.23919/SpliTech61897.2024.10612651,,#4960,Casado-Mansilla 2024,,,1
Using Argumentation Theory to Fight Misinformation,"Figueras, B.C.","To fight misinformation, it is important to verify it, but also to ensure that true information reaches all those spaces that are likely to continue spreading hoaxes. To do this, it is necessary to generate reasoned, persuasive explanations adapted to each space and context. This project is the first to propose fighting misinformation through the automatic generation of explanatory arguments. To achieve this objective, it is necessary to create a tool that is capable of detecting where already debunked hoaxes are being replicated, deciding which is the most appropriate argumentative form to counteract it and, finally, generating the argument that can explain and persuade the spreader of the hoax or the users. For all this, an exhaustive study of the theory of argumentation is necessary, as well as the use of Natural Language Processing (NLP) techniques. Although argumentation has been studied in various contexts, there is no work to date that focuses on the analysis of the argumentation necessary for news verification. Taking advantage of the fact that much of the misinformation that spreads the most is usually verified by humans, in this work, we propose (i) generating a corpus of verifications paired with the disseminating messages associated to these, (ii) using existing theoretical frameworks to analyze the argumentation necessary to fight against misinformation, (iii) using cutting-edge techniques of language generation to explore this application, and finally (iv) exploring the impact that argumentative explanations can have in mitigating misinformation. In addition, we also want to study the use of NLP techniques as pre-bunking tools, which teach users how to detect misinformation. © 2024 Copyright for this paper by its authors.",2024,,CEUR Workshop Proceedings,3797,,114-122,,,,#4963,Figueras 2024,,,1
A comparison of the persuasiveness of human and ChatGPT generated pro-vaccine messages for HPV,"Xia, D.; Song, M.; Zhu, T.","Introduction: Public health messaging is crucial for promoting beneficial health outcomes, and the latest advancements in artificial intelligence offer new opportunities in this field. This study aimed to evaluate the effectiveness of ChatGPT-4 in generating pro-vaccine messages on different topics for Human Papillomavirus (HPV) vaccination. Methods: In this study (N = 60), we examined the persuasive effect of pro-vaccine messages generated by GPT-4 and humans, which were constructed based on 17 factors impacting HPV vaccination. Paired-samples t-tests were used to compare the persuasiveness of these messages. Results: GPT-generated messages were reported as more persuasive than human-generated messages on some influencing factors (e.g., untoward effect, stigmatized perception). Human-generated messages performed better on the message regarding convenience of vaccination. Discussion: This study provides evidence for the viability of ChatGPT, in generating persuasive pro-vaccine messages to influence people’s vaccine attitudes. It is indicated that the feasibility and efficiency of using AI for public health communication. Copyright © 2025 Xia, Song and Zhu.",2024,,Frontiers in Public Health,12,,,,10.3389/fpubh.2024.1515871,,#4975,Xia 2024,,,1
Leveraging LLMs for Mental Health: Detection and Recommendations from Social Discussions,"Aggarwal, V.; Thukral, S.; Patel, K.; Chatterjee, A.","Textual data from social platforms captures various aspects of mental health through discussions around and across issues, while users reach out for help and others sympathize and offer support. We propose a comprehensive framework that leverages Natural Language Processing (NLP) and Generative AI techniques to identify and assess mental health disorders, detect their severity, and create recommendations for behavior change and therapeutic interventions based on users' posts on Reddit. To classify the disorders, we use rule-based labeling methods as well as advanced pre-trained NLP models to extract nuanced semantic features from the data. We fine-tune domain-adapted and generic pre-trained NLP models based on predictions from specialized Large Language Models (LLMs) to improve classification accuracy. Our hybrid approach combines the generalization capabilities of pre-trained models with the domain-specific in-sights captured by LLMs, providing an improved understanding of mental health discourse. Our findings highlight the strengths and limitations of each model, offering valuable insights into their practical applicability. This research potentially facilitates early detection and personalized care to aid practitioners and aims to facilitate timely interventions and improve overall well-being, thereby contributing to the broader field of mental health surveillance and digital health analytics. © 2024 IEEE.",2024,,"Proceedings - 2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT 2024",,,350-354,,10.1109/WI-IAT62293.2024.00055,,#4978,Aggarwal 2024,,,1
Suggestive answers strategy in human-chatbot interaction: a route to engaged critical decision making,"Yamamoto, Y.","In this study, we proposed a novel chatbot interaction strategy based on the suggestive ending of answers. This strategy is inspired by the cliffhanger ending narrative technique, which ends a story without specifying conclusions to spark readers' curiosity as to what will happen next and is often used in television series. Common chatbots provide relevant and comprehensive answers to users' questions. In contrast, chatbots with our proposed strategy end their answers with hints potentially interest-triggering users. The suggestive ending strategy aims to stimulate users' inquisition for critical decision-making, relating to a psychological phenomenon where humans are often urged to finish the uncompleted tasks they have initiated. We demonstrated the implication of our strategy by conducting an online user study involving 300 participants, where they used chatbots to perform three decision-making tasks. We adopted a between-subjects factorial experimental design and compared between the following UIs: (1) plain chatbot—it provides a generated answer when participants issue a question; (2) expositive chatbot—it provides a generated answer for a question, adding short summaries of a positive and negative person's opinion for the answer; (3) suggestive chatbot—it provides a generated answer for a question, which ends with a suggestion of a positive and negative person for the answer. We found that users of the suggestive chatbot were inclined to ask more questions to the bot, engage in prolonged decision-making and information-seeking actions, and formulate their opinions from various perspectives. These findings vary with the users' experience with plain and expositive chatbots. Copyright © 2024 Yamamoto.",2024,,Frontiers in Psychology,15,,,,10.3389/fpsyg.2024.1382234,,#4980,Yamamoto 2024,,,1
Context-Aware Behavioral Tips to Improve Sleep Quality via Machine Learning and Large Language Models,"Corda, E.; Massa, S.M.; Riboni, D.","As several studies demonstrate, good sleep quality is essential for individuals’ well-being, as a lack of restoring sleep may disrupt different physical, mental, and social dimensions of health. For this reason, there is increasing interest in tools for the monitoring of sleep based on personal sensors. However, there are currently few context-aware methods to help individuals to improve their sleep quality through behavior change tips. In order to tackle this challenge, in this paper, we propose a system that couples machine learning algorithms and large language models to forecast the next night’s sleep quality, and to provide context-aware behavior change tips to improve sleep. In order to encourage adherence and to increase trust, our system includes the use of large language models to describe the conditions that the machine learning algorithm finds harmful to sleep health, and to explain why the behavior change tips are generated as a consequence. We develop a prototype of our system, including a smartphone application, and perform experiments with a set of users. Results show that our system’s forecast is correlated to the actual sleep quality. Moreover, a preliminary user study suggests that the use of large language models in our system is useful in increasing trust and engagement. © 2024 by the authors.",2024,,Future Internet,16,2,,,10.3390/fi16020046,,#4988,Corda 2024,,,1
Persuasion-enhanced computational argumentative reasoning through argumentation-based persuasive frameworks,"Ruiz-Dolz, R.; Taverner, J.; Heras Barberá, S.M.; García-Fornes, A.","One of the greatest challenges of computational argumentation research consists of creating persuasive strategies that can effectively influence the behaviour of a human user. From the human perspective, argumentation represents one of the most effective ways to reason and to persuade other parties. Furthermore, it is very common that humans adapt their discourse depending on the audience in order to be more persuasive. Thus, it is of utmost importance to take into account user modelling features for personalising the interactions with human users. Through computational argumentation, we can not only devise the optimal solution, but also provide the rationale for it. However, synergies between computational argumentative reasoning and computational persuasion have not been researched in depth. In this paper, we propose a new formal framework aimed at improving the persuasiveness of arguments resulting from the computational argumentative reasoning process. For that purpose, our approach relies on an underlying abstract argumentation framework to implement this reasoning and extends it with persuasive features. Thus, we combine a set of user modelling and linguistic features through the use of a persuasive function in order to instantiate abstract arguments following a user-specific persuasive policy. From the results observed in our experiments, we can conclude that the framework proposed in this work improves the persuasiveness of argument-based computational systems. Furthermore, we have also been able to determine that human users place a high level of trust in decision support systems when they are persuaded using arguments and when the reasons behind the suggestion to modify their behaviour are provided. © The Author(s) 2023.",2024,,User Modeling and User-Adapted Interaction,34,1,229-258,,10.1007/s11257-023-09370-1,,#4993,Ruiz-Dolz 2024,,,1
A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks,"Debuse, M.; Warnick, S.","In this work we consider the impact of information spread in time-varying social networks, where agents request to follow other agents with aligned opinions while dropping ties to neighbors whose posts are too dissimilar to their own views. Opinion control and rhetorical influence has a very long history, employing various methods including education, persuasion, propaganda, marketing, and manipulation through mis-, dis-, and mal-information. The automation of opinion controllers, however, has only recently become easily deployable at a wide scale, with the advent of large language models (LLMs) and generative AI that can translate the quantified commands from opinion controllers into actual content with the appropriate nuance. Automated agents in social networks can be deployed for various purposes, such as breaking up echo chambers, bridging valuable new connections between agents, or shaping the opinions of a target population-and all of these raise important ethical concerns that deserve serious attention and thoughtful discussion and debate. This paper attempts to contribute to this discussion by considering three archetypal influencing styles observed by human drivers in these settings, comparing and contrasting the impact of these different control methods on the opinions of agents in the network. We will demonstrate the efficacy of current generative AI for generating nuanced content consistent with the command signal from automatic opinion controllers like these, and we will report on frameworks for approaching the relevant ethical considerations. © 2024 IEEE.",2024,,Proceedings of the IEEE Conference on Decision and Control,,,5310-5317,,10.1109/CDC56724.2024.10885979,,#4994,Debuse 2024,,,1
Sniffer: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection,"Qi, P.; Yan, Z.; Hsu, W.; Lee, M.L.","Misinformation is a prevalent societal issue due to its potential high risks. Out-Of-Context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image- text consistency but lack convincing explanations for their judgments, which are essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual rea- soning and explanation generation, they still lack sophisti- cation in understanding and discovering the subtle cross- modal differences. In this paper, we introduce Sniffer,a novel multimodal large language model specifically engi- neered for OOC misinformation detection and explanation. Snifferemploys two-stage instruction tuning on Instruct- BLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities and the sec- ond stage leverages OOC-specific instruction data gener- ated by language-only GPT-4 to fine-tune the model's dis- criminatory powers. Enhanced by external tools and re- trieval, Sniffernot only detects inconsistencies between text and image but also utilizes external knowledge for con- textual verification. Our experiments show that Sniffersurpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. Snifferalso provides accurate and persuasive explanations as val- idated by quantitative and human evaluations. © 2024 IEEE.",2024,,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,,,13052-13062,,10.1109/CVPR52733.2024.01240,,#5002,Qi 2024,,,1
Legitimization of paltry favors effect and chatbot-moderated fundraising,"Lee, S.; Lee, E.; Park, Y.; Park, G.","Labor shortages and the rise of artificial intelligence (AI) technology have accelerated the application of AI chatbot agents in fundraising agencies. This study examines the applicability of the legitimization of the paltry favors (LPF) technique and chatbot image representation in raising the persuasive power of fundraising chatbots. In this study, 609 participants were recruited via a crowdsourcing website to participate in an online survey. Participants were assigned to one of six chatbot conditions (text only vs. robot image vs. human image × standard message vs. LPF message). The results revealed that the LPF message increases the willingness to donate and that self-image concern, requestor need, and guilt significantly mediate the direct effect. The results also suggest the moderating role of perceived anthropomorphism in the association between LPF messages and the willingness to donate via requester needs. The results provide theoretical implications for compliance-gaining studies and practical implications for fundraising organizations and chatbot developers. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",2024,,Current Psychology,43,10,9245-9257,,10.1007/s12144-023-05084-0,,#5005,Lee 2024,,,1
Neural Conversational Agent for Weight Loss Counseling: Protocol for an Implementation and Feasibility Study,"Kotov, A.; Carcone, A.I.; Towner, E.","Background: Obesity is a common, serious and costly chronic disease. Current clinical practice guidelines recommend that providers augment the longitudinal care of people living with obesity with consistent support for the development of self-efficacy and motivation to modify their lifestyle behaviors. Lifestyle behavior change aligns with the goals of motivational interviewing (MI), a client-centered yet directive counseling modality. However, training health care providers to be proficient in MI is expensive and time-consuming, resulting in a lack of trained counselors and limiting the widespread adoption of MI in clinical practice. Artificial intelligence (AI) counselors accessible via the internet can help circumvent these barriers. Objective: The primary objective is to explore the feasibility of conducting unscripted MI-consistent counseling using Neural Agent for Obesity Motivational Interviewing (NAOMI), a large language model (LLM)–based web app for weight loss counseling. The secondary objectives are to test the acceptability and usability of NAOMI’s counseling and examine its ability to shift motivational precursors in a sample of patients with overweight and obesity recruited from primary care clinics. Methods: NAOMI will be developed based on recent advances in deep learning in four stages. In stages 1 and 2, NAOMI will be implemented using an open-source foundation LLM and (1) few-shot learning based on a prompt with task-specific instructions and (2) domain adaptation strategy based on fine-tuning LLM using a large corpus of general psychotherapy and MI treatment transcripts. In stages 3 and 4, we will refine the best of these 2 approaches. Each NAOMI version will be evaluated using a mixed methods approach in which 10 adults (18-65 years) meeting the criteria for overweight or obesity (25.0≥BMI≤39.9) interact with NAOMI and provide feedback. NAOMI’s fidelity to the MI framework will be assessed using the Motivational Interviewing Treatment Integrity scale. Participants’ general perceptions of AI conversational agents and NAOMI specifically will be assessed via Pre- and Post-Interaction Questionnaires. Motivational precursors, such as participants’ confidence, importance, and readiness for changing lifestyle behaviors (eg, diet and activity), will be measured before and after the interaction, and 1 week later. A qualitative analysis of changes in the measures of perceptions of AI agents and counselors and motivational precursors will be performed. Participants will rate NAOMI’s usability and empathic skills post interaction via questionnaire-based assessments along with providing feedback about their experience with NAOMI via a qualitative interview. Results: NAOMI (version 1.0) has been developed. Participant recruitment will commence in September 2024. Data collection activities are expected to conclude in May 2025. Conclusions: If proven effective, LLM-based counseling agents can become a cost-effective approach for addressing the obesity epidemic at a public health level. They can also have a broad, transformative impact on the delivery of MI and other psychotherapeutic treatment modalities extending their reach and broadening access. ©Alexander Kotov, April Idalski Carcone, Elizabeth Towner.",2024,,JMIR Research Protocols,13,,,,10.2196/60361,,#5007,Kotov 2024,,,1
LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial Search for Adaptive Arguments,"Aryan, P.","This paper introduces DebateBrawl, an innovative AI-powered debate platform that integrates Large Language Models (LLMs), Genetic Algorithms (GA), and Adversarial Search (AS) to create an adaptive and engaging debating experience. DebateBrawl addresses the limitations of traditional LLMs in strategic planning by incorporating evolutionary optimization and game-theoretic techniques. The system demonstrates remarkable performance in generating coherent, contextually relevant arguments while adapting its strategy in real-time. Experimental results involving 23 debates show balanced outcomes between AI and human participants, with the AI system achieving an average score of 2.72 compared to the human average of 2.67 out of 10. User feedback indicates significant improvements in debating skills and a highly satisfactory learning experience, with 85% of users reporting improved debating abilities and 78% finding the AI opponent appropriately challenging. The system's ability to maintain high factual accuracy (92% compared to 78% in human-only debates) while generating diverse arguments addresses critical concerns in AI-assisted discourse. DebateBrawl not only serves as an effective educational tool but also contributes to the broader goal of improving public discourse through AI-assisted argumentation. The paper discusses the ethical implications of AI in persuasive contexts and outlines the measures implemented to ensure responsible development and deployment of the system, including robust fact-checking mechanisms and transparency in decision-making processes. © 2024 IEEE.",2024,,"2024 IEEE Conference on Engineering Informatics, ICEI 2024",,,,,10.1109/ICEI64305.2024.10912343,,#5012,Aryan 2024,,,1
Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation,"Saha, S.; Srihari, R.","The proliferation of online misinformation presents a significant challenge, requiring scalable strategies for effective mitigation. While detection methods exist, current reactive approaches, like content flagging and banning, are short-term and insufficient. Additionally, advancements like large language models (LLMs) exacerbate the issue by enabling large-scale creation and dissemination of misinformation. Thus, sustainable, scalable solutions that encourage behavior change and broaden perspectives by persuading misinformants against their viewpoints or broadening their perspectives are needed. To this end, we propose persuasive LLM-based dialogue systems to tackle misinformation. However, challenges arise due to the lack of suitable datasets and formal frameworks for generating persuasive responses. Inspired by existing methods for countering online hate speech, we explore adapting counter-hate response strategies for misinformation. Since misinformation and hate speech often coexist despite differing intentions, we develop classifiers to identify and annotate response strategies from hate-speech counter-responses for use in misinformation scenarios. Human evaluations show a 91% agreement on the applicability of these strategies to misinformation. Next, as a scalable counter-misinformation solution, we create an LLM-based argument graph framework that generates persuasive responses, using the strategies as control codes to adjust the style and content. Human evaluations and case studies demonstrate that our framework generates expert-like responses and is 14% more engaging, 21% more natural, and 18% more factual than the best available alternatives. © 2024 Association for Computational Linguistics.",2024,,"EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,11109-11124,,10.18653/v1/2024.emnlp-main.622,,#5015,Saha 2024,,,1
How Can Client Motivational Language Inform Psychotherapy Agents?,"Hoang, V.; Rogers, E.; Ross, R.","Within Motivational Interviewing (MI), client utterances are coded as for or against a certain behaviour change, along with commitment strength; this is essential to ensure therapists soften rather than persisting goal-related actions in the face of resistance. Prior works in MI agents have been scripted or semi-scripted, limiting users’ natural language expressions. With the aim of automating the MI interactions, we propose and explore the task of automated identification of client motivational language. Employing Large Language Models (LLMs), we compare in-context learning (ICL) and instruction fine-tuning (IFT) with varying training sizes for this identification task. Our experiments show that both approaches can learn under low-resourced settings. Our results demonstrate that IFT, though cheaper, is more stable to prompt choice, and yields better performance with more data. Given the detected motivation, we further present an approach to the analysis of therapists’ strategies for balancing building rapport with clients with advancing the treatment plan. A framework of MI agents is developed using insights from the data and the psychotherapy literature. ©2024 Association for Computational Linguistics.",2024,,"CLPsych 2024 - 9th Workshop on Computational Linguistics and Clinical Psychology, Proceedings of the Workshop",,,23-40,,,,#5028,Hoang 2024,,,1
Persuading across Diverse Domains: A Dataset and Persuasion Large Language Model,"Jin, C.; Ren, K.; Kong, L.; Wang, X.; Song, R.; Chen, H.","Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrieval-based models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT-4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of user's utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io. © 2024 Association for Computational Linguistics.",2024,,Proceedings of the Annual Meeting of the Association for Computational Linguistics,1,,1678-1706,,10.18653/v1/2024.acl-long.92,,#5029,Jin 2024,,,1
Be Persuasive! Automatic Transformation of Virtual Agent’s Head and Facial Behavior,"Cherni, A.; Bertrand, R.; Ochs, M.","The persuasiveness of a virtual agent refers to its ability to influence, persuade, or motivate users to take specific actions or adopt certain attitudes or beliefs. Virtual agents can use its multimodal capabilities, including non-verbal cues to enhance their persuasiveness. In this paper, we present a new tool called THRUST (from neuTral Human face to peRsUaSive virTual face) to automatically generate the head movements and facial expressions of a persuasive virtual character. This tool is based on a machine learning approach from a human videos corpus to identify the non-verbal persuasive cues. A convolution-based model then transforms neutral non-verbal behavior to a persuasive non-verbal behavior simulated on a virtual face. Videos generated by the tool have been evaluated through a subjective perceptive study with about 90 participants. The results show that the virtual agent’s head and facial behaviors generated by the THRUST tool are perceived as persuasive, thus validating the proposed approach. © 2024, Science and Technology Publications, Lda. All rights reserved.",2024,,International Conference on Agents and Artificial Intelligence,1,,359-366,,10.5220/0012429700003636,,#5032,Cherni 2024,,,1
Characterizing the Adoption and Experiences of Users of Artificial Intelligence-Generated Health Information in the United States: Cross-Sectional Questionnaire Study,"Ayo-Ajibola, O.; Davis, R.J.; Lin, M.E.; Riddell, J.; Kravitz, R.L.","Background: OpenAI's ChatGPT is a source of advanced online health information (OHI) that may be integrated into individuals' health information-seeking routines. However, concerns have been raised about its factual accuracy and impact on health outcomes. To forecast implications for medical practice and public health, more information is needed on who uses the tool, how often, and for what. Objective: This study aims to characterize the reasons for and types of ChatGPT OHI use and describe the users most likely to engage with the platform. Methods: In this cross-sectional survey, patients received invitations to participate via the ResearchMatch platform, a nonprofit affiliate of the National Institutes of Health. A web-based survey measured demographic characteristics, use of ChatGPT and other sources of OHI, experience characterization, and resultant health behaviors. Descriptive statistics were used to summarize the data. Both 2-tailed t tests and Pearson chi-square tests were used to compare users of ChatGPT OHI to nonusers. Results: Of 2406 respondents, 21.5% (n=517) respondents reported using ChatGPT for OHI. ChatGPT users were younger than nonusers (32.8 vs 39.1 years, P< .001) with lower advanced degree attainment (BA or higher; 49.9% vs 67%, P< .001) and greater use of transient health care (ED and urgent care; P< .001). ChatGPT users were more avid consumers of general non-ChatGPT OHI (percentage of weekly or greater OHI seeking frequency in past 6 months, 28.2% vs 22.8%, P< .001). Around 39.3% (n=206) respondents endorsed using the platform for OHI 2-3 times weekly or more, and most sought the tool to determine if a consultation was required (47.4%, n=245) or to explore alternative treatment (46.2%, n=239). Use characterization was favorable as many believed ChatGPT to be just as or more useful than other OHIs (87.7%, n=429) and their doctor (81%, n=407). About one-third of respondents requested a referral (35.6%, n=184) or changed medications (31%, n=160) based on the information received from ChatGPT. As many users reported skepticism regarding the ChatGPT output (67.9%, n=336), most turned to their physicians (67.5%, n=349). Conclusions: This study underscores the significant role of AI-generated OHI in shaping health-seeking behaviors and the potential evolution of patient-provider interactions. Given the proclivity of these users to enact health behavior changes based on AI-generated content, there is an opportunity for physicians to guide ChatGPT OHI users on an informed and examined use of the technology. © Oluwatobiloba Ayo-Ajibola, Ryan J Davis, Matthew E Lin, Jeffrey Riddell, Richard L Kravitz.",2024,,Journal of Medical Internet Research,26,,,,10.2196/55138,,#5035,Ayo-Ajibola 2024,,,1
The Impact of Physical Anthropomorphism in Social Robots on User Compliance: The Moderating Effect of Issue Involvement,"Yi, E.; Park, D.-H.","This study investigates how the level of physical anthropomorphism in conversational agents affects their persuasiveness. The experiment compared robots with low and high levels of anthropomorphism, akin to human appearance. Results indicated that while sophisticated physical embodiment did not universally enhance persuasiveness, it did have an interaction effect based on issue involvement. For low involvement issues, persuasiveness increased with higher anthropomorphism. In contrast, for high involvement issues, persuasiveness remained constant across different levels of anthropomorphism but dropped when robots closely resembled humans, possibly due to trust in high-tech robots. The findings highlight the importance of digital agent's persuasive power and ethics in employing and designing robots.  © 2024 IEEE.",2024,,"IEEE International Workshop on Robot and Human Communication, RO-MAN",,,1544-1549,,10.1109/RO-MAN60168.2024.10731171,,#5037,Yi 2024,,,1
Navigating healthier beverage consumption in adolescents using the “R-Ma Bot” chatbot: A usability and evaluation study,"Lee, J.; Lee, H.","Objective: This pilot study aimed to evaluate the usability and effectiveness of a behavior change technique (BCT)-based chatbot developed to promote healthier beverage consumption among adolescents. Methods: The Read and Manage your health roBot (“R-Ma Bot”), designed with 13 BCTs, was tested with 42 adolescents (13 men, 29 women, mean age 15.0 ± 0.7) for 2 weeks. Usability was assessed after the 2-week intervention using a chatbot usability questionnaire, recruitment, retention, participation, and engagement. Scores above 70 out of 100 were considered high usability. Qualitative data from open-ended questions were collected for evaluation. Effectiveness was measured by changes in knowledge, use and impact of nutrition labels, and weekly consumption of sugar, sodium, and caffeine from carbonated and/or energy drinks before and after the 2-week intervention. Results: The score of R-Ma Bot's usability averaged 74.7, with participants addressing it useful, friendly, and easy to use, though they suggested improving unnatural conversation flow. All participants engaged with the chatbot for at least 13 out of 14 days, with over half using it daily for the entire period. After intervention, awareness of nutrition labels increased from 64.3% to 92.9%, and nonreaders decreased from 42.9% to 16.7%. Weekly sugar intake from beverages significantly decreased by 60%, from 13.1 ± 20.1 mg to 7.9 ± 12.8 mg. Conclusions: R-Ma Bot's high usability contributed to high retention and behavioral changes, significantly reduced sugar consumption from beverages and improved awareness of nutrition labels. We suggest integrating strategies that enhance knowledge, motivation, and opportunities through BCTs with youth-friendly design elements in the development of interventions for adolescents. © The Author(s) 2024.",2024,,Digital Health,10,,,,10.1177/20552076241283243,,#5039,Lee 2024,,,1
Mental Wellness Self-Care in Singapore With mindline.sg: A Tutorial on the Development of a Digital Mental Health Platform for Behavior Change,"Weng, J.H.; Hu, Y.; Heaukulani, C.; Tan, C.; Chang, J.K.; Phang, Y.S.; Rajendram, P.; Tan, W.M.; Loke, W.C.; Morris, R.J.T.","Background: Singapore, like the rest of Asia, faces persistent challenges to mental health promotion, including stigma around unwellness and seeking treatment and a lack of trained mental health personnel. The COVID-19 pandemic, which created a surge in mental health care needs and simultaneously accelerated the adoption of digital health solutions, revealed a new opportunity to quickly scale innovative solutions in the region. Objective: In June 2020, the Singaporean government launched mindline.sg, an anonymous digital mental health resource website that has grown to include >500 curated local mental health resources, a clinically validated self-assessment tool for depression and anxiety, an artificial intelligence (AI) chatbot from Wysa designed to deliver digital therapeutic exercises, and a tailored version of the website for working adults called mindline at work. The goal of the platform is to empower Singapore residents to take charge of their own mental health and to be able to offer basic support to those around them through the ease and convenience of a barrier-free digital solution. Methods: Website use is measured through click-level data analytics captured via Google Analytics and custom application programming interfaces, which in turn drive a customized analytics infrastructure based on the open-source platforms Titanium Database and Metabase. Unique, nonbounced (users that do not immediately navigate away from the site), engaged, and return users are reported. Results: In the 2 years following launch (July 1, 2020, through June 30, 2022), the website received >447,000 visitors (approximately 15% of the target population of 3 million), 62.02% (277,727/447,783) of whom explored the site or engaged with resources (referred to as nonbounced visitors); 10.54% (29,271/277,727) of those nonbounced visitors returned. The most popular features on the platform were the dialogue-based therapeutic exercises delivered by the chatbot and the self-assessment tool, which were used by 25.54% (67,626/264,758) and 11.69% (32,469/277,727) of nonbounced visitors. On mindline at work, the rates of nonbounced visitors who engaged extensively (ie, spent ≥40 seconds exploring resources) and who returned were 51.56% (22,474/43,588) and 13.43% (5,853/43,588) over a year, respectively, compared to 30.9% (42,829/138,626) and 9.97% (13,822/138,626), respectively, on the generic mindline.sg site in the same year. Conclusions: The site has achieved desired reach and has seen a strong growth rate in the number of visitors, which required substantial and sustained digital marketing campaigns and strategic outreach partnerships. The site was careful to preserve anonymity, limiting the detail of analytics. The good levels of overall adoption encourage us to believe that mild to moderate mental health conditions and the social factors that underly them are amenable to digital interventions. While mindline.sg was primarily used in Singapore, we believe that similar solutions with local customization are widely and globally applicable. © 2024 JMIR Publications Inc.. All rights reserved.",2024,,Journal of Medical Internet Research,26,,,,10.2196/44443,,#5041,Weng 2024,,,1
Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,"Kolenik, T.; Schiepek, G.; Gams, M.","Background: The importance of computational psychotherapy is increasing due to the record high prevalence of mental health issues worldwide. Despite advancements, current computational psychotherapy systems lack advanced prediction and behavior change mechanisms using conversational agents. Purpose: This work presents a computational psychotherapy system for mental health prediction and behavior change using a conversational agent. It makes two major contributions. First, we introduce a novel, golden standard dataset, comprising panel data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires and qualitative daily diary entries. Second, we present the computational psychotherapy system itself. Hypothesis: We hypothesize that simulating a theory of mind-the human cognitive ability to understand others – in a conversational agent enhances its effectiveness in relieving mental health issues. Methods: The system simulates theory of mind with a cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies. The system was evaluated through a computational experiment on mental health phenomena prediction from text, and an empirical interventional study on relieving mental health issues in 42 participants. Results: The system outperformed state-of-the-art systems in terms of the number of detected categories and detection accuracy (highest accuracy: 91.41% using k-nearest neighbors (kNN); highest accuracy of other systems: 84% using long-short term memory network (LSTM)). The highest accuracy for 7-day forecasting was 87.68%, whereas the other systems were not able to forecast trends. In the study, the system outperformed Woebot, the current state-of-the-art, in reducing stress (p = 0.004) and anxiety (p = 0.008) levels. Conclusion: The confirmation of our hypothesis indicates that incorporating theory of mind simulation in conversational agents significantly enhances their efficacy in computational psychotherapy, offering a promising advancement for mental health interventions and support compared to current state-of-the-art systems. © 2024 Kolenik et al.",2024,,Neuropsychiatric Disease and Treatment,20,,2465-2498,,10.2147/NDT.S417695,,#5061,Kolenik 2024,,,1
A Persuasive Chatbot in an Aging-in-Place Environment,"Chua, A.J.; Liu, S.; Zhang, H.; Fan, X.","With the increase of the senior citizen population worldwide, there is a growing need to create a technology-enabled environment that support older adults in living independently. This paper presents the development of a persuasive chatbot integrated with an aging in place solution, designed to assist senior citizens in leading healthy lifestyles. By utilizing a computational model of adaptive persuasion, the chatbot can effectively encourage users to engage in suitable exercises based on their health conditions and behavioral data collected from the aging in place system. Various persuasion strategies are available, tailored to the user’s motivation and personal profile. A pilot study was conducted to assess the effectiveness of the chatbot in promoting exercise adherence and explore its usability. The study results demonstrate the chatbot’s effectiveness in persuading the majority of participants to follow the recommended exercise routines, and overall user satisfaction with the chatbot’s usage. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",2024,,Lecture Notes in Networks and Systems,794,,347-359,,10.1007/978-3-031-44981-9_29,,#5062,Chua 2024,,,1
Delivering Green Persuasion Strategies with a Conversational Agent: a Pilot Study,"Giudici, M.; Abbo, G.A.; Crovari, P.; Garzotto, F.","Climate change is undeniable. The drastic consequences it may have on our lives make a collective effort crucial. Our research explores how Conversational Agents (CAs) can persuade people into environmentally sustainable behaviors, particularly in domestic spaces where these technologies are becoming increasingly popular. In this research work, we conducted an empirical evaluation (N=29) exploring the effectiveness and stance towards the adoption of different persuasive strategies compared to a CA delivering messages referring to just one persuasion strategy. Furthermore, this contribution reports on a custom dialogue manager's implementation, designed to enable the execution of the experiment. Although study results suggested no significant difference in persuasion effectiveness and usability of the conversational agents, participants reported a significant difference in the perceptions of parasocial interactions and dialogue with the CA, preferring the one delivering multiple persuasive strategies. © 2024 IEEE Computer Society. All rights reserved.",2024,,Proceedings of the Annual Hawaii International Conference on System Sciences,,,811-820,,,,#5064,Giudici 2024,,,1
"Targeting Key Risk Factors for Cardiovascular Disease in At-Risk Individuals: Developing a Digital, Personalized, and Real-Time Intervention to Facilitate Smoking Cessation and Physical Activity","Versluis, A.; Penfornis, K.M.; van der Burg, S.A.; Scheltinga, B.L.; van Vliet, M.H.M.; Albers, N.; Meijer, E.","Health care is under pressure due to an aging population with an increasing prevalence of chronic diseases, including cardiovascular disease. Smoking and physical inactivity are 2 key preventable risk factors for cardiovascular disease. Yet, as with most health behaviors, they are difficult to change. In the interdisciplinary Perfect Fit project, scientists from different fields join forces to develop an evidence-based virtual coach (VC) that supports smokers in quitting smoking and increasing their physical activity. In this Viewpoint paper, intervention content, design, and implementation, as well as lessons learned, are presented to support other research groups working on similar projects. A total of 6 different approaches were used and combined to support the development of the Perfect Fit VC. The approaches used are (1) literature reviews, (2) empirical studies, (3) collaboration with end users, (4) content and technical development sprints, (5) interdisciplinary collaboration, and (6) iterative proof-of-concept implementation. The Perfect Fit intervention integrates evidence-based behavior change techniques with new techniques focused on identity change, big data science, sensor technology, and personalized real-time coaching. Intervention content of the virtual coaching matches the individual needs of the end users. Lessons learned include ways to optimally implement and tailor interactions with the VC (eg, clearly explain why the user is asked for input and tailor the timing and frequency of the intervention components). Concerning the development process, lessons learned include strategies for effective interdisciplinary collaboration and technical development (eg, finding a good balance between end users’ wishes and legal possibilities). The Perfect Fit development process was collaborative, iterative, and challenging at times. Our experiences and lessons learned can inspire and benefit others. Advanced, evidence-based digital interventions, such as Perfect Fit, can contribute to a healthy society while alleviating health care burden. ©Anke Versluis, Kristell M Penfornis, Sven A van der Burg, Bouke L Scheltinga, Milon H M van Vliet, Nele Albers, Eline Meijer.",2024,,JMIR Pediatrics and Parenting,8,,,,10.2196/47730,,#5067,Versluis 2024,,,1
Promoting pro-environmental behaviour spillover through chatbots,"Majid, G.M.; Tussyadiah, I.; Kim, Y.R.; Chen, J.L.","Given proper facilitation, pro-environmental behaviour in tourist destinations may spill over to the daily lives of tourists. Recent advancements in conversational artificial intelligence (AI) may lead to the emergence of more effective technology that will encourage people to become more environmentally friendly. However, limited research has been devoted to understanding the complexity of using such technologies for pro-social nudging. This study applies rigorous scale development procedures to test how advanced conversational AI, like chatbots, can be effective in encouraging pro-environmental behaviour spillover. The conceptual model introduces seven factors that predict people’s intention to use the chatbot and three factors that predict their intention to embrace the target behaviour. Performance expectancy and timing were the most significant in predicting the intention of individuals to adopt the technology. Efficiency and government support were the chief drivers of the intention to adopt pro-environmental behaviour. The findings suggest that although introducing a technology-mediated nudging mechanism can trigger people’s intention to behave pro-environmentally, other factors must be fulfilled in order to ensure sustained behaviour change in society. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",2024,,Journal of Sustainable Tourism,,,,,10.1080/09669582.2024.2393256,,#5068,Majid 2024,,,1
The media literacy dilemma: can ChatGPT facilitate the discernment of online health misinformation?,"Peng, W.; Meng, J.; Ling, T.-W.","Online health misinformation carries serious social and public health implications. A growing prevalence of sophisticated online health misinformation employs advanced persuasive tactics, making misinformation discernment progressively more challenging. Enhancing media literacy is a key approach to improving the ability to discern misinformation. The objective of the current study was to examine the feasibility of using generative AI to dissect persuasive tactics as a media literacy scaffolding tool to facilitate online health misinformation discernment. In a mixed 3 (media literacy tool: control vs. National Library of Medicine [NLM] checklist vs. ChatGPT tool) × 2 (information type: true information vs. misinformation) × 2 (information evaluation difficulty: hard vs. easy) online experiment, we found that using dissecting persuasive strategies of ChatGPT can be equally effective when compared with the NLM checklist, and that information type was a significant moderator such that the ChatGPT tool was more effective in helping people identify true information than misinformation. However, the ChatGPT tool performed worse than control in terms of helping people discern misinformation. No difference was found in terms of perceived usefulness and future use intention of the ChatGPT tool and the NLM checklist. The results suggest that more interactive or conversational features might enhance usefulness of ChatGPT as a media literacy tool. Copyright © 2024 Peng, Meng and Ling.",2024,,Frontiers in Communication,9,,,,10.3389/fcomm.2024.1487213,,#5070,Peng 2024,,,1
Investigating the Role of Fluency and Epistemic Justification in the Context of AI-driven Recruitment,"Parthasarathy, V.P.; Chua, A.Y.K.","The purpose of the study is to understand how message fluency and users’ epistemic justification shift perspectives on AI-driven recruitment. Using the S-O-R as the theoretical foundation, a two-group quasi-experimental design with pretest and posttest measures was carried out among 211 participants to measure their willingness to engage and attitude change. Two major results are gleaned from this study. First, message fluency impacted attitude change. In the fluent condition, participants showed a positive attitude towards the presented information on AI-driven recruitment whereas in the disfluent condition, there was no significant change. Two, users’ epistemic justification determined their willingness to engage with information on AI-driven recruitment and had a positive effect on attitude. This research recommends organizations to maximize message fluency with clear typography, structured layouts, and simplified language for better metacognitive experience. Additionally, organizational leaders can integrate epistemic justification in their processes by including testimony of experts and consensus from multiple sources to ground the information with well-supported evidence. © 2024 IEEE.",2024,,"8th International Conference on Business and Information Management, ICBIM 2024",,,07-Dec,,10.1109/ICBIM63313.2024.10823581,,#5077,Parthasarathy 2024,,,1
Gamification of Behavior Change: Mathematical Principle and Proof-of-Concept Study,"Lieder, F.; Chen, P.-Z.; Prentice, M.; Amo, V.; Tošić, M.","Background: Many people want to build good habits to become healthier, live longer, or become happier but struggle to change their behavior. Gamification can make behavior change easier by awarding points for the desired behavior and deducting points for its omission. Objective: In this study, we introduced a principled mathematical method for determining how many points should be awarded or deducted for the enactment or omission of the desired behavior, depending on when and how often the person has succeeded versus failed to enact it in the past. We called this approach optimized gamification of behavior change. Methods: As a proof of concept, we designed a chatbot that applies our optimized gamification method to help people build healthy water-drinking habits. We evaluated the effectiveness of this gamified intervention in a 40-day field experiment with 1 experimental group (n=43) that used the chatbot with optimized gamification and 2 active control groups for which the chatbot's optimized gamification feature was disabled. For the first control group (n=48), all other features were available, including verbal feedback. The second control group (n=51) received no feedback or reminders. We measured the strength of all participants' water-drinking habits before, during, and after the intervention using the Self-Report Habit Index and by asking participants on how many days of the previous week they enacted the desired habit. In addition, all participants provided daily reports on whether they enacted their water-drinking intention that day. Results: A Poisson regression analysis revealed that, during the intervention, users who received feedback based on optimized gamification enacted the desired behavior more often (mean 14.71, SD 6.57 times) than the active (mean 11.64, SD 6.38 times; P< .001; incidence rate ratio=0.80, 95% CI 0.71-0.91) or passive (mean 11.64, SD 5.43 times; P= .001; incidence rate ratio=0.78, 95% CI 0.69-0.89) control groups. The Self-Report Habit Index score significantly increased in all conditions (P< .001 in all cases) but did not differ between the experimental and control conditions (P > .11 in all cases). After the intervention, the experimental group performed the desired behavior as often as the 2 control groups (P≥.17 in all cases). Conclusions: Our findings suggest that optimized gamification can be used to make digital behavior change interventions more effective. Trial Registration: Open Science Framework (OSF) H7JN8; https://osf.io/h7jn8 © 2024 JMIR Publications Inc.. All rights reserved.",2024,,JMIR Serious Games,12,1,,,10.2196/43078,,#5079,Lieder 2024,,,1
Co-Designing a Smoking Cessation Chatbot: Focus Group Study of End Users and Smoking Cessation Professionals,"Bendotti, H.; Lawler, S.; Ireland, D.; Gartner, C.; Marshall, H.M.","Background: Our prototype smoking cessation chatbot, Quin, provides evidence-based, personalized support delivered via a smartphone app to help people quit smoking. We developed Quin using a multiphase program of co-design research, part of which included focus group evaluation of Quin among stakeholders prior to clinical testing. Objective: This study aimed to gather and compare feedback on the user experience of the Quin prototype from end users and smoking cessation professionals (SCPs) via a beta testing process to inform ongoing chatbot iterations and refinements. Methods: Following active and passive recruitment, we conducted web-based focus groups with SCPs and end users from Queensland, Australia. Participants tested the app for 1-2 weeks prior to focus group discussion and could also log conversation feedback within the app. Focus groups of SCPs were completed first to review the breadth and accuracy of information, and feedback was prioritized and implemented as major updates using Agile processes prior to end user focus groups. We categorized logged in-app feedback using content analysis and thematically analyzed focus group transcripts. Results: In total, 6 focus groups were completed between August 2022 and June 2023; 3 for SCPs (n=9 participants) and 3 for end users (n=7 participants). Four SCPs had previously smoked, and most end users currently smoked cigarettes (n=5), and 2 had quit smoking. The mean duration of focus groups was 58 (SD 10.9; range 46-74) minutes. We identified four major themes from focus group feedback: (1) conversation design, (2) functionality, (3) relationality and anthropomorphism, and (4) role as a smoking cessation support tool. In response to SCPs’ feedback, we made two major updates to Quin between cohorts: (1) improvements to conversation flow and (2) addition of the “Moments of Crisis” conversation tree. Participant feedback also informed 17 recommendations for future smoking cessation chatbot developments. Conclusions: Feedback from end users and SCPs highlighted the importance of chatbot functionality, as this underpinned Quin’s conversation design and relationality. The ready accessibility of accurate cessation information and impartial support that Quin provided was recognized as a key benefit for end users, the latter of which contributed to a feeling of accountability to the chatbot. Findings will inform the ongoing development of a mature prototype for clinical testing. ©Hollie Bendotti, Sheleigh Lawler, David Ireland, Coral Gartner, Henry M Marshall.",2024,,JMIR Human Factors,11,,,,10.2196/56505,,#5080,Bendotti 2024,,,1
Habit Coach: Customising RAG-based chatbots to support behavior change,"Arabi, A.F.M.; Koyuturk, C.; O'Mahony, M.; Calati, R.; Ognibene, D.","This paper presents the iterative development of Habit Coach, a GPT-based chatbot designed to support users in habit change through personalized interaction. Employing a user-centered design approach, we developed the chatbot using a Retrieval-Augmented Generation (RAG) system, which enables behavior personalization without retraining the underlying language model (GPT-4). The system leverages document retrieval and specialized prompts to tailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and narrative therapy techniques. A key challenge in the development process was the difficulty of translating declarative knowledge into effective interaction behaviors. In the initial phase, the chatbot was provided with declarative knowledge about CBT via reference textbooks and high-level conversational goals. However, this approach resulted in imprecise and inefficient behavior, as the GPT model struggled to convert static information into dynamic and contextually appropriate interactions. This highlighted the limitations of relying solely on declarative knowledge to guide chatbot behavior, particularly in nuanced, therapeutic conversations. Over four iterations, we addressed this issue by gradually transitioning towards procedural knowledge, refining the chatbot's interaction strategies and improving its overall effectiveness. In the final evaluation, 5 participants engaged with the chatbot over five consecutive days, receiving individualized CBT interventions. The Self-Report Habit Index (SRHI) was used to measure habit strength before and after the intervention, revealing a reduction in habit strength post-intervention. These results underscore the importance of procedural knowledge in driving effective, personalized behavior change support in RAG-based systems. © 2024 for this paper by its authors.",2024,,CEUR Workshop Proceedings,3903,,57-70,,,,#5082,Arabi 2024,,,1
Engagement with mHealth Alcohol Interventions: User Perspectives on an App or Chatbot-Delivered Program to Reduce Drinking,"Sedotto, R.N.M.; Edwards, A.E.; Dulin, P.L.; King, D.K.","Research suggests participant engagement is a key mediator of mHealth alcohol interventions’ effectiveness in reducing alcohol consumption among users. Understanding the features that promote engagement is critical to maximizing the effectiveness of mHealth-delivered alcohol interventions. The purpose of this study was to identify facilitators and barriers to mHealth alcohol intervention utilization among hazardous-drinking participants who were randomized to use either an app (Step Away) or Artificial Intelligence (AI) chatbot-based intervention for reducing drinking (the Step Away chatbot). We conducted semi-structured interviews from December 2019 to January 2020 with 20 participants who used the app or chatbot for three months, identifying common facilitators and barriers to use. Participants of both interventions reported that tracking their drinking, receiving feedback about their drinking, feeling held accountable, notifications about high-risk drinking times, and reminders to track their drinking promoted continued engagement. Positivity, personalization, gaining insight into their drinking, and daily tips were stronger facilitator themes among bot users, indicating these may be strengths of the AI chatbot-based intervention when compared to a user-directed app. While tracking drinking was a theme among both groups, it was more salient among app users, potentially due to the option to quickly track drinks in the app that was not present with the conversational chatbot. Notification glitches, technology glitches, and difficulty with tracking drinking data were usage barriers for both groups. Lengthy setup processes were a stronger barrier for app users. Repetitiveness of the bot conversation, receipt of non-tailored daily tips, and inability to self-navigate to desired content were reported as barriers by bot users. To maximize engagement with AI interventions, future developers should include tracking to reinforce behavior change self-monitoring and be mindful of repetitive conversations, lengthy setup, and pathways that limit self-directed navigation. © 2024 by the authors.",2024,,Healthcare (Switzerland),12,1,,,10.3390/healthcare12010101,,#5083,Sedotto 2024,,,1
Explanations of Symbolic Reasoning to Effect Patient Persuasion and Education,"Van Woensel, W.; Scioscia, F.; Loseto, G.; Seneviratne, O.; Patton, E.; Abidi, S.","Artificial Intelligence (AI) models can issue smart, context-sensitive recommendations to help patients self-manage their illnesses, including medication regimens, dietary habits, physical activity, and avoiding flare-ups. Instead of merely positing an “edict,” the AI model can also explain why the recommendation was issued: why one should stay indoors (e.g., increased risk of flare-ups), why further calorie intake should be avoided (e.g., met the daily limit), or why the care provider should be contacted (e.g., prescription change). The goal of these explanations is to achieve understanding and persuasion effects, which, in turn, targets education and long-term behavior change. Symbolic AI models facilitate explanations as they are able to offer logical proofs of inferences (or recommendations) from which explanations can be generated. We implemented a modular framework called XAIN (eXplanations for AI in Notation3) to explain symbolic reasoning inferences in a trace-based, contrastive, and counterfactual way. We applied this framework to explain recommendations for Chronic Obstructive Pulmonary Disease (COPD) patients to avoid flare-ups. For evaluation, we propose a questionnaire that captures understanding, persuasion, education, and behavior change, together with traditional XAI metrics including fidelity (soundness, completeness) and interpretability (parsimony, clarity). © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",2024,,Communications in Computer and Information Science,2020 CCIS,,62-71,,10.1007/978-3-031-54303-6_7,,#5086,VanWoensel 2024,,,1
Sequencing conversational turns in peer interactions: An integrated approach for evidence-based conversational agent for just-in-time nicotine cravings intervention,"Singh, T.; Truong, M.; Roberts, K.; Myneni, S.","Background: Risky health behaviors place an enormous toll on public health systems. While relapse prevention support is integrated with most behavior modification programs, the results are suboptimal. Recent advances in artificial intelligence (AI) applications provide us with unique opportunities to develop just-in-time adaptive behavior change solutions. Methods: In this study, we present an innovative framework, grounded in behavioral theory, and enhanced with social media sequencing and communications scenario builder to architect a conversational agent (CA) specialized in the prevention of relapses in the context of tobacco cessation. We modeled peer interaction data (n = 1000) using the taxonomy of behavior change techniques (BCTs) and speech act (SA) theory to uncover the socio-behavioral and linguistic context embedded within the online social discourse. Further, we uncovered the sequential patterns of BCTs and SAs from social conversations (n = 339,067). We utilized grounded theory-based techniques for extracting the scenarios that best describe individuals’ needs and mapped them into the architecture of the virtual CA. Results: The frequently occurring sequential patterns for BCTs were comparison of behavior and feedback and monitoring; for SAs were directive and assertion. Five cravings-related scenarios describing users’ needs as they deal with nicotine cravings were identified along with the kinds of behavior change constructs that are being elicited within those scenarios. Conclusions: AI-led virtual CAs focusing on behavior change need to employ data-driven and theory-linked approaches to address issues related to engagement, sustainability, and acceptance. The sequential patterns of theory and intent manifestations need to be considered when developing effective behavior change CAs. © The Author(s) 2024.",2024,,Digital Health,10,,,,10.1177/20552076241228430,,#5095,Singh 2024,,,1
EcoTrack: A Mobile Application for Real-Time Carbon Footprint Tracking and Sustainable Living,"Dash, A.; Mathur, B.; Patil, P.","Climate change poses a significant threat to the planet, with individual carbon footprints contributing to the global crisis. Despite increased awareness, many individuals struggle to track and reduce their environmental impact due to a lack of accessible tools. This paper presents EcoTrack, an innovative mobile app that helps users track and lower their carbon footprint by monitoring daily activities, including travel, food consumption, and appliance usage. The app offers a comprehensive carbon footprint calculator, a visual progress tracker, and a chatbot that provides personalized sustainability advice. Additionally, EcoTrack includes a resource page with educational content and a rewards system to incentivize eco-friendly behavior. Through user-centered design and integration of AI-powered recommendations, EcoTrack aims to foster long-term behavior change towards sustainability. This paper discusses the app's architecture, implementation, and evaluation, demonstrating its potential to make sustainable living more achievable for users. The results from preliminary testing indicate that EcoTrack effectively raises awareness and encourages users to adopt more sustainable practices. © 2024 IEEE.",2024,,"2024 IEEE International Multi-Conference on Smart Systems and Green Process, IMC-SSGP 2024",,,,,10.1109/IMC-SSGP63352.2024.10919812,,#5098,Dash 2024,,,1
Deception detection in educational AI: challenges for Japanese middle school students in interacting with generative AI robots,"Salem, A.; Sumi, K.","Educational materials that utilize generative AI (e.g., ChatGPT) have been developed, thus, allowing students to learn through conversations with robots or agents. However, if these artificial entities provide incorrect information (hallucinating), it could lead to confusion among students. To investigate whether students can detect lies from these artificial entities, we conducted an experiment using the social robot Furhat and we make it engage in various types of deceptive interactions. Twenty-two Japanese middle school students participated in ten teaching sessions with Furhat using a human and an anime facial appearances while employing different types of deception: Lying, Paltering, Pandering, and Bullshit. The results revealed that the majority of students were deceived by those lies. Additionally, the robot's facial appearance (i.e., social agency) affected both the learning effectiveness and the likelihood of being deceived. We conclude that an anime robot face is recommended to be used as it excelled in learning effectiveness as it attracts students attention. An anime face also provided protection against deceptive techniques due to its low social agency which leads to ineffectiveness in persuasion and deception. This study underscores the importance of preparing AI-based educational tools and scripts carefully to prevent the dissemination of false information produced through generative AI hallucinations to students. Copyright © 2024 Salem and Sumi.",2024,,Frontiers in Artificial Intelligence,7,,,,10.3389/frai.2024.1493348,,#5101,Salem 2024,,,1
Protocol of a mixed-methods evaluation of Perfect Fit: A personalized mHealth intervention with a virtual coach to promote smoking cessation and physical activity in adults,"van Vliet, M.H.M.; Versluis, A.; Chavannes, N.H.; Scheltinga, B.L.; Albers, N.; Penfornis, K.M.; Baccinelli, W.; Meijer, E.","Objective: Adopting healthy behavior is vital for preventing chronic diseases. Mobile health (mHealth) interventions utilizing virtual coaches (i.e., artificial intelligence conversational agents) can offer scalable and cost-effective solutions. Additionally, targeting multiple unhealthy behaviors, like low physical activity and smoking, simultaneously seems beneficial. We developed Perfect Fit, an mHealth intervention with a virtual coach providing personalized feedback to simultaneously promote smoking cessation and physical activity. Through innovative methods (e.g., sensor technology) and iterative development involving end-users, we strive to overcome challenges encountered by mHealth interventions, such as shortage of evidence-based interventions and insufficient personalization. This paper outlines the content of Perfect Fit and the protocol for evaluating its feasibility, acceptability, and preliminary effectiveness, the role of participant characteristics, and the study's feasibility. Methods: A single-arm, mixed-method, real-world evaluation study will be conducted in the Netherlands. We aim to recruit 100 adult daily smokers intending to quit within 6 weeks. The personalized intervention will last approximately 16 weeks. Primary outcomes include Perfect Fit's feasibility and acceptability. Secondary outcomes are preliminary effectiveness and study feasibility, and we will measure participant characteristics. Quantitative data will be collected through questionnaires administered at baseline, post-intervention and 2, 6, and 12 months post-intervention. Qualitative data will be gathered via semi-structured interviews post-intervention. Data analysis will involve descriptive analyses, generalized linear mixed models (quantitative) and the Framework Approach (qualitative), integrating quantitative and qualitative data during interpretation. Conclusions: This study will provide novel insight into the potential of interventions like Perfect Fit, as a multiple health behavior change strategy. Findings will inform further intervention development and help identify methods to foster feasibility and acceptability. Successful mHealth interventions with virtual coaches will prevent chronic diseases and promote public health. © The Author(s) 2024.",2024,,Digital Health,10,,,,10.1177/20552076241300020,,#5105,vanVliet 2024,,,1
MoveONParkinson: developing a personalized motivational solution for Parkinson’s disease management,"Alves, B.; Mota, P.R.; Sineiro, D.; Carmo, R.; Santos, P.; Macedo, P.; Carreira, J.C.; Madeira, R.N.; Dias, S.B.; Pereira, C.M.","Introduction: Despite the effectiveness of exercise-based interventions on symptom management and disease progression, many people with Parkinson’s Disease (PwPD) do not exercise regularly. In line with the ubiquitous use of digital health technology, the MoveONParkinson digital solution was developed, comprising a Web Platform and a Mobile App with a Conversational Agent (CA). The interface features were designed based on the principles of Social Cognitive Theory with the goal of fostering behavior change in PwPD for sustained exercise participation and improved disease management. Methods: Using a mixed methods approach, this study aimed to collect feedback, assess the acceptability of the Mobile App and the Web Platform, and evaluate the usability of the latter. Quantitative data, which included questionnaire responses and the System Usability Scale (SUS) scores, were analyzed using descriptive statistics, heatmaps, and correlation matrices. Qualitative data, comprising semi-structured and thinking-aloud interview transcripts, were subjected to an inductive thematic analysis. A total of 28 participants were involved in the study, comprising 20 physiotherapists (average age: 34.50 ± 10.4), and eight PwPD (average age: 65.75 ± 8.63; mean Hoehn & Yahr: 2.0 (± 0.76)). Results: Three main themes emerged from the thematic analysis of the interviews, namely: Self-management (Theme 1), User Engagement (Theme 2), and Recommendations (Theme 3). The assessment of the Mobile App and the CA (mean score: 4.42/5.0 ± 0.79) suggests that PwPD were able to navigate this interface without notable difficulties. The mean SUS score of 79.50 (± 12.40%) with a 95% confidence interval ranging from 73.70 to 85.30, reveal good usability. Discussion: These findings indicate a high level of acceptability of the MoveONParkinson digital solution, serving as a foundation for assessing its impact on exercise engagement and, subsequently, its influence on symptom management and quality of life of PwPD. Copyright © 2024 Alves, Mota, Sineiro, Carmo, Santos, Macedo, Carreira, Madeira, Dias and Pereira.",2024,,Frontiers in Public Health,12,,,,10.3389/fpubh.2024.1420171,,#5108,Alves 2024,,,1
Development and Evaluation of a Smartphone-Based Chatbot Coach to Facilitate a Balanced Lifestyle in Individuals With Headaches (BalanceUP App): Randomized Controlled Trial,"Ulrich, S.; Gantenbein, A.R.; Zuber, V.; Von Wyl, A.; Kowatsch, T.; Künzli, H.","Background: Primary headaches, including migraine and tension-type headaches, are widespread and have a social, physical, mental, and economic impact. Among the key components of treatment are behavior interventions such as lifestyle modification. Scalable conversational agents (CAs) have the potential to deliver behavior interventions at a low threshold. To our knowledge, there is no evidence of behavioral interventions delivered by CAs for the treatment of headaches. Objective: This study has 2 aims. The first aim was to develop and test a smartphone-based coaching intervention (BalanceUP) for people experiencing frequent headaches, delivered by a CA and designed to improve mental well-being using various behavior change techniques. The second aim was to evaluate the effectiveness of BalanceUP by comparing the intervention and waitlist control groups and assess the engagement and acceptance of participants using BalanceUP. Methods: In an unblinded randomized controlled trial, adults with frequent headaches were recruited on the web and in collaboration with experts and allocated to either a CA intervention (BalanceUP) or a control condition. The effects of the treatment on changes in the primary outcome of the study, that is, mental well-being (as measured by the Patient Health Questionnaire Anxiety and Depression Scale), and secondary outcomes (eg, psychosomatic symptoms, stress, headache-related self-efficacy, intention to change behavior, presenteeism and absenteeism, and pain coping) were analyzed using linear mixed models and Cohen d. Primary and secondary outcomes were self-assessed before and after the intervention, and acceptance was assessed after the intervention. Engagement was measured during the intervention using self-reports and usage data. Results: A total of 198 participants (mean age 38.7, SD 12.14 y; n=172, 86.9% women) participated in the study (intervention group: n=110; waitlist control group: n=88). After the intervention, the intention-to-treat analysis revealed evidence for improved well-being (treatment: β estimate=-3.28, 95% CI -5.07 to -1.48) with moderate between-group effects (Cohen d=-0.66, 95% CI -0.99 to -0.33) in favor of the intervention group. We also found evidence of reduced somatic symptoms, perceived stress, and absenteeism and presenteeism, as well as improved headache management self-efficacy, application of behavior change techniques, and pain coping skills, with effects ranging from medium to large (Cohen d=0.43-1.05). Overall, 64.8% (118/182) of the participants used coaching as intended by engaging throughout the coaching and completing the outro. Conclusions: BalanceUP was well accepted, and the results suggest that coaching delivered by a CA can be effective in reducing the burden of people who experience headaches by improving their well-being. Trial Registration: German Clinical Tr i a l s Register DRKS00017422; https://trialsearch.who.int/Trial2.aspx?TrialID=DRKS00017422. © Sandra Ulrich, Andreas R Gantenbein, Viktor Zuber, Agnes Von Wyl, Tobias Kowatsch, Hansjörg Künzli.",2024,,Journal of Medical Internet Research,26,1,,,10.2196/50132,,#5109,Ulrich 2024,,,1
Explanations Considered Harmful: The Impact of Misleading Explanations on Accuracy in Hybrid Human-AI Decision Making,"Cabitza, F.; Fregosi, C.; Campagner, A.; Natali, C.","Explainable AI (XAI) has the potential to enhance decision-making in human-AI collaborations, yet existing research indicates that explanations can also lead to undue reliance on AI recommendations, a dilemma often referred to as the ‘white box paradox.’ This paradox illustrates how persuasive explanations for incorrect advice might foster inappropriate trust in AI systems. Our study extends beyond the traditional scope of the white box paradox by proposing a framework for examining explanation inadequacy. We specifically investigate how accurate AI advice, when paired with misleading explanations, affects decision-making in logic puzzle tasks. Our findings introduce the concept of the ‘XAI halo effect,’ where participants were influenced by the misleading explanations to the extent that they did not verify the correctness of the advice, despite its accuracy. This effect reveals a nuanced challenge in XAI, where even correct advice can lead to misjudgment if the accompanying explanations are not coherent and contextually relevant. The study highlights the critical need for explanations to be both accurate and relevant, especially in contexts where decision accuracy is paramount. This calls into question the use of explanations in situations where their potential to mislead outweighs their transparency or educational value. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",2024,,Communications in Computer and Information Science,2156 CCIS,,255-269,,10.1007/978-3-031-63803-9_14,,#5113,Cabitza 2024,,,1
Development of an AI behavior change app,"Croghan-Cox, E.; West, R.","Smoking cessation services are an established part of the UK health system, funded by national and local governments. Services are commissioned by Local Authorities (Local Government) according to local needs and are provided by organizations that are paid to deliver services, usually based on achieving a number of 4-week quits per year. Everyone Health is one such commissioned service, delivering Stop Smoking Services across England in 6 Local Authorities, and delivering thousands of 4-week quits per year. To maximize quit rates, Everyone Health has worked with Emeritus Professor Robert West of UCL and British Websites to develop an Artificial Intelligence-driven app to provide treats and advice to those being supported through our services to make and maintain a quit attempt. This oral presentation will describe the process of developing and initial piloting of the app and the plans for evaluation following the rollout. © (2024), (European Publishing). All rights reserved.",2024,,Tobacco Prevention and Cessation,10,,,,10.18332/TPC/194406,,#5115,Croghan-Cox 2024,,,1
Investigating the Potential of a Conversational Agent (Phyllis) to Support Adolescent Health and Overcome Barriers to Physical Activity: Co-Design Study,"Moore, R.; Al-Tamimi, A.-K.; Freeman, E.","Background: Conversational agents (CAs) are a promising solution to support people in improving physical activity (PA) behaviors. However, there is a lack of CAs targeted at adolescents that aim to provide support to overcome barriers to PA. This study reports the results of the co-design, development, and evaluation of a prototype CA called “Phyllis” to support adolescents in overcoming barriers to PA with the aim of improving PA behaviors. The study presents one of the first theory-driven CAs that use existing research, a theoretical framework, and a behavior change model. Objective: The aim of the study is to use a mixed methods approach to investigate the potential of a CA to support adolescents in overcoming barriers to PA and enhance their confidence and motivation to engage in PA. Methods: The methodology involved co-designing with 8 adolescents to create a relational and persuasive CA with a suitable persona and dialogue. The CA was evaluated to determine its acceptability, usability, and effectiveness, with 46 adolescents participating in the study via a web-based survey. Results: The co-design participants were students aged 11 to 13 years, with a sex distribution of 56% (5/9) female and 44% (4/9) male, representing diverse ethnic backgrounds. Participants reported 37 specific barriers to PA, and the most common barriers included a “lack of confidence,” “fear of failure,” and a “lack of motivation.” The CA’s persona, named “Phyllis,” was co-designed with input from the students, reflecting their preferences for a friendly, understanding, and intelligent personality. Users engaged in 61 conversations with Phyllis and reported a positive user experience, and 73% of them expressed a definite intention to use the fully functional CA in the future, with a net promoter score indicating a high likelihood of recommendation. Phyllis also performed well, being able to recognize a range of different barriers to PA. The CA’s persuasive capacity was evaluated in modules focusing on confidence and motivation, with a significant increase in students’ agreement in feeling confident and motivated to engage in PA after interacting with Phyllis. Adolescents also expect to have a personalized experience and be able to personalize all aspects of the CA. Conclusions: The results showed high acceptability and a positive user experience, indicating the CA’s potential. Promising outcomes were observed, with increasing confidence and motivation for PA. Further research and development are needed to create further interventions to address other barriers to PA and assess long-term behavior change. Addressing concerns regarding bias and privacy is crucial for achieving acceptability in the future. The CA’s potential extends to health care systems and multimodal support, providing valuable insights for designing digital health interventions including tackling global inactivity issues among adolescents. © 2024 JMIR Publications Inc.. All rights reserved.",2024,,JMIR Formative Research,8,1,,,10.2196/51571,,#5116,Moore 2024,,,1
Using AI chatbots in climate change mitigation: a moderated serial mediation model,"Lee, S.; Park, Y.; Park, G.","This study examined the effect of chatbots’ emotional expression on climate-change-mitigation behaviour intention and the individual and serial mediating roles of social presence and guilt in the main effect. This study further tested the moderating role of custom addressing in direct and indirect relationships between the aforementioned variables. To that end, 577 American adults were recruited through Amazon Mechanical Turk. After eliminating incomplete responses, the remaining 549 participants were assigned to four conditions (factual vs. emotional conditions x control vs. custom addressing conditions), conversed briefly with a chatbot agent, and responded to a questionnaire. The findings showed that chatbots’ emotional expression yielded higher climate-change-mitigation behavioural intention than factual information and that social presence mediated this relationship. The results did not support the mediating role of guilt but supported serial mediation through social presence and guilt. The results further support the moderating role of custom addressing in the indirect relationship through social presence and serially through social presence and guilt. However, the results do not support the moderating role of custom addressing in direct and indirect relationships through guilt. © 2024 Informa UK Limited, trading as Taylor & Francis Group.",2024,,Behaviour and Information Technology,43,16,4016-4032,,10.1080/0144929X.2023.2298305,,#5119,Lee 2024,,,1
eHealth Communication Intervention to Promote Human Papillomavirus Vaccination Among Middle-School Girls: Development and Usability Study,"Kim, Y.; Lee, H.; Park, J.; Kim, Y.-C.; Kim, D.H.; Lee, Y.-M.","Background: As the age of initiating sexual intercourse has gradually decreased among South Korean adolescents, earlier vaccination of adolescents for human papillomavirus (HPV) is necessary before their exposure to HPV. Health communication includes “cues to action” that lead to preventive health behaviors, and recently, social networking services, which operate with fewer time and space constraints, have been used in various studies as a form of eHealth communication. Objective: This study aims to investigate the feasibility and usability of an eHealth communication intervention for HPV vaccination in middle-school girls aimed at the girls and their mothers. Methods: The eHealth communication intervention for HPV vaccination was developed using a 6-step intervention mapping process: needs assessments, setting program outcomes, selection of a theory-based method and practical strategies, development of the intervention, implementation plan, and testing the validity of the intervention. Results: A review of 10 studies identified effective health communication messages, delivery methods, and theories for HPV vaccination among adolescents. Barriers including low knowledge, perceived threat, and the inconvenience of taking 2 doses of the vaccine were identified through focus groups, suggesting a need for youth-friendly and easy-to-understand information for adolescents delivered via mobile phones. The expected outcomes and the performance objectives are specifically tailored to reflect the vaccination intention. Behavior change techniques were applied using trusted sources and a health belief model. Health messages delivered through a KakaoTalk chatbot improved awareness and self-efficacy. Quality control was ensured with the use of a log system. The experts’ chatbot usability average score was 80.13 (SD 8.15) and the average score of girls was 84.06 (SD 7.61). Conclusions: Future studies need to verify the effectiveness of health communication strategies in promoting HPV vaccination and the effectiveness of scientific intervention using a chatbot as a delivery method for the intervention. ©Youlim Kim, Hyeonkyeong Lee, Jeongok Park, Yong-Chan Kim, Dong Hee Kim, Young-Me Lee.",2024,,JMIR Formative Research,8,,,,10.2196/59087,,#5130,Kim 2024,,,1
ChatGPT-Based Debate Game Application Utilizing Prompt Engineering,"Lee, E.-Y.; Il, N.G.D.; An, G.-H.; Lee, S.; Lim, K.","This paper1 focuses on the implementation of a debate game using ChatGPT, aiming to investigate the feasibility of incorporating large language models into the educational domain through prompt engineering. The study explores strategies to elicit desired outputs from the GPT model by employing the prompt engineering methodology, as provided by Microsoft. Specifically, the game implementation involves the customization of ChatGPT's responses to facilitate a natural progression of debates, varying levels of difficulty, and an evaluation system for assessing the quality of discourse. By leveraging the prompt engineering methodology, we demonstrate that providing specific instructions or case-based prompts improves the accuracy and relevance of ChatGPT's answers. The developed application targets teenagers, enabling them to engage in real-time debates with ChatGPT and enhance their literacy skills. Furthermore, the game fosters the development of logical reasoning, persuasive abilities, effective expression, active participation, and attentive listening while expressing personal opinions, ultimately fostering a sense of accomplishment. Moreover, through debate evaluation and personalized advice, ChatGPT is expected to recognize and address its shortcomings, thereby continuously improving its conversational capabilities. Overall, this research contributes to the understanding of how large language models can be harnessed in educational settings and underscores the potential benefits of prompt engineering techniques in optimizing the outputs of such models.  © 2023 ACM.",2023,,2023 Research in Adaptive and Convergent Systems RACS 2023,,,,,10.1145/3599957.3606244,,#5133,Lee 2023,,,1
"Feasibility and acceptability of Saheli, a WhatsApp Chatbot, on COVID-19 vaccination among pregnant and breastfeeding women in rural North India","El Ayadi, A.M.; Singh, P.; Duggal, M.; Kumar, V.; Kaur, J.; Sharma, P.; Vosburg, K.B.; Diamond-Smith, N.G.","Introduction Pregnant and breastfeeding women are priority targets for COVID-19 vaccination due to adverse maternal and fetal consequences of perinatal COVID-19 infection and the opportunity for protecting infants through maternal antibodies. Saheli ('female friend') is a WhatsApp-deployed chatbot providing evidence-based guidance on COVID-19 for pregnant and breastfeeding women. Objectives To understand the feasibility and acceptability of Saheli and its impact on COVID-19 vaccination. Methods We pilot-tested Saheli among pregnant and breastfeeding participants of pre-existing WhatsApp educational groups led by a community-based non-governmental organisation in Haryana, India from January to March 2022 using a pre/post design. Results 829 unique participants completed precommunity surveys or postcommunity surveys; 238 completed both. 829 individuals used Saheli, including 88% postintervention survey participants. Users reported Saheli was easy to engage with (79%), easy to understand (91%), quick (83%) and met their information needs (97%). 89% indicated it improved their COVID-19 knowledge a lot, 72% recommended it to others and 88% shared chatbot-derived information with others. Most participants received ≥1 COVID-19 vaccine (86% vs 88%, preintervention to postintervention); full vaccination was 55% and 61%, respectively. Vaccination over time increased marginally for ≥1 dose (OR 1.15, 95% CI 0.99 to 1.36) and significantly for 2 doses (OR 1.21, 95% CI 1.09 to 1.34), and increases were significant among pregnant (≥1 dose) and breastfeeding participants (2 doses). Vaccine hesitancy was low. Chatbot use was high, yet individual chatbot engagement did not alter COVID-19 vaccination. Conclusion Chatbots are a promising health education strategy due to high acceptability and deployment potential. Interpreting community chatbot impact must acknowledge the co-occurring constellation of multilevel interventions, community and pandemic factors.  © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.",2023,,BMJ Innovations,9,4,195-206,,10.1136/bmjinnov-2022-001012,,#5137,ElAyadi 2023,,,1
Acceptability of Mobile App-Based Motivational Interviewing and Preferences for App Features to Support Self-Management in Patients With Type 2 Diabetes: Qualitative Study,"Yoon, S.; Tang, H.; Tan, C.M.; Phang, J.K.; Kwan, Y.H.; Low, L.L.","Background: Patients with type 2 diabetes mellitus (T2DM) experience multiple barriers to improving self-management. Evidence suggests that motivational interviewing (MI), a patient-centered communication method, can address patient barriers and promote healthy behavior. Despite the value of MI, existing MI studies predominantly used face-to-face or phone-based interventions. With the growing adoption of smartphones, automated MI techniques powered by artificial intelligence on mobile devices may offer effective motivational support to patients with T2DM. Objective: This study aimed to explore the perspectives of patients with T2DM on the acceptability of app-based MI in routine health care and collect their feedback on specific MI module features to inform our future intervention. Methods: We conducted semistructured interviews with patients with T2DM, recruited from public primary care clinics. All interviews were audio recorded and transcribed verbatim. Thematic analysis was conducted using NVivo. Results: In total, 33 patients with T2DM participated in the study. Participants saw MI as a mental reminder to increase motivation and a complementary care model conducive to self-reflection and behavior change. Yet, there was a sense of reluctance, mainly stemming from potential compromise of autonomy in self-care by the introduction of MI. Some participants felt confident in their ability to manage conditions independently, while others reported already making changes and preferred self-management at their own pace. Compared with in-person MI, app-based MI was viewed as offering a more relaxed atmosphere for open sharing without being judged by health care providers. However, participants questioned the lack of human touch, which could potentially undermine a patient-provider therapeutic relationship. To sustain motivation, participants suggested more features of an ongoing supportive nature such as the visualization of milestones, gamified challenges and incremental rewards according to achievements, tailored multimedia resources based on goals, and conversational tools that are interactive and empathic. Conclusions: Our findings suggest the need for a hybrid model of intervention involving both app-based automated MI and human coaching. Patient feedback on specific app features will be incorporated into the module development and tested in a randomized controlled trial. © Sungwon Yoon, Haoming Tang, Chao Min Tan, Jie Kie Phang, Yu Heng Kwan, Lian Leng Low.",2024,,JMIR Diabetes,9,,,,10.2196/48310,,#5141,Yoon 2024,,,1
Is Persuasion in the eye of the beholder? A study on the use of persuasive technology and its effect,"Mendes, T.R.; Antunes, A.C.","Digital marketing is being revolutionized by technological advancements in many fields, such as artificial intelligence and persuasive technologies. Persuasive technology still generates disagreements, both on the marketer side, by setting up a persuasive system, and on the consumer side, who is responsible for accepting and instilling. This chapter presents a case study, which focuses on a Portuguese sports betting app - Placard. The aim is to examine Placard gamblers' behaviors, attitudes, and preferences, and how their user experience is influenced by the persuasive technologies embedded in this app. The main results allow a conclude that, no matter how much persuasive technology is programmed, and designed to change a certain behavior, it will always depend on the user, as well as on the different factors that influence their decision. The persuasive sports betting platform itself also needs to provide a better user experience to trigger less resistance. These results are discussed for the benefit of future digital marketing strategies and tactics withing online gambling. © 2024, IGI Global. All rights reserved.",2023,,The Use of Artificial Intelligence in Digital Marketing: Competitive Strategies and Tactics,,,142-165,,10.4018/978-1-6684-9324-3.ch006,,#5142,Mendes 2023,,,1
"Integration of ChatGPT Into a Course for Medical Students: Explorative Study on Teaching Scenarios, Students’ Perception, and Applications","Thomae, A.V.; Witt, C.M.; Barth, J.","Background: Text-generating artificial intelligence (AI) such as ChatGPT offers many opportunities and challenges in medical education. Acquiring practical skills necessary for using AI in a clinical context is crucial, especially for medical education. Objective: This explorative study aimed to investigate the feasibility of integrating ChatGPT into teaching units and to evaluate the course and the importance of AI-related competencies for medical students. Since a possible application of ChatGPT in the medical field could be the generation of information for patients, we further investigated how such information is perceived by students in terms of persuasiveness and quality. Methods: ChatGPT was integrated into 3 different teaching units of a blended learning course for medical students. Using a mixed methods approach, quantitative and qualitative data were collected. As baseline data, we assessed students’ characteristics, including their openness to digital innovation. The students evaluated the integration of ChatGPT into the course and shared their thoughts regarding the future of text-generating AI in medical education. The course was evaluated based on the Kirkpatrick Model, with satisfaction, learning progress, and applicable knowledge considered as key assessment levels. In ChatGPT-integrating teaching units, students evaluated videos featuring information for patients regarding their persuasiveness on treatment expectations in a self-experience experiment and critically reviewed information for patients written using ChatGPT 3.5 based on different prompts. Results: A total of 52 medical students participated in the study. The comprehensive evaluation of the course revealed elevated levels of satisfaction, learning progress, and applicability specifically in relation to the ChatGPT-integrating teaching units. Furthermore, all evaluation levels demonstrated an association with each other. Higher openness to digital innovation was associated with higher satisfaction and, to a lesser extent, with higher applicability. AI-related competencies in other courses of the medical curriculum were perceived as highly important by medical students. Qualitative analysis highlighted potential use cases of ChatGPT in teaching and learning. In ChatGPT-integrating teaching units, students rated information for patients generated using a basic ChatGPT prompt as “moderate” in terms of comprehensibility, patient safety, and the correct application of communication rules taught during the course. The students’ ratings were considerably improved using an extended prompt. The same text, however, showed the smallest increase in treatment expectations when compared with information provided by humans (patient, clinician, and expert) via videos. Conclusions: This study offers valuable insights into integrating the development of AI competencies into a blended learning course. Integration of ChatGPT enhanced learning experiences for medical students. © Anita V Thomae, Claudia M Witt, Jürgen Barth.",2024,,JMIR Medical Education,10,,,,10.2196/50545,,#5145,Thomae 2024,,,1
Changing Parent Attitudes Towards HPV Vaccination by Including Adolescents in Multiparty Counseling using Virtual Agents,"Steenstra, I.; Murali, P.; Perkins, R.; Joseph, N.; Paasche-Orlow, M.; Bickmore, T.","Parental permission is required for medical care for children, and decisions may be made without incorporating children’s views, even for adolescents. We explore the impact of including adolescents in virtual agent-based multiparty health counseling to promote Human Papillomavirus (HPV) vaccination. The agent is designed to encourage HPV vaccination for children aged 9-12 by engaging co-present parent/adolescent dyads in an online interaction. Several techniques are incorporated, including HPV education, motivational interviewing, persuasion, modeling, and enablement, to address parents’ intent to vaccinate their children. We conduct a between-subjects randomized study comparing a version of the agent exclusively for the parent, to one that includes the adolescent in the conversation and incorporates the child’s views in counseling strategies. We measure pre- and post-intervention changes in intent to vaccinate, vaccination hesitancy, and knowledge in both the parent and the adolescent, hypothesizing greater improvements in these measures when the adolescent is included in the conversation. We also examine the satisfaction, engagement, and comfort of the parent/child interactions with the virtual agent. We found significant pre-post increases in parent intent to vaccinate their adolescent for both versions of the agent. Our work provides insights into the effectiveness of the virtual agent in promoting HPV vaccination, the impact of child participation on healthcare decision-making, and the user experience of multi-party interaction with the virtual agent. © 2023 Copyright held by the owner/author(s).",2023,,"Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents, IVA 2023",,,,,10.1145/3570945.3607294,,#5167,Steenstra 2023,,,1
The gamification and development of a chatbot to promote oral self-care by adopting behavior change wheel for Taiwanese children,"Chang, W.-J.; Chang, P.-C.; Chang, Y.-H.","Background: Oral health is closely related to general health and quality of life. School-aged children are at a critical stage for developing their self-care ability in oral health. Digital interventions can encourage and facilitate oral self-care in children. Objective: This study aims to present the development of an educational chatbot for school-aged children to address their oral self-care and evaluate its usability. Methods: The development and evaluation of the chatbot for oral self-care consisted of four stages: target behavior analysis, intervention design, system development, and the chatbot evaluation. The target behavior analysis identified barriers to children's engagement in oral self-care based on dentists’ clinical observations; hence, the requirements for achieving the desired behavior were categorized according to the capability-opportunity-motivation behavior model. Interventional functions were created following the behavior change wheel. A menu-driven chatbot was created and evaluated for usability as well as likeability. Results: The barriers and requirements for achieving good behavior in school-aged children's oral self-care were identified by the dental professionals. Intervention strategy incorporated specific functions enriched with gamification features to support school-aged children in developing their abilities for engaging in oral self-care. The intervention functions consist of capability establishment, motivation enhancement, and opportunity creation, which were designed to support children in their oral self-care practices. The designed chatbot was piloted with a convenient sample of 30 school-aged children and their accompanying parents at the pediatric dental clinic. The results indicated good usability, with a mean usability score of 79.91, and high likeability with a mean score of 4.32 out of 5 for the designed chatbot. Conclusions: The educational chatbot incorporated a combination of clinical dentistry practice and guidelines, aiming to promote oral self-care behavior in school-aged children. The designed chatbot achieved high scores for its usability and user likability. © The Author(s) 2024.",2024,,Digital Health,10,,,,10.1177/20552076241256750,,#5174,Chang 2024,,,1
"TherapAI: AI-Powered Therapy Chatbot with Sentiment Analysis, Mood Tracking, and Automated Reporting for Enhanced Mental Health Support","Lal, A.; Dhadwal, P.; Gade, D.; Joy, J.","The global mental health epidemic is creating an urgent need for easily accessible and efficient mental health care. We know that, often times even though it is helpful traditional therapy can be inadequate in terms of accessibility, affordability and scalability. In this paper, we present TherapAI, a therapy chatbot using AI to keep track of user mood through sentiment analysis; offering personalised emotional support 24/7 whilst generating activity summaries for therapists. Through the use of sophisticated Natural Language Processing (NLP), TherapAI tracks behavioral patterns and assists users in managing emotional stress, thereby promoting behavior change. It provides therapists with comprehensive information and allows for real-time emotional assistance through the analysis of user interactions.This could lead to positive testing, with customers having shown more effective emotional control and reporting themselves as satisfied after spending time interacting with the chatbot. According to therapists, those automated reports are a valuable addition and the mood tracking tool provides great data points. As a scalable AI solution, TherapAI can supplement traditional therapy and alleviate substantial gaps in the existing delivery of mental health care. It can also be upgraded to perhaps allow more words in the future. © 2024 IEEE.",2024,,"3rd International Conference on Communication, Control, and Intelligent Systems, CCIS 2024",,,,,10.1109/CCIS63231.2024.10932101,,#5176,Lal 2024,,,1
"Effectiveness of using a chatbot to promote adherence to home physiotherapy after total knee replacement, rationale and design of a randomized clinical trial","Blasco, J.-M.; Díaz-Díaz, B.; Igual-Camacho, C.; Pérez-Maletzki, J.; Hernández-Guilén, D.; Roig-Casasús, S.","Background: Rehabilitation is essential to optimize outcomes after surgical procedures in musculoskeletal disorders. However, adherence to rehabilitation continues to be an important barrier, since compliance with the programs is not always as desired, which may have a negative impact on clinical results. Methods: Randomized controlled trial aimed at to determining the effectiveness of using a virtual assistant (i.e., chatbot) to promote adherence to home rehabilitation. Overall, seventy patients under 75, undergoing total knee replacement, who have a personal smartphone and are familiar with its use, will be assigned into the control (standard care) or the experimental (standard care plus virtual assistant) group. Adherence (primary outcome) will be assessed three months after surgery. The WOMAC questionnaire, knee pain and system usability scale will be also outcomes of interest at three months and one year. Overall, an analysis of variance will look for possible time, group and time*group interactions. Discussion: The expected result is to determine whether the use of a chatbot that interacts with the patient can increase adherence to post-surgical home physiotherapy, and result in better clinical results (functional and pain) than standard care. Trial registration: clinicaltrials.gov id. NCT05363137 © 2023, The Author(s).",2023,,BMC Musculoskeletal Disorders,24,1,,,10.1186/s12891-023-06607-3,,#5182,Blasco 2023,,,1
Utilizing the Fediverse and AI-bots for Youth Engagement During COVID-19 in a Hybrid Preventative Intervention,"Cole, M.; Gary, K.; Meier, M.; Gonzales, N.; Pina, A.; Stoll, R.","Social media is a pervasive platform for delivering targeted interventions, albeit with cautionary ethical consequences. Recently AI robots or “bots” have been combined with social media platforms to enhance interaction, and enact behavior change through increased engagement and adherence to intervention protocols. This paper presents a customized social media platform for promoting engagement and adherence to a prevention intervention protocol. The protocol was originally developed in a group workshop format, and then online during COVID-19. A social media platform was utilized to connect group participants and deliver protocol activities. Bots encouraged participation via positive reinforcement mechanism for the entire group, and to remind a participant of protocol activities. While not a formal study, our exploratory results demonstrate that bots and a social media context support a group leader in increased engagement and adherence to the protocol. Our principal contribution in this paper is demonstrating that a personalized, adaptive instance of a Control Systems Engineering model may improve engagement-related outcomes in brief protocols. © 2024 IEEE Computer Society. All rights reserved.",2024,,Proceedings of the Annual Hawaii International Conference on System Sciences,,,3857-3866,,,,#5188,Cole 2024,,,1
V-care: An application to support lifestyle improvement in children with obesity,"Larizza, C.; Bosoni, P.; Quaglini, S.; Chasseur, M.; Bevolo, V.; Zuccotti, G.; Calcaterra, V.","Background: Obesity is increasing in the pediatric population, and it represents an important risk factor for the life-long development of several diseases. Although health promotion represents the mainstay of obesity prevention and treatment, lifestyle modification programs are often unsuccessful. Objectives: The purpose of this article is to introduce the V-care app, a mobile health platform specifically developed to offer effective interaction and support young people in a long-term obesity treatment, combining different strategies to maximize the results of the lifestyle modification program and minimize the possibility of dropouts. Methods: The V-care app is based on a conventional client–server architecture, but novelties of our approach are the involvement of families in the lifestyle modification program, and the design inspired to psychological/behavioral change theories, with the aim of raising the chance of patients’ compliance to the program. V-care implements a goal-based behavioral intervention, providing specific feedbacks according to the patient's performance. A pilot usability and acceptability study was performed on a sample of thirteen children aged 6–12 years, using a questionnaire with a 5-points Likert scale to evaluate eight system features, identified as essential requirements based on the analysis of strengths and weaknesses of similar systems in literature. Results: The pilot study highlighted very high rate of overall friendliness and perceived utility evaluation, while some critical issues emerged especially for the chatbot section, which may be due to the novelty of the technology. The positive evaluation of the design choices is confirmed by the average score greater than 3 for all the questions. Conclusions: The V-care app represents a digital innovation in the pediatric healthcare, and it could be introduced in children's primary healthcare nationwide, with the aim to offer an intervention program for controlling and preventing childhood obesity. © 2023 Elsevier B.V.",2023,,International Journal of Medical Informatics,177,,,,10.1016/j.ijmedinf.2023.105140,,#5190,Larizza 2023,,,1
Brain versus bot: Distinguishing letters of recommendation authored by humans compared with artificial intelligence,"Preiksaitis, C.; Nash, C.; Gottlieb, M.; Chan, T.M.; Alvarez, A.; Landry, A.","Objectives: Letters of recommendation (LORs) are essential within academic medicine, affecting a number of important decisions regarding advancement, yet these letters take significant amounts of time and labor to prepare. The use of generative artificial intelligence (AI) tools, such as ChatGPT, are gaining popularity for a variety of academic writing tasks and offer an innovative solution to relieve the burden of letter writing. It is yet to be determined if ChatGPT could aid in crafting LORs, particularly in high-stakes contexts like faculty promotion. To determine the feasibility of this process and whether there is a significant difference between AI and human-authored letters, we conducted a study aimed at determining whether academic physicians can distinguish between the two. Methods: A quasi-experimental study was conducted using a single-blind design. Academic physicians with experience in reviewing LORs were presented with LORs for promotion to associate professor, written by either humans or AI. Participants reviewed LORs and identified the authorship. Statistical analysis was performed to determine accuracy in distinguishing between human and AI-authored LORs. Additionally, the perceived quality and persuasiveness of the LORs were compared based on suspected and actual authorship. Results: A total of 32 participants completed letter review. The mean accuracy of distinguishing between human- versus AI-authored LORs was 59.4%. The reviewer's certainty and time spent deliberating did not significantly impact accuracy. LORs suspected to be human-authored were rated more favorably in terms of quality and persuasiveness. A difference in gender-biased language was observed in our letters: human-authored letters contained significantly more female-associated words, while the majority of AI-authored letters tended to use more male-associated words. Conclusions: Participants were unable to reliably differentiate between human- and AI-authored LORs for promotion. AI may be able to generate LORs and relieve the burden of letter writing for academicians. New strategies, policies, and guidelines are needed to balance the benefits of AI while preserving integrity and fairness in academic promotion decisions. © 2023 The Authors. AEM Education and Training published by Wiley Periodicals LLC on behalf of Society for Academic Emergency Medicine.",2023,,AEM Education and Training,7,6,,,10.1002/aet2.10924,,#5192,Preiksaitis 2023,,,1
"Setting Physical Activity Goals with a Virtual Coach: Vicarious Experiences, Personalization and Acceptance","Albers, N.; Hizli, B.; Scheltinga, B.L.; Meijer, E.; Brinkman, W.-P.","Goal-setting is often used in eHealth applications for behavior change as it motivates and helps to stay focused on a desired outcome. However, for goals to be effective, they need to meet criteria such as being specific, measurable, attainable, relevant and time-bound (SMART). Moreover, people need to be confident to reach their goal. We thus created a goal-setting dialog in which the virtual coach Jody guided people in setting SMART goals. Thereby, Jody provided personalized vicarious experiences by showing examples from other people who reached a goal to increase people’s confidence. These experiences were personalized, as it is helpful to observe a relatable other succeed. Data from an online study with a between-subjects with pre-post measurement design (n=39 participants) provide credible support that personalized experiences are seen as more motivating than generic ones. Motivational factors for participants included information about the goal, path to the goal, and the person who accomplished a goal, as well as the mere fact that a goal was reached. Participants also had a positive attitude toward Jody. We see these results as an indication that people are positive toward using a goal-setting dialog with a virtual coach in eHealth applications for behavior change. Moreover, contrary to hypothesized, our observed data give credible support that participants’ self-efficacy was lower after the dialog than before. These results warrant further research on how such dialogs affect self-efficacy, especially whether these lower post-measurements of self-efficacy are associated with people’s more realistic assessment of their abilities. © 2023, The Author(s).",2023,,Journal of Medical Systems,47,1,,,10.1007/s10916-022-01899-9,,#5194,Albers 2023,,,1
Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters,"Potter, Y.; Lai, S.; Kim, J.; Evans, J.; Song, D.","Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs' political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users' political views is required, as their use becomes more widespread. © 2024 Association for Computational Linguistics.",2024,,"EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,4244-4275,,10.18653/v1/2024.emnlp-main.244,,#5195,Potter 2024,,,1
AI Chatbot for Generating Episodic Future Thinking (EFT) Cue Texts for Health,"Ahmadi, S.; Fox, E.A.","We describe an AI-powered chatbot to aid with health improvement by generating Episodic Future Thinking (EFT) cue texts that should reduce delay discounting. In prior studies, EFT has been shown to address maladaptive health behaviors. Those studies involved participants, working with researchers, vividly imagining future events, and writing a description that they subsequently will frequently review, to ensure a shift from an inclination towards immediate rewards. That should promote behavior change, aiding in health tasks such as treatment adherence and lifestyle modifications. The AI chatbot is designed to guide users in generating personalized EFT cues, automating the current labor-intensive interview-based process. This can enhance the efficiency of EFT interventions and make them more accessible, targeting specifically those with limited educational backgrounds or communication challenges. By leveraging AI for EFT intervention, we anticipate broadened access and improved health outcomes across diverse populations. ©2024 IEEE.",2024,,"Proceedings - 2024 IEEE 1st International Conference on Artificial Intelligence for Medicine, Health and Care, AIMHC 2024",,,101-108,,10.1109/AIMHC59811.2024.00027,,#5200,Ahmadi 2024,,,1
Effect of disclosing AI-generated content on prosocial advertising evaluation,"Baek, T.H.; Kim, J.; Kim, J.H.","With advancements in generative artificial intelligence (AI) technology, there is a growing concern about its ethical implications, transparency, and consumer reactions to AI-generated content. Building on the persuasion knowledge model and algorithm aversion literature, this study explores the effects of AI disclosure in prosocial advertising on consumer attitudes and donation intentions. The findings of Study 1 indicate that the initial disclosure of AI-generated content leads to unfavourable attitudes towards ads, with perceived ad credibility serving as a mediating factor. In Study 2, participants who perceived AI as more human-like rather than machine-like tended to experience a diminished negative impact of AI disclosure. Study 3 also highlights the crucial role of perceived ad credibility in influencing donation intentions following the disclosure of AI-generated content. The theoretical and practical implications of our findings for social marketers and nonprofit organizations are discussed further. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",2024,,International Journal of Advertising,,,,,10.1080/02650487.2024.2401319,,#5201,Baek 2024,,,1
"More Victories, Less Cooperation: Assessing Cicero's Diplomacy Play","Wongkamjan, W.; Gu, F.; Wang, Y.; Hermjakob, U.; May, J.; Stewart, B.M.; Kummerfeld, J.K.; Peskoff, D.; Boyd-Graber, J.L.","The boardgame Diplomacy is a challenging setting for communicative and cooperative artificial intelligence. The most prominent communicative Diplomacy AI, Cicero, has excellent strategic abilities, exceeding human players. However, the best Diplomacy players master communication, not just tactics, which is why the game has received attention as an AI challenge. This work seeks to understand the degree to which Cicero succeeds at communication. First, we annotate in-game communication with abstract meaning representation to separate in-game tactics from general language. Second, we run two dozen games with humans and Cicero, totaling over 200 human-player hours of competition. While AI can consistently outplay human players, AI-Human communication is still limited because of AI's difficulty with deception and persuasion. This shows that Cicero relies on strategy and has not yet reached the full promise of communicative and cooperative AI. © 2024 Association for Computational Linguistics.",2024,,Proceedings of the Annual Meeting of the Association for Computational Linguistics,1,,12423-12441,,10.18653/v1/2024.acl-long.672,,#5207,Wongkamjan 2024,,,1
Personality-based tailored explainable recommendation for trustworthy smart learning system in the age of artificial intelligence,"Takami, K.; Flanagan, B.; Dai, Y.; Ogata, H.","In the age of artificial intelligence (AI), trust in AI systems is becoming more important. Explainable recommenders, which explain why an item is recommended, have recently been proposed in the field of learning technology to improve transparency, persuasiveness, and trustworthiness. However, the methods for generating explanations are limited and do not consider the learner’s cognitive perceptions or personality. This study draws inspiration from tailored intervention research in public health and investigates the effectiveness of personality-based tailored explanations by implementing them for the recommended quizzes in an explainable recommender system. High school students (n = 217) were clustered into three distinct profiles labeled Diligent (n = 77), Fearful (n = 72), and Agreeable (n = 68), based on the Big Five personality traits. The students were divided into a tailored intervention group (n = 106) and a control group (n = 111). In the tailored intervention group, personalized explanations for recommended quizzes were provided based on student profiles, with explanations based on quiz characteristics. In the control group, only non-personalized explanations based on quiz characteristics were provided. An 18-day A/B experiment showed that the tailored intervention group had significantly higher recommendation usage than the control group. These results suggest that personality-based tailored explanations with a recommender approach are effective for e-learning engagement and imply improved trustworthiness of AI learning systems. © 2023, The Author(s).",2023,,Smart Learning Environments,10,1,,,10.1186/s40561-023-00282-6,,#5208,Takami 2023,,,1
Exploring Factors Influencing Internet Users' Susceptibility to Deepfake Phishing,"Gal, I.; Bulgurcu, B.","This proposed study investigates the factors that influence susceptibility to deepfake phishing, a modern technique that employs artificial intelligence to create realistic but fake content. Due to the unique nature of social media, we believe that it provides an ample opportunity for attackers to target users that can expect to see well-known individuals on the platforms they use. By focusing on the content of the deepfakes, such as perceived authority, familiarity, and video quality, this paper explores how differences might affect susceptibility. This research utilizes an experimental approach to test the differences in content across a randomized sample of participants. By merging cognitive theories with empirical data, this paper seeks to provide insight into the dynamics of persuasion, deception, and phishing susceptibility in this new digital age, aiming to enhance detection strategies and reduce vulnerability to such sophisticated attacks. © 2024 30th Americas Conference on Information Systems, AMCIS 2024. All rights reserved.",2024,,"30th Americas Conference on Information Systems, AMCIS 2024",,,,,,,#5216,Gal 2024,,,1
Leafy: Enhancing Home Energy Efficiency through Gamified Experience with a Conversational Smart Mirror,"Giudici, M.; Crovari, P.; Garzotto, F.","In the next few years, people will be called upon to try to slow climate change and achieve carbon neutrality collectively. The use of persuasive digital tools and engaging mechanisms can play an important role in matching such objectives. In our research, we explore the usage of a Multimodal Conversational Agent embedded in a Smart Mirror and connected to home automation appliances to help users reduce their energy consumption. The agent employs a variety of gamification techniques to encourage short and long-term sustainable behavior, and it is designed to be an enjoyable and non-intrusive experience. It informs householders about their energy consumption and encourages them regularly to reduce and optimize their electric usage. In order to keep the user engaged, the mirror contains visually appealing components and recommendations for daily challenges.  © 2023 Owner/Author.",2023,,ACM International Conference Proceeding Series,,,128-134,,10.1145/3582515.3609526,,#5217,Giudici 2023,,,1
Responsible Configuration Using LLM-based Sustainability-Aware Explanations,"Lubos, S.; Felfernig, A.; Hotz, L.; Tran, T.N.T.; Polat-Erdeniz, S.; Le, V.-M.; Garber, D.; El-Mansi, M.","Configuration systems play an important role in achieving the sustainable development goals (SDGs) defined by the United Nations. As decision support systems, configurators help users to decide which components or features to include in or exclude from a configuration. An important task of configurators is the provision of explanations which help to achieve goals such as increasing configuration understandability, increasing a user’s trust, and persuading users/customers to include specific configuration components. Our goal in this paper is to introduce the concept of „sustainability-aware explanations“which can help to support the sustainable development goals. The type of explanations we propose in this context are somehow orthogonal to typical explanations used in industrial configuration environments. A major objective in this context is to follow a „less-is-more“principle focusing on different aspects of the idea of „responsible configuration“which refers to configuration techniques explicitly supporting the mentioned sustainability goals. We report the initial results of an evaluation that provide insights on potential impacts of the proposed explanations. © 2024 Copyright for this paper by its authors.",2024,,CEUR Workshop Proceedings,3812,,68-73,,,,#5222,Lubos 2024,,,1
Designing Supportive Chatbots for Blood Donors,"Müller, H.M.; Reuter-Oppermann, M.","Healthcare systems worldwide rely on voluntary blood donations, as blood cannot be produced artificially, but is needed for many surgeries and treatments. In countries like South Africa or Ghana, currently less than 1 % of the population donates blood. Donor education, mobilisation and management are therefore crucial. Chatbots offer easy access to information for all types of donors as well as for blood services to educate (potential) donors. By applying the design science research approach and grounding our research on behaviour change models, we have developed a chatbot for all donor types in South Africa and Ghana. In this work, we present an instantiation of the chatbot and its positive evaluation with non-, first-time, lapsed and regular donors of both countries. © 2024 IEEE Computer Society. All rights reserved.",2024,,Proceedings of the Annual Hawaii International Conference on System Sciences,,,3454-3463,,,,#5230,Müller 2024,,,1
Artificial emotions for charity collection: A serial mediation through perceived anthropomorphism and social presence,"Lee, S.; Park, G.; Chung, J.","Despite the broad application of chatbot agents in online interactions, an ongoing debate persists regarding their persuasive role and human-like emotional disclosure. Our study adds to this debate by exploring the effect of chatbot agents’ emotional disclosure on people's willingness to donate to a charitable cause, and by examining individual and serial mediation between the main effects of perceived anthropomorphism and social presence. To this end, two types of artificial intelligence chatbot agents—one disclosing factual information and another disclosing human-like emotion—were developed and trained using Dialogflow, a natural language processing engine. A total of 619 US residents were recruited through Amazon Mechanical Turk, an online crowdsourcing platform. Of these, 593 participants completed the required conversation with either version of the chatbot agent (factual vs. emotional), as well as the survey questionnaire, and therefore, were included in the final analysis. The participants exhibited a higher willingness to donate when they interacted with a chatbot disclosing human-like emotions than when they were only exposed to factual information. Moreover, this study found both individual and serial mediating roles of perceived anthropomorphism and social presence. Concerning the implications, theoretically, this study adds to the understanding of applying the notion of human interaction to that involving humans and chatbots. Practically, our findings can be of great help in increasing willingness to donate thereby enhancing fund-raising activities. © 2023 Elsevier Ltd",2023,,Telematics and Informatics,82,,,,10.1016/j.tele.2023.102009,,#5233,Lee 2023,,,1
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation,"Wang, H.; Hee, M.S.; Awal, M.R.; Choo, K.T.W.; Lee, R.K.-W.","Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content. Our study underscores the need for caution in applying LLM-generated explanations for content moderation. Code and results are available at https://github.com/Social-AI-Studio/GPT3-HateEval. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved.",2023,,IJCAI International Joint Conference on Artificial Intelligence,2023-August,,6255-6263,,10.24963/ijcai.2023/694,,#5234,Wang 2023,,,1
"Development of “LvL UP 1.0”: a smartphone-based, conversational agent-delivered holistic lifestyle intervention for the prevention of non-communicable diseases and common mental disorders","Castro, O.; Mair, J.L.; Salamanca-Sanabria, A.; Alattas, A.; Keller, R.; Zheng, S.; Jabir, A.; Lin, X.; Frese, B.F.; Lim, C.S.; Santhanam, P.; van Dam, R.M.; Car, J.; Lee, J.; Tai, E.S.; Fleisch, E.; von Wangenheim, F.; Tudor Car, L.; Müller-Riemenschneider, F.; Kowatsch, T.","Background: Non-communicable diseases (NCDs) and common mental disorders (CMDs) are the leading causes of death and disability worldwide. Lifestyle interventions via mobile apps and conversational agents present themselves as low-cost, scalable solutions to prevent these conditions. This paper describes the rationale for, and development of, “LvL UP 1.0″, a smartphone-based lifestyle intervention aimed at preventing NCDs and CMDs. Materials and Methods: A multidisciplinary team led the intervention design process of LvL UP 1.0, involving four phases: (i) preliminary research (stakeholder consultations, systematic market reviews), (ii) selecting intervention components and developing the conceptual model, (iii) whiteboarding and prototype design, and (iv) testing and refinement. The Multiphase Optimization Strategy and the UK Medical Research Council framework for developing and evaluating complex interventions were used to guide the intervention development. Results: Preliminary research highlighted the importance of targeting holistic wellbeing (i.e., both physical and mental health). Accordingly, the first version of LvL UP features a scalable, smartphone-based, and conversational agent-delivered holistic lifestyle intervention built around three pillars: Move More (physical activity), Eat Well (nutrition and healthy eating), and Stress Less (emotional regulation and wellbeing). Intervention components include health literacy and psychoeducational coaching sessions, daily “Life Hacks” (healthy activity suggestions), breathing exercises, and journaling. In addition to the intervention components, formative research also stressed the need to introduce engagement-specific components to maximise uptake and long-term use. LvL UP includes a motivational interviewing and storytelling approach to deliver the coaching sessions, as well as progress feedback and gamification. Offline materials are also offered to allow users access to essential intervention content without needing a mobile device. Conclusions: The development process of LvL UP 1.0 led to an evidence-based and user-informed smartphone-based intervention aimed at preventing NCDs and CMDs. LvL UP is designed to be a scalable, engaging, prevention-oriented, holistic intervention for adults at risk of NCDs and CMDs. A feasibility study, and subsequent optimisation and randomised-controlled trials are planned to further refine the intervention and establish effectiveness. The development process described here may prove helpful to other intervention developers. 2023 Castro, Mair, Salamanca-Sanabria, Alattas, Keller, Zheng, Jabir, Lin, Frese, Lim, Santhanam, van Dam, Car, Lee, Tai, Fleisch, von Wangenheim, Tudor Car, Müller-Riemenschneider and Kowatsch.",2023,,Frontiers in Digital Health,5,,,,10.3389/fdgth.2023.1039171,,#5240,Castro 2023,,,1
Become a Lifesaver - How to Design Conversational Agents to Increase Users’ Intention to Donate Blood,"Pietrantoni, N.; Lichtenberg, S.; Morana, S.","Donating blood is a selfless act that impacts public welfare, potentially saving human lives. However, blood shortage is a rising worldwide issue due to increased demand. Thus, finding ways to animate and motivate potential donors to donate blood is paramount. In this context, conversational agents (CAs) offer a promising approach to educating, promoting, and achieving desired behaviors. In this paper, we conducted an online experimental study (N=303) and investigated the effect of a human-like designed CA and fear-inducing communication on users’ intention to donate. Our results show that users’ intention is driven by perceived persuasiveness rather than perceived humanness and that fear-inducing communication does not significantly affect the intention to donate. Against this background, we provide numerous theoretical and practical implications, contributing to information system literature by enhancing our understanding of how fear-inducing communication is used in CA interactions. © 2023 International Conference on Information Systems, ICIS 2023: ""Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.",2023,,"International Conference on Information Systems, ICIS 2023: ""Rising like a Phoenix: Emerging from the Pandemic and Reshaping Human Endeavors with Digital Technologies""",,,,,,,#5244,Pietrantoni 2023,,,1
"Human favoritism, not AI aversion: People’s perceptions (and bias) toward generative AI, human experts, and human–GAI collaboration in persuasive content generation","Zhang, Y.; Gosline, R.","With the wide availability of large language models and generative AI, there are four primary paradigms for human–AI collaboration: human-only, AI-only (ChatGPT-4), augmented human (where a human makes the final decision with AI output as a reference), or augmented AI (where the AI makes the final decision with human output as a reference). In partnership with one of the world’s leading consulting firms, we enlisted professional content creators and ChatGPT-4 to create advertising content for products and persuasive content for campaigns following the aforementioned paradigms. First, we find that, contrary to the expectations of some of the existing algorithm aversion literature on conventional predictive AI, the content generated by generative AI and augmented AI is perceived as of higher quality than that produced by human experts and augmented human experts. Second, revealing the source of content production reduces—but does not reverse—the perceived quality gap between human-and AI-generated content. This bias in evaluation is predominantly driven by human favoritism rather than AI aversion: Knowing that the same content is created by a human expert increases its (reported) perceived quality, but knowing that AI is involved in the creation process does not affect its perceived quality. Further analysis suggests this bias is not due to a ‘quality prime’ as knowing the content they are about to evaluate comes from competent creators (e.g., industry professionals and state-of-the-art AI) without knowing exactly that the creator of each piece of content does not increase participants’ perceived quality. © The Author(s), 2023.",2023,,Judgment and Decision Making,18,,,,10.1017/JDM.2023.37,,#5253,Zhang 2023,,,1
Working With AI to Persuade: Examining a Large Language Model's Ability to Generate Pro-Vaccination Messages,"Karinshak, E.; Liu, S.X.; Park, J.S.; Hancock, J.T.","Artificial Intelligence (AI) is a transformative force in communication and messaging strategy, with potential to disrupt traditional approaches. Large language models (LLMs), a form of AI, are capable of generating high-quality, humanlike text. We investigate the persuasive quality of AI-generated messages to understand how AI could impact public health messaging. Specifically, through a series of studies designed to characterize and evaluate generative AI in developing public health messages, we analyze COVID-19 pro-vaccination messages generated by GPT-3, a state-of-the-art instantiation of a large language model. Study 1 is a systematic evaluation of GPT-3's ability to generate pro-vaccination messages. Study 2 then observed peoples' perceptions of curated GPT-3-generated messages compared to human-authored messages released by the CDC (Centers for Disease Control and Prevention), finding that GPT-3 messages were perceived as more effective, stronger arguments, and evoked more positive attitudes than CDC messages. Finally, Study 3 assessed the role of source labels on perceived quality, finding that while participants preferred AI-generated messages, they expressed dispreference for messages that were labeled as AI-generated. The results suggest that, with human supervision, AI can be used to create effective public health messages, but that individuals prefer their public health messages to come from human institutions rather than AI sources. We propose best practices for assessing generative outputs of large language models in future social science research and ways health professionals can use AI systems to augment public health messaging.  © 2023 ACM.",2023,,Proceedings of the ACM on Human-Computer Interaction,7,CSCW1,,,10.1145/3579592,,#5257,Karinshak 2023,,,1
Sustainability of Nature Parks by Changing Tourist Behavior Using Donations and Generative AI,"Sato, R.; Shimazaki, Y.; Yamaguchi, T.","Nature parks are one of the most popular tourist destinations in the world. Still, they are sometimes abandoned with little use despite their elegant landscapes, or they can be damaged by the influx of people who come for localized and temporary promotions, so it is necessary to ensure that a certain number of tourists continue to visit in a dispersed manner. While theme parks that charge admission fees may control the number of tourists by setting dynamic prices, this approach is not suitable for nature parks that are free of charge. Some nature parks collect donations, but this is not directly aimed at controlling the number of tourists. Therefore, we propose a system to boost and control the number of tourists by providing information to lure tourists to the best spots scattered throughout the vast nature park and offering unique amusement in return for their donations. We developed a smartphone application that amuses response to tourists' donations. In addition, we provided a tourist guide reflecting the status of sightseeing spots in real-time, and 89.6% of the subjects positively evaluated the guide. Furthermore, we observed that having the AI suggest ideas for making sightseeing spots more attractive had the effect of imagining the actions tourists would want to take at those spots. © 2023 IEEE.",2023,,"Proceedings - 2023 International Conference on Computational Science and Computational Intelligence, CSCI 2023",,,1014-1021,,10.1109/CSCI62032.2023.00168,,#5262,Sato 2023,,,1
Optimizing appreciation and persuasion of embodied conversational agents for health behavior change: A design experiment and focus group study,"Kramer, L.L.; van Velsen, L.; Mulder, B.C.; ter Stal, S.; de Vet, E.","Embodied Conversational Agents (ECAs) can increase user engagement and involvement and can strengthen the effect of an intervention on health outcomes that is provided via an ECA. However, evidence regarding the effectiveness of ECAs on health outcomes is still limited. In this article, we report on a study that has the goal to identify the effect of a match between a health topic and the ECAs’ appearance on ratings of personality characteristics, persuasiveness and intention to use. We report on an online experiment with three different ECAs and three different health topics, conducted among 732 older adults. We triangulated the quantitative results with qualitative insights from a focus group. The results reveal that older adults prefer an ECA that has an appearance matching a certain health topic, resulting in higher ratings on persuasiveness and intention to use. Personality characteristics should be measured embedded within a health topic, but are not rated higher because of a match. We furthermore provide guidelines for designing the content of the ECA. © The Author(s) 2023.",2023,,Health Informatics Journal,29,3,,,10.1177/14604582231183390,,#5264,Kramer 2023,,,1
Be a Miracle - Designing Conversational Agents to Influence Users’ Intention Regarding Organ Donation,"Pietrantoni, N.; Greulich, R.S.; Morana, S.","The increasing need for organ donations remains a worldwide challenge as transplant waiting lists grow and donation rates persist at constant levels. The increasing popularity of conversational agents (CAs) has prompted new strategies for educating and persuading individuals to adjust their cognitive and behavioral beliefs and become donors. However, how CAs should be designed to modify uninformed users’ intention to donate remains unclear. Against this background, we conducted an online experiment (N=134) to examine the impact of a human-like CA design on users' intention to become organ donors. Based on the three-factor theory of anthropomorphism and the elaboration likelihood model, we derive three theoretical mechanisms to understand the influence of a CAs human-like design on users’ intention to donate. The findings show that perceived anthropomorphism does not directly impact persuasion and empathy but is mediated via perceived usefulness to influence the intention to donate. © 2023 International Conference on Information Systems, ICIS 2023: ""Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.",2023,,"International Conference on Information Systems, ICIS 2023: ""Rising like a Phoenix: Emerging from the Pandemic and Reshaping Human Endeavors with Digital Technologies""",,,,,,,#5268,Pietrantoni 2023,,,1
Striking an Emotional Chord: Effects of Emotional Appeals and Chatbot Anthropomorphism on Persuasive Science Communication,"Wang, J.; Peng, L.","Chatbots have been used to achieve persuasive goals in various communication contexts. This research investigates how chatbot anthropomorphism intersects with emotional appeals to influence persuasive outcomes in science communication by conducting two experiments in the contexts of skin cancer prevention and biodiversity conservation. The findings showed a matching effect between emotional appeals and anthropomorphic cues: For a chatbot with more anthropomorphic cues, fear appeals were more persuasive than hope appeals; in contrast, for a less anthropomorphic chatbot, hope appeals were more effective. A key psychological mechanism underlying the relationships was personal risk perceptions but only for fear appeals. © The Author(s) 2023.",2023,,Science Communication,45,4,485-511,,10.1177/10755470231194583,,#5272,Wang 2023,,,1
3D Real-Time Conversational Agents: Do Facial Expressions and Camera Angles Persuade Human?,"Anggia, P.; Sumi, K.","This paper aims to uncover state-of-the-art on persuasion through a dialogue system between human and virtual agents. 37 participants interacted with six virtual agents in different conditions of facial expressions and camera angles. Through empirical measurements using the Big Five theory, we discover that virtual agents' friendliness becomes a solid persuading factor to our predominantly extroverted participants. The experiment deduces that openness, conscientiousness, and extraversion personalities are easily persuaded by our virtual agents, unlike reversed-conscientiousness and neuroticism personalities. © The 2023 International Conference on Artificial Life and Robotics (ICAROB2023), Feb. 9 to 12, on line, Oita, Japan.",2023,,Proceedings of International Conference on Artificial Life and Robotics,,,942-950,,,,#5275,Anggia 2023,,,1
Development of a Conversational Agent for Individuals Ambivalent About Quitting Smoking: Protocol for a Proof-of-Concept Study,"Nair, U.S.; Greene, K.; Marhefka, S.; Kosyluk, K.; Galea, J.T.","Background: Cigarette smoking is the leading preventable cause of disease and death in the United States. Despite the availability of a plethora of evidence-based smoking cessation resources, less than one-third of individuals who smoke seek cessation services, and individuals using these services are often those who are actively contemplating quitting smoking. There is a distinct dearth of low-cost, scalable interventions to support smokers not ready to quit (ambivalent smokers). Such interventions can assist in gradually promoting smoking behavior changes in this target population until motivation to quit arises, at which time they can be navigated to existing evidence-based smoking cessation interventions. Conversational agents or chatbots could provide cessation education and support to ambivalent smokers to build motivation and navigate them to evidence-based resources when ready to quit. Objective: The goal of our study is to test the proof-of-concept of the development and preliminary feasibility and acceptability of a smoking cessation support chatbot. Methods: We will accomplish our study aims in 2 phases. In phase 1, we will survey 300 ambivalent smokers to determine their preferences and priorities for a smoking cessation support chatbot. A “forced-choice experiment” will be administered to understand participants’ preferred characteristics (attributes) of the proposed chatbot prototype. The data gathered will be used to program the prototype. In phase 2, we will invite 25 individuals who smoke to use the developed prototype. For this phase, participants will receive an overview of the chatbot and be encouraged to use the chatbot and engage and interact with the programmed attributes and components for a 2-week period. Results: At the end of phase 1, we anticipate identifying key attributes that ambivalent smokers prefer in a smoking cessation support chatbot. At the end of phase 2, chatbot acceptability and feasibility will be assessed. The study was funded in June 2022, and data collection for both phases of the study is currently ongoing. We expect study results to be published by December 2023. Conclusions: Study results will yield a smoking behavior change chatbot prototype developed for ambivalent smokers that will be ready for efficacy testing in a larger study. © Uma S Nair, Karah Greene, Stephanie Marhefka, Kristin Kosyluk, Jerome T Galea.",2023,,JMIR Research Protocols,12,,,,10.2196/44041,,#5276,Nair 2023,,,1
Assessing the Feasibility of a Text-Based Conversational Agent for Asthma Support: Protocol for a Mixed Methods Observational Study,"Calvo, R.A.; Peters, D.; Moradbakhti, L.; Cook, D.; Rizos, G.; Schuller, B.; Kallis, C.; Wong, E.; Quint, J.","Background: Despite efforts, the UK death rate from asthma is the highest in Europe, and 65% of people with asthma in the United Kingdom do not receive the professional care they are entitled to. Experts have recommended the use of digital innovations to help address the issues of poor outcomes and lack of care access. An automated SMS text messaging–based conversational agent (ie, chatbot) created to provide access to asthma support in a familiar format via a mobile phone has the potential to help people with asthma across demographics and at scale. Such a chatbot could help improve the accuracy of self-assessed risk, improve asthma self-management, increase access to professional care, and ultimately reduce asthma attacks and emergencies. Objective: The aims of this study are to determine the feasibility and usability of a text-based conversational agent that processes a patient’s text responses and short sample voice recordings to calculate an estimate of their risk for an asthma exacerbation and then offers follow-up information for lowering risk and improving asthma control; assess the levels of engagement for different groups of users, particularly those who do not access professional services and those with poor asthma control; and assess the extent to which users of the chatbot perceive it as helpful for improving their understanding and self-management of their condition. Methods: We will recruit 300 adults through four channels for broad reach: Facebook, YouGov, Asthma + Lung UK social media, and the website Healthily (a health self-management app). Participants will be screened, and those who meet inclusion criteria (adults diagnosed with asthma and who use WhatsApp) will be provided with a link to access the conversational agent through WhatsApp on their mobile phones. Participants will be sent scheduled and randomly timed messages to invite them to engage in dialogue about their asthma risk during the period of study. After a data collection period (28 days), participants will respond to questionnaire items related to the quality of the interaction. A pre- and postquestionnaire will measure asthma control before and after the intervention. Results: This study was funded in March 2021 and started in January 2022. We developed a prototype conversational agent, which was iteratively improved with feedback from people with asthma, asthma nurses, and specialist doctors. Fortnightly reviews of iterations by the clinical team began in September 2022 and are ongoing. This feasibility study will start recruitment in January 2023. The anticipated completion of the study is July 2023. A future randomized controlled trial will depend on the outcomes of this study and funding. Conclusions: This feasibility study will inform a follow-up pilot and larger randomized controlled trial to assess the impact of a conversational agent on asthma outcomes, self-management, behavior change, and access to care. © Rafael A Calvo, Dorian Peters, Laura Moradbakhti, Darren Cook, Georgios Rizos, Bjoern Schuller, Constantinos Kallis, Ernie Wong, Jennifer Quint.",2023,,JMIR Research Protocols,12,,,,10.2196/42965,,#5280,Calvo 2023,,,1
Study-Buddy: A Knowledge Graph-Powered Learning Companion for School Students,"Martinez, F.; Collarana, D.; Calvaresi, D.; Arispe, M.; Florida, C.; Calbimonte, J.-P.","Large Language Models (LLMs) have the potential to substantially improve educational tools for students. However, they face limitations, including factual accuracy, personalization, and the lack of control over the sources of information. This paper presents Study-Buddy, a prototype of a conversational AI assistant for school students to address the above-mentioned limitations. Study-Buddy embodies an AI assistant based on a knowledge graph, LLMs models, and computational persuasion. It is designed to support educational campaigns as a hybrid AI solution. The demonstrator showcases interactions with Study-Buddy and the crucial role of the Knowledge Graph for the bot to present the appropriate activities to the students. A video demonstrating the main features of Study-Buddy is available at: https://youtu.be/DHPTsN1RI9o. © The Author(s), under exclusive license to Springer Nature Switzerland AG. 2023.",2023,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13998 LNCS,,133-137,,10.1007/978-3-031-43458-7_25,,#5289,Martinez 2023,,,1
Persuasive Dialogue Corpus: Graph-Based Approach Combining Persuader and Persuadee Perspectives,"Allamudi, M.; Scrivner, O.","Persuasion is omnipresent in our daily communication. As a mechanism for changing or forming one’s opinion or behavior, persuasive dialogues and their strategies have gained interest for developing intelligent conversational systems. Given the complexity of this task, persuasion systems, especially dealing in conversations that require ‘no action’ by the user but rather a change in opinion or belief, require specialized annotated corpora and the understanding of logical structure, natural language, and persuasive strategies. The sparsity of available annotated data and a wide range of proposed models make it challenging for developing strategic chatbots specific to user needs. To address these issues, this study introduces a novel framework combining a replicable data collection tool and a topic-independent annotation schema for designing an argument-graph corpus and incorporating both persuader and persuadee perspectives, essential for building smart conversational agents. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",2023,,Lecture Notes in Networks and Systems,561 LNNS,,607-621,,10.1007/978-3-031-18344-7_43,,#5297,Allamudi 2023,,,1
"Stampede Theory: Human Nature, Technology, and Runaway Social Realities","Feldman, P.","Stampede Theory: Human Nature, Technology, and Runaway Social Realities explores the biological, evolutionary and technological systems that drive troubling patterns of behavior among groups while also proposing actions to combat harm. The book discusses different ways that living beings coordinate and how the emergence of communication technologies has changed behaviors. As the problem of echo chambers and misinformation grows, it is crucial to understand underlying causes and provide solutions—this book does just that by pulling from multiple fields to produce a coherent story about how social realities are created and how they can create resilient communities or reinforce damaging beliefs. This interdisciplinary approach rests on three primary pillars: 1) How information systems affect the distribution of ideas, information, influence and belief; 2. Technology-mediated communication between individuals and groups, from stories pressed into clay tablets to “likes” on social media; 3) The sociology of behavioral bias in groups ranging from teams to nations. Because of its interdisciplinary foundations, the book includes chapters that address behavioral economics, cults, artificial intelligence, and the individual psychology of belief.  This will be a valuable resource for a range of readers, from political and social scientists to decision-makers in government and business, scientists in the fields of machine learning and AI, and more. © 2023 Elsevier Inc. All rights reserved.",2023,,"Stampede Theory: Human Nature, Technology, and Runaway Social Realities",,,1-249,,10.1016/C2022-0-02795-1,,#5301,Feldman 2023,,,1
The social psychology of eating,"Catellani, P.; Carfora, V.","Our eating decisions are guided by several psychological dimensions: cognitive, emotional, value-based, social, and behavioural. The social psychology of eating helps us understand these dimensions and how we can promote healthy and sustainable eating to improve people's wellbeing. What is most important in deciding what we want to eat? What drives people to go vegan? Do we tend to eat more when we are nervous? Does it change our behavior when we sit at the table with others? Why do we put off starting the diet until the next week? How does online and offline communication influence our eating behaviour? Is it possible to help people change their eating habits thanks to artificial intelligence? These and other questions are answered in this book, with up-to-date literature references and pointers to the most promising developments in the field. An essential text for undergraduate and graduate students, as well as researchers in the fields of psychology and nutrition. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.",2023,,The Social Psychology of Eating,,,1-164,,10.1007/978-3-031-35070-2,,#5304,Catellani 2023,,,1
A Digital Coach to Promote Emotion Regulation Skills,"Hopman, K.; Richards, D.; Norberg, M.M.","There is growing awareness that effective emotion regulation is critical for health, adjustment and wellbeing. Emerging evidence suggests that interventions that promote flexible emotion regulation may have the potential to reduce the incidence and prevalence of mental health problems in specific at-risk populations. The challenge is how best to engage with at risk populations, who may not be actively seeking assistance, to deliver this early intervention approach. One possible solution is via digital technology and development, which has rapidly accelerated in this space. Such rapid growth has, however, occurred at the expense of developing a deep understanding of key elements of successful program design and specific mechanisms that influence health behavior change. This paper presents a detailed description of the design, development and evaluation of an emotion regulation intervention conversational agent (ERICA) who acts as a digital coach. ERICA uses interactive conversation to encourage self-reflection and to support and empower users to learn a range of cognitive emotion regulation strategies including Refocusing, Reappraisal, Planning and Putting into Perspective. A pilot evaluation of ERICA was conducted with 138 university students and confirmed that ERICA provided a feasible and highly usable method for delivering an emotion regulation intervention. The results also indicated that ERICA was able to develop a therapeutic relationship with participants and increase their intent to use a range of cognitive emotion regulation strategies. These findings suggest that ERICA holds potential to be an effective approach for delivering an early intervention to support mental health and wellbeing. ERICA’s dialogue, embedded with interactivity, therapeutic alliance and empathy cues, provide the basis for the development of other psychoeducation interventions. © 2023 by the authors.",2023,,Multimodal Technologies and Interaction,7,6,,,10.3390/mti7060057,,#5306,Hopman 2023,,,1
Changing users’ health behaviour intentions through an embodied conversational agent delivering explanations based on users’ beliefs and goals,"Abdulrahman, A.; Richards, D.; Bilgin, A.A.","Interventions to improve health and well-being abound. Whether they are designed for prevention, maintenance or improvement, a key challenge is the motivation of the user to change their current behaviours, such as persisting or taking new actions. To encourage someone to change their behaviour requires persuading them to change their goals and/or their beliefs about the behaviour or their ability to perform it. Our embodied conversational agent (ECA) uses explanations based on the goals and beliefs of the user to promote a sense of personalisation and engagement with the treatment plan which could form a bond as the dyad develop shared goals and tasks together. To keep our message minimal and understand whether belief-based or goal-based explanations are more efficacious in changing behaviour intention, we collected data in the context of a scenario where the ECA seeks to change four behaviours recommended to help students manage their study stress. Our findings suggest that when the behaviour requires a change in desire, we need goal-based explanation, when adoption of the behaviour requires addressing a barrier we need belief-based explanation and warrant future investigation. Further, the stratified analysis suggested that more tailoring to the student’s context could provide more motivation to change. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",2023,,Behaviour and Information Technology,42,9,1338-1356,,10.1080/0144929X.2022.2073269,,#5313,Abdulrahman 2023,,,1
Large Language Models respond to Influence like Humans,"Griffin, L.D.; Kleinberg, B.; Mozes, M.; Mai, K.; Vau, M.; Caldwell, M.; Mavor-Parker, A.","Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement boosts a later truthfulness test rating. Analysis of newly collected data from human and LLM-simulated subjects (1000 of each) showed the same pattern of effects in both populations; although with greater per statement variability for the LLM. The second study concerns a specific mode of influence – populist framing of news to increase its persuasion and political mobilization. Newly collected data from simulated subjects was compared to previously published data from a 15-country experiment on 7286 human participants. Several effects from the human study were replicated by the simulated study, including ones that surprised the authors of the human study by contradicting their theoretical expectations; but some significant relationships found in human data were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence. © 2023 Association for Computational Linguistics.",2023,,Proceedings of the Annual Meeting of the Association for Computational Linguistics,,,15-24,,,,#5314,Griffin 2023,,,1
Persuading to Prepare for Quitting Smoking with a Virtual Coach: Using States and User Characteristics to Predict Behavior,"Albers, N.; Neerincx, M.A.; Brinkman, W.-P.","Despite their prevalence in eHealth applications for behavior change, persuasive messages tend to have small effects on behavior. Conditions or states (e.g., confidence, knowledge, motivation) and characteristics (e.g., gender, age, personality) of persuadees are two promising components for more effective algorithms for choosing persuasive messages. However, it is not yet sufficiently clear how well considering these components allows one to predict behavior after persuasive attempts, especially in the long run. Since collecting data for many algorithm components is costly and places a burden on users, a better understanding of the impact of individual components in practice is welcome. This can help to make an informed decision on which components to use. We thus conducted a longitudinal study in which a virtual coach persuaded 671 daily smokers to do preparatory activities for quitting smoking and becoming more physically active, such as envisioning one's desired future self. Based on the collected data, we designed a Reinforcement Learning (RL)-approach that considers current and future states to maximize the effort people spend on their activities. Using this RL-approach, we found, based on leave-one-out cross-validation, that considering states helps to predict both behavior and future states. User characteristics and especially involvement in the activities, on the other hand, only help to predict behavior if used in combination with states rather than alone. We see these results as supporting the use of states and involvement in persuasion algorithms. Our dataset is available online. © 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",2023,,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",2023-May,,717-726,,,,#5316,Albers 2023,,,1
Towards personalized persuasive dialogue generation for adversarial task oriented dialogue setting,"Tiwari, A.; Khandwe, A.; Saha, S.; Ramnani, R.; Maitra, A.; Sengupta, S.","In recent years, task-oriented virtual assistants have gained huge popularity and demand in both research and industry communities. The primary aim of a task-oriented dialogue agent is to assist end-users in accomplishing a task successfully and satisfactorily. Existing virtual agents have acquired proficiency in assisting users in solving simple tasks such as restaurant bookings. However, they operate under the deterministic presumption that end-users will have a servable task objective, which makes them inadequate under adversarial situations such as goal unavailability. On the other hand, human agents accomplish users’ tasks even in many goal unavailability scenarios by persuading them towards a similar goal to the user's proposed task. Motivated by the limitation, the current work proposes and builds a novel transformer-based context-aware personalized persuasive virtual assistant (CoPersUasive VA), which also serves end-users in task unavailability situations. The proposed CoPersUasive VA recognizes goal conflicts through user sentiment and identifies an appropriate persuasion strategy using ongoing dialogue context and user personality. Depending on users’ proposed goals, it finds a similar servable goal and persuades them with the identified persuasion strategy. The obtained experimental results and detailed post-analysis firmly establish that the proposed model effectively enhances the capability of task-oriented virtual assistants to deal with the task failures caused by goal unavailability. The obtained findings also suggest the crucial role of dialogue context in identifying an appropriate and appealing persuasion strategy. The proposed CoPersUasive model could easily be adapted to any other domain by fine-tuning the model on an underlying task. Furthermore, we developed a personalized persuasive multi-intent conversational dialogue corpus annotated with intent, slot, sentiment, and dialogue act for electronic domain. © 2022",2023,,Expert Systems with Applications,213,,,,10.1016/j.eswa.2022.118775,,#5318,Tiwari 2023,,,1
The Effects of Expressing Empathy/Autonomy Support Using a COVID-19 Vaccination Chatbot: Experimental Study in a Sample of Belgian Adults,"Trzebiński, W.; Claessens, T.; Buhmann, J.; De Waele, A.; Hendrickx, G.; Van Damme, P.; Daelemans, W.; Poels, K.","Background: Chatbots are increasingly used to support COVID-19 vaccination programs. Their persuasiveness may depend on the conversation-related context. Objective: This study aims to investigate the moderating role of the conversation quality and chatbot expertise cues in the effects of expressing empathy/autonomy support using COVID-19 vaccination chatbots. Methods: This experiment with 196 Dutch-speaking adults living in Belgium, who engaged in a conversation with a chatbot providing vaccination information, used a 2 (empathy/autonomy support expression: present vs absent) × 2 (chatbot expertise cues: expert endorser vs layperson endorser) between-subject design. Chatbot conversation quality was assessed through actual conversation logs. Perceived user autonomy (PUA), chatbot patronage intention (CPI), and vaccination intention shift (VIS) were measured after the conversation, coded from 1 to 5 (PUA, CPI) and from –5 to 5 (VIS). Results: There was a negative interaction effect of chatbot empathy/autonomy support expression and conversation fallback (CF; the percentage of chatbot answers “I do not understand” in a conversation) on PUA (PROCESS macro, model 1, B=–3.358, SE 1.235, t186=2.718, P=.007). Specifically, empathy/autonomy support expression had a more negative effect on PUA when the CF was higher (conditional effect of empathy/autonomy support expression at the CF level of +1SD: B=–.405, SE 0.158, t186=2.564, P=.011; conditional effects nonsignificant for the mean level: B=–0.103, SE 0.113, t186=0.914, P=.36; conditional effects nonsignificant for the –1SD level: B=0.031, SE=0.123, t186=0.252, P=.80). Moreover, an indirect effect of empathy/autonomy support expression on CPI via PUA was more negative when CF was higher (PROCESS macro, model 7, 5000 bootstrap samples, moderated mediation index=–3.676, BootSE 1.614, 95% CI –6.697 to –0.102; conditional indirect effect at the CF level of +1SD: B=–0.443, BootSE 0.202, 95% CI –0.809 to –0.005; conditional indirect effects nonsignificant for the mean level: B=–0.113, BootSE 0.124, 95% CI –0.346 to 0.137; conditional indirect effects nonsignificant for the –1SD level: B=0.034, BootSE 0.132, 95% CI –0.224 to 0.305). Indirect effects of empathy/autonomy support expression on VIS via PUA were marginally more negative when CF was higher. No effects of chatbot expertise cues were found. Conclusions: The findings suggest that expressing empathy/autonomy support using a chatbot may harm its evaluation and persuasiveness when the chatbot fails to answer its users’ questions. The paper adds to the literature on vaccination chatbots by exploring the conditional effects of chatbot empathy/autonomy support expression. The results will guide policy makers and chatbot developers dealing with vaccination promotion in designing the way chatbots express their empathy and support for user autonomy. ©Wojciech Trzebiński, Toni Claessens, Jeska Buhmann, Aurélie De Waele, Greet Hendrickx, Pierre Van Damme, Walter Daelemans, Karolien Poels.",2023,,JMIR Formative Research,7,,,,10.2196/41148,,#5324,Trzebiński 2023,,,1
A Technology-Enhanced Intervention for Violence and Substance Use Prevention Among Young Black Men: Protocol for Adaptation and Pilot Testing,"Emezue, C.; Karnik, N.S.; Reeder, B.; Schoeny, M.; Layfield, R.; Zarling, A.; Julion, W.","Background: Black boys and men from disinvested communities are disproportionately survivors and perpetrators of youth violence. Those presenting to emergency departments with firearm-related injuries also report recent substance use. However, young Black men face several critical individual and systemic barriers to accessing trauma-focused prevention programs. These barriers contribute to service avoidance, the exacerbation of violence recidivism, substance use relapse, and a revolving-door approach to prevention. In addition, young Black men are known to be digital natives. Therefore, technology-enhanced interventions offer a pragmatic and promising opportunity to mitigate these barriers, provide vital life skills for self-led behavior change, and boost service engagement with vital community resources. Objective: The study aims to systematically adapt and pilot-test Boosting Violence-Related Outcomes Using Technology for Empowerment, Risk Reduction, and Life Skills Preparation in Youth Based on Acceptance and Commitment Therapy (BrotherlyACT), a culturally congruent, trauma-focused digital psychoeducational and service-engagement tool tailored to young Black men aged 15-24 years. BrotherlyACT will incorporate microlearning modules, interactive safety planning tools for risk assessment, goal-setting, mindfulness practice, and a service-engagement conversational agent or chatbot to connect young Black men to relevant services. Methods: The development of BrotherlyACT will occur in 3 phases. In phase 1, we will qualitatively investigate barriers and facilitators influencing young Black men’s willingness to use violence and substance use prevention services with 15-30 young Black men (aged 15-24 years) who report perpetrating violence and substance use in the past year and 10 service providers (aged >18 years; any gender; including health care providers, street outreach workers, social workers, violence interrupters, community advocates, and school staff). Both groups will be recruited from community and pediatric emergency settings. In phase 2, a steering group of topic experts (n=3-5) and a youth and community advisory board comprising young Black men (n=8-12) and service providers (n=5-10) will be involved in participatory design, alpha testing, and beta testing sessions to develop, refine, and adapt BrotherlyACT based on an existing skills-based program (Achieving Change Through Values-Based Behavior). We will use user-centered design principles and the Assessment, Decision, Administration, Production, Topical, Experts, Integration, Training, and Testing framework to guide this adaptation process (phase 2). In phase 3, a total of 60 young Black men will pilot-test the adapted BrotherlyACT over 10 weeks in a single-group, pretest-posttest design to determine its feasibility and implementation outcomes. Results: Phase 1 data collection began in September 2021. Phases 2 and 3 are scheduled to start in June 2023 and end in September 2024. Conclusions: The development and testing of BrotherlyACT is a crucial first step in expanding an evidence-based psychoeducational and service-mediating intervention for young Black men involved in violence. This colocation of services shifts the current prevention strategy from telling them why to change to teaching them how. © Chuka Emezue, Niranjan S Karnik, Blaine Reeder, Michael Schoeny, Rickey Layfield, Amie Zarling, Wrenetha Julion.",2023,,JMIR Research Protocols,12,,,,10.2196/43842,,#5350,Emezue 2023,,,1
BONE+: AN AI-DRIVEN CHATBOT PLATFORM FOR PROMOTING BEHAVIOR CHANGE IN BONE HEALTH AND OSTEOPOROSIS PREVENTION,"Tsai, T.-H.; Hsieh, Y.-S.; Ho, C.-S.; Pei, Y.-C.; Hsu, C.-L.","Osteoporosis is a prevalent medical condition characterized by a reduction in bone mass and density, which significantly increases the risk of fractures. In order to promote optimal bone health, it is essential to acquire relevant information and adopt a lifestyle that safeguards bone integrity. In light of this, the present study employed the Health Belief Model as a framework to design and develop the Bone+ mobile health chatbot platform. Leveraging artificial intelligence technology, the platform utilizes image analysis of X-rays to assess bone density and predict the likelihood of bone loss. Moreover, it offers comprehensive information on osteoporosis prevention, empowering users to make well-informed decisions regarding their bone health. The results obtained from the heuristic evaluation questionnaire of the chatbot platform demonstrated satisfactory usability, thereby further supporting its efficacy as a valuable tool in osteoporosis prevention. © ICT 2023.All rights reserved.",2023,,"Proceedings of the International Conferences on ICT, Society, and Human Beings 2023, ICT 2023; and e-Health 2023, EH 2023; Connected Smart Cities 2023, CSC 2023; and Big Data Analytics, Data Mining and Computational Intelligence 2023, BigDaCI 2023",,,303-307,,,,#5351,Tsai 2023,,,1
Changebots - Designing Chatbots to Support Blood Donor Behaviour Change,"Müller, H.M.; Reuter-Oppermann, M.","Even though blood products cannot be produced artificially, but are important for many surgeries and treatments, less than 1 % of the population donates blood in countries like South Africa or Ghana. Therefore, efficient and successful blood donor mobilisation and management are important. We argue that a chatbot offers easy access to information for all types of donors and can support the transition of non-, first-time or lapsed donors to regular donors. By applying the design science research methodology, we have developed a chatbot for all donor types in South Africa and Ghana. We performed two design cycles, collaborating with experts from three blood services and grounding our research on existing and derived behavioural change models. The chatbot was positively evaluated in two workshops that included focus group discussions and online surveys. © 2023 International Conference on Information Systems, ICIS 2023: ""Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.",2023,,"International Conference on Information Systems, ICIS 2023: ""Rising like a Phoenix: Emerging from the Pandemic and Reshaping Human Endeavors with Digital Technologies""",,,,,,,#5381,Müller 2023,,,1
A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,"Brown, A.; Kumar, A.T.; Melamed, O.; Ahmed, I.; Wang, Y.H.; Deza, A.; Morcos, M.; Zhu, L.; Maslej, M.; Minian, N.; Sujaya, V.; Wolff, J.; Doggett, O.; Iantorno, M.; Ratto, M.; Selby, P.; Rose, J.","Background: The motivational interviewing (MI) approach has been shown to help move ambivalent smokers toward the decision to quit smoking. There have been several attempts to broaden access to MI through text-based chatbots. These typically use scripted responses to client statements, but such nonspecific responses have been shown to reduce effectiveness. Recent advances in natural language processing provide a new way to create responses that are specific to a client's statements, using a generative language model. Objective: This study aimed to design, evolve, and measure the effectiveness of a chatbot system that can guide ambivalent people who smoke toward the decision to quit smoking with MI-style generative reflections. Methods: Over time, 4 different MI chatbot versions were evolved, and each version was tested with a separate group of ambivalent smokers. A total of 349 smokers were recruited through a web-based recruitment platform. The first chatbot version only asked questions without reflections on the answers. The second version asked the questions and provided reflections with an initial version of the reflection generator. The third version used an improved reflection generator, and the fourth version added extended interaction on some of the questions. Participants' readiness to quit was measured before the conversation and 1 week later using an 11-point scale that measured 3 attributes related to smoking cessation: readiness, confidence, and importance. The number of quit attempts made in the week before the conversation and the week after was surveyed; in addition, participants rated the perceived empathy of the chatbot. The main body of the conversation consists of 5 scripted questions, responses from participants, and (for 3 of the 4 versions) generated reflections. A pretrained transformer-based neural network was fine-tuned on examples of high-quality reflections to generate MI reflections. Results: The increase in average confidence using the nongenerative version was 1.0 (SD 2.0; P=.001), whereas for the 3 generative versions, the increases ranged from 1.2 to 1.3 (SD 2.0-2.3; P<.001). The extended conversation with improved generative reflections was the only version associated with a significant increase in average importance (0.7, SD 2.0; P<.001) and readiness (0.4, SD 1.7; P=.01). The enhanced reflection and extended conversations exhibited significantly better perceived empathy than the nongenerative conversation (P=.02 and P=.004, respectively). The number of quit attempts did not significantly change between the week before the conversation and the week after across all 4 conversations. Conclusions: The results suggest that generative reflections increase the impact of a conversation on readiness to quit smoking 1 week later, although a significant portion of the impact seen so far can be achieved by only asking questions without the reflections. These results support further evolution of the chatbot conversation and can serve as a basis for comparison against more advanced versions. © 2023 Berghahn Journals. All rights reserved.",2023,,JMIR Mental Health,10,1,,,10.2196/49132,,#5391,Brown 2023,,,1
Persuasive strategy implementation choices and their effectiveness: towards personalised persuasive systems,"Oyebode, O.; Orji, R.","Persuasive systems motivate behaviour change using persuasive strategies (PS) which are often implemented in various ways. However, whether or not the effectiveness of PS varies depending on implementation choices is yet to be investigated via an empirical study. We conduct a large-scale study of 568 participants to investigate if and how individuals at different Stages of Change (SoC) respond to different implementations of each strategy in the same system. We also explore why the implementations motivate behaviour change using ARCS motivation model. Our results show that people’s SoC plays a significant role in the perceived effectiveness of different implementations of the same strategy and that the implementations motivate for different reasons. For instance, people at the precontemplation stage are motivated by reward strategy implemented as badges because it increases their Confidence, while people in the preparation stage prefer reward implemented as points to build their Confidence. Our work links SoC theory with motivation theory and Persuasive Systems Design (PSD) model to offer practical guidelines for tailoring PS implementations to individuals to motivate behaviour change. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",2023,,Behaviour and Information Technology,42,13,2176-2209,,10.1080/0144929X.2022.2112612,,#5411,Oyebode 2023,,,1
Automating Extremism: Mapping the Affective Roles of Artificial Agents in Online Radicalization,"Mantello, P.; Ho, T.M.; Podoletz, L.","This chapter argues that social media robots, more commonly known as “bots,” are becoming a formative tool of online radicalization. Besides pushing targeted and personalized content perceived by users as more persuasive, algorithmic innovations in “affect recognition” have given artificially intelligent agents the ability to exploit the emotional state of social media users. On encrypted social media channels, such social media bots are especially useful to extremist groups which lack human and financial resources to enact large-scale psychological warfare campaigns. As newer generations of social bots grow their ability to read and respond to human emotions and, in turn, increase their anthropomorphic tendencies, we argue that these automated headhunters will play a dominant role in online radicalization. This chapter undertakes a qualitative study of the affective role of conversational AI in establishing an emotional relationship with potential recruits in online radicalization. We also assess current efforts by Western security agencies, social media companies, and academic researchers to counter online radicalization strategies by extremist organizations. Our chapter points to the need of greater cross-platform multi-disciplinary research, study of bots that operate in foreign languages, and detection of extremist content in multimodal forms such as memes, music, videos, and selfies. © The Editor(s) (if applicable) and The Author(s), under exclusive licence to Springer Nature Switzerland AG 2023.",2023,,The Palgrave Handbook of Malicious Use of AI and Psychological Security,,,81-103,,10.1007/978-3-031-22552-9_4,,#5425,Mantello 2023,,,1
Can the Internet of Things Persuade Me? An Investigation Into Power Dynamics in Human-Internet of Things Interaction,"Kang, H.; Kim, K.J.; Wang, S.","The advent of artificial intelligence (AI) and the Internet of Things (IoT) has revolutionized user experience with objects. Things can perform social roles and convey persuasive messages to users, posing an important research question for communication and human-computer interaction researchers: What are the factors and underlying mechanisms that shape persuasive effects of IoT? Bridging the reactance theory and the computers are social actors paradigm, this study focuses on how power dynamics are shaped in human-IoT interactions and its implications on persuasion. Specifically, the study examines the effects of the social role assigned to the IoT mobile app agent and the scope of IoT controlled by the app on users’ perceived power and subsequent persuasive outcomes. The results reveal that when the mobile IoT app is for controlling a smart home, the servant (vs. companion) agent elicits greater perceived power over IoT for users, leading to less threat-to-freedom and better persuasive outcomes, including attitude, intention, and actual behavior. However, such a difference is not observed when the mobile app is for controlling a single smart device (i.e., smart fridge). The study findings offer valuable implications for communication practitioners interested in using IoT as a persuasive tool. Copyright © 2022 Kang, Kim and Wang.",2022,,Frontiers in Psychology,13,,,,10.3389/fpsyg.2022.883110,,#5433,Kang 2022,,,1
Usability Testing of a Social Media Chatbot for Increasing Physical Activity Behavior,"Larbi, D.; Denecke, K.; Gabarron, E.","Digital interventions for increasing physical activity behavior have shown great potential, especially those with social media. Chatbots, also known as conversational agents, have emerged in healthcare in relation to digital interventions and have proven effective in promoting physical activity among adults. The study’s objective is to explore users’ experiences with a social media chatbot. The concept and the prototype development of the social media chatbot MYA were realized in three steps: requirement analysis, concept development, and implementation. MYA’s design includes behavior change techniques effective in increasing physical activity through digital interventions. Participants in a usability study answered a survey with the Chatbot Usability Questionnaire (CUQ), which is comparable to the Systems Usability Scale. The mean CUQ score was below 68, the benchmark for average usability. The highest mean CUQ score was 64.5 for participants who thought MYA could help increase their physical activity behavior. The lowest mean CUQ score was 40.6 for participants aged between 50 and 69 years. Generally, MYA was considered to be welcoming, very easy to use, realistic, engaging, and informative. However, some technical issues were identified. A good and diversified user experience promotes prolonged chatbot use. Addressing identified issues will enhance users’ interaction with MYA. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",2022,,Journal of Personalized Medicine,12,5,,,10.3390/jpm12050828,,#5435,Larbi 2022,,,1
Fired by an algorithm? Exploration of conformism with biased intelligent decision support systems in the context of workplace discipline,"Bartosiak, M.L.; Modlinski, A.","Purpose: The importance of artificial intelligence in human resource management has grown substantially. Previous literature discusses the advantages of AI implementation at a workplace and its various consequences, often hostile, for employees. However, there is little empirical research on the topic. The authors address this gap by studying if individuals oppose biased algorithm recommendations regarding disciplinary actions in an organisation. Design/methodology/approach: The authors conducted an exploratory experiment in which the authors evaluated 76 subjects over a set of 5 scenarios in which a biased algorithm gave strict recommendations regarding disciplinary actions at a workplace. Findings: The authors’ results suggest that biased suggestions from intelligent agents can influence individuals who make disciplinary decisions. Social implications: The authors’ results contribute to the ongoing debate on applying AI solutions to HR problems. The authors demonstrate that biased algorithms may substantially change how employees are treated and show that human conformity towards intelligent decision support systems is broader than expected. Originality/value: The authors’ paper is among the first to show that people may accept recommendations that provoke moral dilemmas, bring adverse outcomes, or harm employees. The authors introduce the problem of “algorithmic conformism” and discuss its consequences for HRM. © 2022, Marcin Lukasz Bartosiak and Artur Modlinski.",2022,,Career Development International,27,06-Jul,601-615,,10.1108/CDI-06-2022-0170,,#5442,Bartosiak 2022,,,1
Developing Misinformation Immunity: How to Reason-Check Fallacious News in a Human–Computer Interaction Environment,"Musi, E.; Carmi, E.; Reed, C.; Yates, S.; O’Halloran, K.","To counter the fake news phenomenon, the scholarly community has attempted to debunk and prebunk disinformation. However, misinformation still constitutes a major challenge due to the variety of misleading techniques and their continuous updates which call for the exercise of critical thinking to build resilience. In this study we present two open access chatbots, the Fake News Immunity Chatbot and the Vaccinating News Chatbot, which combine Fallacy Theory and Human–Computer Interaction to inoculate citizens and communication gatekeepers against misinformation. These chatbots differ from existing tools both in function and form. First, they target misinformation and enhance the identification of fallacious arguments; and second, they are multiagent and leverage discourse theories of persuasion in their conversational design. After having described both their backend and their frontend design, we report on the evaluation of the user interface and impact on users’ critical thinking skills through a questionnaire, a crowdsourced survey, and a pilot qualitative experiment. The results shed light on the best practices to design user-friendly active inoculation tools and reveal that the two chatbots are perceived as increasing critical thinking skills in the current misinformation ecosystem. © The Author(s) 2023.",2023,,Social Media and Society,9,1,,,10.1177/20563051221150407,,#5446,Musi 2023,,,1
"A Smartphone Healthcare Application, CALO mama Plus, to Promote Weight Loss: A Randomized Controlled Trial","Nakata, Y.; Sasai, H.; Gosho, M.; Kobayashi, H.; Shi, Y.; Ohigashi, T.; Mizuno, S.; Murayama, C.; Kobayashi, S.; Sasaki, Y.","Mobile applications are increasingly used in healthcare. We have developed a smartphone healthcare application, CALO mama Plus, that can register daily diet, exercise, mood, and sleep quality, calculate dietary intake, and provide advice using artificial intelligence technology. This 3-month randomized controlled trial tested the hypothesis that CALO mama Plus could promote body weight reduction in Japanese adults with overweight or obesity. We recruited office workers as participants. The key eligibility criteria were an age of 20–65 years and a body mass index of 23–40 kg/m2. The primary outcome was body weight change over 3 months. We enrolled 141 participants and randomly assigned them to the intervention (n = 72) and control (n = 69) groups. The intervention group used CALO mama Plus, and the control group did not receive any intervention. The change in body weight was −2.4 ± 4.0 kg and −0.7 ± 3.3 kg in the intervention and control groups, respectively. An analysis of covariance adjusted for related variables showed a significant between-group difference in body weight change (−1.60 kg; 95% confidence interval −2.83 to −0.38; p = 0.011). The present study suggests that CALO mama Plus effectively promotes weight loss. © 2022 by the authors.",2022,,Nutrients,14,21,,,10.3390/nu14214608,,#5450,Nakata 2022,,,1
Investigating the Impact of AIGC Features on the Users' Perception of the Persuasiveness of AIGC: A Perspective of Two-dimensional Awe Emotions,"Liu, X.; Yang, B.; Cheng, H.; Sun, Y.","The development of Artificial Intelligence (AI) continues to profoundly influence the human-intelligence interaction. As AI-Generated Content(AIGC) progressively approaches, and in some instances, even surpasses human-created content, it augments the natural human-intelligence interaction experience, offering users convenient and efficient information services. However, it also raises the issue of the users' perception of the persuasiveness of AIGC. Consequently, there is an imperative to empirically investigate the users' perception of the persuasiveness of AIGC. Drawing upon the Stimulus-Organism-Response (SOR) theory, this paper introduces two novel variables, namely positive and negative awe, to construct a comprehensive model that elucidates the factors influencing the users' perception of the persuasiveness of AIGC. To empirically test this model, we gathers a dataset comprising 298 valid responses through a web-based questionnaire. We employ the Partial Least Squares Structural Equation Modeling (PLS-SEM) technique for rigorous statistical analysis. The findings of this study reveal that AIGC's cognitive and relational competencies exert a significantly positive impact on the elicitation of positive awe among users, while AIGC's cognitive and emotional competencies are associated with a significantly negative effect on the generation of negative awe among users. Furthermore, this study shows that positive awe has a notably favorable influence on the users' perception of the persuasiveness of AIGC, negative awe has a negative effect on the users' perception of the persuasiveness of AIGC. Innovatively, this paper introduces the concept of awe as a pivotal mechanism influencing the users' perceptions of the persuasiveness of AIGC. Through rigorous empirical analysis, this paper provides advice for technology companies on enhancing the users' perception of the persuasiveness of AIGC. © 2023 International Consortium for Electronic Business. All rights reserved.",2023,,Proceedings of the International Conference on Electronic Business (ICEB),23,,409-418,,,,#5461,Liu 2023,,,1
Scaling up interactive argumentation by providing counterarguments with a chatbot,"Altay, S.; Schwartz, M.; Hacquin, A.-S.; Allard, A.; Blancke, S.; Mercier, H.","Discussion is more convincing than standard, unidirectional messaging, but its interactive nature makes it difficult to scale up. We created a chatbot to emulate the most important traits of discussion. A simple argument pointing out the existence of a scientific consensus on the safety of genetically modified organisms (GMOs) already led to more positive attitudes towards GMOs, compared with a control message. Providing participants with good arguments rebutting the most common counterarguments against GMOs led to much more positive attitudes towards GMOs, whether the participants could immediately see all the arguments or could select the most relevant arguments in a chatbot. Participants holding the most negative attitudes displayed more attitude change in favour of GMOs. Participants updated their beliefs when presented with good arguments, but we found no evidence that an interactive chatbot proves more persuasive than a list of arguments and counterarguments. © 2022, The Author(s), under exclusive licence to Springer Nature Limited.",2022,,Nature Human Behaviour,6,4,579-592,,10.1038/s41562-021-01271-w,,#5462,Altay 2022,,,1
"An artificially intelligent, natural language processing chatbot designed to promote COVID-19 vaccination: A proof-of-concept pilot study","Zhou, S.; Silvasstar, J.; Clark, C.; Salyers, A.J.; Chavez, C.; Bull, S.S.","Objective: Our goal is to establish the feasibility of using an artificially intelligent chatbot in diverse healthcare settings to promote COVID-19 vaccination. Methods: We designed an artificially intelligent chatbot deployed via short message services and web-based platforms. Guided by communication theories, we developed persuasive messages to respond to users’ COVID-19-related questions and encourage vaccination. We implemented the system in healthcare settings in the U.S. between April 2021 and March 2022 and logged the number of users, topics discussed, and information on system accuracy in matching responses to user intents. We regularly reviewed queries and reclassified responses to better match responses to query intents as COVID-19 events evolved. Results: A total of 2479 users engaged with the system, exchanging 3994 COVID-19 relevant messages. The most popular queries to the system were about boosters and where to get a vaccine. The system's accuracy rate in matching responses to user queries ranged from 54% to 91.1%. Accuracy lagged when new information related to COVID emerged, such as that related to the Delta variant. Accuracy increased when we added new content to the system. Conclusions: It is feasible and potentially valuable to create chatbot systems using AI to facilitate access to current, accurate, complete, and persuasive information on infectious diseases. Such a system can be adapted to use with patients and populations needing detailed information and motivation to act in support of their health. © The Author(s) 2023.",2023,,Digital Health,9,,,,10.1177/20552076231155679,,#5463,Zhou 2023,,,1
How to Use Artificial Intelligence to Improve Entrepreneurial Attitude in Business Simulation Games: Implications From a Quasi-Experiment,"Chen, J.; Chen, Y.; Ou, R.; Wang, J.; Chen, Q.","Business simulation games (BSGs) have been widely used in entrepreneurship education with positive effects. However, there are still some deficiencies in the BSGs, such as limited guidance, low uncertainty and limited simulation environment, which make it impossible to exert the maximum effect. Artificial intelligence (AI) can solve the above shortcomings. The combination of AI and BSGs is the possible development direction of BSGs. But how to effectively combine BSGs with AI is still an open question. Using a quasi-experimental design, this study uses fuzzy-set qualitative comparative analysis to analyze how participants’ entrepreneurial attitude changes in BSGs. The results show that BSGs can effectively improve entrepreneurial attitude, and there are four types of promotion configurations. These four configurations consist of five antecedent conditions. According to the above conclusions, AI can improve entrepreneurial attitude in BSGs in various ways, such as simulating competitors, providing targeted feedback for failures, and improving game experience. The contribution of this paper is to highlight the possibility of combining AI with BSGs, and to provide suggestions on how AI can intervene in BSGs. Copyright © 2022 Chen, Chen, Ou, Wang and Chen.",2022,,Frontiers in Psychology,13,,,,10.3389/fpsyg.2022.856085,,#5464,Chen 2022,,,1
Users’ needs for a digital smoking cessation application and how to address them: A mixed-methods study,"Albers, N.; Neerincx, M.A.; Penfornis, K.M.; Brinkman, W.-P.","Background: Despite their increasing prevalence and potential, eHealth applications for behavior change suffer from a lack of adherence and from dropout. Advances in virtual coach technology provide new opportunities to improve this. However, these applications still do not always offer what people need. We, therefore, need a better understanding of people’s needs and how to address these, based on both actual experiences of users and their reflections on envisioned scenarios. Methods: We conducted a longitudinal study in which 671 smokers interacted with a virtual coach in five sessions. The virtual coach assigned them a new preparatory activity for quitting smoking or increasing physical activity in each session. Participants provided feedback on the activity in the next session. After the five sessions, participants were asked to describe barriers and motivators for doing their activities. In addition, they provided their views on videos of scenarios such as receiving motivational messages. To understand users’ needs, we took a mixed-methods approach. This approach triangulated findings from qualitative data, quantitative data, and the literature. Results: We identified 14 main themes that describe people’s views of their current and future behaviors concerning an eHealth application. These themes relate to the behaviors themselves, the users, other parties involved in a behavior, and the environment. The most prevalent theme was the perceived usefulness of behaviors, especially whether they were informative, helpful, motivating, or encouraging. The timing and intensity of behaviors also mattered. With regards to the users, their perceived importance of and motivation to change, autonomy, and personal characteristics were major themes. Another important role was played by other parties that may be involved in a behavior, such as general practitioners or virtual coaches. Here, the themes of companionableness, accountability, and nature of the other party (i.e., human vs AI) were relevant. The last set of main themes was related to the environment in which a behavior is performed. Prevalent themes were the availability of sufficient time, the presence of prompts and triggers, support from one’s social environment, and the diversity of other environmental factors. We provide recommendations for addressing each theme. Conclusions: The integrated method of experience-based and envisioning-based needs acquisition with a triangulate analysis provided a comprehensive needs classification (empirically and theoretically grounded). We expect that our themes and recommendations for addressing them will be helpful for designing applications for health behavior change that meet people’s needs. Designers should especially focus on the perceived usefulness of application components. To aid future work, we publish our dataset with user characteristics and 5,074 free-text responses from 671 people. Copyright 2022 Albers et al.",2022,,PeerJ,10,,,,10.7717/peerj.13824,,#5469,Albers 2022,,,1
Nonusage Attrition of Adolescents in an mHealth Promotion Intervention and the Role of Socioeconomic Status: Secondary Analysis of a 2-Arm Cluster-Controlled Trial,"Maenhout, L.; Peuters, C.; Cardon, G.; Crombez, G.; DeSmet, A.; Compernolle, S.","Background: Mobile health (mHealth) interventions may help adolescents adopt healthy lifestyles. However, attrition in these interventions is high. Overall, there is a lack of research on nonusage attrition in adolescents, particularly regarding the role of socioeconomic status (SES). Objective: The aim of this study was to focus on the role of SES in the following three research questions (RQs): When do adolescents stop using an mHealth intervention (RQ1)? Why do they report nonusage attrition (RQ2)? Which intervention components (ie, self-regulation component, narrative, and chatbot) prevent nonusage attrition among adolescents (RQ3)? Methods: A total of 186 Flemish adolescents (aged 12-15 years) participated in a 12-week mHealth program. Log data were monitored to measure nonusage attrition and usage duration for the 3 intervention components. A web-based questionnaire was administered to assess reasons for attrition. A survival analysis was conducted to estimate the time to attrition and determine whether this differed according to SES (RQ1). Descriptive statistics were performed to map the attrition reasons, and Fisher exact tests were used to determine if these reasons differed depending on the educational track (RQ2). Mixed effects Cox proportional hazard regression models were used to estimate the associations between the use duration of the 3 components during the first week and attrition. An interaction term was added to the regression models to determine whether associations differed by the educational track (RQ3). Results: After 12 weeks, 95.7% (178/186) of the participants stopped using the app. 30.1% (56/186) of the adolescents only opened the app on the installation day, and 44.1% (82/186) stopped using the app in the first week. Attrition at any given time during the intervention period was higher for adolescents from the nonacademic educational track compared with those from the academic track. The other SES indicators (family affluence and perceived financial situation) did not explain attrition. The most common reasons for nonusage attrition among participants were perceiving that the app did not lead to behavior change, not liking the app, thinking that they already had a sufficiently healthy lifestyle, using other apps, and not being motivated by the environment. Attrition reasons did not differ depending on the educational track. More time spent in the self-regulation and narrative components during the first week was associated with lower attrition, whereas chatbot use duration was not associated with attrition rates. No moderating effects of SES were observed in the latter association. Conclusions: Nonusage attrition was high, especially among adolescents in the nonacademic educational track. The reported reasons for attrition were diverse, with no statistical differences according to the educational level. The duration of the use of the self-regulation and narrative components during the first week may prevent attrition for both educational tracks. © Laura Maenhout, Carmen Peuters, Greet Cardon, Geert Crombez, Ann DeSmet, Sofie Compernolle",2022,,JMIR mHealth and uHealth,10,5,,,10.2196/36404,,#5470,Maenhout 2022,,,1
Does humanization or machinization make the IoT persuasive? The effects of source orientation and social presence,"Kang, H.; Kim, K.J.","The advent of Internet of Things (IoT) technology has revolutionized both the roles and functions of everyday objects and how users interact with them. Using artificial intelligence (AI) and an advanced capacity for communication, smart objects can now function as communication sources and deliver persuasive messages. This study investigates how different types of agency and source cues shape the persuasiveness of a smart object via social presence. When users interacted with a smart object that exerted its own agency, they sensed greater social presence when the object used machine cues rather than human cues. Conversely, when users interacted with a smart object that allowed the user to exercise their own agency, human cues, rather than machine cues, produced greater feelings of social presence, which enhanced the persuasiveness of the messages conveyed by the object. However, the persuasive effects of social presence were reversed when the interaction prompted AI anxiety in the user. © 2021",2022,,Computers in Human Behavior,129,,,,10.1016/j.chb.2021.107152,,#5476,Kang 2022,,,1
Designing a feature selection method based on explainable artificial intelligence,"Zacharias, J.; von Zahn, M.; Chen, J.; Hinz, O.","Nowadays, artificial intelligence (AI) systems make predictions in numerous high stakes domains, including credit-risk assessment and medical diagnostics. Consequently, AI systems increasingly affect humans, yet many state-of-the-art systems lack transparency and thus, deny the individual’s “right to explanation”. As a remedy, researchers and practitioners have developed explainable AI, which provides reasoning on how AI systems infer individual predictions. However, with recent legal initiatives demanding comprehensive explainability throughout the (development of an) AI system, we argue that the pre-processing stage has been unjustifiably neglected and should receive greater attention in current efforts to establish explainability. In this paper, we focus on introducing explainability to an integral part of the pre-processing stage: feature selection. Specifically, we build upon design science research to develop a design framework for explainable feature selection. We instantiate the design framework in a running software artifact and evaluate it in two focus group sessions. Our artifact helps organizations to persuasively justify feature selection to stakeholders and, thus, comply with upcoming AI legislation. We further provide researchers and practitioners with a design framework consisting of meta-requirements and design principles for explainable feature selection. © 2022, The Author(s).",2022,,Electronic Markets,32,4,2159-2184,,10.1007/s12525-022-00608-1,,#5483,Zacharias 2022,,,1
Motivational Interviewing Conversational Agent for Parents as Proxies for Their Children in Healthy Eating: Development and User Testing,"Smriti, D.; Annie Kao, T.-S.; Rathod, R.; Shin, J.Y.; Peng, W.; Williams, J.; Mujib, M.I.; Colosimo, M.; Huh-Yoo, J.","Background: Increased adoption of off-the-shelf conversational agents (CAs) brings opportunities to integrate therapeutic interventions. Motivational Interviewing (MI) can then be integrated with CAs for cost-effective access to it. MI can be especially beneficial for parents who often have low motivation because of limited time and resources to eat healthy together with their children. Objective: We developed a Motivational Interviewing Conversational Agent (MICA) to improve healthy eating in parents who serve as a proxy for health behavior change in their children. Proxy relationships involve a person serving as a catalyst for behavior change in another person. Parents, serving as proxies, can bring about behavior change in their children. Methods: We conducted user test sessions of the MICA prototype to understand the perceived acceptability and usefulness of the MICA prototype by parents. A total of 24 parents of young children participated in 2 user test sessions with MICA, approximately 2 weeks apart. After parents’ interaction with the MICA prototype in each user test session, we used qualitative interviews to understand parents’ perceptions and suggestions for improvements in MICA. Results: Findings showed participants’ perceived usefulness of MICAs for helping them self-reflect and motivating them to adopt healthier eating habits together with their children. Participants further suggested various ways in which MICA can help them safely manage their children’s eating behaviors and provide customized support for their proxy needs and goals. Conclusions: We have discussed how the user experience of CAs can be improved to uniquely offer support to parents who serve as proxies in changing the behavior of their children. We have concluded with implications for a larger context of designing MI-based CAs for supporting proxy relationships for health behavior change. ©Diva Smriti, Tsui-Sui Annie Kao, Rahil Rathod, Ji Youn Shin, Wei Peng, Jake Williams, Munif Ishad Mujib, Meghan Colosimo, Jina Huh-Yoo.",2022,,JMIR Human Factors,9,4,,,10.2196/38908,,#5496,Smriti 2022,,,1
Supporting Autonomous Motivation for Physical Activity With Chatbots During the COVID-19 Pandemic: Factorial Experiment,"Wlasak, W.; Zwanenburg, S.P.; Paton, C.","Background: Although physical activity can mitigate disease trajectories and improve and sustain mental health, many people have become less physically active during the COVID-19 pandemic. Personal information technology, such as activity trackers and chatbots, can technically converse with people and possibly enhance their autonomous motivation to engage in physical activity. The literature on behavior change techniques (BCTs) and self-determination theory (SDT) contains promising insights that can be leveraged in the design of these technologies; however, it remains unclear how this can be achieved. Objective: This study aimed to evaluate the feasibility of a chatbot system that improves the user's autonomous motivation for walking based on BCTs and SDT. First, we aimed to develop and evaluate various versions of a chatbot system based on promising BCTs. Second, we aimed to evaluate whether the use of the system improves the autonomous motivation for walking and the associated factors of need satisfaction. Third, we explored the support for the theoretical mechanism and effectiveness of various BCT implementations. Methods: We developed a chatbot system using the mobile apps Telegram (Telegram Messenger Inc) and Google Fit (Google LLC). We implemented 12 versions of this system, which differed in 3 BCTs: goal setting, experimenting, and action planning. We then conducted a feasibility study with 102 participants who used this system over the course of 3 weeks, by conversing with a chatbot and completing questionnaires, capturing their perceived app support, need satisfaction, physical activity levels, and motivation. Results: The use of the chatbot systems was satisfactory, and on average, its users reported increases in autonomous motivation for walking. The dropout rate was low. Although approximately half of the participants indicated that they would have preferred to interact with a human instead of the chatbot, 46.1% (47/102) of the participants stated that the chatbot helped them become more active, and 42.2% (43/102) of the participants decided to continue using the chatbot for an additional week. Furthermore, the majority thought that a more advanced chatbot could be very helpful. The motivation was associated with the satisfaction of the needs of competence and autonomy, and need satisfaction, in turn, was associated with the perceived system support, providing support for SDT underpinnings. However, no substantial differences were found across different BCT implementations. Conclusions: The results provide evidence that chatbot systems are a feasible means to increase autonomous motivation for physical activity. We found support for SDT as a basis for the design, laying a foundation for larger studies to confirm the effectiveness of the selected BCTs within chatbot systems, explore a wider range of BCTs, and help the development of guidelines for the design of interactive technology that helps users achieve long-term health benefits. © JMIR Formative Research 2023.",2023,,JMIR Formative Research,7,,,,10.2196/38500,,#5507,Wlasak 2023,,,1
The Effect of a mHealth App (KENPO-app) for Specific Health Guidance on Weight Changes in Adults With Obesity and Hypertension: Pilot Randomized Controlled Trial,"Sakane, N.; Suganuma, A.; Domichi, M.; Sukino, S.; Abe, K.; Fujisaki, A.; Kanazawa, A.; Sugimoto, M.","Background: Commercial smartphone apps that promote self-monitoring of weight loss are widely available. The development of disease-specific apps has begun, but there is no app for specific health guidance (SHG) to prevent metabolic syndrome, type 2 diabetes, and cardiovascular diseases in middle-aged adults in Japan. Objective: This study aimed to determine the efficacy of an SHG mobile health app in facilitating weight loss in Japanese adults with obesity and hypertension. Methods: In a 12-week, statistician-blinded, randomized parallel controlled trial, 78 overweight and obese men aged 40-69 years were assigned in a 1:1 ratio to either the usual support plus KENPO-app group (intervention group) or the active control group. KENPO-app (release April 10, 2019; OMRON Healthcare Co., Ltd.) was developed by the study team and focus groups and uses behavior change techniques (ie, self-monitoring and goal-setting theory). This app was developed for SHG based on the four specific health checkups and guidance system in Japan: (1) focusing primarily on achieving the target (weight loss of ≥2 kg); (2) assessing healthy eating, exercise habits, smoking habits, relaxation, and self-weighing; (3) providing information on the results of specific health checkups; and (4) starting an intervention period of 6 months with the interim assessment at 3 months. The initial assessment explored the following: personality traits (4 types), health checkup data concerns (10 items), symptom concerns (10 items), and the aim of the intervention (weight loss, improving fitness, symptoms, laboratory data). Chatbot-supported health information on health and health behavior was selected from 392 quizzes based on app data and was provided to participants. The KENPO-app had chatbot-supported feedback and information provision combined with a self-monitoring tool (weight, steps, and blood pressure). Data on active exercise, healthy eating, and healthy lifestyle habits were obtained using a web-based self-administered questionnaire at baseline and 12 weeks. Results: The trial’s retention rate was 95% (74/78). The adherence to daily self-weighing, wearing the pedometer, and blood pressure monitoring in the KENPO-app group was significantly higher than those in the active control group. Compared with the active control group, the median body weight and BMI of the intervention group significantly decreased at 3 months (–0.4, IQR –2.0 to 0.6 kg vs –1.1, IQR –2.7 to –0.5 kg; P=.03; –0.1, IQR –0.6 to 0.3 kg vs –0.4, IQR –0.8 to –0.2 kg; P=.02, respectively). The intervention increased the percentage of participants who self-reported taking ≥8000 steps, eating vegetables before rice, eating slowly, and relaxing. Personality traits were associated with the degree of weight loss in the intervention group. Conclusions: The SHG-specific KENPO-app was feasible and induced modest but significant weight loss in adults with obesity. © Naoki Sakane, Akiko Suganuma, Masayuki Domichi, Shin Sukino, Keiko Abe, Akiyoshi Fujisaki, Ai Kanazawa, Mamiko Sugimoto.",2023,,JMIR mHealth and uHealth,11,,,,10.2196/43236,,#5513,Sakane 2023,,,1
"SlimMe, a Chatbot With Artificial Empathy for Personal Weight Management: System Design and Finding","Rahmanti, A.R.; Yang, H.-C.; Bintoro, B.S.; Nursetyo, A.A.; Muhtar, M.S.; Syed-Abdul, S.; Li, Y.-C.J.","As the obesity rate continues to increase persistently, there is an urgent need to develop an effective weight loss management strategy. Nowadays, the development of artificial intelligence (AI) and cognitive technologies coupled with the rapid spread of messaging platforms and mobile technology with easier access to internet technology offers professional dietitians an opportunity to provide extensive monitoring support to their clients through a chatbot with artificial empathy. This study aimed to design a chatbot with artificial empathic motivational support for weight loss called “SlimMe” and investigate how people react to a diet bot. The SlimMe infrastructure was built using Dialogflow as the natural language processing (NLP) platform and LINE mobile messenger as the messaging platform. We proposed a text-based emotion analysis to simulate artificial empathy responses to recognize the user's emotion. A preliminary evaluation was performed to investigate the early-stage user experience after a 7-day simulation trial. The result revealed that having an artificially empathic diet bot for weight loss management is a fun and exciting experience. The use of emoticons, stickers, and GIF images makes the chatbot response more interactive. Moreover, the motivational support and persuasive messaging features enable the bot to express more empathic and engaging responses to the user. In total, there were 1,007 bot responses from 892 user input messages. Of these, 67.38% (601/1,007) of the chatbot-generated responses were accurate to a relevant user request, 21.19% (189/1,007) inaccurate responses to a relevant request, and 10.31% (92/1,007) accurate responses to an irrelevant request. Only 1.12% (10/1,007) of the chatbot does not answer. We present the design of an artificially empathic diet bot as a friendly assistant to help users estimate their calorie intake and calories burned in a more interactive and engaging way. To our knowledge, this is the first chatbot designed with artificial empathy features, and it looks very promising in promoting long-term weight management. More user interactions and further data training and validation enhancement will improve the bot's in-built knowledge base and emotional intelligence base. Copyright © 2022 Rahmanti, Yang, Bintoro, Nursetyo, Muhtar, Syed-Abdul and Li.",2022,,Frontiers in Nutrition,9,,,,10.3389/fnut.2022.870775,,#5514,Rahmanti 2022,,,1
Is Natural Necessary? Human Voice Versus Synthetic Voice for Intelligent Virtual Agents,"Abdulrahman, A.; Richards, D.","The use of intelligent virtual agents (IVA) to support humans in social contexts will depend on their social acceptability. Acceptance will be related to the human’s perception of the IVAs as well as the IVAs’ ability to respond and adapt their conversation appropriately to the human. Adaptation implies computer-generated speech (synthetic speech), such as text-to-speech (TTS). In this paper, we present the results of a study to investigate the effect of voice type (human voice vs. synthetic voice) on two aspects: (1) the IVA’s likeability and voice impression in the light of co-presence, and (2) the interaction outcome, including human–agent trust and behavior change intention. The experiment included 118 participants who interacted with either the virtual advisor with TTS or the virtual advisor with human voice to gain tips for reducing their study stress. Participants in this study found the voice of the virtual advisor with TTS to be more eerie, but they rated both agents, with recorded voice and with TTS, similarly in terms of likeability. They further showed a similar attitude towards both agents in terms of co-presence and building trust. These results challenge previous studies that favor human voice over TTS, and suggest that even if human voice is preferred, TTS can deliver equivalent benefits. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",2022,,Multimodal Technologies and Interaction,6,7,,,10.3390/mti6070051,,#5517,Abdulrahman 2022,,,1
Effectiveness of embodied conversational agents for managing academic stress at an Indian University (ARU) during COVID-19,"Nelekar, S.; Abdulrahman, A.; Gupta, M.; Richards, D.","Stress has become one of the major reasons for many mental health related issues among students of all age groups, which has resulted in devastating personal losses including suicide. Societal and familial pressure to succeed is high, particularly in developing countries where education is highly valued as a key enabler. As part of stress management during the COVID-19 pandemic, demand for online intelligent virtual advisors has risen and, consequently, the need for personalised explanation that is culturally sensitive to the user's context is essential to improve the user's understanding of and trust in the recommendations provided by the virtual advisor. This paper presents the mAnaging stRess at University embodied conversational agent (ECA) that has been adapted for Indian university students from an explainable agent that was found to help Western students reduce their stress by providing study tips with explanations based on the student's beliefs and/or goals. We conducted a research study with sixty students which measured the impact of providing three different patterns of tailored explanations (belief-based, goal-based, and belief and goal-based explanation) on the students' intentions to change the recommended behaviours and the relationship built with the ECA. The experimental results indicate that there was stress reduction across all student groups provided with different types of explanations. Further, the students showed trust and a good working alliance with the conversational agent, along with an intention to change behaviour across all types of explanations. However, it was observed that the user context played an important role in behaviour change intention and hence explanations could be tailored further, making them culturally more relevant to Indian students. Practitioner notes What is already known about this topic Embodied conversational agents (ECAs) have been mostly developed, applied and shown to be effective in developed countries. Hence, their design and development are mostly guided by the intended user's needs and preferences. In a Western context, ECAs have been found to be beneficial for reducing study stress in university students. There is a pertinent need for use of low cost, effective technology that can aid academic stress reduction in higher educational institutions in developing countries owing to their high youth populations, lack of adequate mental healthcare facilities and associated social stigma. What this paper adds The adaptation and use of ECAs to reduce study stress in higher education students in a developing country is evaluated. The ECA technology is adapted for an Indian context in terms of its physical appearance, colour, speech dialect and dialog content so that it is culturally more aligned to the target population. The ECA engages in an empathic conversation tailored for the Indian students and their COVID-19 context providing them with explanation-backed behaviour recommendations that take their beliefs and goals into account. The ECA provides three types of explanation: belief-only; goal-only; and both belief and goal. Results of a study carried out in an Indian university with 61 students, randomly assigned to one of the explanation types, to capture their demographics, study stress statistics, behaviour change intentions and trust/working alliance with the conversational agent. The major findings include stress reduction across all explanation groups, development of a positive relationship between the ECA and the students regardless of its explanation pattern, and changes in behaviour intentions across all types of explanations for all recommended behaviours. However, differences in change intentions for certain behaviours indicate further tailoring of explanations is required based on the user context. Implications for practice and/or policy The ECA technology has shown promise in terms of stress reduction amongst Indian students. Higher Education Institutions in developing countries could utilise low-cost and widely accessible ECAs to overcome lack of access to human-based support and reluctance to use available services due to stigmatized attitudes to mental health issues. This technology can be further improved and deployed into a larger number of Indian educational institutions leading to a widespread impact on overall student health and wellbeing. Digital technologies to support mental health have become more prominent during the COVID-19 pandemic, at least in Western countries. The ECA technology evaluated in our study demonstrates its viability and potential value for use in developing countries, with appropriate tailoring. © 2021 British Educational Research Association.",2022,,British Journal of Educational Technology,53,3,491-511,,10.1111/bjet.13174,,#5524,Nelekar 2022,,,1
Chatbots: A Framework for Improving Information Security Behaviours using ChatGPT,"Gundu, T.","This paper proposes a framework for improving information security behaviours using ChatGPT, a natural language processing chatbot. The framework leverages the Theory of Planned Behaviour (TPB) and the Persuasion Theory to promote secure behaviours through targeted interventions such as education, training, gamification, security tips, reminders, and nudges. ChatGPT can provide personalized and interactive training on information security best practices, deliver relevant threat alerts and tips, and assist with security assessments. Gamification can be used to increase engagement and retention of information security knowledge. The use of nudges and reminders can help sustain secure behaviours over time. Overall, the ChatGPT framework offers a promising approach for organizations looking to enhance their cybersecurity posture by promoting a culture of information security and empowering individuals with the knowledge and tools to protect sensitive information. The framework was then validated by prompting ChatGPT to raise awareness. The responses were then validated by experts to verify if the content from ChatGPT was good. © 2023, IFIP International Federation for Information Processing.",2023,,IFIP Advances in Information and Communication Technology,674,,418-431,,10.1007/978-3-031-38530-8_33,,#5525,Gundu 2023,,,1
Impact of adaptive multimodal empathic behavior on the user interaction,"Iop, A.; Zojaji, S.; Peters, C.","When modeling life-like Embodied Conversational Agents (ECAs), conveying politeness through verbal and nonverbal behaviors with persuasive intents is a significant challenge, as it underlies the conventional set of behavioral rules that govern human communication. In the present study, we explore the adherence to such rules in the context of joining a small, freestanding conversational group of agents in VR. In particular, we focus on the behavior adopted by participants while walking towards the agents, and on whether ECAs were treated in the same way human agents normally are. 45 test subjects were invited by an ECA to walk towards the group by applying one of six possible politeness strategies; after freely joining the group, they were asked to rate the agent's politeness according to four distinct aspects (Clarity, Face loss, Positive face, and Negative face). Across all strategies, in 48% of the trials participants were successfully persuaded to join the group at an inconvenient location. Out of those trials, participants adhered to social conventions by not crossing the convex empty space between the group members (o-space) in 75% of them on average. Additionally, analysis of verbal and nonverbal behaviors in ECAs shows that direct request strategies are more effective than indirect ones, although in some cases they may be perceived as less polite.  © 2022 Owner/Author.",2022,,IVA 2022 - Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents,,,,,10.1145/3514197.3549676,,#5530,Iop 2022,,,1
Robots as My Future Colleagues: Changing Attitudes Toward Collaborative Robots by Means of Experience-Based Workshops,"Leoste, J.; Viik, T.; López, J.S.M.; Kangur, M.; Vunder, V.; Mollard, Y.; Õun, T.; Tammo, H.; Paekivi, K.","Artificial intelligence-driven robots are increasingly being introduced in various workplaces. Research implies that people’s negative attitudes toward intelligent and collaborative robots might hinder their willingness to use them. We propose that interactive educational activities such as specialized workshops help people to overcome such negative attitudes. We designed a two-day workshop that introduced two quasi-industrial robots (Poppy Ergo Jr and ClearBot) to 16 university students. Students’ attitudes were qualitatively measured before and after the workshop. The results imply that the workshop helped students to increase their understanding of the nature of the intelligent collaborative robots. More precisely, robots became to be seen as empowering tools, rather than friends or enemies. Interestingly, there were significant gender differences, as the female participants had a greater tendency to view robots as animated objects. We concluded that specialized workshops effectively lead participants to become aware of various promising opportunities for their robotic co-workers in the possible future. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",2022,,"Smart Innovation, Systems and Technologies",249,,163-175,,10.1007/978-981-16-3930-2_13,,#5532,Leoste 2022,,,1
Adding Dynamic Norm to Environmental Information in Messages Promoting the Reduction of Meat Consumption,"Carfora, V.; Zeiske, N.; van der Werff, E.; Steg, L.; Catellani, P.","Research has shown that dynamic norm information can promote pro-environmental actions (i.e. information indicating that a growing number of people are behaving pro-environmentally). Yet, the question remains whether adding dynamic norm information would increase the effectiveness of information on the environmental consequences of behavior. We compared the effects of environmental information with versus without dynamic norm information on encouraging reductions in meat consumption, and whether effects would depend on receivers’ intrinsic motivation. We also explored whether message effectiveness would vary according to receivers’ intrinsic motivation to reduce meat consumption. In total 197 volunteers participated in a one-month messaging intervention through a chatbot. Results showed that both environmental and environmental + dynamic norm messages increased positive attitude towards reducing meat consumption, and decreased meat consumption. These effects were still present at follow-up (i.e. one month after the intervention stopped). Interestingly, both messages particularly persuaded receivers with a relatively weak intrinsic motivation to reduce meat consumption, while environmental only messages were even counterproductive in receivers with a relatively strong intrinsic motivation. These results advance our comprehension of the effects of messages aimed at reducing meat consumption. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",2022,,Environmental Communication,16,7,900-919,,10.1080/17524032.2022.2062019,,#5542,Carfora 2022,,,1
Artificial intelligence and sustainability: Solutions to social and environmental challenges,"Taghikhah, F.; Erfani, E.; Bakhshayeshi, I.; Tayari, S.; Karatopouzis, A.; Hanna, B.","Human-induced climate change and deterioration of our natural environment are critical problems. Advanced technologies, such as artificial intelligence (AI), offer potential for the development of solutions. Machines that learn by acquiring knowledge and perform human-like tasks can help humans reduce their intense use of natural resources and improve environmental governance for more sustainable living. This chapter provides evidence of AI’s potential to create new opportunities to overcome major socioenvironmental challenges, using two case studies. The first case study explains how machine intelligence can support the design and implementation of interventions for promoting proenvironmental behavior in societies. The next section discusses AI-powered strategies for efficient management of wildfires and renewable energy production. The conclusions offer insights into the ability of AI to influence larger trends in global sustainability and suggest data-driven interventions and leadership strategies to accelerate progress in combating climate change. © 2022 Elsevier Inc. All rights reserved.",2022,,Artificial Intelligence and Data Science in Environmental Sensing,,,93-108,,10.1016/B978-0-323-90508-4.00006-X,,#5546,Taghikhah 2022,,,1
Harnessing Artificial Intelligence for Health Message Generation: The Folic Acid Message Engine,"Schmälzle, R.; Wilcox, S.","Background: Communication campaigns using social media can raise public awareness; however, they are difficult to sustain. A barrier is the need to generate and constantly post novel but on-topic messages, which creates a resource-intensive bottleneck. Objective: In this study, we aim to harness the latest advances in artificial intelligence (AI) to build a pilot system that can generate many candidate messages, which could be used for a campaign to suggest novel, on-topic candidate messages. The issue of folic acid, a B-vitamin that helps prevent major birth defects, serves as an example; however, the system can work with other issues that could benefit from higher levels of public awareness. Methods: We used the Generative Pretrained Transformer-2 architecture, a machine learning model trained on a large natural language corpus, and fine-tuned it using a data set of autodownloaded tweets about #folicacid. The fine-tuned model was then used as a message engine, that is, to create new messages about this topic. We conducted a web-based study to gauge how human raters evaluate AI-generated tweet messages compared with original, human-crafted messages. Results: We found that the Folic Acid Message Engine can easily create several hundreds of new messages that appear natural to humans. Web-based raters evaluated the clarity and quality of a human-curated sample of AI-generated messages as on par with human-generated ones. Overall, these results showed that it is feasible to use such a message engine to suggest messages for web-based campaigns that focus on promoting awareness. Conclusions: The message engine can serve as a starting point for more sophisticated AI-guided message creation systems for health communication. Beyond the practical potential of such systems for campaigns in the age of social media, they also hold great scientific potential for the quantitative analysis of message characteristics that promote successful communication. We discuss future developments and obvious ethical challenges that need to be addressed as AI technologies for health persuasion enter the stage. © Ralf Schmälzle, Shelby Wilcox.",2022,,Journal of Medical Internet Research,24,1,,,10.2196/28858,,#5548,Schmälzle 2022,,,1
Can digital personal assistants persuade people to exercise?,"Paay, J.; Kjeldskov, J.; Papachristos, E.; Hansen, K.M.; Jørgensen, T.; Overgaard, K.L.","Digital personal assistants (DPAs) have recently grown in popularity because they are both a commercially available new technology and reasonably affordable to the average household. This opens opportunities for new ways to assist people in everyday activities in their homes through voice-interaction. Physical activity has significant health benefits, and yet globally, 1 in 4 adults are not active enough. To address this, we investigate the persuasive potential of DPAs in increasing people’s physical activity at home. We conducted a study with 48 participants to understand the effect of applying three of Fogg’s persuasive principles to the design of a DPA exercise programme: Suggestion, Virtual Reward, and Praise. Our findings show that DPAs have the potential, within their current technical and reactive capabilities, to persuade people to increase their physical activity at home, using Suggestion to encourage physical effort, Virtual Reward to encourage endurance, and Praise to create reassurance for beginners. Based on this, we offer three alternate perspectives for developing persuasive DPAs. We also discuss limitations of the study and suggest future research directions around using persuasion with DPAs. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",2022,,Behaviour and Information Technology,41,2,416-432,,10.1080/0144929X.2020.1814412,,#5556,Paay 2022,,,1
A Conversational Goal Setting Buddy for Student Learning,"Chen, Y.; Zhu, T.; Bhalla, S.; Aggarwal, N.; Gee, L.","Time management and goal setting skills are essential for student academic success in higher education. Meanwhile, the recent advances of chatbots offer new opportunities to support goal setting in a conversational way. In this preliminary research, we investigate the effects of chatbots as a conversational goal setting tool for student learning. We developed a chatbot called Sammy that invites students to pledge their study goals and reflect on their goal completion. We conducted a 7-day study among 70 undergraduate students. Analysis on pre- and post-study surveys indicated a significant improvement in student perceived time management in learning goals. Analysis on student daily check-in with Sammy showed an upward trend of student satisfaction in goal completion and confidence in future goal setting. In the future, we will conduct a more comprehensive mixed-method analysis, improve the functionalities and usability of Sammy, and conduct longitudinal studies. © 2022 28th Americas Conference on Information Systems, AMCIS 2022. All Rights Reserved.",2022,,"28th Americas Conference on Information Systems, AMCIS 2022",,,,,,,#5559,Chen 2022,,,1
Raising Awareness of Smartphone Overuse among University Students: A Persuasive Systems Approach,"Abreu, C.; Campos, P.F.","Smartphone overuse can lead to a series of physical, mental and social disturbances. This problem is more prevalent among young adults as compared to other demographic groups. Additionally, university students are already undergoing high cognitive loads and stress conditions; therefore, they are more susceptible to smartphone addiction and its derived problems. In this paper, we present a novel approach where a conversational mobile agent uses persuasive messages exploring the reflective mind to raise users’ awareness of their usage and consequently induce reduction behaviors. We conducted a four-week study with 16 university students undergoing stressful conditions—a global lockdown during their semester—and evaluated the impact of the agent on smartphone usage reduction and the perceived usefulness of such an approach. Results show the efficacy of self-tracking in the behavior change process: 81% of the users reduced their usage time, and all of them mentioned that having a conversational agent alerting them about their usage was useful. Before this experiment, only 68% of them considered such an approach could be useful. In conclusion, users deemed it essential to have an engaging conversational agent on their smartphones, in terms of helping them become more aware of usage times. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",2022,,Informatics,9,1,,,10.3390/informatics9010015,,#5563,Abreu 2022,,,1
Weight loss in a digital app-based diabetes prevention program powered by artificial intelligence,"Graham, S.A.; Pitter, V.; Hori, J.H.; Stein, N.; Branch, O.H.","Objective: The National Diabetes Prevention Program (DPP) reduces diabetes incidence and associated medical costs but is typically staffing-intensive, limiting scalability. We evaluated an alternative delivery method with 3933 members of a program powered by conversational Artificial Intelligence (AI) called Lark DPP that has full recognition from the Centers for Disease Control and Prevention (CDC). Methods: We compared weight loss maintenance at 12 months between two groups: 1) CDC qualifiers who completed ≥4 educational lessons over 9 months (n = 191) and 2) non-qualifiers who did not complete the required CDC lessons but provided weigh-ins at 12 months (n = 223). For a secondary aim, we removed the requirement for a 12-month weight and used logistic regression to investigate predictors of weight nadir in 3148 members. Results: CDC qualifiers maintained greater weight loss at 12 months than non-qualifiers (M = 5.3%, SE =.8 vs. M = 3.3%, SE =.8; p =.015), with 40% achieving ≥5%. The weight nadir of 3148 members was 4.2% (SE =.1), with 35% achieving ≥5%. Male sex (β =.11; P =.009), weeks with ≥2 weigh-ins (β =.68; P <.0001), and days with an AI-powered coaching exchange (β =.43; P <.0001) were associated with a greater likelihood of achieving ≥5% weight loss. Conclusions: An AI-powered DPP facilitated weight loss and maintenance commensurate with outcomes of other digital and in-person programs not powered by AI. Beyond CDC lesson completion, engaging with AI coaching and frequent weighing increased the likelihood of achieving ≥5% weight loss. An AI-powered program is an effective method to deliver the DPP in a scalable, resource-efficient manner to keep pace with the prediabetes epidemic. © The Author(s) 2022.",2022,,Digital Health,8,,,,10.1177/20552076221130619,,#5564,Graham 2022,,,1
Designing Persuasive Food Conversational Recommender Systems With Nudging and Socially-Aware Conversational Strategies,"Pecune, F.; Callebert, L.; Marsella, S.","Unhealthy eating behavior is a major public health issue with serious repercussions on an individual’s health. One potential solution to overcome this problem, and help people change their eating behavior, is to develop conversational systems able to recommend healthy recipes. One challenge for such systems is to deliver personalized recommendations matching users’ needs and preferences. Beyond the intrinsic quality of the recommendation itself, various factors might also influence users’ perception of a recommendation. In this paper, we present Cora, a conversational system that recommends recipes aligned with its users’ eating habits and current preferences. Users can interact with Cora in two different ways. They can select pre-defined answers by clicking on buttons to talk to Cora or write text in natural language. Additionally, Cora can engage users through a social dialogue, or go straight to the point. Cora is also able to propose different alternatives and to justify its recipes recommendation by explaining the trade-off between them. We conduct two experiments. In the first one, we evaluate the impact of Cora’s conversational skills and users’ interaction mode on users’ perception and intention to cook the recommended recipes. Our results show that a conversational recommendation system that engages its users through a rapport-building dialogue improves users’ perception of the interaction as well as their perception of the system. In the second evaluation, we evaluate the influence of Cora’s explanations and recommendation comparisons on users’ perception. Our results show that explanations positively influence users’ perception of a recommender system. However, comparing healthy recipes with a decoy is a double-edged sword. Although such comparison is perceived as significantly more useful compared to one single healthy recommendation, explaining the difference between the decoy and the healthy recipe would actually make people less likely to use the system. Copyright © 2022 Pecune, Callebert and Marsella.",2022,,Frontiers in Robotics and AI,8,,,,10.3389/frobt.2021.733835,,#5570,Pecune 2022,,,1
Audience-Centric Natural Language Generation via Style Infusion,"Moorjani, S.; Krishnan, A.; Sundaram, H.; Maslowska, E.; Sankar, A.","Adopting contextually appropriate, audience-tailored linguistic styles is critical to the success of user-centric language generation systems (e.g., chatbots, computer-aided writing, dialog systems). While existing approaches demonstrate textual style transfer with large volumes of parallel or non-parallel data, we argue that grounding style on audience-independent external factors is innately limiting for two reasons. First, it is difficult to collect large volumes of audience-specific stylistic data. Second, some stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to define without audience feedback. In this paper, we propose the novel task of style infusion - infusing the stylistic preferences of audiences in pretrained language generation models. Since humans are better at pairwise comparisons than direct scoring - i.e., is Sample-A more persuasive/polite/empathic than Sample-B - we leverage limited pairwise human judgments to bootstrap a style analysis model and augment our seed set of judgments. We then infuse the learned textual style in a GPT-2 based text generator while balancing fluency and style adoption. With quantitative and qualitative assessments, we show that our infusion approach can generate compelling stylized examples with generic text prompts. The code and data are accessible at https://github.com/CrowdDynamicsLab/StyleInfusion. © 2022 Association for Computational Linguistics.",2022,,Findings of the Association for Computational Linguistics: EMNLP 2022,,,1919-1932,,10.18653/v1/2022.findings-emnlp.453,,#5577,Moorjani 2022,,,1
Technology-Assisted Motivational Interviewing: Developing a Scalable Framework for Promoting Engagement with Tobacco Cessation Using NLP and Machine Learning,"Saiyed, A.; Layton, J.; Borsari, B.; Cheng, J.; Kanzaveli, T.; Tsvetovat, M.; Satterfield, J.","Motivational interviewing (MI) improves readiness for smoking cessation but can be time-intensive, require substantial expertise, and patients must still be linked with evidence-based cessation programs sensitive to local resources and patient preferences. Technology-assisted MI may provide a more efficient way to promote readiness and facilitate behavior change. This study developed the Technology Assisted Motivational Interviewing Coach (TAMI), a digital conversational agent that incorporates machine learning models to deliver MI for tobacco cessation and create tailored quit plans. This manuscript describes and evaluates the architecture and nested machine learning models within TAMI leveraged during the pilot clinical trial.  © 2022 The Author(s).",2022,,Procedia Computer Science,206,,121-131,,10.1016/j.procs.2022.09.091,,#5585,Saiyed 2022,,,1
Hypocrisy Induction: Using Chatbots to Promote COVID-19 Social Distancing,"Kim, W.; Ryoo, Y.","Considering widespread resistance to COVID-19 preventive measures, the authors draw on hypocrisy induction theory to examine whether online chatbots can be used to induce hypocrisy and increase compliance with social distancing guidelines. The experiment demonstrates that when a chatbot induces hypocrisy by reminding participants that they have failed to comply with social distancing recommendations, they feel guilty about violating social norms. To reinstate confidence in their personal standards, they form favorable attitudes toward the chatbot ad and establish intentions to comply with recommendations. Interestingly, the persuasive power of hypocrisy induction differs depending on the level of anthropomorphism of the chatbot. When a humanlike chatbot reminds them of their hypocritical behavior, participants feel higher levels of guilt and act more desirably, but a machinelike chatbot is not effective for creating guilt or generating compliance.  © Copyright 2022, Mary Ann Liebert, Inc., publishers 2022.",2022,,"Cyberpsychology, Behavior, and Social Networking",25,1,27-36,,10.1089/cyber.2021.0057,,#5593,Kim 2022,,,1
Chatbot as a Persuasive Technology to Promote Responsible Recycling in the City of Lima,"Flores, K.A.F.; Perez, J.J.G.; Sanchez, L.M.C.","Persuasive technologies have gained popularity around the world. This type of technology aims to change the behavior of people in different circumstances using the persuasion method. Within this classification of technologies we have virtual assistants whose function is to interact in a personalized way with the user. This research aims to provide a solution to the low levels of recycling habits in citizens, which is caused by inefficient programs and/or activities applied by authorities, educational centers and households of socioeconomic levels. low fees by promoting recycling issues. We proposed the use of a virtual assistant to increase knowledge and show the benefits of recycling, in order to generate an improvement in the behavior of the population. To validate this research, two knowledge tests on recycling issues were administered to 69 university students at the beginning and end of the process. After using our virtual assistant, the participants appeared and increased their knowledge by 70.49%.  © 2022 IEEE.",2022,,"Proceedings - 2022 IEEE International Conference on Agents, ICA 2022",,,01-May,,10.1109/ICA55837.2022.00007,,#5596,Flores 2022,,,1
Giving Social Robots a Conversational Memory for Motivational Experience Sharing,"Saravanan, A.; Tsfasman, M.; Neerincx, M.A.; Oertel, C.","In ongoing and consecutive conversations with persons, a social robot has to determine which aspects to remember and how to address them in the conversation. In the health domain, important aspects concern the health-related goals, the experienced progress (expressed sentiment) and the ongoing motivation to pursue them. Despite the progress in speech technology and conversational agents, most social robots lack a memory for such experience sharing. This paper presents the design and evaluation of a conversational memory for personalized behavior change support conversations on healthy nutrition via memory-based motivational rephrasing. The main hypothesis is that referring to previous sessions improves motivation and goal attainment, particularly when references vary. In addition, the paper explores how far motivational rephrasing affects user's perception of the conversational agent (the virtual Furhat). An experiment with 79 participants was conducted via Zoom, consisting of three conversation sessions. The results showed a significant increase in participants' change in motivation when multiple references to previous sessions were provided. © 2022 IEEE.",2022,,"RO-MAN 2022 - 31st IEEE International Conference on Robot and Human Interactive Communication: Social, Asocial, and Antisocial Robots",,,985-992,,10.1109/RO-MAN53752.2022.9900677,,#5600,Saravanan 2022,,,1
An Artificial Intelligence Chatbot for Young People's Sexual and Reproductive Health in India (SnehAI): Instrumental Case Study,"Wang, H.; Gupta, S.; Singhal, A.; Muttreja, P.; Singh, S.; Sharma, P.; Piterova, A.","Background: Leveraging artificial intelligence (AI)-driven apps for health education and promotion can help in the accomplishment of several United Nations sustainable development goals. SnehAI, developed by the Population Foundation of India, is the first Hinglish (Hindi + English) AI chatbot, deliberately designed for social and behavioral changes in India. It provides a private, nonjudgmental, and safe space to spur conversations about taboo topics (such as safe sex and family planning) and offers accurate, relatable, and trustworthy information and resources. Objective: This study aims to use the Gibson theory of affordances to examine SnehAI and offer scholarly guidance on how AI chatbots can be used to educate adolescents and young adults, promote sexual and reproductive health, and advocate for the health entitlements of women and girls in India. Methods: We adopted an instrumental case study approach that allowed us to explore SnehAI from the perspectives of technology design, program implementation, and user engagement. We also used a mix of qualitative insights and quantitative analytics data to triangulate our findings. Results: SnehAI demonstrated strong evidence across fifteen functional affordances: Accessibility, multimodality, nonlinearity, compellability, queriosity, editability, visibility, interactivity, customizability, trackability, scalability, glocalizability, inclusivity, connectivity, and actionability. SnehAI also effectively engaged its users, especially young men, with 8.2 million messages exchanged across a 5-month period. Almost half of the incoming user messages were texts of deeply personal questions and concerns about sexual and reproductive health, as well as allied topics. Overall, SnehAI successfully presented itself as a trusted friend and mentor; the curated content was both entertaining and educational, and the natural language processing system worked effectively to personalize the chatbot response and optimize user experience. Conclusions: SnehAI represents an innovative, engaging, and educational intervention that enables vulnerable and hard-to-reach population groups to talk and learn about sensitive and important issues. SnehAI is a powerful testimonial of the vital potential that lies in AI technologies for social good. © 2022 Journal of Medical Internet Research. All rights reserved.",2022,,Journal of Medical Internet Research,24,1,,,10.2196/29969,,#5618,Wang 2022,,,1
The CoroPrevention-SDM Approach: A Technology-supported Shared Decision Making Approach for a Comprehensive Secondary Prevention Program for Cardiac Patients,"Bonneux, C.; Mahmood, D.Y.; Scherrenberg, M.; Falter, M.; Ruiz, G.R.; Kindermans, H.; Hansen, D.; Laaksonen, R.; Dendale, P.; Coninx, K.","After a cardiac event, secondary prevention is recommended to foster recovery and reduce the risk of recurrent events. European guidelines and EAPC position statements on prevention of cardiovascular diseases recommend a holistic approach that actively engages patients by using shared decision making (SDM). It has been demonstrated that telerehabilitation can be a feasible and effective add-on or alternative compared to conventional in-hospital secondary prevention. However, till date, there is no eHealth solution that offers a holistic approach for secondary prevention that includes SDM. In this paper, we present the CoroPrevention-SDM approach, a technology-supported shared decision making approach for a comprehensive secondary prevention program for cardiac patients. The CoroPrevention Tool Suite consists of three applications that support patients and caregivers in following this approach: 1) a caregiver dashboard that includes decision support systems and supports SDM, 2) a patient mobile application that supports patients in making behaviour changes in their daily life, and 3) an extended ePRO application that collects patient reported outcomes and patient preferences. In a formative usability study, we assessed patients’ and caregivers’ opinion about our approach. The study indicated that both are willing to use our proposed approach to collaboratively set behavioural goals during SDM encounters. Copyright © 2022 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",2022,,"International Conference on Information and Communication Technologies for Ageing Well and e-Health, ICT4AWE - Proceedings",,,59-69,,10.5220/0011042300003188,,#5619,Bonneux 2022,,,1
Peer Buddy or Expert? - On the Avatar Design of a Virtual Coach for Obesity Patients,"Weimann, T.; Fischer, M.; Schlieter, H.","Morbid obesity in association with comorbidities is a considerable burden for the healthcare systems worldwide. Long-term weight loss maintenance requires sustainable behavioral changes but poor adherence is a significant problem in obesity care today and patients often relapse. Prior research has found conversational agents with of a humanoid representation (avatar) embodying the role of a virtual coach useful for the interface of health behavior change support systems. Regarding the avatar design, the coach could, e.g., take the role of an obese “peer buddy” or a lean “expert”. Based on requirements and design principles derived from the literature, the present study investigates how the avatar should be designed. Therefore, two patient surveys were conducted to evaluate static and dynamic representations of potential coaches. The results suggest that patients welcome the concept and lean “expert” coaches might be more suitable in an obesity context. Design implications for future research are derived and discussed. © 2022 IEEE Computer Society. All rights reserved.",2022,,Proceedings of the Annual Hawaii International Conference on System Sciences,2022-January,,3835-3844,,,,#5620,Weimann 2022,,,1
MIA,"Samrose, S.; Hoque, E.","Since online discussion platforms can limit the perception of social cues, effective collaboration over videochat requires additional attention to conversational skills. However, self-affirmation and defensive bias theories indicate that feedback may appear confrontational, especially when users are not motivated to incorporate them. We develop a feedback chatbot that employs Motivational Interviewing (MI), a directive counseling method that encourages commitment to behavior change, with the end goal of improving the user's conversational skills. We conduct a within-subject study with 21 participants in 8 teams to evaluate our MI-agent 'MIA' and a non-MI-agent 'Roboto'. After interacting with an agent, participants are tasked with conversing over videochat to evaluate candidate résumés for a job circular. Our quantitative evaluation shows that the MI-agent effectively motivates users, improves their conversational skills, and is likable. Through a qualitative lens, we present the strategies and the cautions needed to fulfill individual and team goals during group discussions. Our findings reveal the potential of the MI technique to improve collaboration and provide examples of conversational tactics important for optimal discussion outcomes. © 2022 ACM.",2022,,Proceedings of the ACM on Human-Computer Interaction,6,GROUP,,,10.1145/3492864,,#5629,Samrose 2022,,,1
Cyberbullying Mitigation by a Proxy Persuasion of a Chat Member Hijacked by a Chatbot,"Ueda, T.; Nakanishi, J.; Kuramoto, I.; Baba, J.; Yoshikawa, Y.; Ishiguro, H.","In this study, to mitigate cyberbullying in communication tools, we propose a method in which a chatbot pretending to be a member of a chat group defends the victim by proxy without permission (i.e., hijacking and proxy persuasion). An experiment was conducted to verify the effectiveness of the proposed method, the results of which showed that it increases the defending behavior of a user who was temporarily hijacked by the chatbot. However, compared with the existing method, the proposed method was found to have a problem with the continued use of the system.  © 2021 ACM.",2021,,"HAI 2021 - Proceedings of the 9th International User Modeling, Adaptation and Personalization Human-Agent Interaction",,,202-208,,10.1145/3472307.3484177,,#5632,Ueda 2021,,,1
Persuasive robots should avoid authority: The effects of formal and real authority on persuasion in human-robot interaction,"Saunderson, S.P.; Nejat, G.","Social robots must take on many roles when interacting with people in everyday settings, some of which may be authoritative, such as a nurse, teacher, or guard. It is important to investigate whether and how authoritative robots can influence people in applications ranging from health care and education to security and in the home. Here, we present a human-robot interaction study that directly investigates the effect of a robot's peer or authority role (formal authority) and control of monetary rewards and penalties (real authority) on its persuasive influence. The study consisted of a social robot attempting to persuade people to change their answers to the robot's suggestion in a series of challenging attention and memory tasks. Our results show that the robot in a peer role was more persuasive than when in an authority role, contrary to expectations from human-human interactions. The robot was also more persuasive when it offered rewards over penalties, suggesting that participants perceived the robot's suggestions as a less risky option than their own estimates, in line with prospect theory. In general, the results show an aversion to the persuasive influence of authoritative robots, potentially due to the robot's legitimacy as an authority figure, its behavior being perceived as dominant, or participant feelings of threatened autonomy. This paper explores the importance of persuasion for robots in different social roles while providing critical insight into the perception of robots in these roles, people's behavior around these robots, and the development of human-robot relationships. © 2021 American Association for the Advancement of Science. All rights reserved.",2021,,Science Robotics,6,58,,,10.1126/scirobotics.abd5186,,#5633,Saunderson 2021,,,1
Grounding behaviours with conversational interfaces: effects of embodiment and failures,"Kontogiorgos, D.; Pereira, A.; Gustafson, J.","Conversational interfaces that interact with humans need to continuously establish, maintain and repair common ground in task-oriented dialogues. Uncertainty, repairs and acknowledgements are expressed in user behaviour in the continuous efforts of the conversational partners to maintain mutual understanding. Users change their behaviour when interacting with systems in different forms of embodiment, which affects the abilities of these interfaces to observe users’ recurrent social signals. Additionally, humans are intellectually biased towards social activity when facing anthropomorphic agents or when presented with subtle social cues. Two studies are presented in this paper examining how humans interact in a referential communication task with wizarded interfaces in different forms of embodiment. In study 1 (N = 30), we test whether humans respond the same way to agents, in different forms of embodiment and social behaviour. In study 2 (N = 44), we replicate the same task and agents but introduce conversational failures disrupting the process of grounding. Findings indicate that it is not always favourable for agents to be anthropomorphised or to communicate with non-verbal cues, as human grounding behaviours change when embodiment and failures are manipulated. © 2021, The Author(s).",2021,,Journal on Multimodal User Interfaces,15,2,239-254,,10.1007/s12193-021-00366-y,,#5643,Kontogiorgos 2021,,,1
Let's Talk It Out: A Chatbot for Effective Study Habit Behavioral Change,"Tian, X.; Risha, Z.; Ahmed, I.; Lekshmi Narayanan, A.B.; Biehl, J.","Research has shown study habits and skills to be correlated with academic success, calling for a deeper comprehension of these behaviors and processes to design effective interventions for struggling students. Chatbots have recently been used as a persuasive technology to help support behavioral change, making them an intriguing design space for students' study habits and skills. This paper investigated the feasibility of using chatbots for promoting behavioral change of college students majoring in Computer Science (CS). We conducted semi-structured interviews with CS peer-tutors and surveyed university freshmen to understand students' study habits and identify technical intervention opportunities. Inspired by the findings, we designed StudyBuddy, a chatbot prototype deployed in Slack that periodically sends tips, provides assessments of students' study habits via surveys, helps the students break down assignments, recommends academic resources, and sends reminders. We evaluated the usability of the prototype in-depth with 8 students (both first-year and senior students) and 5 course instructors followed by a large scale evaluative survey (n=117) using video of the prototype. Our research identified important design challenges such as building trust and preserving privacy, limiting interaction costs, and supporting both immediate and long-term sustainable support. Likewise, we proposed design recommendations that demonstrate context awareness, personalize the experience based on user preferences, and adapt over time as students mature and grow. © 2021 ACM.",2021,,Proceedings of the ACM on Human-Computer Interaction,5,CSCW1,,,10.1145/3449171,,#5645,Tian 2021,,,1
Motivational Embodied Conversational Agent for Brain Injury Rehabilitation,"Hocking, J.; Maeder, A.","The design and development of a motivational embodied conversational agent for brain injury rehabilitation is discussed. Results for initial prototype design and implementation, and alpha and beta testing phases are presented. Key aspects identified during development included supporting user engagement via personalization and choice-making; integrating behaviour change principles into dialogues; addressing clinical needs of cognitive fatigue and memory loss within conversation structure; and optimizing feasibility of use in a real-life clinical setting.  © 2021 The authors and IOS Press.",2021,,Studies in Health Technology and Informatics,277,,37-46,,10.3233/SHTI210026,,#5657,Hocking 2021,,,1
Customizability in conversational agents and their impact on health engagement,"Paul, S.C.; Bartmann, N.; Clark, J.L.","Conversational agents (CAs) are effective tools for health behavior change, yet little research investigates the mechanisms through which they work. In accordance with the Computer as Social Actors (CASA) paradigm, we suggest that agents are perceived as human-like actors and hence influence behavior much as human coaches might. As such, agents should be designed to resemble ideal interaction patterns—for example, by resembling their users. Our study will test this paradigm by testing the impact of customization on similarity and reciprocity, which in turn are hypothesized to improve perceptions of the agent and compliance with the agent's recommendations. © 2021 Wiley Periodicals LLC.",2021,,Human Behavior and Emerging Technologies,3,5,1141-1152,,10.1002/hbe2.320,,#5663,Paul 2021,,,1
Arguetutor: An adaptive dialog-based learning system for argumentation skills,"Wambsganss, T.; Kung, T.; Sollner, M.; Leimeister, J.M.","Techniques from Natural-Language-Processing ofer the opportunities to design new dialog-based forms of human-computer interaction as well as to analyze the argumentation quality of texts. This can be leveraged to provide students with adaptive tutoring when doing a persuasive writing exercise. To test if individual tutoring for students' argumentation will help them to write more convincing texts, we developed ArgueTutor, a conversational agent that tutors students with adaptive argumentation feedback in their learning journey.We compared ArgueTutor with 55 students to a traditional writing tool. We found students using ArgueTutor wrote more convincing texts with a better quality of argumentation compared to the ones using the alternative approach. The measured level of enjoyment and ease of use provides promising results to use our tool in traditional learning settings. Our results indicate that dialog-based learning applications combined with NLP text feedback have a benefcial use to foster better writing skills of students. © 2021 ACM.",2021,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3411764.3445781,,#5668,Wambsganss 2021,,,1
EREBOTS: Privacy-compliant agent-based platform for multi-scenario personalized health-assistant chatbots,"Calvaresi, D.; Calbimonte, J.-P.; Siboni, E.; Eggenschwiler, S.; Manzo, G.; Hilfiker, R.; Schumacher, M.","Context. Asynchronous messaging is increasingly used to support human–machine inter-actions, generally implemented through chatbots. Such virtual entities assist the users in activities of different kinds (e.g., work, leisure, and health-related) and are becoming ingrained into humans’ habits due to factors including (i) the availability of mobile devices such as smartphones and tablets, (ii) the increasingly engaging nature of chatbot interactions, (iii) the release of dedicated APIs from messaging platforms, and (iv) increasingly complex AI-based mechanisms to power the bots’ behav-iors. Nevertheless, most of the modern chatbots rely on state machines (implementing conversational rules) and one-fits-all approaches, neglecting personalization, data-stream privacy management, multi-topic management/interconnection, and multimodal interactions. Objective. This work ad-dresses the challenges above through an agent-based framework for chatbot development named EREBOTS. Methods. The foundations of the framework are based on the implementation of (i) multi-front-end connectors and interfaces (i.e., Telegram, dedicated App, and web interface), (ii) enabling the configuration of multi-scenario behaviors (i.e., preventive physical conditioning, smoking cessa-tion, and support for breast-cancer survivors), (iii) online learning, (iv) personalized conversations and recommendations (i.e., mood boost, anti-craving persuasion, and balance-preserving physical exercises), and (v) responsive multi-device monitoring interface (i.e., doctor and admin). Results. EREBOTS has been tested in the context of physical balance preservation in social confinement times (due to the ongoing pandemic). Thirteen individuals characterized by diverse age, gender, and country distribution have actively participated in the experimentation, reporting advancements in the physical balance and overall satisfaction of the interaction and exercises’ variety they have been proposed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",2021,,Electronics (Switzerland),10,6,Jan-30,,10.3390/electronics10060666,,#5670,Calvaresi 2021,,,1
Questioning ‘what makes us human’: How audiences react to an artificial intelligence–driven show,"Eagle, R.; Lander, R.; Hall, P.D.","I am Echoborg is promoted as ‘a show created afresh each time by the audience in conversation with an artificial intelligence (AI)’. The show demonstrates how AI in a creative and performance context can raise questions about the technology’s ethical use for persuasion and compliance, and how humans can reclaim agency. This audience study focuses on a consecutive three-night run in Bristol, UK in October 2019. The different outcomes of each show illustrate the unpredictability of audience interactions with conversational AI and how the collective dynamic of audience members shapes each performance. This study analyses (1) how I am Echoborg facilitates audience cocreation in a live performance context, (2) the show’s capacity to provoke nuanced understandings of the potential for AI and (3) the ability for intelligent technology to facilitate social interaction and group collaboration. This audience study demonstrates how the show inspires debate beyond binary conclusions (i.e. AI as good or bad) and how audiences can understand potential creative uses of AI, including as a tool for cocreating entertainment with (not just for) them. © 2021 The Authors.",2021,,Cognitive Computation and Systems,3,2,91-99,,10.1049/ccs2.12018,,#5671,Eagle 2021,,,1
Evaluating XAI: A comparison of rule-based and example-based explanations,"van der Waa, J.; Nieuwburg, E.; Cremers, A.; Neerincx, M.","Current developments in Artificial Intelligence (AI) led to a resurgence of Explainable AI (XAI). New methods are being researched to obtain information from AI systems in order to generate explanations for their output. However, there is an overall lack of valid and reliable evaluations of the effects on users' experience of, and behavior in response to explanations. New XAI methods are often based on an intuitive notion what an effective explanation should be. Rule- and example-based contrastive explanations are two exemplary explanation styles. In this study we evaluate the effects of these two explanation styles on system understanding, persuasive power and task performance in the context of decision support in diabetes self-management. Furthermore, we provide three sets of recommendations based on our experience designing this evaluation to help improve future evaluations. Our results show that rule-based explanations have a small positive effect on system understanding, whereas both rule- and example-based explanations seem to persuade users in following the advice even when incorrect. Neither explanation improves task performance compared to no explanation. This can be explained by the fact that both explanation styles only provide details relevant for a single decision, not the underlying rational or causality. These results show the importance of user evaluations in assessing the current assumptions and intuitions on effective explanations. © 2020 Elsevier B.V.",2021,,Artificial Intelligence,291,,,,10.1016/j.artint.2020.103404,,#5679,vanderWaa 2021,,,1
Conversational agent for healthy lifestyle behavior change: Web-Based feasibility study,"Dhinagaran, D.A.; Sathish, T.; Soong, A.; Theng, Y.-L.; Best, J.; Car, L.T.","Background: The rising incidence of chronic diseases is a growing concern, especially in Singapore, which is one of the high-income countries with the highest prevalence of diabetes. Interventions that promote healthy lifestyle behavior changes have been proven to be effective in reducing the progression of prediabetes to diabetes, but their in-person delivery may not be feasible on a large scale. Novel technologies such as conversational agents are a potential alternative for delivering behavioral interventions that promote healthy lifestyle behavior changes to the public. Objective: The aim of this study is to assess the feasibility and acceptability of using a conversational agent promoting healthy lifestyle behavior changes in the general population in Singapore. Methods: We performed a web-based, single-arm feasibility study. The participants were recruited through Facebook over 4 weeks. The Facebook Messenger conversational agent was used to deliver the intervention. The conversations focused on diet, exercise, sleep, and stress and aimed to promote healthy lifestyle behavior changes and improve the participants’ knowledge of diabetes. Messages were sent to the participants four times a week (once for each of the 4 topics of focus) for 4 weeks. We assessed the feasibility of recruitment, defined as at least 75% (150/200) of our target sample of 200 participants in 4 weeks, as well as retention, defined as 33% (66/200) of the recruited sample completing the study. We also assessed the participants’ satisfaction with, and usability of, the conversational agent. In addition, we performed baseline and follow-up assessments of quality of life, diabetes knowledge and risk perception, diet, exercise, sleep, and stress. Results: We recruited 37.5% (75/200) of the target sample size in 1 month. Of the 75 eligible participants, 60 (80%) provided digital informed consent and completed baseline assessments. Of these 60 participants, 56 (93%) followed the study through till completion. Retention was high at 93% (56/60), along with engagement, denoted by 50% (30/60) of the participants communicating with the conversational agent at each interaction. Acceptability, usability, and satisfaction were generally high. Preliminary efficacy of the intervention showed no definitive improvements in health-related behavior. Conclusions: The delivery of a conversational agent for healthy lifestyle behavior change through Facebook Messenger was feasible and acceptable. We were unable to recruit our planned sample solely using the free options in Facebook. However, participant retention and conversational agent engagement rates were high. Our findings provide important insights to inform the design of a future randomized controlled trial. ©Dhakshenya Ardhithy Dhinagaran, Thirunavukkarasu Sathish, AiJia Soong, Yin-Leng Theng, James Best, Lorainne Tudor Car. Originally published in JMIR Formative Research",2021,,JMIR Formative Research,5,12,,,10.2196/27956,,#5681,Dhinagaran 2021,,,1
Participatory Development and Pilot Testing of an Adolescent Health Promotion Chatbot,"Maenhout, L.; Peuters, C.; Cardon, G.; Compernolle, S.; Crombez, G.; DeSmet, A.","Background: The use of chatbots may increase engagement with digital behavior change interventions in youth by providing human-like interaction. Following a Person-Based Approach (PBA), integrating user preferences in digital tool development is crucial for engagement, whereas information on youth preferences for health chatbots is currently limited. Objective: The aim of this study was to gain an in-depth understanding of adolescents' expectations and preferences for health chatbots and describe the systematic development of a health promotion chatbot. Methods: Three studies in three different stages of PBA were conducted: (1) a qualitative focus group study (n = 36), (2) log data analysis during pretesting (n = 6), and (3) a mixed-method pilot testing (n = 73). Results: Confidentiality, connection to youth culture, and preferences when referring to other sources were important aspects for youth in chatbots. Youth also wanted a chatbot to provide small talk and broader support (e.g., technical support with the tool) rather than specifically in relation to health behaviors. Despite the meticulous approach of PBA, user engagement with the developed chatbot was modest. Conclusion: This study highlights that conducting formative research at different stages is an added value and that adolescents have different chatbot preferences than adults. Further improvement to build an engaging chatbot for youth may stem from using living databases. Copyright © 2021 Maenhout, Peuters, Cardon, Compernolle, Crombez and DeSmet.",2021,,Frontiers in Public Health,9,,,,10.3389/fpubh.2021.724779,,#5683,Maenhout 2021,,,1
Chatbot Design for a Healthy Life to Celiac Patients: A Study According to a New Behavior Change Model,"Alghamdi, E.; Alnanih, R.","There is an absolute need for technology in our daily life that makes people busy with their smartphones all day long. In the healthcare field, mobile apps have been widely used for the treatment of many diseases. Most of these apps were designed without considering health behavior change models. Celiac disease is a significant public health problem worldwide. In Saudi Arabia, the incidence of celiac disease is 1.5%. Celiac patients have a natural demand for resources to facilitate care and research; however, they have not received much attention in the field of healthcare apps. This study introduced a new health behavior change model based on the existing common models and adapted it to the use of technology for the changing behavior of celiac patients towards healthy suitable food. As proof of concept, the new model was applied to the WhatsApp chatbot for patients with celiac disease. To test the impact of the chatbot, 60 Saudi celiac patients participated in three steps. First, they completed a pre-test questionnaire. Then, the participants were divided into two groups: the control group, which was left without any intervention, and the test group, who used the chatbot for 90 days. Finally, all participants completed the post-test questionnaire. The results confirmed a significant statistical difference between both groups, and the test group improved their healthy life in terms of eating habits, reduced celiac symptoms, and commitment to the treatment plan. © 2021",2021,,International Journal of Advanced Computer Science and Applications,12,10,698-707,,10.14569/IJACSA.2021.0121077,,#5689,Alghamdi 2021,,,1
Multi-Modal Dialogue Policy Learning for Dynamic and Co-operative Goal Setting,"Tiwari, A.; Saha, T.; Saha, S.; Sengupta, S.; Maitra, A.; Ramnani, R.; Bhattacharyya, P.","Developing an adequate and human-like virtual agent has been one of the primary applications of artificial intelligence. In the last few years, task-oriented dialogue systems have gained huge popularity because of their upsurging relevance and positive outcomes. In real-world, users may not always have a predefined and rigid task goal beforehand; they upgrade/downgrade/change their goal component dynamically depending upon their utility value and agent's serving capability. However, existing virtual agents fail to incorporate this dynamic behavior, leading to either unsuccessful task completion or an ungratified user experience. The paper presents an end to end multimodal dialogue system for dynamic and co-operative goal setting, which incorporates i) a multi-modal semantic state representation in policy learning to deal with multi-modal inputs, ii) a goal manager module in a traditional dialogue manager for handling dynamic and goal unavailability scenarios effectively, iii) an accumulative reward (task/persona/sentiment) for task success, personalized persuasion and user-adaptive behavior, respectively. The obtained experimental results and the comparisons with baselines firmly establish the need and efficacy of the proposed system. © 2021 IEEE.",2021,,Proceedings of the International Joint Conference on Neural Networks,2021-July,,,,10.1109/IJCNN52387.2021.9533878,,#5691,Tiwari 2021,,,1
Using Health Information Technology to Engage African American Women on Nutrition and Supplement Use During the Preconception Period,"Gardiner, P.; Bickmore, T.; Yinusa-Nyahkoon, L.; Reichert, M.; Julce, C.; Sidduri, N.; Martin-Howard, J.; Woodhams, E.; Aryan, J.; Zhang, Z.; Fernandez, J.; Loafman, M.; Srinivasan, J.; Cabral, H.; Jack, B.W.","Importance: Healthy nutrition and appropriate supplementation during preconception have important implications for the health of the mother and newborn. The best way to deliver preconception care to address health risks related to nutrition is unknown. Methods: We conducted a secondary analysis of data from a randomized controlled trial designed to study the impact of conversational agent technology in 13 domains of preconception care among 528 non-pregnant African American and Black women. This analysis is restricted to those 480 women who reported at least one of the ten risks related to nutrition and dietary supplement use. Interventions: An online conversational agent, called “Gabby”, assesses health risks and delivers 12 months of tailored dialogue for over 100 preconception health risks, including ten nutrition and supplement risks, using behavioral change techniques like shared decision making and motivational interviewing. The control group received a letter listing their preconception risks and encouraging them to talk to a health care provider. Results: After 6 months, women using Gabby (a) reported progressing forward on the stage of change scale for, on average, 52.9% (SD, 35.1%) of nutrition and supplement risks compared to 42.9% (SD, 35.4) in the control group (IRR 1.22, 95% CI 1.03–1.45, P = 0.019); and (b) reported achieving the action and maintenance stage of change for, on average, 52.8% (SD 37.1) of the nutrition and supplement risks compared to 42.8% (SD, 37.9) in the control group (IRR 1.26, 96% CI 1.08–1.48, P = 0.004). For subjects beginning the study at the contemplation stage of change, intervention subjects reported progressing forward on the stage of change scale for 75.0% (SD, 36.3%) of their health risks compared to 52.1% (SD, 47.1%) in the control group (P = 0.006). Conclusion: The scalability of Gabby has the potential to improve women’s nutritional health as an adjunct to clinical care or at the population health level. Further studies are needed to determine if improving nutrition and supplement risks can impact clinical outcomes including optimization of weight. Clinical Trial Registration: ClinicalTrials.gov, identifier NCT01827215. © Copyright © 2021 Gardiner, Bickmore, Yinusa-Nyahkoon, Reichert, Julce, Sidduri, Martin-Howard, Woodhams, Aryan, Zhang, Fernandez, Loafman, Srinivasan, Cabral and Jack.",2021,,Frontiers in Endocrinology,11,,,,10.3389/fendo.2020.571705,,#5693,Gardiner 2021,,,1
A personalized agent-based chatbot for nutritional coaching,"Calvaresi, D.; Eggenschwiler, S.; Calbimonte, J.-P.; Manzo, G.; Schumacher, M.","Intelligent systems increasingly support users' behavior change, including exercise adherence, smoking cessation, and healthy diet adoption. Their effectiveness is affected by the personalization degree of advice/coaching and HMI mechanisms. This paper proposes a personalized agent-based chatbot platform assisting the user in healthy nutrition via pervasive technologies leveraging dynamical, multi-modal, and personalized interactions. The system provides diet recommendations and tracks the user's food intake and nutritional behaviors to promote a healthy lifestyle. The study concludes with a user study and performance evaluation.  © 2021 ACM.",2021,,ACM International Conference Proceeding Series,,,682-687,,10.1145/3486622.3493992,,#5694,Calvaresi 2021,,,1
"‘Let Us Work Together’– Insights from an Experiment with Conversational Agents on the Relation of Anthropomorphic Design, Dialog Support, and Performance","Lichtenberg, S.; Bührke, J.; Brendel, A.B.; Trang, S.; Diederich, S.; Morana, S.","In the human interaction with CAs, research has shown that elements of persuasive system design, such as praise, are perceived differently when compared to traditional graphical interfaces. In this experimental study, we will extend our knowledge regarding the relation of persuasiveness (namely dialog support), anthropomorphically designed CAs, and task performance. Within a three-conditions-between-subjects design, two instances of the CA are applied within an online experiment with 120 participants. Our results show that anthropomorphically designed CAs increase perceived dialog support and performance but adding persuasive design elements can be counterproductive. Thus, the results are embedded in the discourse of CA design for task support. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",2021,,Lecture Notes in Information Systems and Organisation,47,,299-315,,10.1007/978-3-030-86797-3_20,,#5695,Lichtenberg 2021,,,1
Can a monologue-style ECA more effectively motivate eHealth users in initial distress than textual guidance?,"Scholten, M.R.; Kelders, S.M.; Van Gemert-Pijnen, J.E.W.C.","Stress is a prevalent issue amongst patients with chronic conditions. As eHealth interventions are gaining importance, it becomes more relevant to invoke the possibilities from the eHealth technology itself to provide motivational acts during experiences of stress as to enhance adherence to the intervention. Embodied Conversational Agents (ECA's) also known as ‘robots on screen’ can potentially provide a remedy. Within our eHealth experiment we applied a between-subjects design and experimentally studied the difference in appraisal of motivation and guidance. We deployed a functionally modest, monologue-style ECA and compared them with textual guidance. This way, we filtered out the considerable positive impact of interactive features that go along with dialogue-style ECA's. The study was carried out amongst eHealth users of which half were deliberately put in a stressful pre-condition. The rationale was two-sided; first, we hypothesized that it would induce a need for motivational support. Second, it would provide a fair representation of eHealth users in real life. Furthermore, we investigated hypothesized positive effects from a gender match between participant and ECA. The results demonstrated preferential ECA effects compared to text but only in the no stress conditions. Although our set-up controlled for user distraction by putting the facilitating ECA in a pane separate from the eHealth environment, we suspect that the enduring visual presence of the ECA during task completion had still inhibited distressed users. Discussing this phenomenon, our stance is that the hypothesis that ECA support is always superior to textual guidance is open for re-evaluation. Text may sometimes serve users equally well because it lacks human-like aspects that in stressful circumstances can become confrontational. We discuss the potential of ECA's to motivate, but also elaborate on the caveats. Further implications for the ECA, intervention adherence, and eHealth study fields are discussed in relation to stress. © 2021 The Author(s); Stress; eHealth; Support; Embodied conversational agent; ECA; Persuasive technology © 2021 The Author(s)",2021,,Heliyon,7,3,,,10.1016/j.heliyon.2021.e06509,,#5718,Scholten 2021,,,1
Face the Uncanny: The Effects of Doppelganger Talking Head Avatars on Affect-Based Trust Toward Artificial Intelligence Technology are Mediated by Uncanny Valley Perceptions,"Weisman, W.D.; Peña, J.F.","This experiment (N = 228) examined how exposure to a talking head doppelganger created by an artificial intelligence (AI) program influenced affect-based trust toward AIs. Using a 3 (talking head featuring the participant's or a stranger's face, audio-only condition) by 2 (pro-AI pitch and anti-AI pitch playback) design, we uncovered that exposure to a talking head featuring the participant's face instead of a stranger's face increased uncanny valley perceptions. Furthermore, uncanny valley perceptions mediated the link between exposure to a talking head with the participant's face on affect-based trust. Overall, exposure to a doppelganger talking head, who delivered a persuasive pitch, triggered discomfort on the participant whose features were sourced to craft a synthetic talking head, which in turn decreased affect-based trust attributed to AIs. This phenomenon is rooted in basic psychological mechanisms that underpin the uncanny valley hypothesis. Future studies may test for these findings across different platforms and also provide evidence regarding user mental processing.  © Copyright 2021, Mary Ann Liebert, Inc., publishers 2021.",2021,,"Cyberpsychology, Behavior, and Social Networking",24,3,182-187,,10.1089/cyber.2020.0175,,#5721,Weisman 2021,,,1
Evaluating glycemic control in patients of South Asian origin with type 2 diabetes using a digital therapeutic platform: Analysis of real-world data,"Krishnakumar, A.; Verma, R.; Chawla, R.; Sosale, A.; Saboo, B.; Joshi, S.; Shaikh, M.; Shah, A.; Kolwankar, S.; Mattoo, V.","Background: Digital therapeutics are evidence-based therapeutic interventions driven by high-quality software programs for the treatment, prevention, or management of a medical disorder or disease. Many studies in the western population have shown the effectiveness of mobile app-based digital therapeutics for improving glycemic control in patients with type 2 diabetes (T2D). However, few studies have assessed similar outcomes in the South Asian population. Objective: This study aims to investigate the real-world effectiveness of the Wellthy CARE digital therapeutic for improving glycemic control among the South Asian population of Indian origin. Methods: We analyzed deidentified data from 102 patients with T2D from India enrolled in a 16-week structured self-management program delivered using the Wellthy CARE mobile app. Patients recorded their meals, weight, physical activity, and blood sugar in the app, and they received lessons on self-care behaviors (healthy eating, being active, monitoring, medication adherence, problem solving, healthy coping, and reducing risks); feedback provided by an artificial intelligence-powered chatbot; and periodic interactions with certified diabetes educators via voice calls and chats. The primary outcome of the program was a change in glycated hemoglobin A1c (HbA1c). Secondary outcomes included the difference between preintervention and postintervention fasting blood glucose (FBG) and postprandial blood glucose (PPBG) levels; changes in BMI and weight at the completion of 16 weeks; and the association between program engagement and the changes in HbA1c, FBG, and PPBG levels. Results: At the end of 16 weeks, the average change in HbA1c was -0.49% (n=102; 95% CI -0.73 to 0.25; P<.001). Of all the patients, 63.7% (65/102) had improved HbA1c levels, with a mean change of -1.16% (n=65; 95% CI -1.40 to -0.92; P<.001). The mean preintervention and postintervention FBG levels were 145 mg/dL (n=51; 95% CI 135-155) and 134 mg/dL (n=51; 95% CI 122-146; P=.02) and PPBG levels were 188 mg/dL (n=51; 95% CI 172-203) and 166 mg/dL (n=51; 95% CI 153-180; P=.03), respectively. The mean changes in BMI and weight were -0.47 kg/m2 (n=59; 95% CI -0.22 to -0.71; P<.001) and -1.32 kg (n=59; 95% CI -0.63 to -2.01; P<.001), respectively. There was a stepwise decrease in HbA1c, FBG, and PPBG levels as the program engagement increased. Patients in the highest tertile of program engagement had a significantly higher reduction in HbA1c (-0.84% vs -0.06%; P=.02), FBG (-21.4 mg/dL vs -0.18 mg/dL; P=.02), and PPBG levels (-22.03 mg/dL vs 2.35 mg/dL; P=.002) than those in the lowest tertile. Conclusions: The use of the Wellthy CARE digital therapeutic for patients with T2D showed a significant reduction in the levels of HbA1c, FBG, and PPBG after 16 weeks. A higher level of participation showed improved glycemic control, suggesting the potential of the Wellthy CARE platform for better management of the disease. © 2021 Journal of Medical Internet Research. All rights reserved.",2021,,Journal of Medical Internet Research,23,3,,,10.2196/17908,,#5724,Krishnakumar 2021,,,1
"Effects of a novel mobile health intervention compared to a multi-component behaviour changing program on body mass index, physical capacities and stress parameters in adolescents with obesity: a randomized controlled trial","Stasinaki, A.; Büchter, D.; Shih, C.-H.I.; Heldt, K.; Güsewell, S.; Brogle, B.; Farpour-Lambert, N.; Kowatsch, T.; l’Allemand, D.","Background: Less than 2% of overweight children and adolescents in Switzerland can participate in multi-component behaviour changing interventions (BCI), due to costs and lack of time. Stress often hinders positive health outcomes in youth with obesity. Digital health interventions, with fewer on-site visits, promise health care access in remote regions; however, evidence for their effectiveness is scarce. Methods: This randomized controlled not blinded trial (1:1) was conducted in a childhood obesity center in Switzerland. Forty-one youth aged 10–18 years with body mass index (BMI) > P.90 with risk factors or co-morbidities or BMI > P.97 were recruited. During 5.5 months, the PathMate2 group (PM) received daily conversational agent counselling via mobile app, combined with standardized counselling (4 on-site visits). Controls (CON) participated in a BCI (7 on-site visits). We compared the outcomes of both groups after 5.5 (T1) and 12 (T2) months. Primary outcome was reduction in BMI-SDS (BMI standard deviation score: BMI adjusted for age and sex). Secondary outcomes were changes in body fat and muscle mass (bioelectrical impedance analysis), waist-to-height ratio, physical capacities (modified Dordel-Koch-Test), blood pressure and pulse. Additionally, we hypothesized that less stressed children would lose more weight. Thus, children performed biofeedback relaxation exercises while stress parameters (plasma cortisol, stress questionnaires) were evaluated. Results: At intervention start median BMI-SDS of all patients (18 PM, 13 CON) was 2.61 (obesity > + 2SD). BMI-SDS decreased significantly in CON at T1, but not at T2, and did not decrease in PM during the study. Muscle mass, strength and agility improved significantly in both groups at T2; only PM reduced significantly their body fat at T1 and T2. Average daily PM app usage rate was 71.5%. Cortisol serum levels decreased significantly after biofeedback but with no association between stress parameters and BMI-SDS. No side effects were observed. Conclusions: Equally to BCI, PathMate2 intervention resulted in significant and lasting improvements of physical capacities and body composition, but not in sustained BMI-SDS decrease. This youth-appealing mobile health intervention provides an interesting approach for youth with obesity who have limited access to health care. Biofeedback reduces acute stress and could be an innovative adjunct to usual care. © 2021, The Author(s).",2021,,BMC Pediatrics,21,1,,,10.1186/s12887-021-02781-2,,#5725,Stasinaki 2021,,,1
Interaction With a Motivational Agent to Support Behaviour Change,"Louvet, A.; Ravenet, B.; Clavel, C.; Sabouret, N.","We are developing a virtual agent based on the Motivational Interviewing method to help elderly and inactive individuals change their behavior. The agent engages users in a conversation that aims to highlight reasons why they might want to increase their level of physical activity. In this work, we describe our approach to computerized adaptation of Motivational Interviewing. A study evaluates the outcome of this approach and the system as a whole.  © 2021 Owner/Author.",2021,,"32e Conference Internationale Francophone sur l'Interaction Homme-Machine, IHM 2021 - Annexes des Actes de la Conference",,,,,10.1145/3451148.3458647,,#5729,Louvet 2021,,,1
Promotion of Preconception Care Among Adolescents and Young Adults by Conversational Agent,"Bickmore, T.; Zhang, Z.; Reichert, M.; Julce, C.; Jack, B.","Purpose: Preconception care is important for all women to improve infant and maternal health outcomes and may be especially important for adolescents and young adults. This study assesses the acceptance, usability, and use of an automated intervention to screen women on 108 preconception care risks and address them over the course of a year via a Web-based virtual animated health counselor and compares these measures for the adolescent and young adult users aged 18–25 years with those of users aged 26–34 years. We hypothesize that the younger cohort will have significantly greater use of and satisfaction with the online intervention. Methods: A randomized controlled trial involving a national sample of 528 women was conducted. We present a secondary data analysis on the system use and self-reported usability and satisfaction of the 79 women aged 18–25 years randomized to the intervention group, compared with the 183 women aged 26–34 years in the intervention group. Participants were required to self-identify as female, black or African American, aged 18–34 years, not pregnant, and English-speaking and were recruited through a variety of advertisements and outreach activities. Results: Of the adolescent and young adult participants (aged 18–25 years) enrolled and randomized to the intervention, 20.25% of participants accessed the system 0 times; 29.11%, 1–3 times; and 50.63%, >3 times over the course of a year. At the end of the year, almost all (96.4%) indicated they had either acted on recommendations made by the agent or planned to. Most (75.0%) said they would recommend the system to someone they knew. There were no significant differences between the two age groups on intervention use or satisfaction. Conclusions: Web-based conversational agents are a viable medium for delivering longitudinal preconception care counseling to adolescents and young adults. © 2019",2020,,Journal of Adolescent Health,67,2,S45-S51,,10.1016/j.jadohealth.2019.09.006,,#5735,Bickmore 2020,,,1
Music Recommendation Using Conversational Recommender System with Explanation Facility,"Fernando, G.; Baizal, Z.K.A.; Dharayani, R.","The majority of music recommendation systems use user data such as ratings, likes, feedback, music played as the user model. This is certainly not suitable for new users and users who are new to music. In addition, most of the research related to music recommendation systems focuses on accuracy and pays less attention to user experience. This can be overcome by using an explanation facility which is the reason the system provides recommendations. Explanation facility is able to improve user experience in several factors such as Transparency, Scrutability, Trustworthiness, Effectiveness, Persuasiveness, Efficiency, and Satisfaction. Therefore, in this study, we develop a conversational recommender system with an explanation facility in the music domain. This system works by conducting a conversation between the system and the user, by a chatbot. This system is a knowledge-based recommender system and uses an ontology that serves as a knowledge base for recommending music. We evaluate the system based on two parameters, such as recommendation accuracy and also the influence of explanation for user satisfaction. Tests show that a system with an explanation facility increases user experience more than a system without an explanation facility. In addition, system testing also shows high accuracy (90.48%) and successfully meets user needs.  © 2021 IEEE.",2021,,"2021 International Conference on Data Science and Its Applications, ICoDSA 2021",,,141-145,,10.1109/ICoDSA53588.2021.9617221,,#5751,Fernando 2021,,,1
Motivating Health Behavior Change with Humorous Virtual Agents,"Olafsson, S.; O'Leary, T.K.; Bickmore, T.W.","Research on the psychology of humor indicates that humor can increase the impact of persuasive messages in certain circumstances, suggesting that virtual agents could use humor to improve the effectiveness of motivational counseling with users. To explore this idea, we developed two virtual agents that attempt to motivate users to perform healthy behaviors-either to increase exercise or fruit and vegetable consumption-and systematically vary whether they use humor or not in their counseling conversations. Human-authored jokes were selected from a large corpus based on the nature of the humor (empathetic affiliative humor that serves to build relational closeness), health topic relevance, and subjective ratings of funniness, as well as relevance to user stage of behavior change. We evaluated our agents in a two-treatment counterbalanced within-subjects experiment, where participants interacted with a humorous and non-humorous agent motivating either exercise or healthy diet. We found the interaction with the humorous agent led to a significantly greater change in motivation to engage in the target behavior than interacting with the non-humorous agent. © 2020 ACM.",2020,,"Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents, IVA 2020",,,,,10.1145/3383652.3423915,,#5753,Olafsson 2020,,,1
How Social Cues in Virtual Assistants Influence Concerns and Persuasion: The Role of Voice and a Human Name,"Voorveld, H.A.M.; Araujo, T.","The aim of this study was to test how two important types of social cues used by virtual assistants today can affect consumer concerns and persuasion. These two cues are modality (voice-based via smart speaker, voice-based via a smartphone, or text-based on a smartphone screen) and the adoption of a human name rather than no name. An online scenario-based experiment (n = 180) has shown that participants who were exposed to a voice-based recommendation via a smart speaker were the most concerned about security and found text-based recommendations on a screen to be the most persuasive. Participants who were exposed to a virtual assistant with a human name were less concerned about their autonomy and were more strongly persuaded than those exposed to an assistant without a human name. © 2020 Mary Ann Liebert, Inc., publishers.",2020,,"Cyberpsychology, Behavior, and Social Networking",23,10,689-696,,10.1089/cyber.2019.0205,,#5759,Voorveld 2020,,,1
Influence of virtual agent politeness behaviors on how users join small conversational groups,"Zojaji, S.; Peters, C.; Pelachaud, C.","Politeness behaviors could affect individuals' decisions heavily in their daily lives and may therefore also play an important role in human-agent interactions. This study considers the impact of politeness behaviors made by a virtual agent, already in a small face-to-face conversational group with another agent, on a human participant as they approach to join it in a virtual environment displayed on a monitor. The agent uses five verbal and nonverbal politeness strategies, ranging from indirect and implicit to direct and explicit, in an attempt to influence the participant to join the group at an inconvenient location, which requires more time and effort than a direct route that would ignore the invitation of the agent. In addition to assessing the success of the strategies at influencing participant behavior, the participants' perception of the agent's persuasive behavior is assessed in relation to clarity, face loss, positive face, and negative face. Based on results from a within-subjects experiment with 30 participants, we found that more direct and explicit politeness strategies have a higher level of success when requesting a participant to join a small group at an inconvenient location, but sometimes negatively impact their perception of the agent. A positive politeness strategy was found to be the most effective for both persuasive success and maintaining a positive impression of the agent. © 2020 ACM.",2020,,"Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents, IVA 2020",,,,,10.1145/3383652.3423917,,#5762,Zojaji 2020,,,1
Designing Effective Dialogue Content for a Virtual Coaching Team Using the Interaction Process Analysis and Interpersonal Circumplex Models,"Huizing, G.; Klaassen, R.; Heylen, D.","Much of the research in the field of virtual coaching agents focuses on interactions between a single agent and the user of the application. Another approach could be to give each user a personal virtual coaching team. This makes it possible to present multiple perspectives, and to have coaches with different expertise debate with each other. This could make the content presented more engaging and the system more persuasive. However, currently guidelines and theory to base designs for content for virtual coaching teams on is scarce. In this paper we present a study in which we set out to design content for a virtual coaching team to talk about general health topics with older adults. We based the content for our study on our implementation of two different models from social psychology used to classify interactive behaviour: the Interaction Process Analysis (IPA) and Interpersonal Circumplex (IPC) models. After testing our implementation of the models with a pilot test, we conducted an online study with 242 older adult participants. We compared the content modelled using the IPA model to the content modelled using the IPC model. For the IPA modelled content compared to the IPC modelled content the virtual coaching team came across more positively, the quality of their coaching was perceived to be better, the interaction experience was rated as better, their ability to persuade was better, and their group cohesion (task and social cohesion) was perceived to be better. We conclude that the IPA model is preferred over the IPC model when designing health coaching content for virtual coaching teams, and discuss possible reasons why. Furthermore, we recommend designers of health coaching content to test other models to base content designs on, and to measure the impact of differently modelled content in both more and less sophisticated coaching systems. © 2021, Springer Nature Switzerland AG.",2021,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12684 LNCS,,19-32,,10.1007/978-3-030-79460-6_2,,#5767,Huizing 2021,,,1
Improving the health of young African American women in the preconception period using health information technology: a randomised controlled trial,"Jack, B.W.; Bickmore, T.; Yinusa-Nyahkoon, L.; Reichert, M.; Julce, C.; Sidduri, N.; Martin-Howard, J.; Zhang, Z.; Woodhams, E.; Fernandez, J.; Loafman, M.; Cabral, H.J.","Background: Preconception care focuses on improving women's health before pregnancy as a means to improve their health and future pregnancy outcomes. How to effectively deliver such care is unknown. The aim of this research was to assess the impact of an embodied conversational agent system on preconception risks among African American and Black women. Methods: We did an open-label, randomised controlled trial of women aged 18–34 years, self-identified as African American or Black, or both, and not pregnant, recruited from 35 states in the USA. Sealed allocation envelopes (in permuted blocks of six and eight, prepared using a random number generator) were opened after enrolment. Intervention participants received an online conversational agent called Gabby that assessed 102 preconception risks and delivered 12 months of tailored dialogue using synthesised speech, non-verbal behaviour, visual aids, and health behaviour change techniques such as motivational interviewing. The control group received a letter listing their preconception risks and encouraging them to talk with a clinician. The primary outcome was the proportion of identified risks at the action or maintenance stage of change at months 6 and 12. The study is registered with ClinicalTrials.gov, NCT01827215. Findings: From March 11, 2014, through July 8, 2018, 528 women recruited from 35 states and 242 cities across the USA received the Gabby intervention (n=262) or were assigned to the control group (n=266). Participants identified a mean of 21 preconception risks per woman (SD 9·9). In the intention-to-treat analysis, at 6 months, intervention women reported reaching the action or maintenance stage of change for 50·0% (SD 28·9) of those preconception risks identified compared with 42·7% (28·3) in the control group (incidence rate ratio 1·16, 95% CI 1·07–1·26; p=0·0004). This result persisted at 12 months. Interpretation: The Gabby system has the potential to improve women's preconception health. Further research is needed to determine if improving preconception risks impacts outcomes such as preterm delivery. Funding: National Institute for Minority Health and Health Disparities. © 2020 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license",2020,,The Lancet Digital Health,2,9,e475-e485,,10.1016/S2589-7500(20)30189-8,,#5768,Jack 2020,,,1
SlumberBot: An Interactive Agent for Helping Users Investigate Disturbance Factors of Sleep Quality,"Liu, Y.; Kim, D.-J.; Miao, T.; Chuang, Y.","As sleep health is increasingly becoming important in recent years, many wearable products and mobile apps have been developed to track users' sleep and interpret their sleep quality. Most of the available designs have mainly focused on objective measurements, such as body movement, heart rate, and/or bedroom light, noise level, and temperature. However, due to the lack of users' subjective experience measurements, sleep trackers often fail to provide useful suggestions for improving their sleep. In this study, we developed SlumberBot with conversational chatbot technology to help users capture their subjective sleep experiences and relevant factors in daytime activities as well. With SlumberBot, we conducted a preliminary field study with five participants in a 4-week time period. The result shows that SlumberBot is easy to stay engaged with and supportive of users' self-reflection on contextual factors related to sleep quality. Besides, SlumberBot has shown the potential of triggering short-term behavior changes that would impact their sleep positively. © 2020 Owner/Author.",2020,,ACM International Conference Proceeding Series,,,,,10.1145/3419249.3420091,,#5769,Liu 2020,,,1
Cognitive superiority: Information to power,"Hartley, D.S.; Jobson, K.O.","In a world of accelerating unending change, perpetual surveillance, and increasing connectivity, conflict has become ever more complex. Wars are no longer limited to the traditional military conflict domains-land, sea, air; even space and cyber space. The new battlefield will be the cognitive domain and the new conflict a larger contest for power; a contest for cognitive superiority. Written by experts in military operations research and neuropsychology, this book introduces the concept of cognitive superiority and provides the keys to succeeding within a complex matrix where the only rules are the laws of physics, access to information, and the boundaries of cognition. The book describes the adversarial environment and how it interacts with the ongoing, accelerating change that we are experiencing, irrespective of adversaries. It talks about the ascendant power of information access, pervasive surveillance, personalized persuasion, and emerging new forms of cognition. It profiles salient technologies and science, including persuasion science, artificial intelligence and machine learning (AI/ML), surveillance technologies, complex adaptive systems, network science, directed human modification, and biosecurity. Readers will learn about human and machine cognition, what makes it tick, and why and how we and our technologies are vulnerable. Following in the tradition of Sun-Tsu and von Clausewitz, this book writes a new chapter in the study of warfare and strategy. It is written for those who lead, aspire to leadership, and those who teach or persuade, especially in the fields of political science, military science, computer science, and business. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2021.",2020,,Cognitive Superiority: Information to Power,,,1-308,,10.1007/978-3-030-60184-3,,#5774,Hartley 2020,,,1
Artificially intelligent chatbots in digital mental health interventions: a review,"Boucher, E.M.; Harake, N.R.; Ward, H.E.; Stoeckl, S.E.; Vargas, J.; Minkel, J.; Parks, A.C.; Zilca, R.","Introduction: Increasing demand for mental health services and the expanding capabilities of artificial intelligence (AI) in recent years has driven the development of digital mental health interventions (DMHIs). To date, AI-based chatbots have been integrated into DMHIs to support diagnostics and screening, symptom management and behavior change, and content delivery. Areas covered: We summarize the current landscape of DMHIs, with a focus on AI-based chatbots. Happify Health’s AI chatbot, Anna, serves as a case study for discussion of potential challenges and how these might be addressed, and demonstrates the promise of chatbots as effective, usable, and adoptable within DMHIs. Finally, we discuss ways in which future research can advance the field, addressing topics including perceptions of AI, the impact of individual differences, and implications for privacy and ethics. Expert opinion: Our discussion concludes with a speculative viewpoint on the future of AI in DMHIs, including the use of chatbots, the evolution of AI, dynamic mental health systems, hyper-personalization, and human-like intervention delivery. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",2021,,Expert Review of Medical Devices,18,sup1,37-49,,10.1080/17434440.2021.2013200,,#5802,Boucher 2021,,,1
Persuasive Social Robot Using Reward Power over Repeated Instances of Persuasion,"Hashemian, M.; Couto, M.; Mascarenhas, S.; Paiva, A.; Santos, P.A.; Prada, R.","This paper presents a user-study with Emys social robot that aims to persuade subjects to select a less-desirable choice. In this study, within a game scenario, Emys attempts to use its social power resources, specifically reward power, to persuade the user to select an alternative such that the user indicates less interest in it. This persuasion attempt is repeated over three times to investigate this effect within repeated persuasion instances. The study consists of three conditions: no/low/high reward. The results indicated that the higher reward does not necessarily lead to higher persuasion and the effect of social power does not decay over repeated persuasion instances. © 2021, Springer Nature Switzerland AG.",2021,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12684 LNCS,,63-70,,10.1007/978-3-030-79460-6_6,,#5804,Hashemian 2021,,,1
Promoting fertility awareness and preconception health using a chatbot: a randomized controlled trial,"Maeda, E.; Miyata, A.; Boivin, J.; Nomura, K.; Kumazawa, Y.; Shirasawa, H.; Saito, H.; Terada, Y.","Research question: What are the effects of using a fertility education chatbot, i.e. automatic conversation programme, on knowledge, intentions to improve preconception behaviour and anxiety? Design: A three-armed, randomized controlled trial was conducted using an online social research panel. Participants included 927 women aged 20–34 years who were randomly allocated to one of three groups: a fertility education chatbot (intervention group), a document about fertility and preconception health (control group 1) or a document about an irrelevant topic (control group 2). Participants’ scores on the Cardiff Fertility Knowledge Scale and the State-Trait Anxiety Inventory, their intentions to optimize preconception behaviours, e.g. taking folic acid, and the free-text feedback provided by chatbot users were assessed. Results: A repeated-measures analysis of variance showed significant fertility knowledge gains after the intervention in the intervention group (+9.1 points) and control group 1 (+14.9 points) but no significant change in control group 2 (+1.1 points). Post-test increases in the intentions to optimize behaviours were significantly higher in the intervention group than in control group 2, and were similar to those in control group 1. Post-test state anxiety scores were significantly lower in the intervention group than in control group 1 and control group 2. User feedbacks about the chatbot suggested technical limitations, e.g. low comprehension of users’ words, and pros and cons of using the chatbot, e.g. convenient versus coldness. Conclusions: Providing fertility education using a chatbot improved fertility knowledge and intentions to optimize preconception behaviour without increasing anxiety, but the improvement in knowledge was small. Further technical development and exploration of personal affinity for technology is required. © 2020 Reproductive Healthcare Ltd.",2020,,Reproductive BioMedicine Online,41,6,1133-1143,,10.1016/j.rbmo.2020.09.006,,#5807,Maeda 2020,,,1
Smartphone-based virtual agents to help individuals with sleep concerns during COVID-19 confinement: Feasibility study,"Philip, P.; Dupuy, L.; Morin, C.M.; de Sevin, E.; Bioulac, S.; Taillard, J.; Serre, F.; Auriacombe, M.; Micoulaud-Franchi, J.-A.","Background: The COVID-19 crisis and consequent confinement restrictions have caused significant psychosocial stress and reports of sleep complaints, which require early management, have increased during recent months. To help individuals concerned about their sleep, we developed a smartphone-based app called KANOPEE that allows users to interact with a virtual agent dedicated to autonomous screening and delivering digital behavioral interventions. Objective: Our objective was to assess the feasibility of this app, in terms of inclusion rate, follow-up rate, perceived trust and acceptance of the virtual agent, and effects of the intervention program, in the context of COVID-19 confinement in France. Methods: The virtual agent is an artificial intelligence program using decision tree architecture and interacting through natural body motion and natural voice. A total of 2069 users aged 18 years and above downloaded the free app during the study period (April 22 to May 5, 2020). These users first completed a screening interview based on the Insomnia Severity Index (ISI) conducted by the virtual agent. If the users were positive for insomnia complaints (ISI score >14), they were eligible to join the 2-stage intervention program: (1) complete an electronic sleep diary for 1 week and (2) follow personalized sleep recommendations for 10 days. We collected and analyzed the following measures: sociodemographic information, ISI scores and sleep/wake schedules, and acceptance and trust of the agent. Results: Approximately 76% (1574/2069) of the app users completed the screening interview with the virtual agent. The virtual agent was well accepted by 27.4% (431/1574) of the users who answered the acceptance and trust questionnaires on its usability, satisfaction, benevolence, and credibility. Of the 773 screened users who reported sleep complaints (ISI score >14), 166 (21.5%) followed Step 1 of the intervention, and only 47 of those (28.3%) followed Step 2. Users who completed Step 1 found that their insomnia complaints (baseline mean ISI score 18.56, mean ISI score after Step 1 15.99; P<.001) and nocturnal sleep quality improved significantly after 1 week. Users who completed Step 2 also showed an improvement compared to the initial measures (baseline mean ISI score 18.87, mean ISI score after Step 2 14.68; P<.001). Users that were most severely affected (ISI score >21) did not respond to either intervention. Conclusions: These preliminary results suggest that the KANOPEE app is a promising solution to screen populations for sleep complaints and that it provides acceptable and practical behavioral advice for individuals reporting moderately severe insomnia. © 2020 Journal of Medical Internet Research. All rights reserved.",2020,,Journal of Medical Internet Research,22,12,,,10.2196/24268,,#5808,Philip 2020,,,1
AI-powered recommendations: the roles of perceived similarity and psychological distance on persuasion,"Ahn, J.; Kim, J.; Sung, Y.","Artificial intelligence (AI) plays various roles in our daily lives, such as personal assistant, salesperson, and virtual counselors; thus, it stands out in various fields as a recommendation agent. This study explored the effects of perceived similarity and psychological distance on the persuasion of AI recommendation agents through two experiments. Results of Experiment 1 elucidated that individuals feel more psychologically distant when they interact with AI recommendation agents than with human agents as a result of a different level of perceived similarity. Furthermore, psychological distance plays a mediating role in determining the effectiveness of desirability- vs. feasibility-focused messages in health-related issues. In Experiment 2, we manipulated the AI speaker's level of perceived similarity via anthropomorphism and found that the AI's recommendation with secondary (vs. primary) features is more effective when AI is humanized, and the reverse was found in non-humanized AI conditions. Both theoretical and managerial implications are provided. © 2021 Advertising Association.",2021,,International Journal of Advertising,40,8,1366-1384,,10.1080/02650487.2021.1982529,,#5809,Ahn 2021,,,1
How to Facilitate Explainability of AI for Increased User Trust: Results of a Study with a COVID-19 Risk Calculator,"Ewerz, B.; Hoefler, M.; Marx, C.; Moertl, P.","While the market of smart technologies is steadily increasing, there is much research to be done regarding the interaction between human users and Artificial Intelligence (AI) technologies. Specifically, the field of Explainable Artificial Intelligence (XAI) focuses on making AI explainable to users. To provide a user-centered approach to this growing field, this paper describes a study to investigate possible processes and methods. For this purpose, 20 participants were asked to use an AI system that provided them with the results of a personalized COVID-19 risk calculation. The study results indicate that while participants generally seemed to think that the presented results of the system were accurate, only a few said that they would change their behavior after receiving the results, and many asked for additional information to better understand the results. This paper discusses the findings along with possible approaches to increase behavior change in users of smart systems.  © 2021 Croatian Society MIPRO.",2021,,"2021 44th International Convention on Information, Communication and Electronic Technology, MIPRO 2021 - Proceedings",,,1076-1080,,10.23919/MIPRO52101.2021.9596837,,#5811,Ewerz 2021,,,1
Making Warning Messages Personal: A Big 5 Personality Trait Persuasion Approach,"Aneke, J.; Ardito, C.; Desolda, G.","Several mitigation strategies in form of warning messages against phishing attacks have continued to fail largely due to user negligence. Thus, it is important for researchers to focus not only on the accuracy of the provided recommendations but also on other factors that influence the acceptance of recommendations and the extent to which these recommendations are convincing or persuasive. In this paper, we present our ongoing approach that leverages on the Big 5 Personality trait model and users digital traces harvested from their social networks, thereafter, transformed into a personalized warning message. We argue that stimulating users through personal recommendations evokes an understanding of the implications of their actions or inaction. © 2021, IFIP International Federation for Information Processing.",2021,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12936 LNCS,,456-461,,10.1007/978-3-030-85607-6_57,,#5826,Aneke 2021,,,1
Preliminary evaluation of a mHealth coaching conversational artificial intelligence for the self-care management of people with sickle-cell disease,"Issom, D.Z.; Rochat, J.; Hartvigsen, G.; Lovis, C.","Adherence to the complex set of recommended self-care practices among people with Sickle-Cell Disease (SCD) positively impacts health outcomes. However, few patients possess the required skills (i.e. disease-specific knowledge, adequate levels of self-efficacy). Consequently, adherence rates remain low and only 1% of patients are empowered enough to master the self-care practices. Health coaching and therapeutic patient education have emerged as new approaches to enhance patients' self-management and support health behavior changes. This preliminary feasibility study examined patients' perceived usefulness of the information provided by a chatbot we developed following patient-important requirements collected during our preliminary studies. Participants tested the chatbot and completed a post-test survey. A total of 19 patients were enrolled and 2 withdrew. 15 respondents (15/17, 88%) gave a score of at least 3/4 to the question “The chatbot contains all the information I need”. Results suggest that mHealth coaching apps could be used to promote the knowledge acquisition of recommended health behaviors related to the prevention of SCD main symptoms. © 2020 European Federation for Medical Informatics (EFMI) and IOS Press.",2020,,Studies in Health Technology and Informatics,270,,1361-1362,,10.3233/SHTI200442,,#5832,Issom 2020,,,1
Rebo Junior: Analysis of dialogue structure quality for a reflection guidance chatbot,"Wolfbauer, I.; Pammer-Schindler, V.; Rose, C.P.","Conversational user interfaces open up new opportunities for reflection guidance. This paper presents a computer-mediated dialogue structure for reflecting on learning tasks, Rebo Junior, and its evaluation in the context of apprenticeship training. We answer three research questions. Firstly, how apprentices react to Rebo Junior; secondly, whether Rebo Junior's dialogue structure is apt to lead apprentices in reflective conversations; and thirdly, how user engagement with Rebo Junior develops over time. During three months, 17 apprentices led 153 reflective conversations with Rebo Junior in the context of a training workshop, 117 in phase one and 36 in phase two of the study (five to thirteen interactions per apprentice). We coded interactions manually for coherence, level of reflectivity, and user engagement. Our results show that apprentices react well to the intervention and that the dialogue structure is successful in leading apprentices through different levels of reflection (114 out of 153 showed observable reflection on the learning experience; 133 out of 153 expressed learning or planned behaviour change for future tasks). Furthermore, the interactions between the apprentices and Rebo Junior result in coherent conversations (149 out of 153 were coherent). Contrary to expectations, engagement did not decrease over time in either phase. With the present paper, we therefore publish a dialogue structure for reflecting on learning tasks that has worked extremely well despite no adaptivity in the conversational interface. Overall, we interpret the results of our work as underscoring the importance of dialogue structure quality in conversational agents. © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2020,,CEUR Workshop Proceedings,2676,,,,,,#5836,Wolfbauer 2020,,,1
Persuasive assistant study—moderator variables,"Ehrenbrink, P.","The persuasive assistant study presented 72 participants with a persuasive to-do list application that was controlled via a spoken dialogue assistant. The investigated moderator variables were trait reactance, social agency, and involvement. Results show that trait reactance only moderated the effects of negative cognitions, while social agency interacts with both components of state reactance. No moderating effect of involvement could be shown. Furthermore, the study showed that state reactance can improve the prediction accuracy of the subjects’ global opinion about the technical device. © 2020, Springer Nature Switzerland AG.",2020,,T-Labs Series in Telecommunication Services,,,105-115,,10.1007/978-3-030-30310-5_11,,#5847,Ehrenbrink 2020,,,1
Ex machina: Technological disruption and the future of artificial intelligence in persuasive legal writing,"Campbell, J.","Technology is disrupting the practice of law and revolutionizing how lawyers work. This revolution is made more powerful because it is increasingly coupled with a rigorous and scientific approach to the law. In some ways, law is looking more like a Silicon Valley startup and less like the oak-paneled law firms of the last 200 years. As law, technology, and science merge, the implications for the profession are wide-sweeping. This article explores persuasive legal writing, offering new thoughts on what the future will hold. Specifically, this article pilots a method for applying technology and science to measure, analyze and improve persuasive legal writing, offering it as a proof of concept that anchors the article's broader, and perhaps more controversial assertion. Namely, more powerful and refined persuasive legal writing software tools, fueled by artificial intelligence, should and will disrupt and reshape significant portions of the legal space, including how legal writing is taught and how it is produced. The effect will be to view legal writing as more science, and less art. The next set of luminaries won't rely on anecdote or intuition to teach or create legal writing; they will rely on software and data. © 2020 Asociacion Nacional de Universidades e Instituciones de Educacion Superior A.C. All rights reserved.",2020,,University of Bologna Law Review,5,2,294-326,,10.6092/ISSN.2531-6133/12238,,#5849,Campbell 2020,,,1
A mobile health solution complementing psychopharmacology-supported smoking cessation: Randomized controlled trial,"Carrasco-Hernandez, L.; Jódar-Sánchez, F.; Núñez-Benjumea, F.; Conde, J.M.; González, M.M.; Civit-Balcells, A.; Hors-Fraile, S.; Parra-Calderón, C.L.; Bamidis, P.D.; Ortega-Ruiz, F.","Background: Smoking cessation is a persistent leading public health challenge. Mobile health (mHealth) solutions are emerging to improve smoking cessation treatments. Previous approaches have proposed supporting cessation with tailored motivational messages. Some managed to provide short-term improvements in smoking cessation. Yet, these approaches were either static in terms of personalization or human-based nonscalable solutions. Additionally, long-term effects were neither presented nor assessed in combination with existing psychopharmacological therapies. Objective: This study aimed to analyze the long-term efficacy of a mobile app supporting psychopharmacological therapy for smoking cessation and complementarily assess the involved innovative technology. Methods: A 12-month, randomized, open-label, parallel-group trial comparing smoking cessation rates was performed at Virgen del Rocío University Hospital in Seville (Spain). Smokers were randomly allocated to a control group (CG) receiving usual care (psychopharmacological treatment, n=120) or an intervention group (IG) receiving psychopharmacological treatment and using a mobile app providing artificial intelligence–generated and tailored smoking cessation support messages (n=120). The secondary objectives were to analyze health-related quality of life and monitor healthy lifestyle and physical exercise habits. Safety was assessed according to the presence of adverse events related to the pharmacological therapy. Per-protocol and intention-to-treat analyses were performed. Incomplete data and multinomial regression analyses were performed to assess the variables influencing participant cessation probability. The technical solution was assessed according to the precision of the tailored motivational smoking cessation messages and user engagement. Cessation and no cessation subgroups were compared using t tests. A voluntary satisfaction questionnaire was administered at the end of the intervention to all participants who completed the trial. Results: In the IG, abstinence was 2.75 times higher (adjusted OR 3.45, P=.01) in the per-protocol analysis and 2.15 times higher (adjusted OR 3.13, P=.002) in the intention-to-treat analysis. Lost data analysis and multinomial logistic models showed different patterns in participants who dropped out. Regarding safety, 14 of 120 (11.7%) IG participants and 13 of 120 (10.8%) CG participants had 19 and 23 adverse events, respectively (P=.84). None of the clinical secondary objective measures showed relevant differences between the groups. The system was able to learn and tailor messages for improved effectiveness in supporting smoking cessation but was unable to reduce the time between a message being sent and opened. In either case, there was no relevant difference between the cessation and no cessation subgroups. However, a significant difference was found in system engagement at 6 months (P=.04) but not in all subsequent months. High system appreciation was reported at the end of the study. Conclusions: The proposed mHealth solution complementing psychopharmacological therapy showed greater efficacy for achieving 1-year tobacco abstinence as compared with psychopharmacological therapy alone. It provides a basis for artificial intelligence–based future approaches. © Laura Carrasco-Hernandez, Francisco Jódar-Sánchez, Francisco Núñez-Benjumea, Jesús Moreno Conde, Marco Mesa González, Antón Civit-Balcells, Santiago Hors-Fraile, Carlos Luis Parra-Calderón, Panagiotis D Bamidis, Francisco Ortega-Ruiz.",2020,,JMIR mHealth and uHealth,8,4,,,10.2196/17530,,#5857,Carrasco-Hernandez 2020,,,1
Agent-based on emotional persuasion decision process and model,"Jinghua, W.; Jiali, H.; Jiaying, W.","With the rise of the Internet environment, automatic negotiation has become a research hotspot in the field of artificial intelligence. Especially in the complex negotiation environment, Agent has made the negotiation process more efficient because of its unique intelligence and interaction, and persuasive negotiation takes full consideration of persuasion factors and makes full use of persuasion, making the negotiation process more rational. Because emotion affects human cognition, goal and behavior, it plays an important role in human decision-making process. Therefore, emotion is an integral part of the field of artificial intelligence. Emotion is introduced into the field of Agent persuasion. By constructing emotional Agent, the intelligence and adaptability of Agent are improved and agent persuasion behavior is made. It is more consistent with human behavior, which has become the consensus of scholars. In the related research, the scholars and experts in this field have studied the persuasion based on Agent and the construction of emotional Agent from different angles, but there are the following problems: 1. the research on emotion is not comprehensive, especially the study of emotion calculation; 2. the research on the application of emotional theory to the Agent persuasion field is less; 3. to Agent More research is on the protocol mechanism, less research on the process of persuasion, especially the influence of emotion on the Agent persuasion process. In order to solve these problems effectively, this paper takes emotion as an important factor in the process of Agent persuasion, and focuses on Agent’s emotional persuasion decision-making process. First, the formal model based on the Agent emotional persuasion decision process is defined. Then the emotional persuasion decision-making process is defined and divided. The emotional persuasion decision process based on Agent is divided into four stages: the evaluation of emotional persuasion, the renewal of emotional persuasion, the adjustment of the emotional persuasion target, and the emotional persuasion. Secondly, through the influence of Agent’s personality on emotional trigger threshold, the emotion desalination factor is introduced, Agent emotional trigger function is defined, and the renewal emotion is built in the persuasion state model. Again, the mapping relationship between eight basic emotions and targets is established by defining emotional evaluation factors, and the emotional persuasion target model is constructed. In the end, the emotional persuasion behavior is divided into four categories: incentive emotional persuasion, pleading emotional persuasion, threatening emotional persuasion and antiplea emotional persuasion, and setting up the principle of emotional persuasion and target matching, and constructing emotional persuasion behavior model. Finally, a complete emotional persuasion decision-making process is formed, and the rationality and validity of the model is proved by an example. To illustrate the above decision process and the effectiveness of the model, an example is designed and analyzed. The example shows that emotional persuasion plays an important role in the decision-making process of Agent and the generation of emotional persuasion. If Agent evaluates the current persuasion to produce happy, dependent, relaxed, gentle, and so on, the emotion that has positive pleasure will encourage Agent to generate incentive or plea emotional persuasion. On the contrary, the feelings of negative pleasure, such as hostility, disdain, boredom, and anxiety, may cause Agent to defer or plead. Type of emotional persuasion. Based on this, the validity of the model is verified, and the important influence of emotion on persuasion decision process is also illustrated. © 2020",2020,,Journal of Industrial Engineering and Engineering Management,34,2,231-238,,10.13587/j.cnki.jieem.2020.02.025,,#5860,Jinghua 2020,,,1
Artificial Intelligence and Persuasion: A Construal-Level Account,"Kim, T.W.; Duhachek, A.","Although more individuals are relying on information provided by nonhuman agents, such as artificial intelligence and robots, little research has examined how persuasion attempts made by nonhuman agents might differ from persuasion attempts made by human agents. Drawing on construal-level theory, we posited that individuals would perceive artificial agents at a low level of construal because of the agents’ lack of autonomous goals and intentions, which directs individuals’ focus toward how these agents implement actions to serve humans rather than why they do so. Across multiple studies (total N = 1,668), we showed that these construal-based differences affect compliance with persuasive messages made by artificial agents. These messages are more appropriate and effective when the message represents low-level as opposed to high-level construal features. These effects were moderated by the extent to which an artificial agent could independently learn from its environment, given that learning defies people’s lay theories about artificial agents. © The Author(s) 2020.",2020,,Psychological Science,31,4,363-380,,10.1177/0956797620904985,,#5861,Kim 2020,,,1
End-to-end trainable non-collaborative dialog system,"Li, Y.; Qian, K.; Shi, W.; Yu, Z.","End-to-end task-oriented dialog models have achieved promising performance on collaborative tasks where users willingly coordinate with the system to complete a given task. While in non-collaborative settings, for example, negotiation and persuasion, users and systems do not share a common goal. As a result, compared to collaborate tasks, people use social content to build rapport and trust in these non-collaborative settings in order to advance their goals. To handle social content, we introduce a hierarchical intent annotation scheme, which can be generalized to different non-collaborative dialog tasks. Building upon TransferTransfo (Wolf et al. 2019), we propose an end-to-end neural network model to generate diverse coherent responses. Our model utilizes intent and semantic slots as the intermediate sentence representation to guide the generation process. In addition, we design a filter to select appropriate responses based on whether these intermediate representations fit the designed task and conversation constraints. Our non-collaborative dialog model guides users to complete the task while simultaneously keeps them engaged. We test our approach on our newly proposed ANTISCAM dataset and an existing PERSUASIONFORGOOD dataset. Both automatic and human evaluations suggest that our model outperforms multiple baselines in these two non-collaborative tasks. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",2020,,AAAI 2020 - 34th AAAI Conference on Artificial Intelligence,,,8203-8302,,,,#5876,Li 2020,,,1
A smartphone-based health care chatbot to promote self-management of chronic pain (SELMA): Pilot randomized controlled trial,"Hauser-Ulrich, S.; Künzli, H.; Meier-Peterhans, D.; Kowatsch, T.","Background: Ongoing pain is one of the most common diseases and has major physical, psychological, social, and economic impacts. A mobile health intervention utilizing a fully automated text-based health care chatbot (TBHC) may offer an innovative way not only to deliver coping strategies and psychoeducation for pain management but also to build a working alliance between a participant and the TBHC. Objective: The objectives of this study are twofold: (1) to describe the design and implementation to promote the chatbot painSELfMAnagement (SELMA), a 2-month smartphone-based cognitive behavior therapy (CBT) TBHC intervention for pain self-management in patients with ongoing or cyclic pain, and (2) to present findings from a pilot randomized controlled trial, in which effectiveness, influence of intention to change behavior, pain duration, working alliance, acceptance, and adherence were evaluated. Methods: Participants were recruited online and in collaboration with pain experts, and were randomized to interact with SELMA for 8 weeks either every day or every other day concerning CBT-based pain management (n=59), or weekly concerning content not related to pain management (n=43). Pain-related impairment (primary outcome), general well-being, pain intensity, and the bond scale of working alliance were measured at baseline and postintervention. Intention to change behavior and pain duration were measured at baseline only, and acceptance postintervention was assessed via self-reporting instruments. Adherence was assessed via usage data. Results: From May 2018 to August 2018, 311 adults downloaded the SELMA app, 102 of whom consented to participate and met the inclusion criteria. The average age of the women (88/102, 86.4%) and men (14/102, 13.6%) participating was 43.7 (SD 12.7) years. Baseline group comparison did not differ with respect to any demographic or clinical variable. The intervention group reported no significant change in pain-related impairment (P=.68) compared to the control group postintervention. The intention to change behavior was positively related to pain-related impairment (P=.01) and pain intensity (P=.01). Working alliance with the TBHC SELMA was comparable to that obtained in guided internet therapies with human coaches. Participants enjoyed using the app, perceiving it as useful and easy to use. Participants of the intervention group replied with an average answer ratio of 0.71 (SD 0.20) to 200 (SD 58.45) conversations initiated by SELMA. Participants’ comments revealed an appreciation of the empathic and responsible interaction with the TBHC SELMA. A main criticism was that there was no option to enter free text for the patients’ own comments. Conclusions: SELMA is feasible, as revealed mainly by positive feedback and valuable suggestions for future revisions. For example, the participants’ intention to change behavior or a more homogenous sample (eg, with a specific type of chronic pain) should be considered in further tailoring of SELMA. © Sandra Hauser-Ulrich, Hansjörg Künzli, Danielle Meier-Peterhans, Tobias Kowatsch.",2020,,JMIR mHealth and uHealth,8,4,,,10.2196/15806,,#5884,Hauser-Ulrich 2020,,,1
Interactive Computing Technology in Anti-Bullying Education: The Effects of Conversation-Bot’s Role on K-12 Students’ Attitude Change Toward Bullying Problems,"Young Oh, E.; Song, D.; Hong, H.","The aim of this study was to examine the effects of an anti-bullying activity that utilizes conversational virtual agents (called conversation-bots or chatbots) on students’ attitudes toward bullying problems. An experimental pre- or posttest design with a three-group setting was used. Eighty-nine fifth-grade students were assigned to one of three groups: Conversation with a virtual agent of (a) bully’s role, (b) victim’s role, and (c) teacher’s role. All agents are conversation-bots designed to support learner–computer interactions. The bully agent defends the notion that bullying behaviors are acceptable whereas the victim agent argues that bullying behavior cannot be tolerated. The teacher agent teaches students the types of bullying and its negative aspects. The participants completed an anti-bullying attitude test at pre- and posttest, which included students’ anti-bully, intention, pro-victim, behavior, and self-efficacy factors. The results show that students’ attitudes toward bullying problems changed to more positive responses after the implementation that used the conversation-bot. In addition, the results revealed that the agent’s role had an impact on the students’ attitudes toward the anti-bully factor. Implications and future research regarding the use of conversation-bots in education are discussed. © The Author(s) 2019.",2020,,Journal of Educational Computing Research,58,1,200-219,,10.1177/0735633119839177,,#5886,YoungOh 2020,,,1
Explainable AI meets persuasiveness: Translating reasoning results into behavioral change advice,"Dragoni, M.; Donadello, I.; Eccher, C.","Explainable AI aims at building intelligent systems that are able to provide a clear, and human understandable, justification of their decisions. This holds for both rule-based and data-driven methods. In management of chronic diseases, the users of such systems are patients that follow strict dietary rules to manage such diseases. After receiving the input of the intake food, the system performs reasoning to understand whether the users follow an unhealthy behavior. Successively, the system has to communicate the results in a clear and effective way, that is, the output message has to persuade users to follow the right dietary rules. In this paper, we address the main challenges to build such systems: (i) the Natural Language Generation of messages that explain the reasoner inconsistency; and, (ii) the effectiveness of such messages at persuading the users. Results prove that the persuasive explanations are able to reduce the unhealthy users’ behaviors. © 2020",2020,,Artificial Intelligence in Medicine,105,,,,10.1016/j.artmed.2020.101840,,#5888,Dragoni 2020,,,1
Persuasive Feedback for Fitness Apps: Effects of Construal Level and Communication Style,"Niess, J.; Diefenbach, S.; Woźniak, P.W.","Persuasive technologies to support behaviour change (e.g., fitness trackers) have become increasingly popular among consumers and healthcare providers. However, studies show that such technologies often fail to offer long-term engagement and tangible health benefits. In this regards, the specific design of persuasive messages provided by the device and users’ reactions to it may play a critical role. Our research explores the potential of applying theories of construal level and communication style to formulating feedback messages in self-improvement technologies. Two experiments (N = 190, N = 177) examine the influence of these two factors on goal commitment and affect-based evaluation for situations of fitness goal attainment and failure. Overall, construal level and communication style were relevant factors with independent influence. In the positive situation of goal attainment, high construal level and a friendly communication style resulted in significantly more goal commitment and positive affect than low construal level and a dominant communication style. In the negative situation of failure, results were overall less unambiguous and need to be consolidated by further research. Implications for the design of persuasive feedback are discussed. © 2020, Springer Nature Switzerland AG.",2020,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12064 LNCS,,101-112,,10.1007/978-3-030-45712-9_8,,#5912,Niess 2020,,,1
Designing an AI Health Coach and Studying Its Utility in Promoting Regular Aerobic Exercise,"Mohan, S.; Venkatakrishnan, A.; Hartzler, A.L.","Our research aims to develop interactive, social agents that can coach people to learn new tasks, skills, and habits. In this article, we focus on coaching sedentary, overweight individuals (i.e., ""trainees"") to exercise regularly. We employ adaptive goal setting in which the intelligent health coach generates, tracks, and revises personalized exercise goals for a trainee. The goals become incrementally more difficult as the trainee progresses through the training program. Our approach is model-based - the coach maintains a parameterized model of the trainee's aerobic capability that drives its expectation of the trainee's performance. The model is continually revised based on trainee-coach interactions. The coach is embodied in a smartphone application, NutriWalking, which serves as a medium for coach-trainee interaction. We adopt a task-centric evaluation approach for studying the utility of the proposed algorithm in promoting regular aerobic exercise. We show that our approach can adapt the trainee program not only to several trainees with different capabilities but also to how a trainee's capability improves as they begin to exercise more. Experts rate the goals selected by the coach better than other plausible goals, demonstrating that our approach is consistent with clinical recommendations. Further, in a 6-week observational study with sedentary participants, we show that the proposed approach helps increase exercise volume performed each week.  © 2020 ACM.",2020,,ACM Transactions on Interactive Intelligent Systems,10,2,,,10.1145/3366501,,#5914,Mohan 2020,,,1
Effects of Persuasive Dialogues: Testing Bot Identities and Inquiry Strategies,"Shi, W.; Wang, X.; Oh, Y.J.; Zhang, J.; Sahay, S.; Yu, Z.","Intelligent conversational agents, or chatbots, can take on various identities and are increasingly engaging in more human-centered conversations with persuasive goals. However, little is known about how identities and inquiry strategies influence the conversation's effectiveness. We conducted an online study involving 790 participants to be persuaded by a chatbot for charity donation. We designed a two by four factorial experiment (two chatbot identities and four inquiry strategies) where participants were randomly assigned to different conditions. Findings showed that the perceived identity of the chatbot had significant effects on the persuasion outcome (i.e., donation) and interpersonal perceptions (i.e., competence, confidence, warmth, and sincerity). Further, we identified interaction effects among perceived identities and inquiry strategies. We discuss the findings for theoretical and practical implications for developing ethical and effective persuasive chatbots. Our published data, codes, and analyses serve as the first step towards building competent ethical persuasive chatbots. © 2020 ACM.",2020,,Conference on Human Factors in Computing Systems - Proceedings,,,,,10.1145/3313831.3376843,,#5927,Shi 2020,,,1
"Feasibility of pediatric obesity and prediabetes treatment support through Tess, the AI behavioral coaching chatbot","Stephens, T.N.; Joerin, A.; Rauws, M.; Werk, L.N.","Behavioral intervention technologies (BITs) are unique ways to incorporate the benefits of technology and psychology to address differing health needs through various media, including Internet interventions, mobile apps, and video games. BITs present several possible benefits, including increased dissemination and accessibility, cost-effectiveness, increased engagement, and decreased stigma, especially among youth. A behavioral coaching chatbot, Tess, addresses different facets of behavioral health, such as depression and anxiety. Available 24/7, Tess delivers customized integrative support, psychoeducation, and interventions through brief conversations via existing communication channels (i.e., SMS text messaging and Facebook Messenger). This study assessed the feasibility of integrating Tess in behavioral counseling of adolescent patients (n = 23; Mage = 15.20 years; Rangeage = 9.78-18.54 years; 57% female) coping with weight management and prediabetes symptoms. Tess engaged patients via a preferred method of communication (SMS text messaging) in individualized conversations to promote treatment adherence, behavior change, and overall wellness. Adolescent patients reported experiencing positive progress toward their goals 81% of the time. The 4,123 messages exchanged and patients' reported usefulness ratings (96% of the time) illustrate that adolescents engaged with and viewed this chatbot as helpful. These results highlight the feasibility and benefit of support through artificial intelligence, specifically in a pediatric setting, which could be scaled to serve larger groups of patients. As a partner to clinicians, Tess can continue the therapeutic interaction outside office hours while maintaining patient satisfaction. Due to Tess's capacity for continuous learning, future iterations may have additional features to increase the user experience. © 2019 The Author(s).",2019,,Translational Behavioral Medicine,9,3,440-447,,10.1093/tbm/ibz043,,#5950,Stephens 2019,,,1
Modelling working alliance using user-aware explainable embodied conversational agents for behavior change: Framework and empirical evaluation,"Abdulrahman, A.; Richards, D.","The utilisation of embodied conversational agents (ECAs) to build a human-agent working alliance holds promise to promote health behavior change and improve health outcomes. Although ECAs have been shown to build empathic relationships with users, there is no complete framework to model working alliance. In this paper, we developed a framework that is grounded on theories and findings from social science and artificial intelligence to design a cognitive architecture for a user-aware explainable ECA. An empirical evaluation with 68 undergraduate students found differences in the efficacy of explanation to change behavior intention, build trust and working alliance depending on gender, stress levels and achievement aims; confirming the imperative of incorporating shared planning and user-tailored explanation in one framework. The empirical evaluation was limited in tailoring the explanation to the user's beliefs only; however, the analyses confirmed the need for considering adequate user information such as user's goals and preferences to build a user-aware explainable agent for behavior change towards improved health outcomes. © 40th International Conference on Information Systems, ICIS 2019. All rights reserved.",2019,,"40th International Conference on Information Systems, ICIS 2019",,,,,,,#5952,Abdulrahman 2019,,,1
Evaluation of computer-tailored motivational messaging in a health promotion context,"d’Hondt, J.E.; Nuijten, R.C.Y.; Van Gorp, P.M.E.","Persuasive messages have recently been shown to be more effective when tailored to the personality and preferences of the recipient. However, much of the literature on adaptive persuasion has evaluated the effectiveness of persuasive attempts by the direct reactions to those attempts instead of changes on the longer term (e.g. lifestyle changes). Results of this study suggest that adaptive persuasion improves attitudes towards persuasive attempts, but does not necessarily cause a change in longer term behavior. This was found through a randomized controlled trial evaluating the implementation an adaptive persuasive system in a health promotion intervention. This article provides a detailed description of this evaluation and encourages the research community to (1) become more skeptical towards the longer term effectiveness of adaptive persuasive techniques and (2) design more explicitly for longer term changes in behavior. © Springer Nature Switzerland AG 2019.",2019,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11939 LNAI,,120-133,,10.1007/978-3-030-34974-5_11,,#5963,d’Hondt 2019,,,1
Coerced change-talk with conversational agents promotes confidence in behavior change,"Olafsson, S.; O’Leary, T.; Bickmore, T.","Motivational interviewing is a counseling technique that works, in part, by helping people talk about changing their behavior. We describe two conversational agent-based interventions to increase motivation and confidence to promote physical activity and fruit and vegetable consumption that incorporate principles from motivational interviewing. We also explore the efficacy of constraining input, so users are only allowed to express change talk in their conversation with the agent. In a within-subjects experiment we demonstrate that both interventions are effective at increasing motivation, confidence, and self-efficacy for behavior change during a single counseling session. We also demonstrate that coercing user change talk leads to significantly greater increases in confidence compared to equivalent counseling agents that allow users to express statements about not changing their behavior. © 2019 Association for Computing Machinery.",2019,,PervasiveHealth: Pervasive Computing Technologies for Healthcare,,,31-40,,10.1145/3329189.3329202,,#5971,Olafsson 2019,,,1
"“I Am Here to Assist You Today”: The Role of Entity, Interactivity and Experiential Perceptions in Chatbot Persuasion","Ischen, C.; Araujo, T.; van Noort, G.; Voorveld, H.; Smit, E.","Online users are increasingly exposed to chatbots as one form of AI-enabled media technologies, employed for persuasive purposes, e.g., making product/service recommendations. However, the persuasive potential of chatbots has not yet been fully explored. Using an online experiment (N = 242), we investigate the extent to which communicating with a stand-alone chatbot influences affective and behavioral responses compared to interactive Web sites. Several underlying mechanisms are studied, showing that enjoyment is the key mechanism explaining the positive effect of chatbots (vs. Web sites) on recommendation adherence and attitudes. Contrary to expectations, perceived anthropomorphism seems not to be particularly relevant in this comparison. © 2020 Broadcast Education Association.",2020,,Journal of Broadcasting and Electronic Media,64,4,615-639,,10.1080/08838151.2020.1834297,,#5985,Ischen 2020,,,1
Promoting sustainable mobility beliefs with persuasive and anthropomorphic design: Insights from an experiment with a conversational agent,"Diederich, S.; Lichtenberg, S.; Brendel, A.B.; Trang, S.","Sustainable mobility behavior is increasingly relevant due to the vast environmental impact of current transportation systems. With the growing variety of transportation modes, individual decisions for or against specific mobility options become more and more important and salient beliefs regarding the environmental impact of different modes influence this decision process. While information systems have been recognized for their potential to shape individual beliefs and behavior, design-oriented studies that explore their impact, in particular on environmental beliefs, remain scarce. In this study, we contribute to closing this research gap by designing and evaluating a new type of artifact, a persuasive and human-like conversational agent, in a 2x2 experiment with 225 participants. Drawing on the Theory of Planned Behavior and Social Response Theory, we find empirical support for the influence of persuasive design elements on individual environmental beliefs and discover that anthropomorphic design can contribute to increasing the persuasiveness of artifacts. © 40th International Conference on Information Systems, ICIS 2019. All rights reserved.",2019,,"40th International Conference on Information Systems, ICIS 2019",,,,,,,#6005,Diederich 2019,,,1
Generation of multi-party dialogues among embodied conversational agents to promote active living and healthy diet for subjects suffering from type 2 diabetes,"Das, K.S.J.; Beinema, T.; Op Den Akker, H.; Hermens, H.","Diabetes Mellitus is a chronic condition that is highly prevalent in the geriatric population. Self-management plays a key role in the management of this condition. Leading an active lifestyle and having a healthy meal are cornerstones in managing this condition. Now with the advancement in Information and Communication Technology (ICT), continuous and proactive care from the affected individual's side is possible. A change in the individual's behaviour would be beneficial in pursuing physical activity and adopting a healthy diet. This paper focuses on providing a dialogue based virtual health coaching through multiple Embodied Conversational Agents (ECAs), using strategies such as Motivational Interviewing (MI), Theories of Behaviour Change and Behaviour Change Techniques (BCTs). The dialogues are constructed to induce a behaviour change and promote motivation in the affected subjects to work on improving their physical activity and diet plan. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",2019,,ICT4AWE 2019 - Proceedings of the 5th International Conference on Information and Communication Technologies for Ageing Well and e-Health,,,297-304,,10.5220/0007750602970304,,#6011,Das 2019,,,1
Towards Understanding Emotional Intelligence for Behavior Change Chatbots,"Ghandeharioun, A.; McDuff, D.; Czerwinski, M.; Rowan, K.","A natural conversational interface that allows longitudinal symptom tracking would be extremely valuable in health/wellness applications. However, the task of designing emotionally-aware agents for behavior change is still poorly understood. In this paper, we present the design and evaluation of an emotion-aware chatbot that conducts experience sampling in an empathetic manner. We evaluate it through a human-subject experiment with N=39 participants over the course of a week. Our results show that extraverts preferred the emotion-aware chatbot significantly more than introverts. Also, participants reported a higher percentage of positive mood reports when interacting with the empathetic bot. Finally, we provide guidelines for the design of emotion-aware chatbots for potential use in mHealth contexts. © 2019 IEEE.",2019,,"2019 8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019",,,Aug-14,,10.1109/ACII.2019.8925433,,#6021,Ghandeharioun 2019,,,1
Is argumessage effective? A critical evaluation of the persuasive message generation system,"J. Thomas, R.; Masthoff, J.; Oren, N.","This paper describes an investigation into the effectiveness of ArguMessage, a system that uses argumentation schemes and limited user input to semi-automatically generate persuasive messages encouraging behaviour change that follow specific argumentation patterns. We conducted user studies in the domains of healthy eating and email security to investigate its effectiveness. Our results show that ArguMessage in general supported users in generating messages based on the argumentation schemes. However, there were some issues in particular with copying the example messages, and some system improvements need to be made. Participants were generally satisfied with the messages produced, with the exception of those produced by two schemes (‘Argument from memory with goal’ and ‘Argument from values with goal’) which were removed after the first study. © Springer Nature Switzerland AG 2019.",2019,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11433 LNCS,,87-99,,10.1007/978-3-030-17287-9_8,,#6025,J 2019,,,1
Intelligent virtual assistant for promoting behaviour change in older people with T2D,"Balsa, J.; Neves, P.; Félix, I.; Pereira Guerreiro, M.; Alves, P.; Carmo, M.B.; Marques, D.; Dias, A.; Henriques, A.; Cláudio, A.P.","We present a version of an application prototype developed in the context of the VASelfCare project. This application works as an intelligent anthropomorphic virtual relational agent that has the role of assisting older people with Type 2 Diabetes Mellitus (T2D) in medication adherence and lifestyle changes. In this paper, we focus on the development of the dialogue component of the system and in what we consider one of the main original contributions: the incorporation, in the way the dialogue flows, of Behaviour Change Techniques (BCTs), identified in the context of the Behaviour Change Wheel framework. We also describe the general architecture of the system, including the graphical component. Tests on the prototype pre-requisites were conducted with health professionals and older adults with T2D within five primary care units of the Portuguese National Health Service. Overall, these tests yielded encouraging data and endorsed our approach. © Springer Nature Switzerland AG 2019.",2019,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11804 LNAI,,372-383,,10.1007/978-3-030-30241-2_32,,#6030,Balsa 2019,,,1
Data-centered persuasion: Nudging user's prosocial behavior and designing social innovation,"Shin, Y.; Kim, J.","By employing individuals' behavioral data collected from routinely used devices, a more effective ‘nudging’ toward changes in individuals' attitudes and behavior can be implemented. To examine the persuasive effect on inducing prosocial behaviors, two types of system concepts were designed, namely shared value systems. We manipulated the usage context of these systems and tested the effects with two independent variables: the status of the system agent and the valence of the agent's feedback. As a result, this study revealed that users' satisfaction and intention for prosocial behavior increased after they experienced the shared value system based on the concept of self-preferential value. Interaction effects were also found between the status of the system agent and the valence of the agent's feedback. © 2017 Elsevier Ltd",2018,,Computers in Human Behavior,80,,168-178,,10.1016/j.chb.2017.11.009,,#6046,Shin 2018,,,1
Wenner: A theoretically motivated approach for tailored coaching about physical activity,"Clavel, C.; Whittaker, S.; Blacodon, A.; Martin, J.-C.","In this paper, we describe a new theoretically motivated application framework, Wenner, for developing tailored interactions for an exercise-oriented well-being application. The framework allows us to conduct in-lab and 'in the wild' studies to directly deploy and evaluate the effects of our interventions. We describe two systems that have been developed using the framework. The first WennerStep is an individually tailored text based approach to delivering adaptive persuasive messaging using a smartwatch. The second WennerAgent, is a virtual embodied coach who adaptively reacts to user activity with emotionally appropriate responses. We conclude with a discussion in which we describe outstanding challenges for our approach and relate this to current work on Chatbot deployments for well-being. © 2018 Association for Computing Machinery.",2018,,UbiComp/ISWC 2018 - Adjunct Proceedings of the 2018 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2018 ACM International Symposium on Wearable Computers,,,1669-1675,,10.1145/3267305.3274190,,#6049,Clavel 2018,,,1
Designing for workplace reflection: A chat and voice-based conversational agent,"Kocielnik, R.; Avrahami, D.; Marlow, J.; Lu, D.; Hsieh, G.","Conversational agents stand to play an important role in supporting behavior change and well-being in many domains. With users able to interact with conversational agents through both text and voice, understanding how designing for these channels supports behavior change is important. To begin answering this question, we designed a conversational agent for the workplace that supports workers' activity journaling and self-learning through reflection. Our agent, named Robota, combines chat-based communication as a Slack Bot and voice interaction through a personal device using a custom Amazon Alexa Skill. Through a 3-week controlled deployment, we examine how voice-based and chat-based interaction affect workers' reflection and support self-learning. We demonstrate that, while many current technical limitations exist, adding dedicated mobile voice interaction separate from the already busy chat modality may further enable users to step back and reflect on their work. We conclude with discussion of the implications of our findings to design of workplace self-tracking systems specifically and to behavior-change systems in general. © 2018 Association for Computing Machinery.",2018,,DIS 2018 - Proceedings of the 2018 Designing Interactive Systems Conference,,,881-894,,10.1145/3196709.3196784,,#6079,Kocielnik 2018,,,1
Perceived organizational affiliation and its effects on patient trust: Role modeling with embodied conversational agents,"Zhang, Z.; Bickmore, T.W.; Paasche-Orlow, M.K.","Objective Verbal and non-verbal behaviors, which are known as “relational contextualization cues” relay information about relationships and how they are structured. We developed a computer-simulated provider conducting an informed consent process for clinical research to investigate the effects of a provider's alignment of interests with a patient, the research team, or a neutral party on patient trust in the provider. Methods Participants (N = 43) interacted with a simulated provider for a research informed consent process in a three-arm, counterbalanced, within-subjects experiment. Participants reported their trust in the simulated provider after each treatment. Results Participants successfully recognized the alignment manipulation, and perceived the patient-aligned provider as more trustworthy than the other providers. Participants were also more satisfied with the patient-aligned provider, liked this provider more, expressed more desire to continue working with this provider, and stated that they were significantly more likely to sign the consent form after interacting with this provider compared to the other two. Conclusion Relational contextualization that aligns with the patient increases trust, satisfaction, and willingness to enroll in the context of research informed consent. Practice implications Health providers should align themselves with patients’ interests. © 2017 Elsevier B.V.",2017,,Patient Education and Counseling,100,9,1730-1737,,10.1016/j.pec.2017.03.017,,#6095,Zhang 2017,,,1
Resolving asymmetry of medical information by using AI: Japanese people's change behavior by technology-driven innovation for Japanese health insurance,"Yamasaki, K.; Hosoya, R.","The advancement of the artificial intelligence (AI) technologies in recent years has enabled new technology-driven innovation in many fields in our society. The medical sector is one of the application field of AI that with high expectation for salient outcomes by directly influencing people's life-and-death matters. On the other hand, ethical problems pertaining to AI-based massive utilization of human health and biological data have risen as a new issue. In this paper, we report on the current status of the Japan's medical policies and a case study of a project for building an AI-based medical knowledge creation and information retrieval system aiming at achievement of change in people's health-related behavior. © 2018 Portland International Conference on Management of Engineering and Technology, Inc. (PICMET).",2018,,"PICMET 2018 - Portland International Conference on Management of Engineering and Technology: Managing Technological Entrepreneurship: The Engine for Economic Growth, Proceedings",,,,,10.23919/PICMET.2018.8481824,,#6104,Yamasaki 2018,,,1
Improving adherence in automated e-coaching: A case from insomnia therapy,"Beun, R.J.; Brinkman, W.-P.; Fitrianie, S.; Griffioen-Both, F.; Horsch, C.; Lancee, J.; Spruit, S.","Non-adherence is considered a problem that seriously undermines the outcome of behavior change therapies, in particular of self-help therapies delivered without human interference. This paper presents the design rationale behind a computer system in the domain of adherence enhancing strategies in automated e-coaching. A variety of persuasive strategies is introduced and implemented in a mobile e-coaching system in the domain of insomnia therapy. The system integrates two types of interface elements, i.e. dedicated tools and natural language conversation, to simplify therapy related activities and to include social strategies to improve motivation. We focus on the crucial role of communication and adaptation. © Springer International Publishing Switzerland 2016.",2016,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9638,,276-287,,10.1007/978-3-319-31510-2_24,,#6211,Beun 2016,,,1
Towards diet management with automatic reasoning and persuasive natural language generation,"Anselma, L.; Mazzei, A.","We devise a scenario where the interaction between man and food is mediated by an intelligent system that, on the basis of various factors, encourages or discourages the user to eat a specific dish. The main factors that the system need to account for are (1) the diet that the user intends to follow, (2) the food that s/he has eaten in the last days, and (3) the nutritional values of the dishes and their specific recipes. Automatic reasoning and Natural Language Generation (NLG) play a fundamental role in this project: the compatibility of a food with a diet is formalized as a Simple Temporal Problem (STP), while the NLG tries to motivate the user. In this paper we describe these two facilities and their interface. © Springer International Publishing Switzerland 2015.",2015,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9273,,79-90,,10.1007/978-3-319-23485-4_8,,#6232,Anselma 2015,,,1
Toward conducting motivational interviewing with an on-demand clinician avatar for tailored health behavior change interventions,"Lisetti, C.L.; Yasavur, U.; Visser, U.; Rishe, N.","In this article we describe work-in-progress about the development of avatar-based personalized assistants that can delivered motivational interviewing health behavior change interventions, tailored to its specific users Our approach combines the latest progress in Embodied Conversational Agents (ECAs), believable agents, and dialog systems. We discuss how we use different platforms to aim at providing accessibility of personalized health assistant, anytime anywhere. © 2011 ICST.",2011,,"2011 5th International Conference on Pervasive Computing Technologies for Healthcare and Workshops, PervasiveHealth 2011",,,246-249,,10.4108/icst.pervasivehealth.2011.246078,,#6506,Lisetti 2011,,,1
Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,"Meywirth, S","Adopting and maintaining healthy lifestyle behaviors such as regular exercise and balanced nutrition remain challenging despite their well-documented benefits for preventing chronic diseases and promoting overall well-being. Motivational Interviewing (MI) has emerged as a promising technique to address ambivalence and facilitate behavior change. However, traditional face-to-face delivery of MI interventions is limited by scalability and accessibility issues. Leveraging recent advancements in LLMs, this paper proposes an innovative approach to deliver MI-based coaching for lifestyle behavior change digitally. Following a problem-centered DSR approach, we created an initial prototype based on MI theory and qualitative user interviews using ChatGPT (GPT-3.5). We evaluated our prototype in a qualitative study. Our research outcomes include five design principles and thirteen system requirements. This research enhances the design knowledge base in LLM-based health coaching. It marks an essential first step towards designing LLM-based MI interventions, contributing valuable insights for future research in this emerging field.",2024,,"DESIGN SCIENCE RESEARCH FOR A RESILIENT FUTURE, DESRIST 2024",14621,,81-94,WOS:001285477600006,10.1007/978-3-031-61175-9_6,,#6684,Meywirth 2024,,,1
"Negotiating with LLMs: Prompt Hacks, Skill Gaps, and Reasoning Deficits","Schneider, J; Haag, S; Kruse, LC","Large language models (LLMs) like ChatGPT have reached the 100 million users' barrier in record time and might increasingly enter all areas of our life leading to a diverse set of interactions between those Artificial Intelligence models and humans. While many studies have discussed governance and regulations deductively from first order principles, few studies provide an inductive, data-driven lens based on observing dialogues between humans and LLMs - especially, when it comes to non-collaborative, competitive situations that have the potential to pose a serious threat for people. In this work, we conduct a user study engaging 41 individuals across all age groups in price negotiations with an LLM. We explore how people interact with an LLM, investigating differences in negotiation outcomes and strategies. Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality. We also show that the negotiated prices humans manage to achieve span a broad range, which points to a literacy gap in effectively interacting with LLMs.",2025,,"COMPUTER-HUMAN INTERACTION RESEARCH AND APPLICATIONS, CHIRA 2024, PT II",2371,,238-259,WOS:001473069300015,10.1007/978-3-031-83845-3_15,,#6685,Schneider 2025,,,1
Confronting Core Issues: A Critical Assessment of Attitude Polarization Using Tailored Experiments,"Velez, YR; Liu, P","A long-standing debate in political psychology considers whether individuals update their beliefs and attitudes in the direction of evidence or grow more confident in their convictions when confronted with counter-attitudinal arguments. Though recent studies have shown that instances of the latter tendency, which scholars have termed attitude polarization and ""belief backfire,"" are rarely observed in settings involving hot-button issues or viral misinformation, we know surprisingly little about how participants respond to information targeting deeply held attitudes, a key condition for triggering attitude polarization. We develop a tailored experimental design that measures participants' core issue positions and exposes them to personalized counter-attitudinal information using the large language model GPT-3. We find credible evidence of attitude polarization, but only when arguments are contentious and vitriolic. For lower valence counter-attitudinal arguments, attitude polarization is not detected. We conclude by discussing implications for the study of political cognition and the measurement of attitudes.",2025,,AMERICAN POLITICAL SCIENCE REVIEW,119,2,1036-1053,WOS:001284611200001,10.1017/S0003055424000819,,#6731,Velez 2025,,,1
An Embodied Conversational Agent to Support Wellbeing After Injury: Insights from a Stakeholder Inclusive Design Approach,"Hopman, K; Richards, D; Norberg, MN","Embodied conversational agents (ECAs) are increasingly being included in digital health and wellbeing programs. Whilst initial evaluations of ECAs in this field have been promising, there has been a lack of detail as to how stakeholders and end users have been included in the design process. Stakeholder inclusion has been acknowledged as essential to ensure that ECAs meet expectations, maximize engagement and facilitate behavior change. This study describes an iterative coproduction process used to develop an emotion regulation intervention conversational agent (ERICA) to support people following injury. The coproduction process followed an accelerated creation approach with 13 qualitative interviews being conducted with people who had a lived experience of recovering from injury/adversity. Interviews were completed with the goal to understand barriers and facilitators to engagement, user experience and to identify opportunities for program improvement. Four broad themes emerged from the data: visual look, voice, content and experience. A key finding from this study was a high level of stakeholder apprehension of generative AI and the need for ECA designers to make explicit the specific technology and functions being used within programs. Findings highlight the value of timely stakeholder inclusion in digital intervention design, particularly when knowledge and expectations of generative technology is rapidly changing.",2024,,"PERSUASIVE TECHNOLOGY, PERSUASIVE 2024",14636,,161-175,WOS:001280411900013,10.1007/978-3-031-58226-4_13,,#6815,Hopman 2024,,,1
"Exploring the Impact of Multimodal Long Conversations in VR on Attitudes Toward Behavior Change, Memory Retention, and Cognitive Load","Vankit, SA; Zamanifard, S; Diaz, D; Mousas, C; Richardson, K; Duchowski, AT; Volonte, M","This study examines how multimodal communication strategies (subtitles, visualizations, and their combination), affect memory retention, attitudes toward behavior change, and cognitive load during long conversations (+20 min) in immersive virtual reality (VR). Using embodied conversational agents to educate participants on diabetes and healthy eating, we found that all conditions effectively improved memory retention and behavior change attitudes. However, the combination of multimodal strategies increased cognitive load, suggesting a trade-off between engagement and cognitive demands. These findings highlight the potential of long VR conversations for healthcare education, while emphasizing the importance of balancing cognitive demands and exploring personalization for diverse users.",2025,,COMPUTER ANIMATION AND VIRTUAL WORLDS,36,3,,WOS:001511889400001,10.1002/cav.70023,,#6820,Vankit 2025,,,1
Automatic topic selection for long-term interaction with embodied conversational agents in health coaching: A micro-randomized trial,"Beinema, T; Op den Akker, H; Hurmuz, M; Jansen-Kosterink, S; Hermens, H","Introduction: Embodied Conversational Agents (ECAs) can be included in health coaching applications as virtual coaches. The engagement with these virtual coaches could be improved by presenting users with tailored coaching dialogues. In this article, we investigate if the suggestion of an automatically tailored topic by an ECA leads to higher engagement by the user and thus longer sessions of interaction.Methods: A Micro-Randomized Trial (MRT) was conducted in which two types of interaction with an ECA were compared: (a) the coach suggests a relevant topic to discuss, and (b) the coach asks the user to select a topic from a set of options. Every time the user would interact with the ECA, one of those conditions would be randomly selected. Participants interacted in their daily life with the ECA that was part of a multi-agent health coaching application for 4-8 weeks.Results: In two rounds, 82 participants interacted with the micro-randomized coach a total of 1011 times. Interactions in which the coach took the initiative were found to be of equal length as interactions in which the user was allowed to choose the topic, and the acceptance of topic suggestions was high (71.1% overall, 75.8% for coaching topics).Conclusion: Tailoring coaching conversations with ECAs by letting the coach automatically suggest a topic that is tailored to the user is perceived as a natural variation in the flow of interaction. Future research could focus on improving the novel coaching engine component that supports the topic selection process for these suggestions or on investigating how the amount of initiative and coaching approach by the ECA could be tailored.",2022,,INTERNET INTERVENTIONS-THE APPLICATION OF INFORMATION TECHNOLOGY IN MENTAL AND BEHAVIOURAL HEALTH,27,,,WOS:000770883600005,10.1016/j.invent.2022.100502,,#6829,Beinema 2022,,,1
Don't Walk Between Us: Adherence to Social Conventions When Joining a Small Conversational Group of Agents,"ACM; Iop, A; Zojaji, S; Peters, C","When modeling life-like Embodied Conversational Agents (ECAs), conveying politeness through verbal and nonverbal behaviors with persuasive intents is a significant challenge, as it underlies the conventional set of behavioral rules that govern human communication. In the present study, we explore the adherence to such rules in the context of joining a small, freestanding conversational group of agents in VR. In particular, we focus on the behavior adopted by participants while walking towards the agents, and on whether ECAs were treated in the same way human agents normally are. 45 test subjects were invited by an ECA to walk towards the group by applying one of six possible politeness strategies; after freely joining the group, they were asked to rate the agent's politeness according to four distinct aspects (Clarity, Face loss, Positive face, and Negative face). Across all strategies, in 48% of the trials participants were successfully persuaded to join the group at an inconvenient location. Out of those trials, participants adhered to social conventions by not crossing the convex empty space between the group members (o-space) in 75% of them on average. Additionally, analysis of verbal and nonverbal behaviors in ECAs shows that direct request strategies are more effective than indirect ones, although in some cases they may be perceived as less polite.",2022,,"PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS, IVA 2022",,,,WOS:001118873500014,10.1145/3514197.3549676,,#6857,ACM 2022,,,1
A Model of Social Explanations for a Conversational Movie Recommendation System,"ACM; Pecune, F; Murali, S; Tsai, V; Matsuyama, Y; Cassell, J","A critical aspect of any recommendation process is explaining the reasoning behind each recommendation. These explanations can not only improve users' experiences, but also change their perception of the recommendation quality. This work describes our human-centered design for our conversational movie recommendation agent, which explains its decisions as humans would. After exploring and analyzing a corpus of dyadic interactions, we developed a computational model of explanations. We then incorporated this model in the architecture of a conversational agent and evaluated the resulting system via a user experiment. Our results show that social explanations can improve the perceived quality of both the system and the interaction, regardless of the intrinsic quality of the recommendations.",2019,,PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON HUMAN-AGENT INTERACTION (HAI'19),,,135-143,WOS:000719339300020,10.1145/3349537.3351899,,#6867,ACM 2019,,,1
The impact of chatbot conversational skill on engagement and perceived humanness,"Schuetzler, RM; Grimes, GM; Giboney, JS","Conversational agents (CAs)-frequently operationalized as chatbots-are computer systems that leverage natural language processing to engage in conversations with human users. CAs are often operationalized as chatbots which are used for many applications including technical support, customer service, and digital personal assistants. Despite their widespread use, little research to date has investigated how improving the conversational skill of a CA impacts user perceptions of the agent. To elucidate this relationship, this research uses Social Presence Theory to describe how conversational skill influences perceived social presence and ultimately anthropomorphism of a chatbot. We conducted a series of studies in which 450 participants interacted with CAs exhibiting varying levels of conversational skill. We show that people perceive a more skilled CA to be more socially present and anthropomorphic than a less skilled CA. This research advances the knowledge of computer-human interface in information systems, as CA research to date has largely focused on the technical challenges rather than the behavioral questions of how users interact with CAs.",2020,,JOURNAL OF MANAGEMENT INFORMATION SYSTEMS,37,3,875-900,WOS:000590120000012,10.1080/07421222.2020.1790204,,#6872,Schuetzler 2020,,,1
Evaluating the Impact of Learner Control and Interactivity in Conversational Tutoring Systems for Persuasive Writing,"Wambsganss, T; Benke, I; Maedche, A; Koedinger, K; Käser, T","Conversational tutoring systems (CTSs) offer a promising avenue for individualized learning support, especially in domains like persuasive writing. Although these systems have the potential to enhance the learning process, the specific role of learner control and inter- activity within them remains underexplored. This paper introduces WritingTutor, a CTS designed to guide students through the pro- cess of crafting persuasive essays, with a focus on varying levels of learner control. In an experimental study involving 96 students, we evaluated the effects of high-level learner control, encompassing con- tent navigation and interface appearance control, against a benchmark version of WritingTutor without these features and a static, non- interactive tutoring group. Preliminary findings suggest that tutoring and learner control might enhance the learning experience in terms of enjoyment, ease-of-use, and perceived autonomy. However, these differences are not significant after pair-wise comparison and appear not to translate to significant differences in learning outcomes. This research contributes to the understanding of learner control in CTS, offering empirical insights into its influence on the learning experience.",2024,,INTERNATIONAL JOURNAL OF ARTIFICIAL INTELLIGENCE IN EDUCATION,,,,WOS:001236107900001,10.1007/s40593-024-00409-x,,#6880,Wambsganss 2024,,,1
Customizability in Conversational Agents and Their Impact on Health Engagement (Stage 2),"Paul, SC; Bartmann, N; Clark, JL","Conversational agents (CAs) are effective tools for health behavior change, yet little research investigates the mechanisms through which they work. Following the Computer as Social Actors (CASA) paradigm, we suggest that agents are perceived as human-like actors and hence influence behavior much as human coaches might. As such, agents should be designed to resemble ideal interaction patterns, for example, by resembling their users. In this registered report, we evaluated this paradigm by testing the impact of customization on similarity and reciprocity, which in turn were hypothesized to improve perceptions of the agent and compliance with the agent's recommendations to complete a cognitive training exercise. In an online study, 2437 participants were randomly assigned to one of two surface-level CA customization conditions (present/absent) and to one of two deep-level CA customization conditions (present/absent) in a between-subject experimental design. As part of a conversation flow with a CA, participants assigned to the present surface- and/or deep-level customization conditions were able to choose their preferred CA based on the four personality summaries and/or choose their CA's gender (male/female/agender robotic), avatar (choice between seven avatars corresponding to the chosen gender), and name. While the ability to customize increased similarity to the user and the perceptions of customizability, our findings show that customization did not impact experience or compliance. However, the perceived customizability of the agent was linked to increases in the likeability and usefulness of the agent. We conclude that our work finds no negative effects of customization; yet, its impact on the relationship between the agent and its user is complex and can benefit from more research as merited by its applicability to public health. As aging and ill populations increase the burden on health systems worldwide, CAs have the potential to transform the landscape of accessible care.",2024,,HUMAN BEHAVIOR AND EMERGING TECHNOLOGIES,2024,,,WOS:001350802400001,10.1155/2024/5015913,,#6885,Paul 2024,,,1
Planning Habit: Daily Planning Prompts with Alexa,"Cuadra, A; Bankole, O; Sobolev, M","The widespread adoption of intelligent voice assistants (IVAs), like Amazon's Alexa or Google's Assistant, presents new opportunities for designers of persuasive technologies to explore how to support people's behavior change goals and habits with voice technology. In this work, we explore how to use planning prompts, a technique from behavior science to make specific and effective plans, with IVAs. We design and conduct usability testing (N = 13) on a voice app called Planning Habit that encourages users to formulate daily plans out loud. We identify strategies that make it possible to successfully adapt planning prompts to voice format. We then conduct a week-long online deployment (N = 40) of the voice app in the context of daily productivity. Overall, we find that traditional forms of planning prompts can be adapted to and enhanced by IVA technology.",2021,,PERSUASIVE TECHNOLOGY (PERSUASIVE 2021),12684,,73-87,WOS:000788285600007,10.1007/978-3-030-79460-6_7,,#6889,Cuadra 2021,,,1
Exploring Users' Experiences With a Quick-Response Chatbot Within a Popular Smoking Cessation Smartphone App: Semistructured Interview Study,"Alphonse, A; Stewart, K; Brown, J; Perski, O","Background: Engagement with smartphone apps for smoking cessation tends to be low. Chatbots (ie, software that enables conversations with users) offer a promising means of increasing engagement.Objective: We aimed to explore smokers' experiences with a quick-response chatbot (Quit Coach) implemented within a popular smoking cessation app and identify factors that influence users' engagement with Quit Coach.Methods: In-depth, one-to-one, semistructured qualitative interviews were conducted with adult, past-year smokers who had voluntarily used Quit Coach in a recent smoking cessation attempt (5/14, 36%) and current smokers who agreed to download and use Quit Coach for a minimum of 2 weeks to support a new cessation attempt (9/14, 64%). Verbal reports were audio recorded, transcribed verbatim, and analyzed within a constructivist theoretical framework using inductive thematic analysis.Results: A total of 3 high-order themes were generated to capture users' experiences and engagement with Quit Coach: anthropomorphism of and accountability to Quit Coach (ie, users ascribing human-like characteristics and thoughts to the chatbot, which helped foster a sense of accountability to it), Quit Coach's interaction style and format (eg, positive and motivational tone of voice and quick and easy-to-complete check-ins), and users' perceived need for support (ie, chatbot engagement was motivated by seeking distraction from cravings or support to maintain motivation to stay quit).Conclusions: Anthropomorphism of a quick-response chatbot implemented within a popular smoking cessation app appeared to be enabled by its interaction style and format and users' perceived need for support, which may have given rise to feelings of accountability and increased engagement.",2022,,JMIR FORMATIVE RESEARCH,6,7,,WOS:000854081500008,10.2196/36869,,#6900,Alphonse 2022,,,1
"Can digital health researchers make a difference during the pandemic? Results of the single-arm, chatbot-led Elena plus : Care for COVID-19 interventional study","Ollier, J; Suryapalli, P; Fleisch, E; von Wangenheim, F; Mair, JL; Salamanca-Sanabria, A; Kowatsch, T","Background The current paper details findings from Elena+: Care for COVID-19, an app developed to tackle the collateral damage of lockdowns and social distancing, by offering pandemic lifestyle coaching across seven health areas: anxiety, loneliness, mental resources, sleep, diet and nutrition, physical activity, and COVID-19 information.Methods The Elena+ app functions as a single-arm interventional study, with participants recruited predominantly via social media. We used paired samples T-tests and within subjects ANOVA to examine changes in health outcome assessments and user experience evaluations over time. To investigate the mediating role of behavioral activation (i.e., users setting behavioral intentions and reporting actual behaviors) we use mixed-effect regression models. Free-text entries were analyzed qualitatively.Results Results show strong demand for publicly available lifestyle coaching during the pandemic, with total downloads (N = 7 & PRIME;135) and 55.8% of downloaders opening the app (n = 3,928) with 9.8% completing at least one subtopic (n = 698). Greatest areas of health vulnerability as assessed with screening measures were physical activity with 62% (n = 1,000) and anxiety with 46.5% (n = 760). The app was effective in the treatment of mental health; with a significant decrease in depression between first (14 days), second (28 days), and third (42 days) assessments: F-2,F-38 = 7.01, p = 0.003, with a large effect size (?2G = 0.14), and anxiety between first and second assessments: t(54) = 3.7, p = <0.001 with a medium effect size (Cohen d = 0.499). Those that followed the coaching program increased in net promoter score between the first and second assessment: t(36) = 2.08, p = 0.045 with a small to medium effect size (Cohen d = 0.342). Mediation analyses showed that while increasing number of subtopics completed increased behavioral activation (i.e., match between behavioral intentions and self-reported actual behaviors), behavioral activation did not mediate the relationship to improvements in health outcome assessments.Conclusions Findings show that: (i) there is public demand for chatbot led digital coaching, (ii) such tools can be effective in delivering treatment success, and (iii) they are highly valued by their long-term user base. As the current intervention was developed at rapid speed to meet the emergency pandemic context, the future looks bright for other public health focused chatbot-led digital health interventions.",2023,,FRONTIERS IN PUBLIC HEALTH,11,,,WOS:001070029800001,10.3389/fpubh.2023.1185702,,#6901,Ollier 2023,,,1
"Path Modeling of Visual Attention, User Perceptions, and Behavior Change Intentions in Conversations With Embodied Agents in VR","Vankit, SA; Motti, VG; Do, TD; Zamanifard, S; Diaz, D; Duchowski, AT; Knijnenburg, BP; Volonte, M","This study examines how subtitles and image visualizations influence gaze behavior, working alliance, and behavior change intentions in virtual health conversations with ECAs. Visualizations refer to images on a 3D model TV and text on a virtual whiteboard, both reinforcing key content conveyed by the ECA. Using a 2 x$$ \times $$ 2 factorial design, participants were randomly assigned to one of four conditions: no subtitles or visualizations (Control), subtitles only (SUB), visualizations only (VIS), or both subtitles and visualizations (VISSUB). Structural equation path modeling showed that SUB and VIS individually reduced gaze toward the ECA, whereas VISSUB moderated this reduction, resulting in less gaze loss than the sum of either condition alone. Gaze behavior was positively associated with working alliance, and perceptions of enjoyment and appropriateness influenced engagement, which in turn predicted behavior change intentions. VIS was negatively associated with behavior change intentions, suggesting that excessive visual input may introduce cognitive trade-offs.",2025,,COMPUTER ANIMATION AND VIRTUAL WORLDS,36,3,,WOS:001512330400001,10.1002/cav.70028,,#6902,Vankit 2025,,,1
Attitudes Toward a Virtual Smoking Cessation Coach: Relationship and Willingness to Continue,"Albers, N; Neerincx, MA; Aretz, NL; Ali, M; Ekinci, A; Brinkman, WP","Virtual coaches have the potential to address the low adherence common to eHealth applications for behavior change by, for example, providing motivational support. However, given the multitude of factors affecting users' attitudes toward virtual coaches, more insights are needed on how such virtual coaches can be designed to affect these attitudes in a specific use context positively. Especially valuable are insights that are based on users interacting with such a virtual coach for longer. We thus conducted a study in which more than 500 smokers interacted with the text-based virtual coach Sam in five sessions. In each session, Sam assigned smokers a new preparatory activity for quitting smoking and provided motivational support for doing the activity. Based on a mixed-methods analysis of users' willingness to continue working and their relationship with Sam, we obtained eight themes for users' attitudes toward Sam. These themes relate to whether Sam is seen as human or artificial, specific characteristics of Sam (e.g., caring character), the interaction with Sam, and the relationship with Sam. We used these themes to formulate literature-based recommendations to guide designers of virtual coaches for behavior change. For example, letting the virtual coach get to know users and disclose more information about itself may improve its relationship with users.",2023,,"PERSUASIVE TECHNOLOGY, PERSUASIVE 2023",13832,,265-274,WOS:001286452400017,10.1007/978-3-031-30933-5_17,,#6904,Albers 2023,,,1
Designing Virtual Coaching Solutions,"Schlieter, H; Gand, K; Weimann, TG; Sandner, E; Kreiner, K; Thoma, S; Liu, J; Caprino, M; Corbo, M; Seregni, A; Tropea, P; Del Pino, R; Esteban, JCG; Gabilondo, I; Lacraru, AE; Busnatu, SS","Especially older persons are prone to disabilities and chronic diseases. These chronic conditions pose a worldwide challenge, leading to deteriorating health, economic strain, loss of life, and a decline in the quality of life (QoL). Therefore, healthcare institutions seek to enhance their strategies for disease prevention and management to uphold the well-being of the community. This leads to the need to regain independence and improve QoL to properly rehabilitate the patients. Virtual Coaches (VCs) in the form of Embodied Conversational Agents are seen as a relevant digital intervention to support the continuity of care. The paper at hand reports on a Design Science Research project about implementing a VC solution to support older patients' home rehabilitation. The study underpins four pivotal design principles: Adaptivity, Coaching Strategy, Multi-user Interface, and Sustainable Infrastructure. The final artifact was tested with 80 patients which were supported in continuing their inpatient rehabilitation at home by using a VC. The evaluation shows both positive results for usability and acceptance of the intervention for four different use cases and a positive impact on the QoL. Given the comprehensive clinical evaluation, the system represents a safe and appealing solution for ensuring the continuity of medical rehabilitation care and the access to personalized cognitive and motor function treatments.",2024,,BUSINESS & INFORMATION SYSTEMS ENGINEERING,66,3,377-400,WOS:001234570800001,10.1007/s12599-024-00871-w,,#6918,Schlieter 2024,,,1
Collaboratively Setting Daily Step Goals with a Virtual Coach: Using Reinforcement Learning to Personalize Initial Proposals,"Dierikx, M; Albers, N; Scheltinga, BL; Brinkman, WP","Goal-setting is commonly used in behavior change applications for physical activity. However, for goals to be effective, they need to be tailored to a user's situation (e.g., motivation, progress). One way to obtain such goals is a collaborative process in which a healthcare professional and client set a goal together, thus making use of the professional's expertise and the client's knowledge about their own situation. As healthcare professionals are not always available, we created a dialog with the virtual coach Steph to collaboratively set daily step goals. Since judgments in human decision-making processes are adjusted based on the starting point or anchor, the first step goal proposal Steph makes is likely to influence the user's final goal and self-efficacy. Situational factors impacting physical activity (e.g., motivation, self-efficacy, available time) or how users process information (e.g., mood) may determine which initial proposals are most effective in getting users to reach their underlying previous activity-based recommended step goals. Using data from 117 people interacting with Steph for up to five days, we designed a reinforcement learning algorithm that considers users' current and future situations when choosing an initial step goal proposal. Our simulations show that initial step goal proposals matter: choosing optimal ones based on this algorithm could make it more likely that people move to a situation with high motivation, high self-efficacy, and a favorable daily context. Then, they are more likely to achieve, but also to overachieve, their underlying recommended step goals. Our dataset is publicly available.",2024,,"PERSUASIVE TECHNOLOGY, PERSUASIVE 2024",14636,,100-115,WOS:001280411900009,10.1007/978-3-031-58226-4_9,,#6920,Dierikx 2024,,,1
Explainable AI: The Effect of Contradictory Decisions and Explanations on Users' Acceptance of AI Systems,"Ebermann, C; Selisky, M; Weibelzahl, S","Providing explanations of an artificial intelligence (AI) system has been suggested as a means to increase users' acceptance during the decision-making process. However, little research has been done to examine the psychological mechanism of how these explanations cause a positive or negative reaction in the user. To address this gap, we investigate the effect on user acceptance if decisions and the associated provided explanations contradict between an AI system and the user. An interdisciplinary research model was derived and validated by an experiment with 78 participants. Findings suggest that in decision situations with cognitive misfit users experience negative mood significantly more often and have a negative evaluation of the AI system's support. Therefore, the following article provides further guidance regarding new interdisciplinary approaches for dealing with human-AI interaction during the decision-making process and sheds some light on how explainable AI can increase users' acceptance of such systems.",2023,,INTERNATIONAL JOURNAL OF HUMAN-COMPUTER INTERACTION,39,9,1807-1826,WOS:000876592900001,10.1080/10447318.2022.2126812,,#6977,Ebermann 2023,,,1
Typologies of Persuasive Strategies and Content: A Formalization Using Argumentation,"Guerrero, E; Lindgren, H","Persuasion is an active research topic in artificial intelligence (AI), human-computer interaction (HCI), and social sciences. When persuasive technology has been designed, some HCI guidelines have commonly used disregarding the current AI state of the art, for example, ignoring autonomy and proactive AI behavior. In this paper, a systematic review of HCI persuasive strategies and their corresponding content is mapped to a formal AI approach using argumentation theory. We also present experimental results using as context a mobile application for behavior change in the Swedish context.",2021,,"ADVANCES IN PRACTICAL APPLICATIONS OF AGENTS, MULTI-AGENT SYSTEMS, AND SOCIAL GOOD: THE PAAMS COLLECTION, PAAMS 2021",12946,,101-113,WOS:000791045800009,10.1007/978-3-030-85739-4_9,,#6997,Guerrero 2021,,,1
Creative Artificial Intelligence and Narrative Transportation,"Messingschlager, TV; Appel, M","Artificial intelligence (AI) is increasingly used to accomplish complex tasks, including the creation of artworks and entertainment products. Our focus here is on user responses to AI systems as authors of fictional stories. Across two experiments, we examined how the information that a story was written by AI influences narrative transportation and related experiences. In Experiment 1 (N = 325) the information that an AI had created a short story (contemporary fiction) reduced narrative transportation into this story. Experiment 2 (N = 489) was an extended replication in which genre differences (contemporary fiction vs. science fiction) were addressed. As expected, ostensible AI authorship reduced transportation, but this effect was qualified by genre: Whereas the AI-authorship effect was replicated for contemporary fiction stories, transportation did not differ between human and AI authorship when participants read science fiction stories. Across both experiments, individual differences (openness, affinity for technology and attitude toward AI) did not moderate the effect of AI authorship on any of the dependent variables.",2024,,PSYCHOLOGY OF AESTHETICS CREATIVITY AND THE ARTS,18,5,848-857,WOS:000819645500001,10.1037/aca0000495,,#7049,Messingschlager 2024,,,1
SmileApp: The Design and Evaluation of an mHealth App for Stress Reduction Through Artificial Intelligence and Persuasive Technology,"Orji, J; Chan, G; Ndulue, C; Orji, R","Mobile health apps are becoming popular and capable of addressing a wide range of health-related issues including stress. In this study, we designed, developed, and evaluated an mHealth application called SmileApp to promote positive mood as a means of reducing stress. The design of SmileApp is grounded in psychological theories and integrates artificial intelligence (AI) and persuasive technology (PT). To evaluate SmileApp, we conducted a two-week in-the-wild study involving 72 participants. This was followed by an optional semi-structured interviewwith 23 participants. Quantitative results suggest that SmileApp is usable, and the use of the features can convince users to smile more frequently, as well as feel a sense of companionship and connectedness. Furthermore, qualitative results suggest that SmileApp was a unique design to help users alleviate stress. These results offer valuable insights into innovative approaches for designing mHealth applications that promote positive mood. Moreover, the findings underscore the importance of utilizing technology to support emotional wellbeing. We present a novel approach to promote desired behaviors by motivating users to read supportive messages and play mobile games by smiling.",2024,,"PERSUASIVE TECHNOLOGY, PERSUASIVE 2024",14636,,237-251,WOS:001280411900018,10.1007/978-3-031-58226-4_18,,#7062,Orji 2024,,,1
Beyond Consumption-Relevant Outcomes: The Role of AI Customer Service Chatbots' Communication Styles in Promoting Societal Welfare,"Zhou, YY; Wang, J; Ding, YX; Meng, XY; Gao, A","The application of artificial intelligence (AI) in customer service becomes ubiquitous. In response to the advocacy in the ""2021 Coordinated Plan on Artificial Intelligence"", it is crucial to understand how to leverage AI customer service chatbots for societal welfare. Across two scenario studies and one lab experiment, this research investigates the impact of AI chatbots' communication styles on consumers' subsequent prosocial intentions irrelevant to the AI-human interaction contents. The combined evidence suggests that consumers exhibit higher prosocial intentions after interacting with social-oriented (vs. task-oriented) AI chatbots. The findings reveal the chain-mediating roles of social presence and empathy. Moreover, the current research investigates the boundary effect of consumers' goal focus (process focus vs. outcome focus), and shows that AI chatbots' communication styles have stronger impact on prosocial intentions for customers with outcome focus. These results revealed the important externality of the AI application in marketplace and provide a novel perspective for companies to implement the corporate social responsibility (CSR) strategy.",2025,,JOURNAL OF SYSTEMS SCIENCE AND SYSTEMS ENGINEERING,,,,WOS:001500328200001,10.1007/s11518-025-5674-8,,#7085,Zhou 2025,,,1
Evaluation of Dietary Management Using Artificial Intelligence and Human Interventions: Nonrandomized Controlled Trial,"Okaniwa, F; Yoshida, H","Background: There has been an increase in personal health records with the increased use of wearable devices and smartphone apps to improve health. Traditional health promotion programs by human professionals have limitations in terms of cost and reach. Due to labor shortages and to save costs, there has been a growing emphasis in the medical field on building health guidance systems using artificial intelligence (AI). AI will replace advanced human tasks to some extent in the future. However, it is difficult to sustain behavioral change through technology alone at present.Objective: This study investigates whether AI alone can effectively encourage healthy behaviors or whether human interventions are needed to achieve and sustain health-related behavioral change. We examined the effectiveness of AI and human interventions to encourage dietary management behaviors. In addition, we elucidated the conditions for maximizing the effect of AI on health improvement. We hypothesized that the combination of AI and human interventions will maximize their effectiveness.Methods: We conducted a 3-month experiment by recruiting participants who were users of a smartphone diet management app. We recruited 102 participants and divided them into 3 groups. Treatment group I received text messages using the standard features of the app (AI-based text message intervention). Treatment group II received video messages from a companion, in addition to the text messages (combined text message and human video message intervention by AI). The control group used the app to keep a dietary record, but no feedback was provided (no intervention). We examine the participants' continuity and the effects on physical indicators.Results: Combined AI and video messaging (treatment group II) led to a lower dropout rate from the program compared to the control group, and the Cox proportional-hazards model estimate showed a hazard ratio (HR) of 0.078, which was statistically significant at the 5% level. Further, human intervention with AI and video messaging significantly reduced the body fat percentage (BFP) of participants after 3 months compared to the control group, and the rate of reduction was greater in the group with more individualized intervention. The AI-based text messages affected the BMI but had no significant effect on the BFP.Conclusions: This experiment shows that it is challenging to sustain participants' healthy behavior with AI intervention alone. The results also suggest that even if the health information conveyed is the same, the information conveyed by humans and AI is more effective in improving health than the information sent by AI alone. The support received from the companion in the form of video messages may have promoted voluntary health behaviors. It is noteworthy that companions were competent, even though they were nonexperts. This means that person-to-person communication is crucial for health interventions.",2022,,JMIR FORMATIVE RESEARCH,6,6,,WOS:000854080300005,10.2196/30630,,#7118,Okaniwa 2022,,,1
"""I know it's a deepfake"": the role of AI disclaimers and comprehension in the processing of deepfake parodies","Lu, H; Yuan, SP","Rapid innovations in media technologies have ushered in diverse entertainment avenues, including politically oriented content, presenting both novel opportunities and societal challenges. This study delves into the implications of the burgeoning deepfake phenomenon, particularly focusing on audience interpretation and engagement with deepfake parodies, a quintessential example of ""misinfotainment."" Additionally, it examines the potential impact of artificial intelligence (AI) disclaimers on audience understanding and related consequences. To probe this, two experiments (N = 2,808) were executed featuring parodied politicians adopting opposing viewpoints on the issue of climate change. U.S. participants were either exposed to deepfake videos prefaced with AI disclaimers or without. Results indicate that the inclusion of an AI disclaimer significantly influenced audience comprehension and their ability to recognize the parody. These factors were subsequently associated with enjoyment, discounting, and counterarguing, which in turn showed different relationships with policy support and sharing intentions. This article culminates with insights into the theoretical underpinnings and practical ramifications of these findings.",2024,,JOURNAL OF COMMUNICATION,74,5,,WOS:001249909600001,10.1093/joc/jqae022,,#7125,Lu 2024,,,1
"Can ChatGPT be a debate partner? Developing ChatGPT-based application ""DEBO"" for debate education, findings and limitations","Lee, U; Jeong, Y; Koh, J; Byun, G; Lee, Y; Hwang, Y; Kim, H; Lim, C","Debate is a universally acknowledged competency for its vital role in fostering essential skills such as analytical reasoning, eloquent communication, and persuasive argument construction. This is relevant in both formal educational settings like classrooms and informal venues such as after-school clubs. Traditional debate training methods often face challenges in facilitating personalized learning, primarily due to the difficulty of securing a debate partner. However, the emerging field of generative AI offers a promising alternative. This study evaluates the effectiveness of DEBO, a debate education application that utilizes ChatGPT, an advanced language model by OpenAI. Focusing on university students actively involved in a debate club, six volunteers tested DEBO and participated in interviews to provide insights. The findings reveal that DEBO notably enhances divergent thinking and features a comprehensive analytics dashboard for in-depth performance analysis. However, there are few limitations including occasional shortcomings in debate quality, minor inaccuracies, and the need for faster response times. The study underscores the transformative potential of advanced language models like ChatGPT in reshaping debate education, while also identifying areas for further improvement.",2024,,EDUCATIONAL TECHNOLOGY & SOCIETY,27,2,321-346,WOS:001497043900003,10.30191/ETS.202404_27(2).TP03,,#7149,Lee 2024,,,1
Feasibility of Using an Artificial Intelligence-based Telephone Application for Dietary Assessment and Nudging to Improve the Quality of Food Choices of Female Adolescents in Vietnam: Evidence from a Randomized Pilot Study,"Braga, BC; Nguyen, PH; Tran, LM; Hoang, NT; Bannerman, B; Doyle, F; Folson, G; Gangupantulu, R; Karachiwalla, N; Kolt, B; McCloskey, P; Palloni, G; Tran, THT; Troung, DTT; Hughes, D; Gelli, A","Background: Adolescent nutrition has faced a policy neglect, partly owing to the gaps in dietary intake data for this age group. The Food Recognition Assistance and Nudging Insights (FRANI) is a smartphone application validated for dietary assessment and to in fl uence users toward healthy food choices. Objectives: This study aimed to assess the feasibility (adherence, acceptability, and usability) of FRANI and its effects on food choices and diet quality in female adolescents in Vietnam. Methods: Adolescents ( N = 36) were randomly selected from a public school and allocated into 2 groups. The control group received smartphones with a version of FRANI limited to dietary assessment, whereas the intervention received smartphones with gami fi ed FRANI. After the fi rst 4 wk, both groups used gami fi ed FRANI for further 2 wk. The primary outcome was the feasibility of using FRANI as measured by adherence (the proportion of completed food records), acceptability and usability (the proportion of participants who considered FRANI acceptable and usable according to answers of a Likert questionnaire). Secondary outcomes included the percentage of meals recorded, the Minimum Dietary Diversity for Women (MDDW) and the Eat-Lancet Diet Score (ELDS). Dietary diversity is important for dietary quality, and sustainable healthy diets are important to reduce carbon emissions. Poisson regression models were used to estimate the effect of gami fi ed FRANI on the MDDW and ELDS. Results: Adherence to the application was 82% and the percentage of meals recorded was 97%. Acceptability and usability were 97%. MDDW in the intervention group was 1.07 points (95% CI: 0.98, 1.18; P = 0.13) greater than that in the control (constant = 4.68); however, the difference was not statistically signi fi cant. Moreover, ELDS in the intervention was 1.09 (95% CI: 1.01, 1.18; P = 0.03) points greater than in the control (constant = 3.67). Conclusions: FRANI was feasible and may be effective to in fl uence users toward healthy food choices. Research is needed for FRANI in different contexts and at scale. The trial was registered at the International Standard Randomized Controlled Trial Number as ISRCTN 10681553.",2024,,CURRENT DEVELOPMENTS IN NUTRITION,8,6,,WOS:001244674400001,10.1016/j.cdnut.2023.102063,,#7162,Braga 2024,,,1
Scaffolding the Mastery of Healthy Behaviors with Fittle plus Systems: Evidence-Based Interventions and Theory,"Pirolli, P; Youngblood, GM; Du, HL; Konrad, A; Nelson, L; Springer, A","We present a series of mHealth applications and studies pursued as part of the Fittle+ project. This program of research has the dual aims of (1) bringing scalable evidence-based behavior-change interventions to mHealth and evaluating them and (2) developing theoretically based predictive models to better understand the dynamics of the impact of these interventions on achieving behavior-change goals. Our approach in the Fittle+ systems rests on the idea that to master the complex fabric of a new healthy lifestyle, one must weave together a new set of healthy habits that over-ride the old unhealthy habits. To achieve these aims, we have developed a series of mHealth platforms that provide scaffolding interventions: Behavior-change techniques and associated mHealth interactions (e.g., SMS reminders; chatbot dialogs; user interface functionality; etc.) that provide additional support to the acquisition and maintenance of healthy habits. We present experimental evidence collected so far for statistically significant improvements in behavior change in eating, exercise, and physical activity for the following scaffolding interventions: guided mastery, teaming, self-affirmation, and implementation intentions. We also present predictive computational ACT-R models of daily individual behavior goal success for data collected in guided mastery and implementation intention studies that address goal-striving and habit formation mechanisms.",2021,,HUMAN-COMPUTER INTERACTION,36,2,73-106,WOS:000604738500001,10.1080/07370024.2018.1512414,,#7217,Pirolli 2021,,,1
Evaluating the Effectiveness of Bayesian Knowledge Tracing Model-Based Explainable Recommender,"Takami, K; Flanagan, B; Dai, YL; Ogata, H","Explainable recommendation, which provides an explanation about why a quiz is recommended, helps to improve transparency, persuasiveness, and trustworthiness. However, little research examined the effectiveness of the explainable recommender, especially on academic performance. To survey its effectiveness, the authors evaluate the math academic performance among middle school students (n=115) by giving pre- and post-test questions based evaluation techniques. During the pre- and post-test periods, students were encouraged to use the Bayesian Knowledge Tracing model based explainable recommendation system. To evaluate how well the students were able to do what they could not do, the authors defined growth rate and found recommended quiz clicked counts had a positive effect on the total number of solved quizzes (R=0.343, P=0.005) and growth rate (R=0.297, P=0.017) despite no correlation between the total number of solved quizzes and growth rate. The results suggest that the use of an explainable recommendation system that learns efficiently will enable students to do what they could not do before.",2024,,INTERNATIONAL JOURNAL OF DISTANCE EDUCATION TECHNOLOGIES,22,1,,WOS:001160903100001,10.4018/IJDET.337600,,#7255,Takami 2024,,,1
"Checking the Fact-Checkers: The Role of Source Type, Perceived Credibility, and Individual Differences in Fact-Checking Effectiveness","Liu, XY; Qi, L; Wang, L; Metzger, MJ","This study investigates fact-checking effectiveness in reducing belief in misinformation across various types of fact-check sources (i.e., professional fact-checkers, mainstream news outlets, social media platforms, artificial intelligence, and crowdsourcing). We examine fact-checker credibility perceptions as a mechanism to explain variance in fact-checking effectiveness across sources, while taking individual differences into account (i.e., analytic thinking and alignment with the fact-check verdict). An experiment with 859 participants revealed few differences in effectiveness across fact-checking sources but found that sources perceived as more credible are more effective. Indeed, the data show that perceived credibility of fact-check sources mediates the relationship between exposure to fact-checking messages and their effectiveness for some source types. Moreover, fact-checker credibility moderates the effect of alignment on effectiveness, while analytic thinking is unrelated to fact-checker credibility perceptions, alignment, and effectiveness. Other theoretical contributions include extending the scope of the credibility-persuasion association and the MAIN model to the fact-checking context, and empirically verifying a critical component of the two-step motivated reasoning model of misinformation correction.",2025,,COMMUNICATION RESEARCH,52,6,719-746,WOS:001088445300001,10.1177/00936502231206419,,#7289,Liu 2025,,,1
Exploring factors influencing user perspective of ChatGPT as a technology that assists in healthcare decision making: A cross sectional survey study,"Choudhury, A; Elkefi, S; Tounsi, A","As ChatGPT emerges as a potential ally in healthcare decision-making, it is imperative to investigate how users leverage and perceive it. The repurposing of technology is innovative but brings risks, especially since AI's effectiveness depends on the data it's fed. In healthcare, ChatGPT might provide sound advice based on current medical knowledge, which could turn into misinformation if its data sources later include erroneous information. Our study assesses user perceptions of ChatGPT, particularly of those who used ChatGPT for healthcare-related queries. By examining factors such as competence, reliability, transparency, trustworthiness, security, and persuasiveness of ChatGPT, the research aimed to understand how users rely on ChatGPT for health-related decision-making. A web-based survey was distributed to U.S. adults using ChatGPT at least once a month. Bayesian Linear Regression was used to understand how much ChatGPT aids in informed decision-making. This analysis was conducted on subsets of respondents, both those who used ChatGPT for healthcare decisions and those who did not. Qualitative data from open-ended questions were analyzed using content analysis, with thematic coding to extract public opinions on urban environmental policies. Six hundred and seven individuals responded to the survey. Respondents were distributed across 306 US cities of which 20 participants were from rural cities. Of all the respondents, 44 used ChatGPT for health-related queries and decision-making. In the healthcare context, the most effective model highlights 'Competent + Trustworthy + ChatGPT for healthcare queries', underscoring the critical importance of perceived competence and trustworthiness specifically in the realm of healthcare applications of ChatGPT. On the other hand, the non-healthcare context reveals a broader spectrum of influential factors in its best model, which includes 'Trustworthy + Secure + Benefits outweigh risks + Satisfaction + Willing to take decisions + Intent to use + Persuasive'. In conclusion our study findings suggest a clear demarcation in user expectations and requirements from AI systems based on the context of their use. We advocate for a balanced approach where technological advancement and user readiness are harmonized.",2024,,PLOS ONE,19,3,,WOS:001181701200019,10.1371/journal.pone.0296151,,#7372,Choudhury 2024,,,1
CoArgue: Fostering Lurkers' Contribution to Collective Arguments in Community-based QA Platforms,"ACM; Liu, CZ; Zhou, SX; Liu, DD; Li, JZ; Huang, ZY; Ma, XJ","In Community-Based Question Answering (CQA) platforms, people can participate in discussions about non-factoid topics by marking their stances, providing premises, or arguing for the opinions they support, which forms ""collective arguments"". The sustainable development of collective arguments relies on a big contributor base, yet most of the frequent CQA users are lurkers who seldom speak out. With a formative study, we identified detailed obstacles preventing lurkers from contributing to collective arguments. We consequently designed a processing pipeline for extracting and summarizing augmentative elements from question threads. Based on this we built CoArgue, a tool with navigation and chatbot features to support CQA lurkers' motivation and ability in making contributions. Through a within-subject study (N=24), we found that, compared to a Quora-like baseline, participants perceived CoArgue as significantly more useful in enhancing their motivation and ability to join collective arguments and found the experience to be more engaging and productive.",2023,,"PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023",,,,WOS:001037809504030,10.1145/3544548.3580932,,#7374,ACM 2023,,,1
Context-Aware Chatbot Based on Cyber-Physical Sensing for Promoting Serendipitous Face-to-Face Communication,"Tanaka, H; Motomatsu, H; Nakamura, Y; Arakawa, Y","To promote the face-to-face communication reduced by COVID-19, we proposed and implemented a context-aware Slack chat-bot based on Cyber-Physical sensing that helps colleagues meet more often in the same place. Our system periodically collects the user's internal context through Slack (cyber sensing) and uses small BLE beacons distributed to colleagues and beacon scanners installed in a laboratory to sense physical attendance (physical sensing). In addition, the system notifies the user of recommended actions, such as lunch or coffee break, depending on the context determined by the Cyber-Physical sensors. We deployed the proposed system in a laboratory environment and conducted an initial experiment for six weeks. Experimental results confirmed that our system can encourage serendipitous face-to-face communication during periods when the frequency of attending school and going to work dropped due to COVID-19. It was also found that in an environment such as a laboratory, where a certain level of trust has already been established, the openness of the collected information can further motivate users to participate in the system.",2022,,PERSUASIVE TECHNOLOGY (PERSUASIVE 2022),13213,,230-239,WOS:000784740900018,10.1007/978-3-030-98438-0_18,,#7386,Tanaka 2022,,,1
The efficacy of online physical activity interventions with added mobile elements within adults aged 50 years and over: Randomized controlled trial,"Collombon, EHGM; Bolman, CAW; de Bruijn, GJ; Peels, DA; Verboon, P; Lechner, L","Electronic health (eHealth) and mobile health (mHealth) could stimulate physical activity (PA) in a time-efficient and cost-effective way. This randomized controlled trial aims to investigate effects on moderate-to-vigorous PA (MVPA) of different combined computer- and mobile-based PA interventions targeted at adults aged 50 years and over. Participants (N = 954) were randomly allocated to a basic existing computer-based intervention (Active Plus [AP] or I Move [IM]) supplemented with one of three mobile elements being (1) activity tracker (AT), (2) ecological momentary intervention (EMI), or (3) chatbot (CB) or a control group (CG). MVPA was assessed via the SQUASH at baseline (T0), 3 months (T1), and 6 months (T2) and via accelerometers at T0 and T2. No intervention effects were found on objective (p = .502) and subjective (p = .368) MVPA for main research groups (AP/IM + AT, AP/IM + EMI, AP/IM + CB). Preliminary MVPA findings for subgroups (AP + AT, AP + EMI, AP + CB, IM + AT, IM + EMI, IM + CB) combined with drop-out data showed potential for the computer-based intervention AP with an integrated AT. Based on these preliminary findings, eHealth developers can be recommended to integrate ATs with existing computer-based PA interventions. However, further research is recommended to confirm the findings as a result of the exploratory nature of the subgroup analyses.",2024,,APPLIED PSYCHOLOGY-HEALTH AND WELL BEING,16,4,1921-1943,WOS:001254743800001,10.1111/aphw.12568,,#7395,Collombon 2024,,,1
Examining the Effects of Conversational Chatbots on Changing Conspiracy Beliefs about Science: The Paradox of Interactivity,"Wang, JP; Tanes-Ehle, Z","Conspiracy beliefs are commonly seen during times of uncertainty. This study examined whether a chatbot offering counter-conspiracy information can mitigate conspiracy beliefs and the role of chatbot empathy on its effectiveness. We conducted an online experiment in two different contexts (climate change vs. Covid-19) (N = 189). The results showed that as for Covid-19, participants who interacted with the chatbot with less empathetic expressions showed fewer changes in conspiracy beliefs than those who read the scientific news article. Regarding climate change, a chatbot with more empathetic expressions was more effective in changing conspiracy beliefs than an article, but only for people who can tolerate ambiguity.",2023,,JOURNAL OF BROADCASTING & ELECTRONIC MEDIA,67,1,68-89,WOS:000894643800001,10.1080/08838151.2022.2153842,,#7419,Wang 2023,,,1
Addressing Popular Concerns Regarding COVID-19 Vaccination with Natural Language Argumentation Dialogues,"Chalaguine, L; Hunter, A","Chatbots have the potential of being used as dialogical argumentation systems for behaviour change applications. They thereby offer a cost-effective and scalable alternative to in-person consultations with health professionals that users could engage in from the comfort of their own home. During events like the global COVID-19 pandemic, it is even more important than usual that people are well informed and make conscious decisions that benefit themselves. Getting a COVID-19 vaccine is a prime example of a behaviour that benefits the individual, as well as society as a whole. In this paper, we present a chatbot that engages in dialogues with users who do not want to get vaccinated, with the goal to persuade them to change their stance and get a vaccine. The chatbot is equipped with a small repository of arguments that it uses to counter user arguments on why the user is reluctant to get a vaccine. We evaluate our chatbot in a study with participants.",2021,,"SYMBOLIC AND QUANTITATIVE APPROACHES TO REASONING WITH UNCERTAINTY, ECSQARU 2021",12897,,59-73,WOS:000711926000005,10.1007/978-3-030-86772-0_5,,#7425,Chalaguine 2021,,,1
Conversational Support for Education,"Akcora, DE; Belli, A; Berardi, M; Casola, S; Di Blas, N; Falletta, S; Faraotti, A; Lodi, L; Diaz, DN; Paolini, P; Renzi, F; Vannella, F","This paper describes the development of chatbots that can help learners make the most appropriate use of a large body of content. The purpose of a COntent-based Learning Assistant, COLA, is to suggest optimal educational paths, along with a persuasive and empathic coaching. COLAs are supported by a novel technology, iCHAT, based on the cognitive engine Watson (by IBM).",2018,,"ARTIFICIAL INTELLIGENCE IN EDUCATION, PT II",10948,,14-19,WOS:000877310400003,10.1007/978-3-319-93846-2_3,,#7430,Akcora 2018,,,1
"LvL UP 1.0, a holistic mHealth lifestyle coaching intervention for the prevention of non-communicable diseases and common mental disorders: a mixed methods feasibility study","Mair, J.L.; Jabir, A.I.; Salamanca-Sanabria, A.; Müller-Riemenschneider, F.; Kowatsch, T.","LvL UP is a smartphone-based holistic lifestyle coaching intervention aimed at improving health behaviours, mental well-being, and preventing noncommunicable diseases and common mental disorders. It features a ‘talk and tools’ approach, combining automated health literacy coaching via conversational agent with digital tools such as journaling, life hacks, and slow-paced breathing exercises. An ‘in-the-wild' mixed-methods study was conducted in Singapore to evaluate LvL UP’s feasibility and acceptability to inform a future definitive trial. The app was available on iOS and Android from March to August 2023 and was promoted through online and offline strategies. Data collection included in-app surveys, usage metrics, and interviews, summarised using descriptive statistics and template analysis. The app was downloaded 307 times. Data from 99 active users were analysed. Most users were female and aged 21–35 years with mild to moderate vulnerabilities in physical activity, diet, and depressive symptoms. Engagement was highest during the first eight days, with 9% remaining engaged for up to 50 days. Users rated technology acceptance highly, finding the app enjoyable, easy to use, and informative. Suggested improvements included streamlined onboarding, fixing bugs, shortening dialogues, and adding rewards. The findings support LvL UP’s feasibility and have informed enhancements for future trials.",2025,,,,,,,,3,#7437,Mair 2025,,,1
Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information,"Zhou, J.; Venkatachalam, K.; Choi, M.; Saha, K.; De Choudhury, M.","Background: With the wide adoption of large language models (LLMs) in information creation assistance, it is essential to examine their alignment with human communication styles and values. We situate this study within the context of fact-checking health information, given the critical challenge of correcting misconceptions and building trustworthiness. Recent studies have explored the potential of LLM-based fact-checking, but communication style differences between LLMs and human fact-checkers and associated reader perceptions remain under-explored. In this light, our study evaluates the communication styles of LLMs, focusing on how their explanations differ from those of humans in three core components of health communication: information linguistic features, sender persuasive strategies, and receiver value alignments. Methods: We compiled a dataset of 1498 health misinformation explanations from authoritative fact-checking organizations. Using this dataset, we employed chain-of-thought prompting with zero-shot and few-shot variations to generate LLM fact-checking responses to inaccurate health information. We drew from health communication theories and categorized communication styles along linguistic, persuasion, and value-based dimensions and measured how closely the LLM-generated responses aligned with professional explanations. Then, we examined human preferences with 99 participants who were unaware of LLM involvement and rated randomized fact-checking articles with switching orders. Results: Our findings reveal that LLM-generated articles showed significantly lower scores in persuasive strategies, certainty expressions, and alignment with social values and moral foundations. However, human evaluation demonstrated a strong preference for LLM content, with over 60% responses favoring LLM articles for clarity, completeness, and persuasiveness. Conclusion: Our results suggest that LLMs’ structured approach to presenting information may be more effective at engaging readers despite scoring lower on traditional measures of quality in fact-checking and health communication.",2025,,,,,,,,7,#7441,Zhou 2025,,,1
"Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts","Aldahoul, N.; Ibrahim, H.; Varvello, M.; Rahwan, T.; Zaki, Y.","Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs’ apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.",2025,,,,,,,,9,#7443,Aldahoul 2025,,,1
A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies,"Chen, Z.; Kalla, J.; Le, Q.; Sekhon, J.; Wang, R.","In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the “receive” and “accept” steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between $48-$74 per persuaded voter compared to $100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.",2025,,,,,,,,13,#7447,Chen 2025,,,1
Conversations with AI Chatbots Increase Short-Term Vaccine Intentions But Do Not Outperform Standard Public Health Messaging,"Sehgal, N.K.R.; Rai, S.; Tonneau, M.; Buttenheim, A.; Guntuku, S.C.","Large language model (LLM) based chatbots show promise in persuasive communication, but existing studies often rely on weak controls or focus on belief change rather than behavioral intentions or outcomes. This pre-registered multi-country (US, Canada, UK) randomized controlled trial involving 930 vaccine-hesitant parents evaluated brief (three-minute) multi-turn conversations with LLM-based chatbots against standard public health messaging approaches for increasing human papillomavirus (HPV) vaccine intentions for their children. Participants were randomly assigned to: (1) a weak control (no message), (2) a strong control reflecting the standard of care (reading official public health materials), or (3 and 4) one of two chatbot conditions. One chatbot was prompted to deliver short, conversational responses, while the other used the model’s default output style (longer with bullet points). While chatbot interactions significantly increased self-reported vaccination intent (by 7.1-10.3 points on a 100-point scale) compared to no message, they did not outperform standard public health materials, with the conversational chatbot performing significantly worse. Additionally, while the short-term effects of chatbot interactions faded during a 15-day follow-up, the effects of public health material persisted relative to no message. These findings suggest that while LLMs can effectively shift vaccination intentions in the short-term, their incremental value over existing public health communications is questionable—offering a more tempered view of their persuasive capabilities and highlighting the importance of integrating AI-driven tools alongside, rather than replacing, current public health strategies.",2025,,,,,,,,14,#7448,Sehgal 2025,,,1
Working with Large Language Models to Enhance Messaging Effectiveness for Vaccine Confidence,"Gullison, L.; Fu, F.","Vaccine hesitancy and misinformation are significant barriers to achieving widespread vaccination coverage. Smaller public health departments may not have expertise or resources for effective vaccine messaging. This paper explores the potential of ChatGPT-augmented messaging to promote confidence in vaccination uptake. We conducted a survey where participants chose between a pair of vaccination messages and assessed which was more persuasive and to what extent. In each pair of messages, there was was one original message and one ChatGPT-augmented message. At the end of the survey, participants were told that half of the messages were generated by ChatGPT. Participants were asked for a quantitative and qualitative response about how knowledge of a message’s ChatGPT status changed their impression of the messages. Overall, ChatGPT messages were rated slightly higher than the original messages. ChatGPT messages generally scored higher when they were longer. Respondents did not express major concerns about ChatGPT messaging, nor was there a significant relationship between views on ChatGPT and message ratings. Finally, there was a correlation between whether the message was positioned first or second in the question and the message’s score. Overall, our results point to the potential of ChatGPT to improve vaccination messaging with potential for future research to clarify this human-AI collaborative relationship.",2025,,,,,,,,22,#7456,Gullison 2025,,,1
Labeling Messages as AI-Generated Does Not Reduce Their Persuasive Effects,"Gallegos, I.O.; Shani, C.; Shi, W.; Jurafsky, D.; Willer, R.","As generative artificial intelligence (AI) enables the creation and dissemination of information at massive scale and speed, it is increasingly important to understand how people perceive AI-generated content. One prominent policy proposal requires explicitly labeling AI-generated content to increase transparency and encourage critical thinking about the information, but prior research has not yet tested the effects of such labels. To address this gap, we conducted a survey experiment (N=1601) on a diverse sample of Americans, presenting participants with an AI-generated message about several public policies (e.g., allowing colleges to pay student-athletes), randomly assigning whether participants were told the message was generated by (a) an expert AI model, (b) a human policy expert, or (c) no label. We found that messages were generally persuasive, influencing participants’ views of the policies by 9.74 percentage points on average. However, while 94.6% of participants assigned to the AI and human label conditions believed the authorship labels, labels had no significant effects on participants’ attitude change toward the policies, judgments of message accuracy, nor intentions to share the message with others. These patterns were robust across a variety of participant characteristics, including prior knowledge of the policy, prior experience with AI, political party, education level, or age. Taken together, these results imply that, while authorship labels would likely enhance transparency, they are unlikely to substantially affect the persuasiveness of the labeled content, highlighting the need for alternative strategies to address challenges posed by AI-generated information.",2025,,,,,,,,23,#7457,Gallegos 2025,,,1
Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models,"Zhao, X.; Deng, Y.; Wang, W.; Ng, S.-K.; Chua, T.-S.","Conversational Recommender Systems (CRSs) engage users in multi-turn interactions to deliver personalized recommendations. The emergence of large language models (LLMs) further enhances these systems by enabling more natural and dynamic user interactions. However, a key challenge remains in understanding how personality traits shape conversational recommendation outcomes. Psychological evidence highlights the influence of personality traits on user interaction behaviors. To address this, we introduce an LLM-based personality-aware user simulation for CRSs (PerCRS). The user agent induces customizable personality traits and preferences, while the system agent possesses the persuasion capability to simulate realistic interaction in CRSs. We incorporate multi-aspect evaluation to ensure robustness and conduct extensive analysis from both user and system perspectives. Experimental results demonstrate that state-of-the-art LLMs can effectively generate diverse user responses aligned with specified personality traits, thereby prompting CRSs to dynamically adjust their recommendation strategies. Our experimental analysis offers empirical insights into the impact of personality traits on the outcomes of conversational recommender systems.",2025,,,,,,,,25,#7459,Zhao 2025,,,1
"Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge","Zeng, D.; Legaspi, R.; Sun, Y.; Spirtes, P.; Zhang, K.","We hypothesize that optimal system responses emerge from adaptive strategies grounded in causal and counterfactual knowledge. Counterfactual inference allows us to create hypothetical scenarios to examine the effects of alternative system responses. We enhance this process through causal discovery, which identifies the strategies informed by the underlying causal structure that govern system behaviors. Moreover, we consider the psychological constructs and unobservable noises that might be influencing user-system interactions as latent factors. We show that these factors can be effectively estimated. We employ causal discovery to identify strategy-level causal relationships among user and system utterances, guiding the generation of personalized counterfactual dialogues. We model the user utterance strategies as causal factors, enabling system strategies to be treated as counterfactual actions. Furthermore, we optimize policies for selecting system responses based on counterfactual data. Our results using a real-world dataset on social good demonstrate significant improvements in persuasive system outcomes, with increased cumulative rewards validating the efficacy of causal discovery in guiding personalized counterfactual inference and optimizing dialogue policies for a persuasive dialogue system.",2025,,,,,,,,27,#7461,Zeng 2025,,,1
MemInsight: Autonomous Memory Augmentation for LLM Agents,"Salama, R.; Cai, J.; Yuan, M.; Zhang, Y.; Benajiba, Y.","Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.",2025,,,,,,,,34,#7468,Salama 2025,,,1
Optimizing Influence Campaigns: Nudging Under Bounded Confidence,"Chen, Y.-S.; Zaman, T.","Influence campaigns in online social networks are often run by organizations, political parties, and nation states to influence large audiences. These campaigns are employed through the use of agents in the network that share persuasive content. Yet, their impact might be minimal if the audiences remain unswayed, often due to the bounded confidence phenomenon, where only a narrow spectrum of viewpoints can influence them. Here we show that to persuade under bounded confidence, an agent must nudge its targets to gradually shift their opinions. Using a control theory approach, we show how to construct an agent’s nudging policy under the bounded confidence opinion dynamics model and also how to select targets for multiple agents in an influence campaign on a social network. Simulations on real Twitter networks show that a multi-agent nudging policy can shift the mean opinion, decrease opinion polarization, or even increase it. We find that our nudging based policies outperform other common techniques that do not consider the bounded confidence effect. Finally, we show how to craft prompts for large language models, such as ChatGPT, to generate text-based content for real nudging policies. This illustrates the practical feasibility of our approach, allowing one to go from mathematical nudging policies to real social media content.",2025,,,,,,,,37,#7471,Chen 2025,,,1
Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models,"Yeginbergen, A.; Oronoz, M.; Agerri, R.","This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems. Data and code publicly available.1",2025,,,,,,,,41,#7475,Yeginbergen 2025,,,1
AI-POWERED EPISODIC FUTURE THINKING,"Ahmadi, S.; Rockwell, M.; Stuart, M.; Stein, J.; Fox, E.A.","Episodic Future Thinking (EFT) is an intervention that involves vividly imagining personal future events and experiences in detail. It has shown promise as an intervention to reduce delay discounting—the tendency to devalue delayed rewards in favor of immediate gratification—and to promote behavior change in a range of maladaptive health behaviors. We present EFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model, designed to generate EFT cues for users with lifestyle-related conditions. To evaluate the chatbot, we conducted a user study that included usability assessments and user evaluations based on content characteristics questionnaires, followed by semi-structured interviews. The study provides qualitative insights into participants’ experiences and interactions with the chatbot and its usability. Our findings highlight the potential application of AI chatbots based on Large Language Models (LLMs) in EFT interventions, and offer design guidelines for future behavior-oriented applications.",2025,,,,,,,,42,#7476,Ahmadi 2025,,,1
The Influence of Prior Discourse on Conversational Agent-Driven Decision-Making,"Pilli, S.; Nallur, V.","Persuasion through conversation has been the focus of much research. Nudging is a popular strategy to influence decision-making in physical and digital settings. However, conversational agents employing “nudging” have not received significant attention. We explore the manifestation of cognitive biases—the underlying psychological mechanisms of nudging—and investigate how the complexity of prior dialogue tasks impacts decision-making facilitated by conversational agents. Our research used a between-group experimental design, involving 756 participants randomly assigned to either a simple or complex task before encountering a decision-making scenario. Three scenarios were adapted from Samuelsonś classic experiments on status-quo bias, the underlying mechanism of default nudges. Our results aligned with previous studies in two out of three simple-task scenarios. Increasing task complexity consistently shifted effect-sizes toward our hypothesis, though bias was significant in only one case. These findings inform conversational nudging strategies and highlight inherent biases relevant to behavioural economics.",2025,,,,,,,,44,#7478,Pilli 2025,,,1
Can (A)I Change Your Mind?,"Havin, M.; Kleinman, T.W.; Koren, M.; Dover, Y.; Goldstein, A.","The increasing integration of large language models (LLMs) based conversational agents into everyday life raises critical cognitive and social questions about their potential to influence human opinions. Although previous studies have shown that LLM-based agents can generate persuasive content, these typically involve controlled English-language settings. Addressing this, our preregistered study explored LLMs’ persuasive capabilities in more ecological, unconstrained scenarios, examining both static (written paragraphs) and dynamic (conversations via Telegram) interaction types. Conducted entirely in Hebrew with 200 participants, the study assessed the persuasive effects of both LLM and human interlocutors on controversial civil policy topics. Results indicated that participants adopted LLM and human perspectives similarly, with significant opinion changes evident across all conditions, regardless of interlocutor type or interaction mode. Confidence levels increased significantly in most scenarios. These findings demonstrate LLM-based agents’ robust persuasive capabilities across diverse sources and settings, highlighting their potential impact on shaping public opinions.",2025,,,,,,,,48,#7482,Havin 2025,,,1
AI persuading AI vs AI persuading Humans: LLMs’ Differential Effectiveness in Promoting Pro-Environmental Behavior,"Doudkin, A.; Pataranutaporn, P.; Maes, P.","Pro-environmental behavior (PEB) is vital to combat climate change, yet turning awareness into intention and action remains elusive. We explore large language models (LLMs) as tools to promote PEB, comparing their impact across 3,200 participants: real humans (n=1,200), simulated humans based on actual participant data (n=1,200), and fully synthetic personas (n=1,200). All three participant groups faced personalized or standard chatbots, or static statements, employing four persuasion strategies (moral foundations, future self-continuity, action orientation, or”freestyle” chosen by the LLM). Results reveal a”synthetic persuasion paradox”: synthetic and simulated agents significantly affect their post-intervention PEB stance, while human responses barely shift. Simulated participants better approximate human trends but still overestimate effects. This disconnect underscores LLM’s potential for pre-evaluating PEB interventions but warns of its limits in predicting real-world behavior. We call for refined synthetic modeling and sustained and extended human trials to align conversational AI’s promise with tangible sustainability outcomes.",2025,,,,,,,,51,#7485,Doudkin 2025,,,1
Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions,"Borah, A.; Mihalcea, R.; Pérez-Rosas, V.","Existing challenges in misinformation exposure and susceptibility vary across demographic groups, as some populations are more vulnerable to misinformation than others. Large language models (LLMs) introduce new dimensions to these challenges through their ability to generate persuasive content at scale and reinforcing existing biases. This study investigates the bidirectional persuasion dynamics between LLMs and humans when exposed to misinformative content. We analyze human-to-LLM influence using human-stance datasets and assess LLM-to-human influence by generating LLM-based persuasive arguments. Additionally, we use a multi-agent LLM framework to analyze the spread of misinformation under persuasion among demographic-oriented LLM agents. Our findings show that demographic factors influence susceptibility to misinformation in LLMs, closely reflecting the demographic-based patterns seen in human susceptibility. We also find that, similar to human demographic groups, multi-agent LLMs exhibit echo chamber behavior. This research explores the interplay between humans and LLMs, highlighting demographic differences in the context of misinformation and offering insights for future interventions.",2025,,,,,,,,52,#7486,Borah 2025,,,1
Among Them: A game-based framework for assessing persuasion capabilities of LLMs,"Idziejczak, M.; Korzavatykh, V.; Stawicki, M.; Błądek, I.; Brzezinski, D.","The proliferation of large language models (LLMs) and autonomous AI agents has raised concerns about their potential for automated persuasion and social influence. While existing research has explored isolated instances of LLM-based manipulation, systematic evaluations of persuasion capabilities across different models remain limited. In this paper, we present an Among Us-inspired game framework for assessing LLM deception skills in a controlled environment. The proposed framework makes it possible to compare LLM models by game statistics, as well as quantify in-game manipulation according to 25 persuasion strategies from social psychology and rhetoric. Experiments between 8 popular language models of different types and sizes demonstrate that all tested models exhibit persuasive capabilities, successfully employing 22 of the 25 anticipated techniques. We also find that larger models do not provide any persuasion advantage over smaller models and that longer model outputs are negatively correlated with the number of games won. Our study provides insights into the deception capabilities of LLMs, as well as tools and data for fostering future research on the topic.",2025,,,,,,,,56,#7490,Idziejczak 2025,,,1
Fine-tuning Large Language Models in Behavioral Psychology for Scalable Physical Activity Coaching,"Mantena, S.; Johnson, A.; Oppezzo, M.; Kim, D.S.; Ashley, E.A.","Personalized, smartphone-based coaching improves physical activity but relies on static, human-crafted messages. We introduce My Heart Counts (MHC)-Coach, a large language model fine-tuned on the Transtheoretical Model of Change. MHC-Coach generates messages tailored to an individual’s psychology (their “stage of change”), providing personalized support to foster long-term physical activity behavior change. To evaluate MHC-Coach’s efficacy, 632 participants compared human-expert and MHC-Coach text-based interventions encouraging physical activity. Among messages matched to an individual’s stage of change, 68.0% (N=430) preferred MHC-Coach-generated messages (P < 0.001). Blinded behavioral science experts (N=2) rated MHC-Coach messages higher than human-expert messages for perceived effectiveness (4.4 vs. 2.8) and Transtheoretical Model alignment (4.1 vs. 3.5) on a 5-point Likert scale. This work demonstrates how language models can operationalize behavioral science frameworks for personalized health coaching, promoting long-term physical activity and potentially reducing cardiovascular disease risk at scale.",2025,,,,,,,,63,#7497,Mantena 2025,,,1
"Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust","Sun, Y.; Wang, T.","Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit ‘sycophancy’, a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. However, users’ perceptions of LLM sycophancy and its interplay with other anthropomorphic features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) × 2 (Friendliness: high vs. low) between-subjects experiment (N = 224). Our study uncovered, for the first time, the intricate dynamics between LLM sycophancy and friendliness: When an LLM agent already exhibits a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight the imperative for responsible designs in user-LLM agent interactions.",2025,,,,,,,,75,#7509,Sun 2025,,,1
CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration,"Yang, Y.; Achananuparp, P.; Huang, H.; Ern, C.T.S.; Lim, E.-P.","Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) – a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client’s state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI’s performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client’s state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.",2025,,,,,,,,83,#7517,Yang 2025,,,1
OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable Attitude and Behavior Change,"Pataranutaporn, P.; Doudkin, A.; Maes, P.","Marine ecosystems face unprecedented threats from climate change and plastic pollution, yet traditional environmental education often struggles to translate awareness into sustained behavioral change. This paper presents OceanChat, an interactive system leveraging large language models to create conversational AI agents represented as animated marine creatures—specifically a beluga whale, a jellyfish, and a seahorse—designed to promote environmental behavior (PEB) and foster awareness through personalized dialogue. Through a between-subjects experiment (N=900), we compared three conditions: (1) Static Scientific Information, providing conventional environmental education through text and images; (2) Static Character Narrative, featuring first-person storytelling from 3D-rendered marine creatures; and (3) Conversational Character Narrative, enabling real-time dialogue with AI-powered marine characters. Our analysis revealed that the Conversational Character Narrative condition significantly increased behavioral intentions and sustainable choice preferences compared to static approaches. The beluga whale character demonstrated consistently stronger emotional engagement across multiple measures, including perceived anthropomorphism and empathy. However, impacts on deeper measures like climate policy support and psychological distance were limited, highlighting the complexity of shifting entrenched beliefs. Our work extends research on sustainability interfaces facilitating PEB and offers design principles for creating emotionally resonant, context-aware AI characters. By balancing anthropomorphism with species authenticity, OceanChat demonstrates how interactive narratives can bridge the gap between environmental knowledge and real-world behavior change.",2025,,,,,,,,84,#7518,Pataranutaporn 2025,,,1
Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated Statistics,"Timm, J.; Talele, C.; Haimes, J.","Large Language Models (LLMs) are becoming increasingly persuasive, demonstrating the ability to personalize arguments in conversation with humans by leveraging their personal data. This may have serious impacts on the scale and effectiveness of disinformation campaigns. We studied the persuasiveness of LLMs in a debate setting by having humans (n = 33) engage with LLM-generated arguments intended to change the human’s opinion. We quantified the LLM’s effect by measuring human agreement with the debate’s hypothesis pre- and post-debate and analyzing both the magnitude of opinion change, as well as the likelihood of an update in the LLM’s direction. We compare persuasiveness across established persuasion strategies, including personalized arguments informed by user demographics and personality, appeal to fabricated statistics, and a mixed strategy utilizing both personalized arguments and fabricated statistics. We found that static arguments generated by humans and GPT-4o-mini have comparable persuasive power. However, the LLM outperformed static human-written arguments when leveraging the mixed strategy in an interactive debate setting. This approach had a 51% chance of persuading participants to modify their initial position, compared to 32% for the static human-written arguments. Our results highlight the concerning potential for LLMs to enable inexpensive and persuasive large-scale disinformation campaigns.",2025,,,,,,,,91,#7525,Timm 2025,,,1
Visible Sources and Invisible Risks: Exploring the Impact of Ai Disclosure on Perceived Credibility of Ai-Generated Content,"Lin, T.; Zhang, Y.","With the development of Large Language Models, the capability of AI in generating information has significantly improved, leading to its widespread application in content production. However, the increasing persuasiveness of AI-generated content (AIGC) is also making it increasingly difficult to discern AI-generated misinformation, and AI disclosures are therefore being used to help people counteract the negative effects of misinformation, as well as to increase acceptance of AIGC. In order to assess the effectiveness of AI disclosure applied to popular science articles on social media, we conducted a within-subject experiment (N=419) in the context of the Chinese internet environment. The results indicated that AI disclosure not only made people more likely to believe AI-generated misinformation but also reduced their perceived trust in AI-generated accurate information. This effect diminished as the audience's attitudes toward AI became more negative. For AI-generated misinformation, the moderating effect of negative attitudes towards AI is related to the subject matter of the message. For AI-generated correct information, the negative impact caused by AI disclosure may not exist when the audience's negative attitude towards AI is weak. Furthermore, the level of audience involvement with the information was irrelevant. The study provides new empirical evidence for the debate on the effectiveness of AI disclosure, while also highlighting potential issues in its practical application and discussing the prospects of AI disclosure.",2025,,,,,,,,99,#7533,Lin 2025,,,1
Will you donate money to a chatbot? The effect of chatbot anthropomorphic features and persuasion strategies on willingness to donate,"Novozhilova, E.; Huang, J.; He, L.; Li, Z.; Cummings, J.","This work investigates the causal mechanism behind the effect of chatbot personification and persuasion strategies on users’ perceptions and donation likelihood. In a 2 (personified vs. non-personified chatbot) x 2 (emotional vs. logical persuasion strategy) between-subjects experiment (N = 76), participants engaged with a chatbot that represented a non-profit charitable organization. The results suggest that interaction with a personified chatbot evokes perceived anthropomorphism, however, does not elicit greater willingness to donate. In fact, we found that commonly used anthropomorphic features, like name and narrative, led to negative attitudes toward an AI agent in the donation context. Our results showcase a preference for non-personified chatbots paired with logical persuasion appeal, emphasizing the significance of consistency in chatbot interaction, mirroring human-human engagement. We discuss the importance of moving from exploring the common scenario of chatbot with machine identity vs. chatbot with human identity in light of the recent regulations of AI systems.",2024,,,,,,,,101,#7535,Novozhilova 2024,,,1
Virtual Agent-Based Communication Skills Training to Facilitate Health Persuasion Among Peers,"Nouraei, F.; Rebello, K.; Fallah, M.; Paasche-Orlow, M.; Bickmore, T.","Many laypeople are motivated to improve the health behavior of their family or friends but do not know where to start, especially if the health behavior is potentially stigmatizing or controversial. We present an approach that uses virtual agents to coach community-based volunteers in health counseling techniques, such as motivational interviewing, and allows them to practice these skills in role-playing scenarios. We use this approach in a virtual agent-based system to increase COVID-19 vaccination by empowering users to influence their social network. In a between-subjects comparative design study, we test the effects of agent system interactivity and role-playing functionality on counseling outcomes, with participants evaluated by standardized patients and objective judges. We find that all versions are effective at producing peer counselors who score adequately on a standardized measure of counseling competence, and that participants were significantly more satisfied with interactive virtual agents compared to passive viewing of the training material. We discuss design implications for interpersonal skills training systems based on our findings.",2024,,,,,,,,107,#7541,Nouraei 2024,,,1
"Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation","Cima, L.; Miaschi, A.; Trujillo, A.; Dell’Orletta, F.; Cresci, S.","AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct a LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.",2024,,,,,,,,111,#7545,Cima 2024,,,1
Can Generative Artificial Intelligence Drive Sustainable Behavior? A Consumer Adoption Model for Ai-Driven Recommendations,"Silalahi, A.D.K.","Generative AI (GAI) holds potential to promote sustainable behaviors by personalized recommendations, yet its effectiveness hinges on user trust—a critical area underexplored in current literature. Existing studies often focus narrowly on domain-specific applications without addressing broader trust-building mechanisms or the cognitive and motivational factors essential for sustained engagement with GAI in sustainability contexts. This study aims to bridge these gaps by examining how trust influences the adoption of GAI-driven sustainability recommendations, drawing on the Elaboration Likelihood Model (ELM) and Expectancy-Value Theory (EVT) to create a robust and holistic framework. Drawing on data from sustainability-focused users, we examine how central route constructs (perceived information quality and utility) and peripheral route constructs (anthropomorphism and interaction quality) foster trust, with perceived information complexity and risk as moderating factors. Findings indicate that cognitive engagement through high information quality and utility strengthens trust, while anthropomorphic design elements and interaction quality enhance trust from heuristic path. However, excessive information complexity and privacy concerns weaken trust, emphasizing the need for clear communication and data transparency. This study advances theoretical understanding by extending ELM and EVT to GAI’s role in sustainability, providing a holistic framework that integrates cognitive and motivational trust drivers. These insights not only fill significant gaps in technology adoption research but also offer practical guidance for designing GAI platforms that genuinely support pro-environmental behavior change.",2024,,,,,,,,113,#7547,Silalahi 2024,,,1
PERSUASION WITH LARGE LANGUAGE MODELS: A SURVEY,"Rogiers, A.; Noels, S.; Buyl, M.; De Bie, T.","The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.",2024,,,,,,,,123,#7557,Rogiers 2024,,,1
INTERACTIVE DIALOGUE AGENTS VIA REINFORCEMENT LEARNING ON HINDSIGHT REGENERATIONS,"Hong, J.; Lin, J.; Dragan, A.; Levine, S.","Recent progress on large language models (LLMs) has enabled dialogue agents to generate highly naturalistic and plausible text. However, current LLM language generation focuses on responding accurately to questions and requests with a single effective response. In reality, many real dialogues are interactive, meaning an agent's utterances will influence their conversational partner, elicit information, or change their opinion. Accounting for how an agent can effectively steer a conversation is a crucial ability in many dialogue tasks, from healthcare to preference elicitation. Existing methods for fine-tuning dialogue agents to accomplish such tasks would rely on curating some amount of expert data. However, doing so often requires understanding the underlying cognitive processes of the conversational partner, which is a skill neither humans nor LLMs trained on human data can reliably do. Our key insight is that while LLMs may not be adept at identifying effective strategies for steering conversations a priori, or in the middle of an ongoing conversation, they can do so post-hoc, or in hindsight, after seeing how their conversational partner responds. We use this fact to rewrite and augment existing suboptimal data, and train via offline reinforcement learning (RL) an agent that outperforms both prompting and learning from unaltered human demonstrations. We apply our approach to two domains that require understanding human mental state, intelligent interaction, and persuasion: mental health support, and soliciting charitable donations. Our results in a user study with real humans show that our approach greatly outperforms existing state-of-the-art dialogue agents.",2024,,,,,,,,126,#7560,Hong 2024,,,1
Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions,"Song, T.; Tan, Y.; Zhu, Z.; Feng, Y.; Lee, Y.-C.","Multi-agent systems - systems with multiple independent AI agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of AI agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user's stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents' stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.",2024,,,,,,,,127,#7561,Song 2024,,,1
Coherence-Driven Multimodal Safety Dialogue with Active Learning for Embodied Agents,"Hassan, S.; Chung, H.-Y.; Tan, X.Z.; Alikhani, M.","When assisting people in daily tasks, robots need to accurately interpret visual cues and respond effectively in diverse safety-critical situations, such as sharp objects on the floor. In this context, we present M-CoDAL, a multimodal-dialogue system specifically designed for embodied agents to better understand and communicate in safety-critical situations. The system leverages discourse coherence relations to enhance its contextual understanding and communication abilities. To train this system, we introduce a novel clustering-based active learning mechanism that utilizes an external Large Language Model (LLM) to identify informative instances. Our approach is evaluated using a newly created multimodal dataset comprising 1K safety violations extracted from 2K Reddit images. These violations are annotated using a Large Multimodal Model (LMM) and verified by human annotators. Results with this dataset demonstrate that our approach improves resolution of safety situations, user sentiment, as well as safety of the conversation. Next, we deploy our dialogue system on a Hello Robot Stretch robot and conduct a within-subject user study with real-world participants. In the study, participants role-play two safety scenarios with different levels of severity with the robot and receive interventions from our model and a baseline system powered by OpenAI’s ChatGPT. The study results corroborate and extend the findings from the automated evaluation, showing that our proposed system is more persuasive in a real-world embodied agent setting.",2024,,,,,,,,138,#7573,Hassan 2024,,,1
Human Bias in the Face of AI: The Role of Human Judgement in AI Generated Text Evaluation,"Zhu, T.; Weissburg, I.; Zhang, K.; Wang, W.Y.","As AI advances in text generation, human trust in AI generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as ""Human Generated,"" over those labeled ""AI Generated,"" by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.",2024,,,,,,,,144,#7579,Zhu 2024,,,1
Beyond Turing Test: Can GPT-4 Sway Experts’ Decisions?,"Takayanagi, T.; Takamura, H.; Izumi, K.; Chen, C.-C.","In the post-Turing era, evaluating large language models (LLMs) involves assessing generated text based on readers’ reactions rather than merely its indistinguishability from human-produced content. This paper explores how LLM-generated text impacts readers’ decisions, focusing on both amateur and expert audiences. Our findings indicate that GPT-4 can generate persuasive analyses affecting the decisions of both amateurs and professionals. Furthermore, we evaluate the generated text from the aspects of grammar, convincingness, logical coherence, and usefulness. The results highlight a high correlation between real-world evaluation through audience reactions and the current multi-dimensional evaluators commonly used for generative models. Overall, this paper shows the potential and risk of using generated text to sway human decisions and also points out a new direction for evaluating generated text, i.e., leveraging the reactions and decisions of readers. We release our dataset to assist future research.",2024,,,,,,,,147,#7582,Takayanagi 2024,,,1
Persuasion Games using Large Language Models,"Ramani, G.P.; Karande, S.; Santhosh, V.; Bhatia, Y.","Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape user perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS). We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with user agents through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We continuously analyze the resistance of the user agent to persuasive efforts and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques. We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).",2024,,,,,,,,160,#7595,Ramani 2024,,,1
Sleeper Social Bots: A New Generation of AI Disinformation Bots are Already a Political Threat,"Doshi, J.; Novacic, I.; Fletcher, C.; Sprague, D.; Xia, M.","This paper presents a study on the growing threat of “sleeper social bots,” AI-driven social bots in the political landscape, created to spread disinformation and manipulate public opinion. We based the name sleeper social bots on their ability to pass as humans on social platforms, where they’re embedded like political “sleeper” agents, making them harder to detect and more disruptive. To illustrate the threat these bots pose, our research team at the University of Southern California constructed a demonstration using a private Mastodon server, where ChatGPT-driven bots, programmed with distinct personalities and political viewpoints, engaged in discussions with human participants about a fictional electoral proposition. Our preliminary findings suggest these bots can convincingly pass as human users, actively participate in conversations, and effectively disseminate disinformation. Moreover, they can adapt their arguments based on the responses of human interlocutors, showcasing their dynamic and persuasive capabilities. College students participating in initial experiments failed to identify our bots, underscoring the urgent need for increased awareness and education about the dangers of AI-driven disinformation, and in particular, disinformation spread by bots. The implications of our research point to the significant challenges posed by social bots in the upcoming 2024 U.S. presidential election and beyond.",2024,,,,,,,,165,#7600,Doshi 2024,,,1
DECEPTIVE AI SYSTEMS THAT GIVE EXPLANATIONS ARE MORE CONVINCING THAN HONEST AI SYSTEMS AND CAN AMPLIFY BELIEF IN MISINFORMATION,"Danry, V.; Pataranutaporn, P.; Groh, M.; Epstein, Z.; Maes, P.","Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and erode trust in the truth. We examined the impact of deceptive AI generated explanations on individuals’ beliefs in a pre-registered online experiment with 23,840 observations from 1,192 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that personal factors such as cognitive reflection and trust in AI do not necessarily protect individuals from these effects caused by deceptive AI generated explanations. Instead, our results show that the logical validity of AI generated deceptive explanations, that is whether the explanation has a causal effect on the truthfulness of the AI’s classification, plays a critical role in countering their persuasiveness – with logically invalid explanations being deemed less credible. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation.",2024,,,,,,,,167,#7602,Danry 2024,,,1
NudgeRank: Digital Algorithmic Nudging for Personalized Health,"Chiam, J.; Lim, A.; Teredesai, A.","In this paper we describe NudgeRank™, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17% increase in daily steps and 7.61% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1% open rate compared to baseline systems’ 4%. Demonstrating scalability and reliability, NudgeRank™ operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems.",2024,,,,,,,,173,#7608,Chiam 2024,,,1
Large Language Models can impersonate politicians and other public figures,"Herbold, S.; Trautsch, A.; Kikteva, Z.; Hautli-Janisz, A.","Modern AI technology like Large language models (LLMs) has the potential to pollute the public information sphere with made-up content, which poses a significant threat to the cohesion of societies at large. A wide range of research has shown that LLMs are capable of generating text of impressive quality, including persuasive political speech, text with a pre-defined style, and role-specific content. But there is a crucial gap in the literature: We lack large-scale and systematic studies of how capable LLMs are in impersonating political and societal representatives and how the general public judges these impersonations in terms of authenticity, relevance and coherence. We present the results of a study based on a cross-section of British society that shows that LLMs are able to generate responses to debate questions that were part of a broadcast political debate programme in the UK. The impersonated responses are judged to be more authentic and relevant than the original responses given by people who were impersonated. This shows two things: (1) LLMs can be made to contribute meaningfully to the public political debate and (2) there is a dire need to inform the general public of the potential harm this can have on society.",2024,,,,,,,,179,#7613,Herbold 2024,,,1
Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness,"Kumar, H.; Yoo, S.; Bernuy, A.Z.; Anderson, A.; Kornfield, R.","Although engagement in self-directed wellness exercises typically declines over time, integrating social support such as coaching can sustain it. However, traditional forms of support are often inaccessible due to the high costs and complex coordination. Large Language Models (LLMs) show promise in providing human-like dialogues that could emulate social support. Yet, in-depth, in situ investigations of LLMs to support behavior change remain underexplored. We conducted two randomized experiments to assess the impact of LLM agents on user engagement with mindfulness exercises. First, a single-session study, involved 502 crowdworkers; second, a three-week study, included 54 participants. We explored two types of LLM agents: one providing information and another facilitating self-reflection. Both agents enhanced users' intentions to practice mindfulness. However, only the information-providing LLM, featuring a friendly persona, significantly improved engagement with the exercises. Our findings suggest that specific LLM agents may bridge the social support gap in digital health interventions.",2024,,,,,,,,182,#7616,Kumar 2024,,,1
Designing and Evaluating Multi-Chatbot Interface for Human-AI Communication: Preliminary Findings from a Persuasion Task,"Yoon, S.; Kim, T.E.; Oh, Y.J.","The dynamics of human-AI communication have been reshaped by language models such as ChatGPT. However, extant research has primarily focused on dyadic communication, leaving much to be explored regarding the dynamics of human-AI communication in group settings. The availability of multiple language model chatbots presents a unique opportunity for scholars to better understand the interaction between humans and multiple chatbots. This study examines the impact of multi-chatbot communication in a specific persuasion setting: promoting charitable donations. We developed an online environment that enables multi-chatbot communication and conducted a pilot experiment utilizing two GPT-based chatbots, Save the Children and UNICEF chatbots, to promote charitable donations. In this study, we present our development process of the multi-chatbot interface and present preliminary findings from a pilot experiment. Analysis of qualitative and quantitative feedback are presented, and limitations are addressed.",2024,,,,,,,,187,#7621,Yoon 2024,,,1
Evidence of a Log Scaling Law for Political Persuasion with Large Language Models,"Hackenburg, K.; Tappin, B.M.; Röttger, P.; Bright, J.; Margetts, H.","Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 U.S. political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25, 982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more. Second, mere task completion (coherence, staying on topic) appears to account for larger models' persuasive advantage. These findings suggest that further scaling model size will not much increase the persuasiveness of static LLM-generated messages.",2024,,,,,,,,190,#7624,Hackenburg 2024,,,1
Assessing AI vs Human-Authored Spear Phishing SMS Attacks: An Empirical Study,"Francia, J.; Hansen, D.; Schooley, B.; Murray, S.; Snow, G.","This paper explores the use of Large Language Models (LLMs) in spear phishing message generation and evaluates their performance compared to human-authored counterparts. Our pilot study examines the effectiveness of smishing (SMS phishing) messages created by GPT-4 and human authors, which have been personalized for willing targets. The targets assessed these messages in a modified ranked-order experiment using a novel methodology we call TRAPD (Threshold Ranking Approach for Personalized Deception). Experiments involved ranking each spear phishing message from most to least convincing, providing qualitative feedback, and guessing which messages were human- or AI-generated. Results show that LLM-generated messages are often perceived as more convincing than those authored by humans, particularly job-related messages. Targets also struggled to distinguish between human- and AI-generated messages. We analyze different criteria the targets used to assess the persuasiveness and source of messages. This study aims to highlight the urgent need for further research and improved countermeasures against personalized AI-enabled social engineering attacks.",2024,,,,,,,,193,#7627,Francia 2024,,,1
On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial,"Salvi, F.; Ribeiro, M.H.; Gallotti, R.; West, R.","Can large language models (LLMs) create tailor-made, convincing arguments to promote false or misleading narratives online? Early work has found that LLMs can generate content perceived on par with, or even more persuasive than, human-written messages. However, there is still limited evidence regarding LLMs’ persuasive capabilities in direct conversations with humans—the scenario these models are usually deployed at. In this pre-registered study, we analyze the power of AI-driven persuasion in a controlled, harmless setting. To this end, we created a web-based platform where human participants engaged in short, multi-round debates with either human or LLM opponents. Each participant was randomly assigned to one of four treatment conditions in a two-by-two factorial design: (1) the conversation partner was either another human or an LLM; (2) the conversation partner either had or did not have access to basic sociodemographic information about their opponent (and thus arguments could be personalized). We find that 64.4% of the time, personalized LLM debaters were more persuasive than humans, given that they were not equally persuasive (81.2% relative increase in the odds of higher post-debate agreement; p < 0.01; N = 900). Without personalization, GPT-4 still outperformed humans, but the effect was lower and not statistically significant (p = 0.30). Further, our analysis suggests that LLMs use different strategies from human debaters: their texts are harder to read and have more markers associated with logical and analytical reasoning. Overall, our results suggest that concerns around LLM-based persuasion are meaningful and have important implications for social media governance and the design of new online environments.",2024,,,,,,,,199,#7633,Salvi 2024,,,1
How the Anthropomorphism of an Artificial Intelligence Agent Affects the Correction of Misinformation: A Focus on Authenticity and Behavioral-Response System Level,"Oh, J.; Lim, S.","AI agents are increasingly leveraged to counter misinformation. This study probes their impact on correction, focusing on user interaction authenticity. Examining anthropomorphism and behavioral response, it seeks to discern how perceived authenticity shapes AI agents' persuasive impact in misinformation correction. In a 2x2 experiment with 320 participants correcting misinformation about the HPV vaccine, high anthropomorphism induces greater authenticity in source, message, and interaction compared to low anthropomorphism. Additionally, perceived interaction authenticity mediates the relationship between anthropomorphism and attitudinal change, moderated by the behavioral response system level. These findings provide vital evidence underlining the importance of anthropomorphism and behavioral response in fostering authentic interactions, with practical implications.",2024,,,,,,,,209,#7644,Oh 2024,,,1
Counterfactual Reasoning Using Predicted Latent Personality Dimensions for Optimizing Persuasion Outcome,"Zeng, D.; Legaspi, R.S.; Sun, Y.; Spirtes, P.; Zhang, K.","Customizing persuasive conversations related to the outcome of interest for specific users achieves better persuasion results. However, existing persuasive conversation systems rely on persuasive strategies and encounter challenges in dynamically adjusting dialogues to suit the evolving states of individual users during interactions. This limitation restricts the system’s ability to deliver flexible or dynamic conversations and achieve suboptimal persuasion outcomes. In this paper, we present a novel approach that tracks a user’s latent personality dimensions (LPDs) during ongoing persuasion conversation and generates tailored counterfactual utterances based on these LPDs to optimize the overall persuasion outcome. In particular, our proposed method leverages a Bi-directional Generative Adversarial Network (BiCoGAN) in tandem with a Dialogue-based Personality Prediction Regression (DPPR) model to generate counterfactual data D̃. This enables the system to formulate alternative persuasive utterances that are more suited to the user. Subsequently, we utilize the D3QN model to learn policies for optimized selection of system utterances on D̃. Experimental results we obtained from using the PersuasionForGood dataset demonstrate the superiority of our approach over the existing method, BiCoGAN. The cumulative rewards and Q-values produced by our method surpass ground truth benchmarks, showcasing the efficacy of employing counterfactual reasoning and LPDs to optimize reinforcement learning policy in online interactions.",2024,,,,,,,,217,#7652,Zeng 2024,,,1
"Large Language Models are as persuasive as humans, but why? About the cognitive effort and moral-emotional language of LLM arguments","Carrasco-Farré, C.","Large Language Models (LLMs) are already as persuasive as humans. However, we know very liEle about why. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuaion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.",2024,,,,,,,,218,#7653,Carrasco-Farré 2024,,,1
Feedback-Driven Insight Generation and Recommendation for Health Self-Management,"Susaiyah, A.; Härmä, A.; Reiter, E.; Balloccu, S.; Petković, M.","Purpose: This study aims to investigate the impact of personalized health insights generated from wearable device data on users’ health behaviors. The primary objective is to assess whether user feedback-driven algorithms enhance the relevance and effectiveness of health insights, ultimately influencing positive changes in users’ daily activities. Methods: A two-month field study was conducted with 25 healthy volunteers using Mi Band 6 wearable devices. Participants were divided into test and control groups, and the test group received personalized insights recommended by a neural network-based algorithm fine-tuned by user feedback. The data collected included various health parameters such as calories burned, step count, heart rate, heart minutes, active minutes, sleep duration, sleep time and sleep segments. Insights were provided through a Telegram chatbot and user feedback was collected through a rating system. Results: The study revealed that the test group, which considered user feedback for insight recommendations, showed a significant improvement in daily activity compared to the control group. The relevance of the insights over time, as evidenced by feedback regression trends, showed a notable increase in the test group. Additional analyzes explored the relationship between insight delivery timing, user feedback, and delays, providing insights into user engagement patterns. Conclusion: This research highlights the effectiveness of personalized health insights generated from wearable data in positively influencing user health behaviors. Incorporating feedback from users into recommendation algorithms greatly enhances the relevance and effectiveness of insights, encouraging behavioral improvements. The results emphasize the significance of timing when providing insights and propose potential areas for future investigation, such as utilizing Graph Neural Networks to improve recommendation systems. In general, personalized insights from wearables have the potential to empower individuals to manage their health and well-being effectively.",2024,,,,,,,,232,#7666,Susaiyah 2024,,,1
Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru,"Jelínek, M.; Nichols, E.; Gomez, R.","This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method [13] for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru’s emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru’s liveliness, interactivity, and neutrality. Additionally, our study contributes to the broader understanding of dialogue design in social robotics, offering practical insights for future developments in the field.",2024,,,,,,,,239,#7673,Jelínek 2024,,,1
Consolidating Strategies for Countering Hate Speech Using Persuasive Dialogues,"Saha, S.; Srihari, R.","Hateful comments are prevalent on social media platforms. Although tools for automatically detecting, flagging, and blocking such false, offensive, and harmful content online have lately matured, such reactive and brute force methods alone provide short-term and superficial remedies while the perpetrators persist. With the public availability of large language models which can generate articulate synthetic and engaging content at scale, there are concerns about the rapid growth of dissemination of such malicious content on the web. There is now a need to focus on deeper, long-term solutions that involve engaging with the human perpetrator behind the source of the content to change their viewpoint or at least bring down the rhetoric using persuasive means. To do that, we propose defining and experimenting with controllable strategies for generating counterarguments to hateful comments in online conversations. We experiment with controlling response generation using features based on (i) argument structure and reasoning-based Walton argument schemes, (ii) counter-argument speech acts, and (iii) human characteristics-based qualities such as Big-5 personality traits and human values. Using automatic and human evaluations, we determine the best combination of features that generate fluent, argumentative, and logically sound arguments for countering hate. We further share the developed computational models for automatically annotating text with such features, and a silver-standard annotated version of an existing hate speech dialog corpora.",2024,,,,,,,,254,#7688,Saha 2024,,,1
The Persuasive Power of Large Language Models,"Breum, S.M.; Egdal, D.V.; Mortensen, V.G.; Møller, A.G.; Aiello, L.M.","The increasing capability of Large Language Models to act as human-like social agents raises two important questions in the area of opinion dynamics. First, whether these agents can generate effective arguments that could be injected into the online discourse to steer the public opinion. Second, whether artificial agents can interact with each other to reproduce dynamics of persuasion typical of human social systems, opening up opportunities for studying synthetic social systems as faithful proxies for opinion dynamics in human populations. To address these questions, we designed a synthetic persuasion dialogue scenario on the topic of climate change, where a ‘convincer’ agent generates a persuasive argument for a ‘skeptic’ agent, who subsequently assesses whether the argument changed its internal opinion state. Different types of arguments were generated to incorporate different linguistic dimensions underpinning psycho-linguistic theories of opinion change. We then asked human judges to evaluate the persuasiveness of machine-generated arguments. Arguments that included factual knowledge, markers of trust, expressions of support, and conveyed status were deemed most effective according to both humans and agents, with humans reporting a marked preference for knowledge-based arguments. Our experimental framework lays the groundwork for future in-silico studies of opinion dynamics, and our findings suggest that artificial agents have the potential of playing an important role in collective processes of opinion formation in online social media.",2023,,,,,,,,258,#7692,Breum 2023,,,1
Impact of a Health Action Process Approach model-based mobile health intervention on health behaviours of brucella patients: protocol for an intervention randomized controlled trial,"Wang, J.; Jiang, Y.; Yu, Z.; Chen, J.; Wu, Y.","Background Brucellosis, one of the most common zoonotic diseases globally, is a serious public health problem. Effective health management is crucial in helping brucellosis patients develop and master self-management behavioural competencies. In recent years, mHealth has emerged as a popular and highly effective intervention for improving health-related outcomes for individuals with brucellosis. This study aims to explore the use of mHealth intervention, guided by the Health Action Process Approach theoretical model, to enhance the self-management level of brucellosis patients. Methods This study will be a single-blind single-centre randomized controlled trial conducted at the Fourth Central Hospital of Baoding City, Hebei Province. The study was approved by the Ethics Committee of the Fourth Central Hospital of Baoding City. The registration number of this experiment is ChiCTR2200064320. The estimated sample size will be 58 participants. Participants will be randomly assigned to either the mobile health intervention group or the routine education group. The mobile health education intervention will be delivered through an artificial intelligence terminal WeChat platform, while the routine management group will receive routine discharge education and a brochure on ""Brucella Health Education"". The primary outcome indicators will be the rate of health behaviour formation and health literacy. Secondary outcomes will be the measurement of psychological variables and behaviour change. Questionnaires were administered at baseline and at 3, 6, 9, and 12 months of follow-up to assess changes in health behaviour formation and health literacy among the respondents. Discussion This study will help to fill the research gap of postoperative health management of brucellosis patients in China based on health theory guidance. It will be the first health intervention study in China to apply the HAPA theoretical model as a guide for a group of brucellosis patients. The purpose of this study is to evaluate the utility of the HAPA model as a theoretical guide for improving health behaviors in patients with brucellosis and to confirm the validity of the HAPA theory as an effective theory to guide the improvement of health behaviors in patients.",2023,,,,,,,,269,#7703,Wang 2023,,,1
Artificial Intelligence Can Persuade Humans on Political Issues,"Bai, H.; Voelkel, J.G.; Eichstaedt, J.C.; Willer, R.","The emergence of large language models (LLMs) that leverage deep learning and web-scale corpora has made it possible for artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets in the US and globally. Here, we investigate whether existing, openly-available LLMs are capable of influencing humans’ political attitudes, an ability recently regarded as the unique purview of other humans. Across three preregistered experiments featuring diverse samples of Americans (total N=4,836), we find consistent evidence that messages generated by LLMs (OpenAI’s GPT 3 and 3.5 models) are able to persuade humans across a number of policy issues, including highly polarized issues, such as an assault weapon ban, a carbon tax, and a paid parental-leave program. Overall, LLM-generated messages were as persuasive as messages crafted by lay humans. Our results show LLMs can persuade humans, even on highly polarized policy issues. As the capacity of LLMs is expected to improve substantially in the near future, these results suggest that LLMs may change political discourse, calling for immediate attention for the identification and regulation of potential misuse of LLMs.",2023,,,,,,,,274,#7708,Bai 2023,,,1
Fostering User Engagement in the Critical Reflection of Arguments,"Weber, K.; Aicher, A.; Minker, W.; Ultes, S.; André, E.","A natural way to resolve different points of view and form opinions is through exchanging arguments and knowledge. Facing the vast amount of available information on the internet, people tend to focus on information consistent with their beliefs. Especially when the issue is controversial, information is often selected that does not challenge one's beliefs. To support a fair and unbiased opinion-building process, we propose a chatbot system that engages in a deliberative dialogue with a human. In contrast to persuasive systems, the envisioned chatbot aims to provide a diverse and representative overview - embedded in a conversation with the user. To account for a reflective and unbiased exploration of the topic, we enable the system to intervene if the user is too focused on their pre-existing opinion. Therefore we propose a model to estimate the users' reflective engagement (RUE), defined as their critical thinking and open-mindedness. We report on a user study with 58 participants to test our model and the effect of the intervention mechanism, discuss the implications of the results, and present perspectives for future work. The results show a significant effect on both user reflection and total user focus, proving our proposed approach's validity.",2023,,,,,,,,278,#7712,Weber 2023,,,1
Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models,"O'Gara, A.","Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called Hoodwinked, inspired by Mafia and Among Us. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger persuasive skills during discussions. To evaluate the ability of AI agents to deceive humans, we make this game publicly available at https://hoodwinked.ai.",2023,,,,,,,,283,#7717,O'Gara 2023,,,1
From psychological traits to safety warnings: three studies on recommendations in a smart home environment,"Cena, F.; Gena, C.; Mattutino, C.; Mioli, M.; Vernero, F.","In this paper, we report on three experiments we have carried out in the context of the EMPATHY project, with the aim of helping users make beBer configuraDon choices in a smart home environment, and discuss our results. We found that there are psychological traits, such as Need for CogniDon, which influence the way individuals tend to use recommendaDons, that there are non obvious relaDonships between the perceived usefulness of recommendaDons in different domains and individuals' ability to exploit suggesDons on configuraDon choices, and that detailed, easy-to-understand security explanaDons are more persuasive than simple security warnings, when it comes to make decisions on the applicability of rules which might cause privacy and security risks.",2023,,,,,,,,287,#7721,Cena 2023,,,1
Artificial Intelligence can facilitate selfish decisions by altering the appearance of interaction partners,"Köbis, N.; Lorenz-Spreen, P.; Ajaj, T.; Hertwig, R.; Rahwan, I.","The ethical and psychological consequences of using Artificial Intelligence (AI) to manipulate our perception of others is an increasing phenomenon as image-altering filters proliferate on social media and video conferencing technologies. Here, we investigate the potential impact of a particular appearance-altering technology-blur filters-to investigate how individuals' behavior changes towards others. Our results consistently indicate an increase in selfish behavior at the expense of blurred individuals, suggesting blur filters can facilitate moral disengagement via depersonalization. These findings underscore the urgency for broader ethical discussions on AI technologies that alter our perception of others, encompassing transparency, consent, and the consequences of knowing that others can manipulate one's appearance. We highlight the potential role of anticipatory experiments in informing and developing responsible guidelines and policies ahead of technological reality.",2023,,,,,,,,288,#7722,Köbis 2023,,,1
Building Extractive Question Answering System to Support Human-AI Health Coaching Model for Sleep Domain,"Bojic, I.; Ong, Q.C.; Joty, S.; Car, J.","Non-communicable diseases (NCDs) are a leading cause of global deaths,necessitating a focus on primary prevention and lifestyle behavior change.Health coaching, coupled with Question Answering (QA) systems, has thepotential to transform preventive healthcare. This paper presents ahuman-Artificial Intelligence (AI) health coaching model incorporating adomain-specific extractive QA system. A sleep-focused dataset, SleepQA, wasmanually assembled and used to fine-tune domain-specific BERT models. The QAsystem was evaluated using automatic and human methods. A data-centricframework enhanced the system\'s performance by improving passage retrieval andquestion reformulation. Although the system did not outperform the baseline inautomatic evaluation, it excelled in the human evaluation of real-worldquestions. Integration into a Human-AI health coaching model was tested in apilot Randomized Controlled Trial (RCT).",2023,,,,,,,,289,#7723,Bojic 2023,,,1
Human Choice Prediction in Language-based Persuasion Games: Simulation-based Off-Policy Evaluation,"Shapira, E.; Madmon, O.; Apel, R.; Tennenholtz, M.; Reichart, R.","Recent advances in Large Language Models (LLMs) have spurred interest in designing LLM-based agents for tasks that involve interaction with human and artificial agents. This paper addresses a key aspect in the design of such agents: predicting human decisions in off-policy evaluation (OPE). We focus on language-based persuasion games, where an expert aims to influence the decision-maker through verbal messages. In our OPE framework, the prediction model is trained on human interaction data collected from encounters with one set of expert agents, and its performance is evaluated on interactions with a different set of experts. Using a dedicated application, we collected a dataset of 87K decisions from humans playing a repeated decision-making game with artificial agents. To enhance off-policy performance, we propose a simulation technique involving interactions across the entire agent space and simulated decision-makers. Our learning strategy yields significant OPE gains, e.g., improving prediction accuracy in the top 15% challenging cases by 7.1%.1",2023,,,,,,,,294,#7728,Shapira 2023,,,1
"Development, Implementation, and Usability Evaluation of a Chatbot to Effectively Provide Information on the Human Papillomavirus Vaccine","Alzughaibi, S.; Luo, J.; Nazareth, D.; Wu, M.; Fink, J.T.","The Human papillomavirus (HPV) vaccine has been shown to provide protection against HPV infections, which can cause genital warts and some forms of cancer. Lack of information about the virus and the vaccine represents one of the major challenges to reaching national HPV vaccination goals. Advancements in natural language processing have facilitated the effectiveness of chatbots in interacting with users. Chatbots have been increasingly employed in health and medical care settings, assisting physicians during consultations, helping with behavior change issues, and supporting patients and the elderly in their daily lives. A potential opportunity to improve vaccination rates is to provide health information through unconventional methods by utilizing chatbots. This work describes the design, implementation, and evaluation of a chatbot called GARDI, created to disseminate information about HPV infection and vaccine. The Chatbot Usability Questionnaire (CUQ) was employed to evaluate GARDI. The mean CUQ score was 66.6, which is marginally below the suggested benchmark for evaluation of chatbots. An examination of CUQ subscales indicated that GARDI was considered to be easy to use and navigate, provided useful, appropriate, and informative responses, and welcoming in nature.",2023,,,,,,,,303,#7737,Alzughaibi 2023,,,1
CatAlyst: Domain-Extensible Intervention for Preventing Task Procrastination Using Large Generative Models,"Arakawa, R.; Yakura, H.; Goto, M.","CatAlyst uses generative models to help workers’ progress by influencing their task engagement instead of directly contributing to their task outputs. It prompts distracted workers to resume their tasks by generating a continuation of their work and presenting it as an intervention that is more context-aware than conventional (predetermined) feedback. The prompt can function by drawing their interest and lowering the hurdle for resumption even when the generated continuation is insufficient to substitute their work, while recent human-AI collaboration research aiming at work substitution depends on a stable high accuracy. This frees CatAlyst from domain-specific model-tuning and makes it applicable to various tasks. Our studies involving writing and slide-editing tasks demonstrated CatAlyst’s effectiveness in helping workers swiftly resume tasks with a lowered cognitive load. The results suggest a new form of human-AI collaboration where large generative models publicly available but imperfect for each individual domain can contribute to workers’ digital well-being.",2023,,,,,,,,313,#7747,Arakawa 2023,,,1
Examining the Effectiveness of Social Cues Embedded in Ai-Driven Healthcare Chatbots: Trust in a Warm Human vs. A Competent Machine,"Huang, M.; Ki, E.-J.","AI-powered healthcare chatbots with their great conversational potential promise to bring more benefits to the communication and relationships between health organizations and their publics by enhancing trust in human-computer interactions. To this end, this study conducted an experiment to examine how the interaction of different social cues shapes positive behavioral and relational outcomes through trust in chatbots. Results revealed that when users interacted with a healthcare chatbot with human source cues, they felt greater trust in the chatbot that showed warmth rather than competence. Conversely, when users interacted with a healthcare chatbot with machine source cues, a competent, rather than warm, conversational style generated greater trust in the chatbot. Trust in this human-computer interaction further enhanced users’ continued usage intention and recommendation adherence as well as relational trust and satisfaction with the health organization. These findings advance the literature on healthcare chatbots as social actors involved in organization-public communication and provide guidance for message persuasion and relationship management.",2023,,,,,,,,318,#7752,Huang 2023,,,1
Interacting with Non-Cooperative User: A New Paradigm for Proactive Dialogue Policy,"Lei, W.; Zhang, Y.; Song, F.; Yang, Z.; Chua, T.-S.","Proactive dialogue system is able to lead the conversation to a goal topic and has advantaged potential in bargain, persuasion and negotiation. Current corpus-based learning manner limits its practical application in real-world scenarios. To this end, we contribute to advance the study of the proactive dialogue policy to a more natural and challenging setting, i.e., interacting dynamically with users. Further, we call attention to the non-cooperative user behavior — the user talks about off-path topics when he/she is not satisfied with the previous topics introduced by the agent. We argue that the targets of reaching the goal topic quickly and maintaining a high user satisfaction are not always converge, because the topics close to the goal and the topics user preferred may not be the same. Towards this issue, we propose a new solution named I-Pro that can learn Proactive policy in the Interactive setting. Specifically, we learn the trade-off via a learned goal weight, which consists of four factors (dialogue turn, goal completion difficulty, user satisfaction estimation, and cooperative degree). The experimental results demonstrate I-Pro significantly outperforms baselines in terms of effectiveness and interpretability.",2022,,,,,,,,344,#7778,Lei 2022,,,1
Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue,"Chen, M.; Shi, W.; Yan, F.; Sahay, S.; Yu, Z.","Complex conversation settings such as persuasion involve communicating changes in attitude or behavior, so users' perspectives need to be addressed, even when not directly related to the topic. In this work, we contribute a novel modular dialogue system framework that seamlessly integrates factual information and social content into persuasive dialogue. Our framework is generalizable to any dialogue tasks that have mixed social and task contents. We conducted a study that compared user evaluations of our framework versus a baseline end-to-end generation model. We found our framework was evaluated more favorably in all dimensions including competence and friendliness, compared to the end-toend model which does not explicitly handle social content or factual questions.",2022,,,,,,,,350,#7784,Chen 2022,,,1
SalienTrack: providing salient information for semi-automated feedback in self-tracking with explainable AI,"Wang, Y.; Liu, J.; Park, H.; Kay, J.; Lim, B.Y.","Self-tracking can improve people's awareness of their unhealthy behaviors and support reflection to inform behavior change. Increasingly, new technologies make tracking easier, leading to large amounts of tracked data. However, much of that information is not salient for reflection and self-awareness. To tackle this burden for reflection, we created the SalienTrack framework, which aims to 1) identify salient tracking events, 2) select the salient details of those events, 3) explain why they are informative, and 4) present the details as manually elicited or automatically shown feedback. We implemented SalienTrack in the context of nutrition tracking. To do this, we first conducted a field study to collect photo-based mobile food tracking over 1-5 weeks. We then report how we used this data to train an explainable-AI model of salience. Finally, we created interfaces to present salient information and conducted a formative user study to gain insights about how SalienTrack could be integrated into an interface for reflection. Our key contributions are the SalienTrack framework, a demonstration of its implementation for semi-automated feedback in an important and challenging self-tracking context and a discussion of the broader uses of the framework.",2021,,,,,,,,371,#7805,Wang 2021,,,1
A STORYTELLING ROBOT MANAGING PERSUASIVE AND ETHICAL STANCES VIA ACT-R: AN EXPLORATORY STUDY,"Augello, A.; Città, G.; Gentile, M.; Lieto, A.","We present a storytelling robot, controlled via the ACT-R cognitive architecture, able to adopt different persuasive techniques and ethical stances while conversing about some topics concerning COVID-19. The main contribution of the paper consists in the proposal of a needs-driven model that guides and evaluates, during the dialogue, the use (if any) of persuasive techniques available in the agent procedural memory. The portfolio of persuasive techniques tested in such a model ranges from the use of storytelling, to framing techniques and rhetorical-based arguments. To the best of our knowledge, this represents the first attempt of building a persuasive agent able to integrate a mix of explicitly grounded cognitive assumptions about dialogue management, storytelling and persuasive techniques as well as ethical attitudes. The paper presents the results of an exploratory evaluation of the system on 63 participants.",2021,,,,,,,,372,#7806,Augello 2021,,,1
Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues,"Joshi, R.; Balachandran, V.; Vashishth, S.; Black, A.W.; Tsvetkov, Y.","To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DIALOGRAPH, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DIALOGRAPH explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues.1",2021,,,,,,,,378,#7812,Joshi 2021,,,1
Strategic argumentation dialogues for persuasion: Framework and experiments based on modelling the beliefs and concerns of the persuadee,"Hadoux, E.; Hunter, A.; Polberg, S.","Persuasion is an important and yet complex aspect of human intelligence. When undertaken through dialogue, the deployment of good arguments, and therefore counterarguments, clearly has a significant effect on the ability to be successful in persuasion. Two key dimensions for determining whether an argument is “good” in a particular dialogue are the degree to which the intended audience believes the argument and counterarguments, and the impact that the argument has on the concerns of the intended audience. In this paper, we present a framework for modelling persuadees in terms of their beliefs and concerns, and for harnessing these models in optimizing the choice of move in persuasion dialogues. Our approach is based on the Monte Carlo Tree Search which allows optimization in real-time. We provide empirical results of a study with human participants showing that our automated persuasion system based on this technology is superior to a baseline system that does not take the beliefs and concerns into account in its strategy.",2021,,,,,,,,388,#7822,Hadoux 2021,,,1
Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration,"Shi, W.; Li, Y.; Sahay, S.; Yu, Z.","Persuasion dialogue systems reflect the machine’s ability to make strategic moves beyond verbal communication, and therefore differentiate themselves from task-oriented or open-domain dialogue systems and have their own unique values. However, the repetition and inconsistency problems still persist in dialogue response generation and could substantially impact user experience and impede the persuasion outcome. Besides, although reinforcement learning (RL) approaches have achieved big success in strategic tasks such as games, they require a sophisticated user simulator to provide real-time feedback to the dialogue system, which limits the application of RL on persuasion dialogues. To address these issues towards a better persuasion dialogue system, we apply RL to refine a language model baseline without user simulators, and distill sentence-level information about repetition, inconsistency, and task relevance through rewards. Moreover, to better accomplish the persuasion task, the model learns from human demonstration to imitate human persuasion behavior and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback.",2020,,,,,,,,397,#7831,Shi 2020,,,1
FitChat: Conversational artificial intelligence interventions for encouraging physical activity in older adults,"Wiratunga, N.; Cooper, K.; Wijekoon, A.; Reiter, E.; Martin, K.","Delivery of digital behaviour change interventions which encourage physical activity has been tried in many forms. Most often interventions are delivered as text notifications, but these do not promote interaction. Advances in conversational AI have improved natural language understanding and generation, allowing AI chatbots to provide an engaging experience with the user. For this reason, chatbots have recently been seen in healthcare delivering digital interventions through free text or choice selection. In this work, we explore the use of voice based AI chatbots as a novel mode of intervention delivery, specifically targeting older adults to encourage physical activity. We co-created “FitChat”, an AI chatbot, with older adults and we evaluate the first prototype using Think Aloud Sessions. Our thematic evaluation suggests that older adults prefer voice based chat over text notifications or free text entry and that voice is a powerful mode for encouraging motivation.",2020,,,,,,,,414,#7848,Wiratunga 2020,,,1
Exploring the role of common model of cognition in designing adaptive coaching interactions for health behavior change,"Mohan, S.","Our research aims to develop intelligent collaborative agents that are human-aware - they can model, learn, and reason about their human partner's physiological, cognitive, and affective states. In this paper, we study how adaptive coaching interactions can be designed to help people develop sustainable healthy behaviors. We leverage the common model of cognition - CMC [31] - as a framework for unifying several behavior change theories that are known to be useful in human-human coaching. We motivate a set of interactive system desiderata based on the CMC-based view of behavior change. Then, we propose PARCoach - an interactive system that addresses the desiderata. PARCoach helps a trainee pick a relevant health goal, set an implementation intention, and track their behavior. During this process, the trainee identifies a specific goal-directed behavior as well as the situational context in which they will perform it. PARCcoach uses this information to send notifications to the trainee, reminding them of their chosen behavior and the context. We report the results from a 4-week deployment with 60 participants. Our results support the CMC-based view of behavior change and demonstrate that the desiderata for proposed interactive system design is useful in producing behavior change.",2019,,,,,,,,427,#7861,Mohan 2019,,,1
Alternating recurrent dialog model with large-scale pre-trained language models,"Wu, Q.; Zhang, Y.; Li, Y.; Yu, Z.","Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models has suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Recurrent Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of large pre-trained language models. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with the state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In the PersuasionForGood task, ARDM is capable of generating human-like responses to persuade people to donate to a charity.1",2019,,,,,,,,429,#7863,Wu 2019,,,1
CoachAI: A conversational agent assisted health coaching platform,"Fadhil, A.; Schiavo, G.; Wang, Y.","Poor lifestyle represents a health risk factor and is the leading cause of morbidity and chronic conditions. The impact of poor lifestyle can be significantly altered by individual behavior change. Although the current shift in healthcare towards a long-lasting modifiable behavior, however, with increasing caregiver workload and individuals' continuous needs of care, there is a need to ease caregiver's work while ensuring continuous interaction with users. This paper describes the design and validation of CoachAI, a conversational agent-assisted health coaching system to support health intervention delivery to individuals and groups. CoachAI instantiates a text-based healthcare chatbot system that bridges the remote human coach and the users. This research provides three main contributions to the preventive healthcare & healthy lifestyle promotion: (1) it presents the conversational agent to aid the caregiver; (2) it aims to decrease caregiver's workload and enhance care given to users, by handling (automating) repetitive caregiver tasks; and (3) it presents a domain-independent mobile health conversational agent for health intervention delivery. We will discuss our approach and analyze the results of a one-month validation study on physical activity, healthy diet and stress management.",2019,,,,,,,,443,#8813,Fadhil 2019,,,1
Is two better than one? effects of multiple agents on user persuasion,"Kantharaju, R.B.; De Franco, D.; Pease, A.; Pelachaud, C.","Virtual humans need to be persuasive in order to promote behaviour change in human users. While several studies have focused on understanding the numerous aspects that influence the degree of persuasion, most of them are limited to dyadic interactions. In this paper, we present an evaluation study focused on understanding the effects of multiple agents on user's persuasion. Along with gender and status (authoritative &peer), we also look at type of focus employed by the agent i. e., user-directed where the agent aims to persuade by addressing the user directly and vicarious where the agent aims to persuade the user, who is an observer, indirectly by engaging another agent in the discussion. Participants were randomly assigned to one of the 12 conditions and presented with a persuasive message by one or several virtual agents. A questionnaire was used to measure perceived interpersonal attitude, credibility and persuasion. Results indicate that credibility positively affects persuasion. In general, multiple agent setting, irrespective of the focus, was more persuasive than single agent setting. Although, participants favored user-directed setting and reported it to be persuasive and had an increased level of trust in the agents, the actual change in persuasion score reflects that vicarious setting was the most effective in inducing behaviour change. In addition to this, the study also revealed that authoritative agents were the most persuasive.",2019,,,,,,,,445,#8815,Kantharaju 2019,,,1
AI model GPT-3 (dis)informs us better than humans,"Spitale, Giovanni; Biller-Andorno, Nikola; Germani, Federico","Artificial intelligence (AI) is changing the way we create and evaluate information, and this is happening during an infodemic, which has been having marked effects on global health. Here, we evaluate whether recruited individuals can distinguish disinformation from accurate information, structured in the form of tweets, and determine whether a tweet is organic or synthetic, i.e., whether it has been written by a Twitter user or by the AI model GPT-3. The results of our preregistered study, including 697 participants, show that GPT-3 is a double-edge sword: In comparison with humans, it can produce accurate information that is easier to understand, but it can also produce more compelling disinformation. We also show that humans cannot distinguish between tweets generated by GPT-3 and written by real Twitter users. Starting from our results, we reflect on the dangers of AI for disinformation and on how information campaigns can be improved to benefit global health.",2023,,Science Advances,9,26,eadh1850,,10.1126/sciadv.adh1850,,#8963,Spitale 2023,,,1
AI-Generated Messages Can Be Used to Persuade Humans on Policy Issues,"Bai, Hui; Voelkel, Jan; Muldowney, Shane; Eichstaedt, Johannes; Willer, Robb","The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets in the U.S. and globally. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans’ political attitudes. Across three pre-registered experiments (total N = 4,829), we find consistent evidence that assigning participants to read persuasive messages generated by LLMs can lead to attitude change across a range of policies, including highly polarized policies, such as an assault weapons ban, a carbon tax, and a paid parental-leave program. Overall, we found LLM-generated messages were similarly effective in influencing policy attitudes as were messages crafted by lay humans. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages cheaply and at massive scale.",2025,,,,,,,10.31219/osf.io/stakv_v5,,#8966,Bai 2025,,,1
AI-Mediated Communication: How the Perception that Profile Text was Written by AI Affects Trustworthiness,"Jakesch, Maurice; French, Megan; Ma, Xiao; Hancock, Jeffrey T.; Naaman, Mor","We are entering an era of AI-Mediated Communication (AI-MC) where interpersonal communication is not only mediated by technology, but is optimized, augmented, or generated by artificial intelligence. Our study takes a first look at the potential impact of AI-MC on online self-presentation. In three experiments we test whether people find Airbnb hosts less trustworthy if they believe their profiles have been written by AI. We observe a new phenomenon that we term the Replicant Effect: Only when participants thought they saw a mixed set of AI- and human-written profiles, they mistrusted hosts whose profiles were labeled as or suspected to be written by AI. Our findings have implications for the design of systems that involve AI technologies in online self-presentation and chart a direction for future work that may upend or augment key aspects of Computer-Mediated Communication theory.",2019,,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,,,Jan-13,,10.1145/3290605.3300469,,#8967,Jakesch 2019,,,1
"An Empathy-Driven, Conversational Artificial Intelligence Agent (Wysa) for Digital Mental Well-Being: Real-World Data Evaluation Mixed-Methods Study","Inkster, Becky; Sarda, Shubhankar; Subramanian, Vinod","Background: A World Health Organization 2017 report stated that major depression affects almost 5% of the human population. Major depression is associated with impaired psychosocial functioning and reduced quality of life. Challenges such as shortage of mental health personnel, long waiting times, perceived stigma, and lower government spends pose barriers to the alleviation of mental health problems. Face-to-face psychotherapy alone provides only point-in-time support and cannot scale quickly enough to address this growing global public health challenge. Artificial intelligence (AI)-enabled, empathetic, and evidence-driven conversational mobile app technologies could play an active role in filling this gap by increasing adoption and enabling reach. Although such a technology can help manage these barriers, they should never replace time with a health care professional for more severe mental health problems. However, app technologies could act as a supplementary or intermediate support system. Mobile mental well-being apps need to uphold privacy and foster both short- and long-term positive outcomes. Objective: This study aimed to present a preliminary real-world data evaluation of the effectiveness and engagement levels of an AI-enabled, empathetic, text-based conversational mobile mental well-being app, Wysa, on users with self-reported symptoms of depression. Methods: In the study, a group of anonymous global users were observed who voluntarily installed the Wysa app, engaged in text-based messaging, and self-reported symptoms of depression using the Patient Health Questionnaire-9. On the basis of the extent of app usage on and between 2 consecutive screening time points, 2 distinct groups of users (high users and low users) emerged. The study used mixed-methods approach to evaluate the impact and engagement levels among these users. The quantitative analysis measured the app impact by comparing the average improvement in symptoms of depression between high and low users. The qualitative analysis measured the app engagement and experience by analyzing in-app user feedback and evaluated the performance of a machine learning classifier to detect user objections during conversations. Results: The average mood improvement (ie, difference in pre- and post-self-reported depression scores) between the groups (ie, high vs low users; n=108 and n=21, respectively) revealed that the high users group had significantly higher average improvement (mean 5.84 [SD 6.66]) compared with the low users group (mean 3.52 [SD 6.15]); Mann-Whitney P=.03 and with a moderate effect size of 0.63. Moreover, 67.7% of user-provided feedback responses found the app experience helpful and encouraging. Conclusions: The real-world data evaluation findings on the effectiveness and engagement levels of Wysa app on users with self-reported symptoms of depression show promise. However, further work is required to validate these initial findings in much larger samples and across longer periods.",2018,,JMIR mHealth and uHealth,6,11,e12106,,10.2196/12106,,#8972,Inkster 2018,,,1
Anthropomorphism brings us closer: The mediating role of psychological distance in User–AI assistant interactions,"Li, Xinge; Sung, Yongjun","In the current era, interacting with Artificial Intelligence (AI) has become an everyday activity. Understanding the interaction between humans and AI is of potential value because, in future, such interactions are expected to become more pervasive. Two studies—one survey and one experiment—were conducted to demonstrate positive effects of anthropomorphism on interactions with smart-speaker-based AI assistants and to examine the mediating role of psychological distance in this relationship. The results of Study 1, an online survey, showed that participants with a higher tendency to anthropomorphize their AI assistant/s evaluated it/them more positively, and this effect was mediated by psychological distance. In Study 2, the hypotheses were tested in a more sophisticated experiment. Again, the results indicated that, in the high-anthropomorphism (vs. low-anthropomorphism) condition, participants had more positive attitudes toward the AI assistant, and the effect was mediated by psychological distance. Though several studies have demonstrated the effect of anthropomorphism, few have probed the underlying mechanism of anthropomorphism thoroughly. The current research not only contributes to the anthropomorphism literature, but also provides direction to research on facilitating human–AI interaction.",2021,,Computers in Human Behavior,118,,106680,,10.1016/j.chb.2021.106680,,#8973,Li 2021,,,1
Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect,"Longoni, Chiara; Cian, Luca","Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).",2022,,Journal of Marketing,86,1,91-108,,10.1177/0022242920957347,,#8978,Longoni 2022,,,1
Behavioral and neural evidence on consumer responses to human doctors and medical artificial intelligence,"Yun, Jin Ho; Lee, Eun-Ju; Kim, Dong Hyun","Will consumers accept artificial intelligence (AI) as a medical care provider? On the basis of evolution theory, we investigate the implicit psychological mechanisms that underlie consumers’ interactions with medical AI and a human doctor. In a behavioral investigation (Study 1), consumers expressed a positive intention to use medical AI's healthcare services when it used personalized rather than mechanical conversation. However, neural investigation (Study 2) using functional magnetic resonance imaging revealed that some consumers’ implicit attitudes toward medical AI differed from their expressed behavioral intentions. The brain areas linked with implicitly apathetic emotions were activated even when medical AI used a personalized conversation, whereas consumers’ brains were activated in areas associated with prosociality when they interacted with a human doctor who used a personalized conversation. On the basis of our neural evidence, consumers perceive an identical personalized conversation differently when it is offered by a medical AI versus a human doctor. These findings have implications for the area of human–AI interactions and medical decision-making and suggest that replacing human doctors with medical AI is still an unrealistic proposition.",2021,,Psychology & Marketing,38,4,610-625,,10.1002/mar.21445,,#8985,Yun 2021,,,1
Can AI enhance people's support for online moderation and their openness to dissimilar political views?,"Wojcieszak, Magdalena; Thakur, Arti; Ferreira Gonçalves, João Fernando; Casas, Andreu; Menchen-Trevino, Ericka; Boon, & Miriam","Although artificial intelligence is blamed for many societal challenges, it also has underexplored potential in political contexts online. We rely on six preregistered experiments in three countries (N = 6,728) to test the expectation that AI and AI-assisted humans would be perceived more favorably than humans (a) across various content moderation, generation, and recommendation scenarios and (b) when exposing individuals to counter-attitudinal political information. Contrary to the preregistered hypotheses, participants see human agents as more just than AI across the scenarios tested, with the exception of news recommendations. At the same time, participants are not more open to counter-attitudinal information attributed to AI rather than a human or an AI-assisted human. These findings, which—with minor variations—emerged across countries, scenarios, and issues, suggest that human intervention is preferred online and that people reject dissimilar information regardless of its source. We discuss the theoretical and practical implications of these findings. Lay Summary: In the era of unprecedented political divides and misinformation, artificial intelligence (AI) and algorithms are often seen as the culprits. In contrast to these dominant narratives, we argued that AI might be seen as being less biased than a human in online political contexts. We relied on six preregistered experiments in three countries (the United Sates, Spain, Poland) to test whether internet users perceive AI and AI-assisted humans more favorably than simply humans; (a) across various distinct scenarios online, and (b) when exposing people to opposing political information on a range of contentious issues. Contrary to our expectations, human agents were consistently perceived more favorably than AI except when recommending news. These findings suggest that people prefer human intervention in most online political contexts. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",2021,,Journal of Computer-Mediated Communication,26,4,223-243,,10.1093/jcmc/zmab006,,#8991,Wojcieszak 2021,,,1
Can AI reduce motivated reasoning in news consumption? Investigating the role of attitudes towards AI and prior-opinion in shaping trust perceptions of news,"Wischnewski, M.; Krämer, N.","A central role in understanding the interaction between humans and AI plays the notion of trust. Especially research from social and cognitive psychology has shown, however, that individuals’ perceptions of trust can be biased. In this empirical investigation, we focus on the single and combined effects of attitudes towards AI and motivated reasoning in shaping such biased trust perceptions in the context of news consumption. In doing so, we rely on insights from works on the machine heuristic and motivated reasoning. In a 2 (author) x 2 (congruency) between-subjects online experiment, we asked N = 477 participants to read a news article purportedly written either by AI or a human author. We manipulated whether the article represented pro or contra arguments of a polarizing topic, to elicit motivated reasoning. We also assessed participants’ attitudes towards AI in terms of competence and objectivity. Through multiple linear regressions, we found that (a) increased perceptions of AI as objective and ideologically unbiased increased trust perceptions, whereas (b), in cases where participants were swayed by their prior opinion to trust content more when they agreed with the content, the AI author reduced such biased perceptions. Our results indicate that it is crucial to account for attitudes towards AI and motivated reasoning to accurately represent trust perceptions.",2022,,HHAI2022: Augmenting Human Intellect,,,184-198,,10.3233/FAIA220198,,#8992,Wischnewski 2022,,,1
Can we be friends with Mitsuku? A longitudinal study on the process of relationship formation between humans and a social chatbot,"Croes, Emmelyn A. J.; Antheunis, Marjolijn L.","This explorative study investigated (a) whether social attraction, self-disclosure, interaction quality, intimacy, empathy and communicative competence play a role in getting-acquainted interactions between humans and a chatbot, and (b) whether humans can build a relationship with a chatbot. Although human-machine communication research suggests that humans can develop feelings for computers, this does not automatically imply that humans experience feelings of friendship with a chatbot. In this longitudinal study, 118 participants had seven interactions with chatbot Mitsuku over a 3-week period. After each interaction participants filled out a questionnaire. The results showed that the social processes decreased after each interaction and feelings of friendship were low. In line with the ABCDE model of relationship development, the social processes that aid relationship continuation decrease, leading to deterioration of the relationship. Furthermore, a novelty effect was at play after the first interaction, after which the chatbot became predictable and the interactions less enjoyable.",2021,,Journal of Social and Personal Relationships,38,1,279-300,,10.1177/0265407520959463,,#8994,Croes 2021,,,1
Comparing the persuasiveness of role-playing large language models and human experts on polarized U.S. political issues,"Hackenburg, Kobi; Ibrahim, Lujain; Tappin, Ben; Tsakiris, Manos","Advances in large language models (LLMs) could significantly disrupt political communication. In a large-scale pre-registered experiment (n=4,955), we prompted GPT-4 to generate persuasive messages impersonating the language and beliefs of U.S. political parties – a technique we term “partisan role-play” –  and directly compared their persuasiveness to that of human persuasion experts. In aggregate, the persuasive impact of role-playing messages generated by GPT-4 was not significantly different from that of non-role-playing messages. However, the persuasive impact of GPT-4 rivaled, and on some issues exceeded, that of the human experts. Taken together, our findings suggest that — contrary to popular concern — instructing current LLMs to role-play as partisans offers limited persuasive advantage, but also that current LLMs can rival and even exceed the persuasiveness of human experts. These results potentially portend widespread adoption of AI tools by persuasion campaigns, with important implications for the role of AI in politics and democracy.",2023,,,,,,,10.31219/osf.io/ey8db,,#9001,Hackenburg 2023,,,1
Debate Helps Supervise Unreliable Experts,"Michael, Julian; Mahdi, Salsabila; Rein, David; Petty, Jackson; Dirani, Julien; Padmakumar, Vishakh; Bowman, Samuel R.","As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important. How can we supervise unreliable experts, which have access to the truth but may not accurately report it, to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own? In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage. In our debates, one expert argues for the correct answer, and the other for an incorrect answer. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%. Debates are also more efficient, being 68% of the length of consultancies. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill). Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.",2023,,,,,,,10.48550/arXiv.2311.08702,,#9006,Michael 2023,,,1
Does the Correspondence Bias Apply to Social Robots?: Dispositional and Situational Attributions of Human Versus Robot Behavior,"Edwards, Autumn; Edwards, Chad","Increasingly, people interact with embodied machine communicators and are challenged to understand their natures and behaviors. The Fundamental Attribution Error (FAE, sometimes referred to as the correspondence bias) is the tendency for individuals to over-emphasize personality-based or dispositional explanations for other people’s behavior while under-emphasizing situational explanations. This effect has been thoroughly examined with humans, but do people make the same causal inferences when interpreting the actions of a robot? As compared to people, social robots are less autonomous and agentic because their behavior is wholly determined by humans in the loop, programming, and design choices. Nonetheless, people do assign robots agency, intentionality, personality, and blame. Results of an experiment showed that participants made correspondent inferences when evaluating both human and robot speakers, attributing their behavior to underlying attitudes even when it was clearly coerced. However, they committed a stronger correspondence bias in the case of the robot–an effect driven by the greater dispositional culpability assigned to robots committing unpopular behavior–and they were more confident in their attitudinal judgments of robots than humans. Results demonstrated some differences in the global impressions of humans and robots based on behavior valence and choice. Judges formed more generous impressions of the robot agent when its unpopular behavior was coerced versus chosen; a tendency not displayed when forming impressions of the human agent. Implications of attributing robot behavior to disposition, or conflating robot actors with their actions, are addressed.",2022,,Frontiers in Robotics and AI,8,,,,10.3389/frobt.2021.788242,,#9012,Edwards 2022,,,1
Durably reducing conspiracy beliefs through dialogues with AI,"Costello, Thomas; Pennycook, Gordon; Rand, David","Conspiracy theories are a paradigmatic example of beliefs that, once adopted, are extremely difficult to dispel. Influential psychological theories propose that conspiracy beliefs are uniquely resistant to counterevidence because they satisfy important needs and motivations. Here, we raise the possibility that previous attempts to correct conspiracy beliefs have been unsuccessful merely because they failed to deliver counterevidence that was sufficiently compelling and tailored to each believer’s specific conspiracy theory (which vary dramatically from believer to believer). To evaluate this possibility, we leverage recent developments in generative artificial intelligence (AI) to deliver well-argued, person-specific debunks to a total of N = 2,190 conspiracy theory believers. Participants in our experiments provided detailed, open-ended explanations of a conspiracy theory they believed, and then engaged in a 3 round dialogue with a frontier generative AI model (GPT-4 Turbo) which was instructed to reduce each participant’s belief in their conspiracy theory (or discuss a banal topic in a control condition). Across two experiments, we find robust evidence that the debunking conversation with the AI reduced belief in conspiracy theories by roughly 20%. This effect did not decay over 2 months time, was consistently observed across a wide range of different conspiracy theories, and occurred even for participants whose conspiracy beliefs were deeply entrenched and of great importance to their identities. Furthermore, although the dialogues were focused on a single conspiracy theory, the intervention spilled over to reduce beliefs in unrelated conspiracies, indicating a general decrease in conspiratorial worldview, as well as increasing intentions to challenge others who espouse their chosen conspiracy. These findings highlight that even many people who strongly believe in seemingly fact-resistant conspiratorial beliefs can change their minds in the face of sufficient evidence.",2024,,,,,,,10.31234/osf.io/xcwdn,,#9015,Costello 2024,,,1
Emotional Support from AI Chatbots: Should a Supportive Partner Self-Disclose or Not?,"Meng, Jingbo; Dai, Yue (Nancy)","This study examined how and when a chatbot’s emotional support was effective in reducing people’s stress and worry. It compared emotional support from chatbot versus human partners in terms of its process and conditional effects on stress/worry reduction. In an online experiment, participants discussed a personal stressor with a chatbot or a human partner who provided none, or either one or both of emotional support and reciprocal self-disclosure. The results showed that emotional support from a conversational partner was mediated through perceived supportiveness of the partner to reduce stress and worry among participants, and the link from emotional support to perceived supportiveness was stronger for a human than for a chatbot. A conversational partner’s reciprocal self-disclosure enhanced the positive effect of emotional support on worry reduction. However, when emotional support was absent, a solely self-disclosing chatbot reduced even less stress than a chatbot not providing any response to participants’ stress.In recent years, AI chatbots have increasingly been used to provide empathy and support to people who are experiencing stressful times. This study compared emotional support from a chatbot compared to that of a human who provided support. We were interested in examining which approach could best effectively reduce people’s worry and stress. When either a person or a chatbot was able to engage with a stressed individual and tell that individual about their own experiences, they were able to build rapport. We found that this type of reciprocal self-disclosure was effective in calming the worry of the individual. Interestingly, if a chatbot only reciprocally self-disclosed but offered no emotional support, the outcome was worse than if the chatbot did not respond to people at all. This work will help in the development of supportive chatbots by providing insights into when and what they should self-disclose.",2021,,Journal of Computer-Mediated Communication,26,4,207-222,,10.1093/jcmc/zmab005,,#9020,Meng 2021,,,1
Empathetic Persuasion: Reinforcing Empathy and Persuasiveness in Dialogue Systems,"Samad, Azlaan Mustafa; Mishra, Kshitij; Firdaus, Mauajama; Ekbal, Asif","Persuasion is an intricate process involving empathetic connection between two individuals. Plain persuasive responses may make a conversation non-engaging. Even the most well-intended and reasoned persuasive conversations can fall through in the absence of empathetic connection between the speaker and listener. In this paper, we propose a novel task of incorporating empathy when generating persuasive responses. We develop an empathetic persuasive dialogue system by fine-tuning a maximum likelihood Estimation (MLE)-based language model in a reinforcement learning (RL) framework. To design feedbacks for our RL-agent, we define an effective and efficient reward function considering consistency, repetitiveness, emotion and persuasion rewards to ensure consistency, non-repetitiveness, empathy and persuasiveness in the generated responses. Due to lack of emotion annotated persuasive data, we first annotate the existing Persuaion For Good dataset with emotions, then build transformer based classifiers to provide emotion based feedbacks to our RL agent. Experimental results confirm that our proposed model increases the rate of generating persuasive responses as compared to the available state-of-the-art dialogue models while making the dialogues empathetically more engaging and retaining the language quality in responses.",2022,,Findings of the Association for Computational Linguistics: NAACL 2022,,,844-856,,10.18653/v1/2022.findings-naacl.63,,#9021,Samad 2022,,,1
Expl(AI)ned: The Impact of Explainable Artificial Intelligence on Users’ Information Processing,"Bauer, Kevin; von Zahn, Moritz; Hinz, Oliver","Because of a growing number of initiatives and regulations, predictions of modern artificial intelligence (AI) systems increasingly come with explanations about why they behave the way they do. In this paper, we explore the impact of feature-based explanations on users’ information processing. We designed two complementary empirical studies where participants either made incentivized decisions on their own, with the aid of opaque predictions, or with explained predictions. In Study 1, laypeople engaged in the deliberately abstract investment game task. In Study 2, experts from the real estate industry estimated listing prices for real German apartments. Our results indicate that the provision of feature-based explanations paves the way for AI systems to reshape users’ sense making of information and understanding of the world around them. Specifically, explanations change users’ situational weighting of available information and evoke mental model adjustments. Crucially, mental model adjustments are subject to the confirmation bias so that misconceptions can persist and even accumulate, possibly leading to suboptimal or biased decisions. Additionally, mental model adjustments create spillover effects that alter user behavior in related yet disparate domains. Overall, this paper provides important insights into potential downstream consequences of the broad employment of modern explainable AI methods. In particular, side effects of mental model adjustments present a potential risk of manipulating user behavior, promoting discriminatory inclinations, and increasing noise in decision making. Our findings may inform the refinement of current efforts of companies building AI systems and regulators that aim to mitigate problems associated with the black-box nature of many modern AI systems. History: Alessandro Acquisti, senior editor; Jason Chan, associate editor. Funding: This work was supported by the Deutsche Forschungsgemeinschaft (DFG) (Projek 449023539), Volkswagen Foundation (ML2MT), and LeibnizInstitute for Financial Research SAFE. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2023.1199 .",2023,,Information Systems Research,34,4,1582-1602,,10.1287/isre.2023.1199,,#9023,Bauer 2023,,,1
Exposure to robot preachers undermines religious commitment,"Jackson, Joshua Conrad; Yam, Kai Chi; Tang, Pok Man; Liu, Ting; Shariff, Azim","Over the last decade, robots continue to infiltrate the workforce, permeating occupations that once seemed immune to automation. This process seems to be inevitable because robots have ever-expanding capabilities. However, drawing from theories of cultural evolution and social learning, we propose that robots may have limited influence in domains that require high degrees of “credibility”; here we focus on the automation of religious preachers as one such domain. Using a natural experiment in a recently automated Buddhist temple (Study 1) and a fully randomized experiment in a Taoist temple (Study 2), we consistently show that religious adherents perceive robot preachers—and the institutions which employ them—as less credible than human preachers. This lack of credibility explains reductions in religious commitment after people listen to robot (vs. human) preachers deliver sermons. Study 3 conceptually replicates this finding in an online experiment and suggests that religious elites require perceived minds (agency and patiency) to be credible, which is partly why robot preachers inspire less credibility than humans. Our studies support cultural evolutionary theories of religion and suggest that escalating religious automation may induce religious decline. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",2023,,Journal of Experimental Psychology: General,152,12,3344-3358,,10.1037/xge0001443,,#9025,Jackson 2023,,,1
"Facing the Artificial: Understanding Affinity, Trustworthiness, and Preference for More Realistic Digital Humans","Seymour, Mike; Yuan, Lingyao; Dennis, Alan; Riemer, Kai","In recent years, companies have been developing more realistic looking human faces for digital, virtual agents controlled by artificial intelligence (AI). But how do users feel about interacting with such virtual agents? We used a controlled lab experiment to examine users’ perceived trustworthiness, affinity, and preference towards a real human travel agent appearing via video (i.e., Skype) as well as in the form of a very human-realistic avatar; half of the participants were (deceptively) told the avatar was a virtual agent controlled by AI while the other half were told the avatar was controlled by the same human travel agent. Results show that participants rated the video human agent more trustworthy, had more affinity for him, and preferred him to both avatar versions. Users who believed the avatar was a virtual agent controlled by AI reported the same level of affinity, trustworthiness, and preferences towards the agent as those who believed it was controlled by a human. Thus, use of a realistic digital avatar lowered affinity, trustworthiness, and preferences, but how the avatar was controlled (by human or machine) had no effect. The conclusion is that improved visual fidelity alone makes a significant positive difference and that users are not averse to advanced AI simulating human presence, some may even be anticipating such an advanced technology.",2020,,,,,,,,,#9026,Seymour 2020,,,1
Getting to Know Each Other: The Role of Social Dialogue in Recovery from Errors in Social Robots,"Lucas, Gale M.; Boberg, Jill; Traum, David; Artstein, Ron; Gratch, Jonathan; Gainer, Alesia; Johnson, Emmanuel; Leuski, Anton; Nakano, Mikio","This work explores the extent to which social dialogue can mitigate (or exacerbate) the loss of trust caused when robots make conversational errors. Our study uses a NAO robot programmed to persuade users to agree with its rankings on two tasks. We perform two manipulations: (1) The timing of conversational errors - the robot exhibited errors either in the first task, the second task, or neither; (2) The presence of social dialogue - between the two tasks, users either engaged in a social dialogue with the robot or completed a control task. We found that the timing of the errors matters: replicating previous research, conversational errors reduce the robot's influence in the second task, but not on the first task. Social dialogue interacts with the timing of errors, acting as an intensifier: social dialogue helps the robot recover from prior errors, and actually boosts subsequent influence; but social dialogue backfires if it is followed by errors, because it extends the period of good performance, creating a stronger contrast effect with the subsequent errors. The design of social robots should therefore be more careful to avoid errors after periods of good performance than early on in a dialogue.",2018,,Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction,,,344-351,,10.1145/3171221.3171258,,#9030,Lucas 2018,,,1
Human-level play in the game of Diplomacy by combining language models with strategic reasoning,"Meta Fundamental AI Research Diplomacy Team (FAIR); Bakhtin, Anton; Brown, Noam; Dinan, Emily; Farina, Gabriele; Flaherty, Colin; Fried, Daniel; Goff, Andrew; Gray, Jonathan; Hu, Hengyuan; Jacob, Athul Paul; Komeili, Mojtaba; Konath, Karthik; Kwon, Minae; Lerer, Adam; Lewis, Mike; Miller, Alexander H.; Mitts, Sasha; Renduchintala, Adithya; Roller, Stephen; Rowe, Dirk; Shi, Weiyan; Spisak, Joe; Wei, Alexander; Wu, David; Zhang, Hugh; Zijlstra, Markus","Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.",2022,,Science,378,6624,1067-1074,,10.1126/science.ade9097,,#9038,MetaFundamentalAIResearchDiplomacyTeam(FAIR) 2022,,,1
Interpersonal impressions of a social robot versus human in the context of performance evaluations,"Edwards, Chad; Edwards, Autumn; Albrehi, Fatima; Spence, Patric","Extending previous research on the Computers Are Social Actors paradigm and the human-to-human interaction script, this study examines the interpersonal impressions of a social robot evaluator versus a human evaluator in a performance evaluation context. A between-subjects experiment was conducted to measure participants’ impressions of the credibility, task and social attractiveness, and social presence of a social robot or a human evaluator of their public speaking performances. Additionally, participants rated the competence of the feedback they received. Both evaluators (human and social robot) garnered mostly “favorable” interpersonal impressions. Moreover, both agents’ feedback was rated favorably. In all cases, the human evaluator was rated significantly higher than the social robot evaluator. Implications for the use of social robots in educational evaluator roles and for future human–robot interaction research are discussed.",2021,,Communication Education,70,2,165-182,,10.1080/03634523.2020.1802495,,#9048,Edwards 2021,,,1
"Large Language Models Can Argue in Convincing Ways About Politics, But Humans Dislike AI Authors: implications for Governance","Palmer, Alexis; Spirling, Arthur","All politics relies on rhetorical appeals, and the ability to make arguments is considered perhaps uniquely human. But as recent times have seen successful large language model (LLM) applications to similar endeavours, we explore whether these approaches can out-compete humans in making appeals for/against various positions in US politics. We curate responses from crowdsourced workers and an LLM and place them in competition with one another. Human (crowd) judges make decisions about the relative strength of their (human v machine) efforts. We have several empirical ‘possibility’ results. First, LLMs can produce novel arguments that convince independent judges at least on a par with human efforts. Yet when informed about an orator’s true identity, judges show a preference for human over LLM arguments. This may suggest voters view such models as potentially dangerous; we think politicians should be aware of related ‘liar’s dividend’ concerns.",2023,,Political Science,75,3,281-291,,10.1080/00323187.2024.2335471,,#9054,Palmer 2023,,,1
Large Language Models Can Enhance Persuasion Through Linguistic Feature Alignment,"Shin, Minkyu; Kim, Jin","Although large language models (LLMs) are reshaping various aspects of human life, our current understanding of their impacts remains somewhat constrained. Here we investigate the impact of LLMs on human communication, using data on consumer complaints in the financial industry. By employing an AI detection tool on more than 820K complaints gathered by the Consumer Financial Protection Bureau (CFPB), we find a sharp increase in the likely use of LLMs shortly after the release of ChatGPT. Moreover, the likely LLM usage was positively correlated with message persuasiveness (i.e., increased likelihood of obtaining relief from financial firms). Computational linguistic analyses suggest that the positive correlation may be explained by LLMs’ enhancement of various linguistic features. Based on the results of these observational studies, we hypothesize that LLM usage may enhance a comprehensive set of linguistic features, increasing message persuasiveness to receivers with heterogeneous linguistic preferences (i.e., linguistic feature alignment). We test this hypothesis in preregistered experiments and find support for it. As an instance of early empirical demonstrations of LLM usage for enhancing persuasion, our research highlights the transformative potential of LLMs in human communication.",2024,,,,,,,,,#9055,Shin 2024,,,1
Measuring the Persuasiveness of Language Models,"Durmus, Esin; Lovitt, Liane; Tamkin, Alex; Ritchie, Stuart; Clark, Jack; Ganguli, Deep","Anthropic developed a way to test how persuasive language models (LMs) are, and analyzed how persuasiveness scales across different versions of Claude.",2024,,,,,,,,,#9064,Durmus 2024,,,1
People devalue generative AI’s competence but not its advice in addressing societal and personal challenges,"Böhm, Robert; Jörling, Moritz; Reiter, Leonhard; Fuchs, Christoph","The release of ChatGPT and related tools have made generative artificial intelligence (AI) easily accessible for the broader public. We conducted four preregistered experimental studies (total N = 3308; participants from the US) to investigate people’s perceptions of generative AI and the advice it generates on how to address societal and personal challenges. The results indicate that when individuals are (vs. are not) aware that the advice was generated by AI, they devalue the author’s competence but not the content or the intention to share and follow the advice on how to address societal challenges (Study 1) and personal challenges (Studies 2a and 2b). Study 3 further shows that individuals’ preference to receive advice from AI (vs. human experts) increases when they gained positive experience with generative AI advice in the past. The results are discussed regarding the nature of AI aversion in the context of generative AI and beyond.",2023,,Communications Psychology,1,1,32,,10.1038/s44271-023-00032-x,,#9079,Böhm 2023,,,1
Persuasiveness of arguments with AI-source labels,"Teigen, Cassandra; Madsen, Jens Koed; George, Nicole Lauren; Yousefi, Sayeh","This paper sought to understand the impact of labelling an argument as AI-generated compared to human-authored, and how factors such as portrayals of expertise and the nature of arguments presented (narrative versus statistical) may affect the persuasiveness of the arguments. Three domains were explored: health, finance, and politics. We show that arguments with AI source labels, both non-expert and expert, were rated by participants as less persuasive than when they had their counterpart human-authored source labels attached. Moreover, although the statistical arguments were found to be more persuasive than the narrative arguments, this did not affect the impact of an AI source label, with a significant interaction effect only being seen for the domain of politics for the expert AI source. The study explored the role of attitude towards AI on the impact of source labels as an exploratory analysis and found no significant interaction effect across the three domains.",2024,,Proceedings of the Annual Meeting of the Cognitive Science Society,46,0,,,,,#9083,Teigen 2024,,,1
Quantifying the Impact of Large Language Models on Collective Opinion Dynamics,"Li, Chao; Su, Xing; Han, Haoying; Xue, Cong; Zheng, Chunmo; Fan, Chao","The process of opinion expression and exchange is a critical component of democratic societies. As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned. However, the knowledge about how LLMs affect the process of opinion expression and exchange of social opinion networks is very limited. Here, we create an opinion network dynamics model to encode the opinions of LLMs, cognitive acceptability and usage strategies of individuals, and simulate the impact of LLMs on opinion dynamics in a variety of scenarios. The outcomes of the simulations inform about effective demand-oriented opinion network interventions. The results from this study suggested that the output opinion of LLMs has a unique and positive effect on the collective opinion difference. The marginal effect of cognitive acceptability on collective opinion formation is nonlinear and shows a decreasing trend. When people partially rely on LLMs, the exchange process of opinion becomes more intense and the diversity of opinion becomes more favorable. In fact, there is 38.6% more opinion diversity when people all partially rely on LLMs, compared to prohibiting the use of LLMs entirely. The optimal diversity of opinion was found when the fractions of people who do not use, partially rely on, and fully rely on LLMs reached roughly 4:12:1. Our experiments also find that introducing extra agents with opposite/neutral/random opinions, we can effectively mitigate the impact of biased/toxic output from LLMs. Our findings provide valuable insights into opinion dynamics in the age of LLMs, highlighting the need for customized interventions tailored to specific scenarios to address the drawbacks of improper output and use of LLMs.",2024,,,,,,,10.2139/ssrn.4688547,,#9087,Li 2024,,,1
Resistance to Medical Artificial Intelligence,"Longoni, Chiara; Bonezzi, Andrea; Morewedge, Carey K","Artificial intelligence (AI) is revolutionizing healthcare, but little is known about consumer receptivity to AI in medicine. Consumers are reluctant to utilize healthcare provided by AI in real and hypothetical choices, separate and joint evaluations. Consumers are less likely to utilize healthcare (study 1), exhibit lower reservation prices for healthcare (study 2), are less sensitive to differences in provider performance (studies 3A–3C), and derive negative utility if a provider is automated rather than human (study 4). Uniqueness neglect, a concern that AI providers are less able than human providers to account for consumers’ unique characteristics and circumstances, drives consumer resistance to medical AI. Indeed, resistance to medical AI is stronger for consumers who perceive themselves to be more unique (study 5). Uniqueness neglect mediates resistance to medical AI (study 6), and is eliminated when AI provides care (a) that is framed as personalized (study 7), (b) to consumers other than the self (study 8), or (c) that only supports, rather than replaces, a decision made by a human healthcare provider (study 9). These findings make contributions to the psychology of automation and medical decision making, and suggest interventions to increase consumer acceptance of AI in medicine.",2019,,Journal of Consumer Research,46,4,629-650,,10.1093/jcr/ucz013,,#9093,Longoni 2019,,,1
"Same benefits, different communication patterns: Comparing Children's reading with a conversational agent vs. a human partner","Xu, Ying; Wang, Dakuo; Collins, Penelope; Lee, Hyelim; Warschauer, Mark","Storybook reading accompanied by adult-guided conversation provides a stimulating context for children's language development. Conversational agents powered by artificial intelligence, such as smart speakers, are prevalent in children's homes and have the potential to engage children in storybook reading as language partners. However, little research has explored the effectiveness of using conversational agents to support children's language development. This study examined how an automated conversational agent can read stories to children via a smart speaker while asking questions and providing contingent feedback. Using a randomized experiment among 90 children aged three to six years, this study compared these children's story comprehension and verbal engagement in storybook reading with a conversational agent versus an adult. The conversational agent's guided conversation was found to be as supportive in improving children's story comprehension as that provided by an adult language partner. At the same time, this study uncovered a number of differences in children's verbal engagement when interacting with a conversational agent versus with an adult. Specifically, children who read with the conversational agent responded to questions with better intelligibility, whereas those who read with an adult responded to questions with higher productivity, lexical diversity, and topical relevance. And the two groups responded to questions with a similar level of accuracy. In addition, questions requiring high cognitive demand amplified the differences in of verbal engagement between the conversational agent and adult partner. The study offers important implications for developing and researching conversational agent systems to support children's language development.",2021,,Computers & Education,161,,104059,,10.1016/j.compedu.2020.104059,,#9098,Xu 2021,,,1
Should I Trust the Artificial Intelligence to Recruit? Recruiters’ Perceptions and Behavior When Faced With Algorithm-Based Recommendation Systems During Resume Screening,"Lacroux, Alain; Martin-Lacroux, Christelle","Resume screening assisted by decision support systems that incorporate artificial intelligence is currently undergoing a strong development in many organizations, raising technical, managerial, legal and ethical issues. The purpose of the present paper is to better understand the reactions of recruiters when they are offered algorithm-based recommendations during resume screening.Two polarized attitudes have been identified in the literature on users' reactions to algorithm-based recommendations: algorithm aversion, which reflects a general distrust and preference for human recommendations; and automation bias, which corresponds to an overconfidence in the decisions or recommendations made by algorithmic decision support systems (ADSS). Drawing on results obtained in the field of automated decision support areas, we make the general hypothesis that recruiters trust human experts more than ADSS, because they distrust algorithms for subjective decisions such as recruitment. An experiment on résumé screening was conducted on a sample of professionals (N=694) involved in the screening of job applications. They were asked to study a job offer, then evaluate two fictitious résumés in a 2×2 factorial design with manipulation of the type of recommendation (no recommendation/algorithmic recommendation/human expert recommendation) and of the consistency of the recommendations (consistent vs. inconsistent recommendation).Our results support the general hypothesis of preference for human recommendations: recruiters exhibit a higher level of trust toward human expert recommendations compared with algorithmic recommendations. However, we also found that recommendation’s consistence has a differential and unexpected impact on decisions: in the presence of an inconsistent algorithmic recommendation, recruiters favored the unsuitable over the suitable résumé. Our results also show that specific personality traits (extraversion, neuroticism and self-confidence) are associated with a differential use of algorithmic recommendations. Implications for research and HR policies are finally discussed.",2022,,Frontiers in Psychology,13,,,,10.3389/fpsyg.2022.895997,,#9103,Lacroux 2022,,,1
Susceptibility to Influence of Large Language Models,"Griffin, Lewis D.; Kleinberg, Bennett; Mozes, Maximilian; Mai, Kimberly T.; Vau, Maria; Caldwell, Matthew; Marvor-Parker, Augustine","Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.",2023,,,,,,,10.48550/arXiv.2303.06074,,#9112,Griffin 2023,,,1
The Attenuating Effect of Intelligent Agents and Agent Autonomy on Managers' Ability to Diffuse Responsibility for and Engage in Earnings Management,"Kipp, Peter C.; Curtis, Mary B.; Li, Ziyin","Advances in IT suggest that computerized intelligent agents (IAs) may soon occupy many roles that presently employ human agents. A significant concern is the ethical conduct of those who use IAs, including their possible utilization by managers to engage in earnings management. We investigate how financial reporting decisions are affected when they are supported by the work of an IA versus a human agent, with varying autonomy. In an experiment with experienced managers, we vary agent type (human versus IA) and autonomy (more versus less), finding that managers engage in less aggressive financial reporting decisions with IAs than with human agents, and engage in less aggressive reporting decisions with less autonomous agents than with more autonomous agents. Managers' perception of control over their agent and ability to diffuse their own responsibility for financial reporting decisions explain the effect of agent type and autonomy on managers' financial reporting decisions.",2020,,Accounting Horizons,34,4,143-164,,10.2308/HORIZONS-19-133,,#9116,Kipp 2020,,,1
The Effect of Multimodal Emotional Expression and Agent Appearance on Trust in Human-Agent Interaction,"Torre, Ilaria; Carrigan, Emma; McDonnell, Rachel; Domijan, Katarina; McCabe, Killian; Harte, Naomi","Emotional expressivity can boost trust in human-human and human-machine interaction. As a multimodal phenomenon, previous research argued that a mismatch in the expressive channels provides evidence of joint audio-video emotional processing. However, while previous work studied this from the point of view of emotion recognition and processing, not much is known about what effect a multimodal agent would have on a human-agent interaction task. Also, agent appearance could influence this interaction too. Here we manipulated the agent’s multimodal emotional expression (”smiling face” and ”smiling voice”, or both) and agent type (photorealistic or cartoon-like virtual human) and assessed people’s trust toward this agent. We measured trust using a mixed-methods approach, combining behavioural data from a survival task, questionnaire ratings and qualitative comments. These methods gave different results: while people commented on the importance of emotional expressivity in the agent’s voice, this factor had limited influence on trusting behaviours; while people rated the cartoon-like agent on several traits higher than the photorealistic one, the agent’s style also was not the most influential feature on people’s trusting behaviour. These results highlight the contribution of a mixed-methods approach in human-machine interaction, as both explicit and implicit perception and behaviour will contribute to the success of the interaction.",2019,,"Proceedings of the 12th ACM SIGGRAPH Conference on Motion, Interaction and Games",,,01-Jun,,10.1145/3359566.3360065,,#9118,Torre 2019,,,1
The effect of source disclosure on evaluation of AI-generated messages: A two-part study,"Lim, Sue; Schmälzle, Ralf","Advancements in artificial intelligence (AI) over the last decade demonstrate that machines can exhibit communicative behavior and influence how humans think, feel, and behave. In fact, the recent development of ChatGPT has shown that large language models (LLMs) can be leveraged to generate high-quality communication content at scale and across domains, suggesting that they will be increasingly used in practice. However, many questions remain about how knowing the source of the messages influences recipients' evaluation of and preference for AI-generated messages compared to human-generated messages. This paper investigated this topic in the context of vaping prevention messaging. In Study 1, which was pre-registered, we examined the influence of source disclosure on people's evaluation of AI-generated health prevention messages compared to human-generated messages. We found that source disclosure (i.e., labeling the source of a message as AI vs. human) significantly impacted the evaluation of the messages but did not significantly alter message rankings. In a follow-up study (Study 2), we examined how the influence of source disclosure may vary by the participants' negative attitudes towards AI. We found a significant moderating effect of negative attitudes towards AI on message evaluation, but not for message selection. However, for those with moderate levels of negative attitudes towards AI, source disclosure decreased the preference for AI-generated messages. Overall, the results of this series of studies showed a slight bias against AI-generated messages once the source was disclosed, adding to the emerging area of study that lies at the intersection of AI and communication.",2024,,Computers in Human Behavior: Artificial Humans,2,1,100058,,10.1016/j.chbah.2024.100058,,#9119,Lim 2024,,,1
The Str(AI)ght Scoop: Artificial Intelligence Cues Reduce Perceptions of Hostile Media Bias,"Cloudy, Joshua; Banks, Jaime; Bowman, Nicholas David","In the face of increasing public distrust for journalistic institutions, stories sourced from artificially intelligent (AI) journalists have the potential to lower hostile media bias by activating the machine heuristic—a mental shortcut assuming machines are more unbiased, systematic, and accurate than are humans. An online experiment targeting issue partisans found support for the prediction: a story presented as sourced from an AI journalist activated the machine heuristic that, in turn, mitigated hostile media bias. This mediation effect was moderated: perceived bias was more strongly reduced as partisan-attitude extremity increased.",2023,,Digital Journalism,11,9,1577-1596,,10.1080/21670811.2021.1969974,,#9126,Cloudy 2023,,,1
Tools or peers? Impacts of anthropomorphism level and social role on emotional attachment and disclosure tendency towards intelligent agents,"Zhang, Andong; Patrick Rau, Pei-Luen","Owing to the development of anthropomorphic intelligent agent (IA) designs, users consider IAs as more than just inanimate tools. Previous studies have reported that anthropomorphic features can promote users' social feedback and aid in establishing intimate human–agent relationships. The present study examined the main and interaction effects of anthropomorphism level (a human-like IA vs. robot-like IA) and social role (servant vs. mentor) on emotional attachment, information disclosure tendency, and satisfaction in a smart home. The study participants were randomly assigned into four groups with balanced gender. The results indicate that high anthropomorphism and mentor role can positively predict users' emotional attachment. Additionally, users tend to disclose more personal information to the human-servant and robot-mentor IAs than the human-mentor and robot-servant IAs. Interestingly, social presence was determined to be a positive and significant mediator between anthropomorphic design and emotional attachment. The study findings highlight the importance of social role in anthropomorphic IA design and explain the mechanism of establishing effective human–agent relationships. Moreover, both theoretical and practical implications of these findings are analyzed.",2023,,Computers in Human Behavior,138,,107415,,10.1016/j.chb.2022.107415,,#9129,Zhang 2023,,,1
Trust in an AI versus a Human teammate: The effects of teammate identity and performance on Human-AI cooperation,"Zhang, Guanglu; Chong, Leah; Kotovsky, Kenneth; Cagan, Jonathan","Recent advances in artificial intelligence (AI) enable researchers to create more powerful AI agents that are becoming competent teammates for humans. However, human distrust of AI is a critical factor that may impede human-AI cooperation. Although AI agents have been endowed with anthropomorphic traits, such as a human-like appearance, in prior studies to improve human trust in AI, it is still an open question whether humans have more trust in an AI teammate and achieve better human-AI joint performance if they are deceived about the identity of their AI teammate as another human. This research assesses the effects of teammate identity (“human” vs. AI) and teammate performance (low-performing vs. high-performing AI) on human-AI cooperation through a human subjects study. The results of this study show that humans behaviorally trust the AI more than another human by accepting their AI teammate's decisions more often. In addition, teammate performance has a significant effect on human-AI joint performance in the study, while teammate identity does not. These results caution against deceiving humans about the identity of AI in future applications involving human-AI cooperation.",2023,,Computers in Human Behavior,139,,107536,,10.1016/j.chb.2022.107536,,#9137,Zhang 2023,,,1
Understanding Effects of Algorithmic vs. Community Label on Perceived Accuracy of Hyper-partisan Misinformation,"Jia, Chenyan; Boltz, Alexander; Zhang, Angie; Chen, Anqing; Lee, Min Kyung","Hyper-partisan misinformation has become a major public concern. In order to examine what type of misinformation label can mitigate hyper-partisan misinformation sharing on social media, we conducted a 4 (label type: algorithm, community, third-party fact-checker, and no label) X 2 (post ideology: liberal vs. conservative) between-subjects online experiment (N = 1,677) in the context of COVID-19 health information. The results suggest that for liberal users, all labels reduced the perceived accuracy and believability of fake posts regardless of the posts' ideology. In contrast, for conservative users, the efficacy of the labels depended on whether the posts were ideologically consistent: algorithmic labels were more effective in reducing the perceived accuracy and believability of fake conservative posts compared to community labels, whereas all labels were effective in reducing their belief in liberal posts. Our results shed light on the differing effects of various misinformation labels dependent on people's political ideology.",2022,,Proceedings of the ACM on Human-Computer Interaction,6,CSCW2,Jan-27,,10.1145/3555096,,#9138,Jia 2022,,,1
Use of the Chatbot “Vivibot” to Deliver Positive Psychology Skills and Promote Well-Being Among Young People After Cancer Treatment: Randomized Controlled Feasibility Trial,"Greer, Stephanie; Ramo, Danielle; Chang, Yin-Juei; Fu, Michael; Moskowitz, Judith; Haritatos, Jana","Background Positive psychology interventions show promise for reducing psychosocial distress associated with health adversity and have the potential to be widely disseminated to young adults through technology. Objective This pilot randomized controlled trial examined the feasibility of delivering positive psychology skills via the Vivibot chatbot and its effects on key psychosocial well-being outcomes in young adults treated for cancer. Methods Young adults (age 18-29 years) were recruited within 5 years of completing active cancer treatment by using the Vivibot chatbot on Facebook messenger. Participants were randomized to either immediate access to Vivibot content (experimental group) or access to only daily emotion ratings and access to full chatbot content after 4 weeks (control). Created using a human-centered design process with young adults treated for cancer, Vivibot content includes 4 weeks of positive psychology skills, daily emotion ratings, video, and other material produced by survivors, and periodic feedback check-ins. All participants were assessed for psychosocial well-being via online surveys at baseline and weeks 2, 4, and 8. Analyses examined chatbot engagement and open-ended feedback on likability and perceived helpfulness and compared experimental and control groups with regard to anxiety and depression symptoms and positive and negative emotion changes between baseline and 4 weeks. To verify the main effects, follow-up analyses compared changes in the main outcomes between 4 and 8 weeks in the control group once participants had access to all chatbot content. Results Data from 45 young adults (36 women; mean age: 25 [SD 2.9]; experimental group: n=25; control group: n=20) were analyzed. Participants in the experimental group spent an average of 74 minutes across an average of 12 active sessions chatting with Vivibot and rated their experience as helpful (mean 2.0/3, SD 0.72) and would recommend it to a friend (mean 6.9/10; SD 2.6). Open-ended feedback noted its nonjudgmental nature as a particular benefit of the chatbot. After 4 weeks, participants in the experimental group reported an average reduction in anxiety of 2.58 standardized t-score units, while the control group reported an increase in anxiety of 0.7 units. A mixed-effects models revealed a trend-level (P=.09) interaction between group and time, with an effect size of 0.41. Those in the experimental group also experienced greater reductions in anxiety when they engaged in more sessions (z=–1.9, P=.06). There were no significant (or trend level) effects by group on changes in depression, positive emotion, or negative emotion. Conclusions The chatbot format provides a useful and acceptable way of delivering positive psychology skills to young adults who have undergone cancer treatment and supports anxiety reduction. Further analysis with a larger sample size is required to confirm this pattern.",2019,,JMIR mHealth and uHealth,7,10,e15018,,10.2196/15018,,#9140,Greer 2019,,,1
Using Psychological Artificial Intelligence (Tess) to Relieve Symptoms of Depression and Anxiety: Randomized Controlled Trial,"Fulmer, Russell; Joerin, Angela; Gentile, Breanna; Lakerink, Lysanne; Rauws, Michiel","Background: Students in need of mental health care face many barriers including cost, location, availability, and stigma. Studies show that computer-assisted therapy and 1 conversational chatbot delivering cognitive behavioral therapy (CBT) offer a less-intensive and more cost-effective alternative for treating depression and anxiety. Although CBT is one of the most effective treatment methods, applying an integrative approach has been linked to equally effective posttreatment improvement. Integrative psychological artificial intelligence (AI) offers a scalable solution as the demand for affordable, convenient, lasting, and secure support grows. Objective: This study aimed to assess the feasibility and efficacy of using an integrative psychological AI, Tess, to reduce self-identified symptoms of depression and anxiety in college students. Methods: In this randomized controlled trial, 75 participants were recruited from 15 universities across the United States. All participants completed Web-based surveys, including the Patient Health Questionnaire (PHQ-9), Generalized Anxiety Disorder Scale (GAD-7), and Positive and Negative Affect Scale (PANAS) at baseline and 2 to 4 weeks later (T2). The 2 test groups consisted of 50 participants in total and were randomized to receive unlimited access to Tess for either 2 weeks (n=24) or 4 weeks (n=26). The information-only control group participants (n=24) received an electronic link to the National Institute of Mental Health’s (NIMH) eBook on depression among college students and were only granted access to Tess after completion of the study. Results: A sample of 74 participants completed this study with 0% attrition from the test group and less than 1% attrition from the control group (1/24). The average age of participants was 22.9 years, with 70% of participants being female (52/74), mostly Asian (37/74, 51%), and white (32/74, 41%). Group 1 received unlimited access to Tess, with daily check-ins for 2 weeks. Group 2 received unlimited access to Tess with biweekly check-ins for 4 weeks. The information-only control group was provided with an electronic link to the NIMH’s eBook. Multivariate analysis of covariance was conducted. We used an alpha level of .05 for all statistical tests. Results revealed a statistically significant difference between the control group and group 1, such that group 1 reported a significant reduction in symptoms of depression as measured by the PHQ-9 (P=.03), whereas those in the control group did not. A statistically significant difference was found between the control group and both test groups 1 and 2 for symptoms of anxiety as measured by the GAD-7. Group 1 (P=.045) and group 2 (P=.02) reported a significant reduction in symptoms of anxiety, whereas the control group did not. A statistically significant difference was found on the PANAS between the control group and group 1 (P=.03) and suggests that Tess did impact scores. Conclusions: This study offers evidence that AI can serve as a cost-effective and accessible therapeutic agent. Although not designed to appropriate the role of a trained therapist, integrative psychological AI emerges as a feasible option for delivering support. Trial Registration: International Standard Randomized Controlled Trial Number: ISRCTN61214172; https://doi.org/10.1186/ISRCTN61214172.",2018,,JMIR Mental Health,5,4,e9782,,10.2196/mental.9782,,#9142,Fulmer 2018,,,1
Watch Me Improve—Algorithm Aversion and Demonstrating the Ability to Learn,"Berger, Benedikt; Adam, Martin; Rühr, Alexander; Benlian, Alexander","Owing to advancements in artificial intelligence (AI) and specifically in machine learning, information technology (IT) systems can support humans in an increasing number of tasks. Yet, previous research indicates that people often prefer human support to support by an IT system, even if the latter provides superior performance – a phenomenon called algorithm aversion. A possible cause of algorithm aversion put forward in literature is that users lose trust in IT systems they become familiar with and perceive to err, for example, making forecasts that turn out to deviate from the actual value. Therefore, this paper evaluates the effectiveness of demonstrating an AI-based system’s ability to learn as a potential countermeasure against algorithm aversion in an incentive-compatible online experiment. The experiment reveals how the nature of an erring advisor (i.e., human vs. algorithmic), its familiarity to the user (i.e., unfamiliar vs. familiar), and its ability to learn (i.e., non-learning vs. learning) influence a decision maker’s reliance on the advisor’s judgement for an objective and non-personal decision task. The results reveal no difference in the reliance on unfamiliar human and algorithmic advisors, but differences in the reliance on familiar human and algorithmic advisors that err. Demonstrating an advisor’s ability to learn, however, offsets the effect of familiarity. Therefore, this study contributes to an enhanced understanding of algorithm aversion and is one of the first to examine how users perceive whether an IT system is able to learn. The findings provide theoretical and practical implications for the employment and design of AI-based systems.",2021,,Business & Information Systems Engineering,63,1,55-68,,10.1007/s12599-020-00678-5,,#9143,Berger 2021,,,1
What to expect from opening up ‘black boxes’? Comparing perceptions of justice between human and automated agents,"Schlicker, Nadine; Langer, Markus; Ötting, Sonja K.; Baum, Kevin; König, Cornelius J.; Wallach, Dieter","Advances in artificial intelligence contribute to increasing automation of decisions. In a healthcare-scheduling context, this study compares effects of decision agents and explanations for decisions on decision-recipients’ perceptions of justice. In a 2 (decision agent: automated vs. human) × 3 (explanation: no explanation vs. equality-explanation vs. equity-explanation) between-subjects online study, 209 healthcare professionals were asked to put themselves in a situation where their vacation request was denied by either a human or an automated agent. Participants either received no explanation or an explanation based on equality or equity norms. Perceptions of interpersonal justice were stronger for the human agent. Additionally, participants perceived human agents as offering more voice and automated agents as being more consistent in decision-making. When given no explanation, perceptions of informational justice were impaired only for the human decision agent. In the study's second part, participants took the perspective of a decision-maker and were given the choice to delegate decision-making to an automated system. Participants who delegated an unpleasant decision to the system frequently externalized responsibility and showed different response patterns when confronted by a decision-recipient who asked for a rationale for the decision.",2021,,Computers in Human Behavior,122,,106837,,10.1016/j.chb.2021.106837,,#9144,Schlicker 2021,,,1
"When expert recommendation contradicts peer opinion: Relative social influence of valence, group identity and artificial intelligence","Wang, Jinping; Molina, Maria D.; Sundar, S. Shyam","Whom do we trust more, the recommendation of an expert or public opinion from a crowd of other users of the site? Does it matter if the expert belongs to our in-group? And, what, if anything, would change if an Artificial Intelligence (AI) system was the recommender rather than a human expert? In order to answer these research questions, we conducted a between-subjects online experiment, informed by MAIN Model (Sundar, 2008), which posits that interface cues signaling different types of sources can influence perceived credibility of content by triggering distinct cognitive heuristics. Participants were assigned to a scenario wherein the expert review contrasted the peer rating about recommending photos for business profiles, with systematic variations in expert review valence (negative vs. positive), expert identity (ingroup vs. outgroup vs. no identity), and agent type (human vs. AI). Results show that positive ratings are more influential on user judgements. However, for negative ratings, human ingroup members generated greater effects than no-identity experts. Moreover, AI systems were as influential as human experts, suggesting the potential for AI to substitute human experts for online recommendations.",2020,,Computers in Human Behavior,107,,106278,,10.1016/j.chb.2020.106278,,#9146,Wang 2020,,,1
When humans and computers induce social stress through negative feedback: Effects on performance and subjective state,"Thuillard, S.; Adams, M.; Jelmini, G.; Schmutz, S.; Sonderegger, A.; Sauer, J.","People increasingly work with autonomous systems, which progressively take over functions previously performed exclusively by humans. This may lead to situations in which automated agents give negative performance feedback, which represents an important work-related social stressor. Little is known about how negative feedback provided by computers (as opposed to humans) affects human performance and subjective state. A first experiment (N = 60) focused on the influence of human feedback on performance. After participants had performed a cognitive task, they received a manipulated performance feedback (either positive or negative) from a human (comparing to a control with no feedback) and subsequent performance on several cognitive tasks and the participants' subjective state was measured. The results showed that while negative feedback had a negative influence on several subjective state measures, performance remained unimpaired. In a second experiment (N = 89), participants received manipulated negative feedback by a human or by a computer (or no feedback at all) after having completed an ability test. Subsequent performance was measured on attention tasks and creativity tasks and participants' subjective state was assessed. Although participants felt stressed by both negative computer and human feedback, subsequent performance was again not impaired. However, computer feedback was rated as being less fair than human feedback. Overall, our findings show that there are costs of protecting one's performance against negative feedback and they call for caution regarding the use of negative feedback by both human and automated agents in work settings.",2022,,Computers in Human Behavior,133,,107270,,10.1016/j.chb.2022.107270,,#9147,Thuillard 2022,,,1
