<?xml version="1.0" encoding="UTF-8"?>
<xml><records><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Goldstein, Josh A</author><author>Chao, Jason</author><author>Grossman, Shelby</author><author>Stamos, Alex</author><author>Tomz, Michael</author></authors></contributors><titles><title>How persuasive is AI-generated propaganda?</title><secondary-title>PNAS Nexus</secondary-title></titles><periodical><full-title>PNAS Nexus</full-title><abbr-1>PNAS Nexus</abbr-1></periodical><pages>pgae034</pages><volume>3</volume><number>2</number><issue>2</issue><dates><year>2024</year><pub-dates><date>2024-02-01</date></pub-dates></dates><isbn>2752-6542</isbn><electronic-resource-num>10.1093/pnasnexus/pgae034</electronic-resource-num><abstract>Can large language models, a form of artificial intelligence (AI), generate persuasive propaganda? We conducted a preregistered survey experiment of US respondents to investigate the persuasiveness of news articles written by foreign propagandists compared to content generated by GPT-3 davinci (a large language model). We found that GPT-3 can create highly persuasive text as measured by participants’ agreement with propaganda theses. We further investigated whether a person fluent in English could improve propaganda persuasiveness. Editing the prompt fed to GPT-3 and/or curating GPT-3’s output made GPT-3 even more persuasive, and, under certain conditions, as persuasive as the original propaganda. Our findings suggest that propagandists could use AI to create convincing content with limited effort.</abstract><remote-database-name>Silverchair</remote-database-name><urls><web-urls><url>https://doi.org/10.1093/pnasnexus/pgae034</url></web-urls><pdf-urls><url>internal-pdf://7890/Goldstein и др. - 2024 - How persuasive is AI-generated propaganda.pdf</url></pdf-urls><text-urls><url>internal-pdf://7891/7610937.html</url></text-urls></urls><access-date>2025-06-05 11:10:01</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Chae, Je Hoon</author><author>Tewksbury, David</author></authors></contributors><titles><title>Perceiving AI intervention does not compromise the persuasive effect of fact-checking</title><secondary-title>New Media &amp; Society</secondary-title></titles><periodical><full-title>New Media &amp; Society</full-title></periodical><pages>14614448241286881</pages><dates><year>2024</year><pub-dates><date>2024-10-21</date></pub-dates></dates><isbn>1461-4448</isbn><electronic-resource-num>10.1177/14614448241286881</electronic-resource-num><abstract>Efforts to scale up fact-checking through technology, such as artificial intelligence (AI), are increasingly being suggested and tested. This study examines whether previously observed effects of reading fact-checks remain constant when readers are aware of AI’s involvement in the fact-checking process. We conducted three online experiments (N = 3,978), exposing participants to fact-checks identified as either human-generated or AI-assisted, simulating cases where AI fully generates the fact-check or automatically retrieves human fact-checks. Our findings indicate that the persuasive effect of fact-checking, specifically in increasing truth discernment, persists even among participants without a positive prior attitude toward AI. Additionally, in some cases, awareness of AI’s role reduced perceived political bias in fact-checks among Republicans. Finally, neither AI-generated nor human fact-checks significantly affected participants’ feelings toward or their perceptions of the competence of the targeted politicians.</abstract><remote-database-name>SAGE Journals</remote-database-name><language>EN</language><urls><web-urls><url>https://doi.org/10.1177/14614448241286881</url></web-urls><pdf-urls><url>internal-pdf://7893/Chae и Tewksbury - 2024 - Perceiving AI intervention does not compromise the.pdf</url></pdf-urls></urls><access-date>2025-06-05 11:10:28</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>Schneider, Johannes</author><author>Haag, Steffi</author><author>Kruse, Leona Chandra</author></authors><secondary-authors><author>Plácido da Silva, Hugo</author><author>Cipresso, Pietro</author></secondary-authors></contributors><titles><title>Negotiating with LLMs: Prompt Hacks, Skill Gaps, and Reasoning Deficits</title><short-title>Negotiating with LLMs</short-title></titles><periodical><full-title>Computer-Human Interaction Research and Applications</full-title></periodical><pages>238-259</pages><keywords><keyword>Large language model</keyword><keyword>Negotiation</keyword><keyword>Prompt Hacking</keyword><keyword>Reasoning</keyword></keywords><dates><year>2025</year><pub-dates><date>2025</date></pub-dates></dates><publisher>Springer Nature Switzerland</publisher><isbn>978-3-031-83845-3</isbn><electronic-resource-num>10.1007/978-3-031-83845-3_15</electronic-resource-num><abstract>Large language models (LLMs) like ChatGPT have reached the 100 million users’ barrier in record time and might increasingly enter all areas of our life leading to a diverse set of interactions between those Artificial Intelligence models and humans. While many studies have discussed governance and regulations deductively from first order principles, few studies provide an inductive, data-driven lens based on observing dialogues between humans and LLMs – especially, when it comes to non-collaborative, competitive situations that have the potential to pose a serious threat for people. In this work, we conduct a user study engaging 41 individuals across all age groups in price negotiations with an LLM. We explore how people interact with an LLM, investigating differences in negotiation outcomes and strategies. Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality. We also show that the negotiated prices humans manage to achieve span a broad range, which points to a literacy gap in effectively interacting with LLMs.</abstract><remote-database-name>Springer Link</remote-database-name><language>en</language><urls><pdf-urls><url>internal-pdf://8082/Schneider et al. - 2025 - Negotiating with LLMs Prompt Hacks, Skill Gaps, a.pdf</url></pdf-urls></urls><custom1>Cham</custom1><custom3>Computer-Human Interaction Research and Applications</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Wack, Morgan</author><author>Ehrett, Carl</author><author>Linvill, Darren</author><author>Warren, Patrick</author></authors></contributors><titles><title>Generative propaganda: Evidence of AI’s impact from a state-backed disinformation campaign</title><secondary-title>PNAS Nexus</secondary-title><short-title>Generative propaganda</short-title></titles><periodical><full-title>PNAS Nexus</full-title><abbr-1>PNAS Nexus</abbr-1></periodical><pages>pgaf083</pages><volume>4</volume><number>4</number><issue>4</issue><dates><year>2025</year><pub-dates><date>2025-04-01</date></pub-dates></dates><isbn>2752-6542</isbn><electronic-resource-num>10.1093/pnasnexus/pgaf083</electronic-resource-num><abstract>Can AI bolster state-backed propaganda campaigns, in practice? Growing use of AI and large language models has drawn attention to the potential for accompanying tools to be used by malevolent actors. Though recent laboratory and experimental evidence has substantiated these concerns in principle, the usefulness of AI tools in the production of propaganda campaigns has remained difficult to ascertain. Drawing on the adoption of generative-AI techniques by a state-affiliated propaganda site with ties to Russia, we test whether AI adoption enabled the website to amplify and enhance its production of disinformation. First, we find that the use of generative-AI tools facilitated the outlet’s generation of larger quantities of disinformation. Second, we find that use of generative-AI coincided with shifts in the volume and breadth of published content. Finally, drawing on a survey experiment comparing perceptions of articles produced prior to and following the adoption of AI tools, we show that the AI-assisted articles maintained their persuasiveness in the postadoption period. Our results illustrate how generative-AI tools have already begun to alter the size and scope of state-backed propaganda campaigns.</abstract><remote-database-name>Silverchair</remote-database-name><urls><web-urls><url>https://doi.org/10.1093/pnasnexus/pgaf083</url></web-urls><pdf-urls><url>internal-pdf://7897/Wack и др. - 2025 - Generative propaganda Evidence of AI’s impact fro.pdf</url></pdf-urls><text-urls><url>internal-pdf://7898/8097936.html</url></text-urls></urls><access-date>2025-06-05 11:11:30</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Allan, Kevin</author><author>Azcona, Jacobo</author><author>Sripada, Somayajulu</author><author>Leontidis, Georgios</author><author>Sutherland, Clare A. M.</author><author>Phillips, Louise H.</author><author>Martin, Douglas</author></authors></contributors><titles><title>Stereotypical bias amplification and reversal in an experimental model of human interaction with generative artificial intelligence</title><secondary-title>Royal Society Open Science</secondary-title></titles><periodical><full-title>Royal Society Open Science</full-title></periodical><pages>241472</pages><volume>12</volume><number>4</number><issue>4</issue><keywords><keyword>bias amplification</keyword><keyword>bias in AI</keyword><keyword>generative AI</keyword><keyword>human–AI interaction</keyword><keyword>large language models</keyword><keyword>stereotypes</keyword></keywords><dates><year>2025</year><pub-dates><date>2025-04-09</date></pub-dates></dates><electronic-resource-num>10.1098/rsos.241472</electronic-resource-num><abstract>Stereotypical biases are readily acquired and expressed by generative artificial intelligence (AI), causing growing societal concern about these systems amplifying existing human bias. This concern rests on reasonable psychological assumptions, but stereotypical bias amplification during human–AI interaction relative to pre-existing baseline levels has not been demonstrated. Here, we use previous psychological work on gendered character traits to capture and control gender stereotypes expressed in character descriptions generated by Open AI’s GPT3.5. In four experiments (N = 782) with a first impressions task, we find that unexplained (‘black-box’) character recommendations using stereotypical traits already convey a potent persuasive influence significantly amplifying baseline stereotyping within first impressions. Recommendations that are counter-stereotypical eliminate and effectively reverse human baseline bias, but these stereotype-challenging influences propagate less well than reinforcing influences from stereotypical recommendations. Critically, the bias amplification and reversal phenomena occur when GPT3.5 elaborates on the core stereotypical content, although GPT3.5’s explanations propagate counter-stereotypical influence more effectively and persuasively than black-box recommendations. Our findings strongly imply that without robust safeguards, generative AI will amplify existing bias. But with safeguards, existing bias can be eliminated and even reversed. Our novel approach safely allows such effects to be studied in various contexts where gender and other bias-inducing social stereotypes operate.</abstract><remote-database-name>royalsocietypublishing.org (Atypon)</remote-database-name><urls><web-urls><url>https://royalsocietypublishing.org/doi/10.1098/rsos.241472</url></web-urls><pdf-urls><url>internal-pdf://7900/Allan и др. - 2025 - Stereotypical bias amplification and reversal in a.pdf</url></pdf-urls></urls><access-date>2025-06-05 11:11:56</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Ahn, Jungyong</author><author>Kim, Jungwon</author><author>Sung, Yongjun</author></authors></contributors><titles><title>The effect of gender stereotypes on artificial intelligence recommendations</title><secondary-title>Journal of Business Research</secondary-title></titles><periodical><full-title>Journal of Business Research</full-title><abbr-1>Journal of Business Research</abbr-1></periodical><pages>50-59</pages><volume>141</volume><keywords><keyword>AI agent</keyword><keyword>AI recommendations</keyword><keyword>Artificial Intelligence (AI)</keyword><keyword>Gender stereotypes</keyword></keywords><dates><year>2022</year><pub-dates><date>2022-03-01</date></pub-dates></dates><isbn>0148-2963</isbn><electronic-resource-num>10.1016/j.jbusres.2021.12.007</electronic-resource-num><abstract>This study explores the effects of gender stereotypes on evaluating artificial intelligence (AI) recommendations. We predict that gender stereotypes will affect human-AI interactions, resulting in somewhat different persuasive effects of AI recommendations for utilitarian vs. hedonic products. We found that participants in the male AI agent condition gave higher competence scores than in the female AI agent condition. Contrariwise, perceived warmth was higher in the female AI agent condition than in the male condition. More importantly, a significant interaction effect between AI gender and product type was found, suggesting that participants showed more positive attitudes toward the AI recommendations when the male AI recommended a utilitarian (vs. hedonic) product. Conversely, a hedonic product was evaluated more positively when advised by the female (vs. male) AI agent.</abstract><remote-database-name>ScienceDirect</remote-database-name><urls><web-urls><url>https://www.sciencedirect.com/science/article/pii/S0148296321009206</url></web-urls><pdf-urls><url>internal-pdf://7905/Ahn и др. - 2022 - The effect of gender stereotypes on artificial int.pdf</url></pdf-urls><text-urls><url>internal-pdf://7902/S0148296321009206.html</url></text-urls></urls><access-date>2025-06-05 11:12:13</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Hackenburg, Kobi</author><author>Tappin, Ben M.</author><author>Röttger, Paul</author><author>Hale, Scott A.</author><author>Bright, Jonathan</author><author>Margetts, Helen</author></authors></contributors><titles><title>Scaling language model size yields diminishing returns for single-message political persuasion</title><secondary-title>Proceedings of the National Academy of Sciences</secondary-title></titles><periodical><full-title>Proceedings of the National Academy of Sciences</full-title></periodical><pages>e2413443122</pages><volume>122</volume><number>10</number><issue>10</issue><dates><year>2025</year><pub-dates><date>2025-03-11</date></pub-dates></dates><electronic-resource-num>10.1073/pnas.2413443122</electronic-resource-num><abstract>Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 US political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25,982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence that model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are only slightly more persuasive than models smaller in size by an order of magnitude or more. Second, we find that the association between language model size and persuasiveness shrinks toward zero and is no longer statistically significant once we adjust for mere task completion (coherence, staying on topic), a pattern that highlights task completion as a potential mediator of larger models’ persuasive advantage. Given that current frontier models are already at ceiling on this task completion metric in our setting, taken together, our results suggest that further scaling model size may not much increase the persuasiveness of static LLM-generated political messages.</abstract><remote-database-name>pnas.org (Atypon)</remote-database-name><urls><web-urls><url>https://www.pnas.org/doi/10.1073/pnas.2413443122</url></web-urls><pdf-urls><url>internal-pdf://7907/Hackenburg и др. - 2025 - Scaling language model size yields diminishing ret.pdf</url></pdf-urls></urls><access-date>2025-06-05 11:21:39</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>Fang, Jingchao</author><author>Arechiga, Nikos</author><author>Namikoshi, Keiichi</author><author>Bravo, Nayeli</author><author>Hogan, Candice</author><author>Shamma, David A.</author></authors></contributors><titles><title>On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments</title><tertiary-title>IVA '24</tertiary-title><short-title>On LLM Wizards</short-title></titles><periodical><full-title>Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents</full-title></periodical><pages>1–11</pages><dates><year>2024</year><pub-dates><date>Декабрь 26, 2024</date></pub-dates></dates><publisher>Association for Computing Machinery</publisher><isbn>9798400706257</isbn><electronic-resource-num>10.1145/3652988.3673967</electronic-resource-num><abstract>The Wizard of Oz (WoZ) method is a widely adopted research approach where a human Wizard “role-plays” a not readily available technology and interacts with participants to elicit user behaviors and probe the design space. With the growing ability for modern large language models (LLMs) to role-play, one can apply LLMs as Wizards in WoZ experiments with better scalability and lower cost than the traditional approach. However, methodological guidance on responsibly applying LLMs in WoZ experiments and a systematic evaluation of LLMs’ role-playing ability are lacking. Through two LLM-powered WoZ studies, we take the first step towards identifying an experiment lifecycle for researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings that involve Wizards role-played by LLMs. We also contribute a heuristic-based evaluation framework that allows the estimation of LLMs’ role-playing ability in WoZ experiments and reveals LLMs’ behavior patterns at scale.</abstract><remote-database-name>ACM Digital Library</remote-database-name><urls><web-urls><url>https://dl.acm.org/doi/10.1145/3652988.3673967</url></web-urls><pdf-urls><url>internal-pdf://7915/Fang и др. - 2024 - On LLM Wizards Identifying Large Language Models'.pdf</url></pdf-urls></urls><access-date>2025-06-05</access-date><custom1>New York, NY, USA</custom1><custom3>Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Ahn, Jungyong</author><author>, Kim ,Jungwon</author><author>and Sung, Yongjun</author></authors></contributors><titles><title>AI-powered recommendations: the roles of perceived similarity and psychological distance on persuasion</title><secondary-title>International Journal of Advertising</secondary-title><short-title>AI-powered recommendations</short-title></titles><periodical><full-title>International Journal of Advertising</full-title></periodical><pages>1366-1384</pages><volume>40</volume><number>8</number><issue>8</issue><keywords><keyword>Artificial intelligence (AI)</keyword><keyword>anthropomorphism</keyword><keyword>construal level theory (CLT)</keyword><keyword>psychological distance</keyword><keyword>recommendation agent</keyword></keywords><dates><year>2021</year><pub-dates><date>2021-12-30</date></pub-dates></dates><isbn>0265-0487</isbn><electronic-resource-num>10.1080/02650487.2021.1982529</electronic-resource-num><abstract>Artificial intelligence (AI) plays various roles in our daily lives, such as personal assistant, salesperson, and virtual counselors; thus, it stands out in various fields as a recommendation agent. This study explored the effects of perceived similarity and psychological distance on the persuasion of AI recommendation agents through two experiments. Results of Experiment 1 elucidated that individuals feel more psychologically distant when they interact with AI recommendation agents than with human agents as a result of a different level of perceived similarity. Furthermore, psychological distance plays a mediating role in determining the effectiveness of desirability- vs. feasibility-focused messages in health-related issues. In Experiment 2, we manipulated the AI speaker's level of perceived similarity via anthropomorphism and found that the AI's recommendation with secondary (vs. primary) features is more effective when AI is humanized, and the reverse was found in non-humanized AI conditions. Both theoretical and managerial implications are provided.</abstract><remote-database-name>Taylor and Francis+NEJM</remote-database-name><urls><web-urls><url>https://doi.org/10.1080/02650487.2021.1982529</url></web-urls><pdf-urls><url>internal-pdf://7919/Ahn и др. - 2021 - AI-powered recommendations the roles of perceived.pdf</url></pdf-urls></urls><access-date>2025-06-05 11:25:09</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Xia, Dengke</author><author>Song, Mengyao</author><author>Zhu, Tingshao</author></authors></contributors><titles><title>A comparison of the persuasiveness of human and ChatGPT generated pro-vaccine messages for HPV</title><secondary-title>Frontiers in Public Health</secondary-title></titles><periodical><full-title>Frontiers in Public Health</full-title><abbr-1>Front Public Health</abbr-1></periodical><pages>1515871</pages><volume>12</volume><dates><year>2025</year><pub-dates><date>2025-1-16</date></pub-dates></dates><isbn>2296-2565</isbn><electronic-resource-num>10.3389/fpubh.2024.1515871</electronic-resource-num><abstract>Introduction&#xD;Public health messaging is crucial for promoting beneficial health outcomes, and the latest advancements in artificial intelligence offer new opportunities in this field. This study aimed to evaluate the effectiveness of ChatGPT-4 in generating pro-vaccine messages on different topics for Human Papillomavirus (HPV) vaccination.&#xD;&#xD;Methods&#xD;In this study (N = 60), we examined the persuasive effect of pro-vaccine messages generated by GPT-4 and humans, which were constructed based on 17 factors impacting HPV vaccination. Paired-samples t-tests were used to compare the persuasiveness of these messages.&#xD;&#xD;Results&#xD;GPT-generated messages were reported as more persuasive than human-generated messages on some influencing factors (e.g., untoward effect, stigmatized perception). Human-generated messages performed better on the message regarding convenience of vaccination.&#xD;&#xD;Discussion&#xD;This study provides evidence for the viability of ChatGPT, in generating persuasive pro-vaccine messages to influence people’s vaccine attitudes. It is indicated that the feasibility and efficiency of using AI for public health communication.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11780328/</url></web-urls><pdf-urls><url>internal-pdf://7929/Xia и др. - 2025 - A comparison of the persuasiveness of human and Ch.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11780328/</url></text-urls></urls><access-date>2025-06-06 11:56:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Dehnert, Marco</author><author>Mongeau, Paul A</author></authors></contributors><titles><title>Persuasion in the Age of Artificial Intelligence (AI): Theories and Complications of AI-Based Persuasion</title><secondary-title>Human Communication Research</secondary-title><short-title>Persuasion in the Age of Artificial Intelligence (AI)</short-title></titles><periodical><full-title>Human Communication Research</full-title><abbr-1>Human Communication Research</abbr-1></periodical><pages>386-403</pages><volume>48</volume><number>3</number><issue>3</issue><dates><year>2022</year><pub-dates><date>2022-07-01</date></pub-dates></dates><isbn>1468-2958</isbn><electronic-resource-num>10.1093/hcr/hqac006</electronic-resource-num><abstract>Artificial intelligence (AI) has profound implications for both communication and persuasion. We consider how AI complicates and promotes rethinking of persuasion theory and research. We define AI-based persuasion as a symbolic process in which a communicative-AI entity generates, augments, or modifies a message—designed to convince people to shape, reinforce, or change their responses—that is transmitted to human receivers. We review theoretical perspectives useful for studying AI-based persuasion—the Computers Are Social Actors (CASA) paradigm, the Modality, Agency, Interactivity, and Navigability (MAIN) model, and the heuristic-systematic model of persuasion—to explicate how differences in AI complicate persuasion in two ways. First, thin AI exhibits few (if any) machinic (i.e., AI) cues, social cues might be available, and communication is limited and indirect. Second, thick AI exhibits ample machinic and social cues, AI presence is obvious, and communication is direct and interactive. We suggest avenues for future research in each case.</abstract><remote-database-name>Silverchair</remote-database-name><urls><web-urls><url>https://doi.org/10.1093/hcr/hqac006</url></web-urls><pdf-urls><url>internal-pdf://7931/Dehnert и Mongeau - 2022 - Persuasion in the Age of Artificial Intelligence (.pdf</url></pdf-urls><text-urls><url>internal-pdf://7932/6564679.html</url></text-urls></urls><access-date>2025-06-10 08:20:35</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Schmälzle, Ralf</author><author>Wilcox, Shelby</author></authors></contributors><titles><title>Harnessing Artificial Intelligence for Health Message Generation: The Folic Acid Message Engine</title><secondary-title>Journal of Medical Internet Research</secondary-title><short-title>Harnessing Artificial Intelligence for Health Message Generation</short-title></titles><periodical><full-title>Journal of Medical Internet Research</full-title><abbr-1>J Med Internet Res</abbr-1></periodical><pages>e28858</pages><volume>24</volume><number>1</number><issue>1</issue><dates><year>2022</year><pub-dates><date>2022-1-18</date></pub-dates></dates><isbn>1439-4456</isbn><electronic-resource-num>10.2196/28858</electronic-resource-num><abstract>Background&#xD;Communication campaigns using social media can raise public awareness; however, they are difficult to sustain. A barrier is the need to generate and constantly post novel but on-topic messages, which creates a resource-intensive bottleneck.&#xD;&#xD;Objective&#xD;In this study, we aim to harness the latest advances in artificial intelligence (AI) to build a pilot system that can generate many candidate messages, which could be used for a campaign to suggest novel, on-topic candidate messages. The issue of folic acid, a B-vitamin that helps prevent major birth defects, serves as an example; however, the system can work with other issues that could benefit from higher levels of public awareness.&#xD;&#xD;Methods&#xD;We used the Generative Pretrained Transformer-2 architecture, a machine learning model trained on a large natural language corpus, and fine-tuned it using a data set of autodownloaded tweets about #folicacid. The fine-tuned model was then used as a message engine, that is, to create new messages about this topic. We conducted a web-based study to gauge how human raters evaluate AI-generated tweet messages compared with original, human-crafted messages.&#xD;&#xD;Results&#xD;We found that the Folic Acid Message Engine can easily create several hundreds of new messages that appear natural to humans. Web-based raters evaluated the clarity and quality of a human-curated sample of AI-generated messages as on par with human-generated ones. Overall, these results showed that it is feasible to use such a message engine to suggest messages for web-based campaigns that focus on promoting awareness.&#xD;&#xD;Conclusions&#xD;The message engine can serve as a starting point for more sophisticated AI-guided message creation systems for health communication. Beyond the practical potential of such systems for campaigns in the age of social media, they also hold great scientific potential for the quantitative analysis of message characteristics that promote successful communication. We discuss future developments and obvious ethical challenges that need to be addressed as AI technologies for health persuasion enter the stage.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8808340/</url></web-urls><pdf-urls><url>internal-pdf://7936/Schmälzle и Wilcox - 2022 - Harnessing Artificial Intelligence for Health Mess.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8808340/</url></text-urls></urls><access-date>2025-06-10 13:27:34</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Mendel, T</author><author>Singh, N</author><author>Mann, DM</author><author>Wiesenfeld, B</author><author>Nov, O</author></authors></contributors><titles><title>Laypeople's Use of and Attitudes Toward Large Language Models and Search Engines for Health Queries: Survey Study</title><secondary-title>JOURNAL OF MEDICAL INTERNET RESEARCH</secondary-title></titles><periodical><full-title>JOURNAL OF MEDICAL INTERNET RESEARCH</full-title><abbr-1>JOURNAL OF MEDICAL INTERNET RESEARCH</abbr-1></periodical><volume>27</volume><dates><year>2025</year><pub-dates><date>2025</date></pub-dates></dates><electronic-resource-num>10.2196/64290</electronic-resource-num><abstract>Background: Laypeople have easy access to health information through large language models (LLMs), such as ChatGPT, and search engines, such as Google. Search engines transformed health information access, and LLMs offer a new avenuefor answering laypeople's questions. Objective: We aimed to comparethefrequency of use and attitudes toward LLMsand search engines as well as their comparative relevance, usefulness, ease of use, and trustworthiness in responding to health queries. Methods: We conducted a screening survey to comparethe demographics of LLM users and nonusers seeking health information, analyzing results with logistic regression. LLM users from the screening survey were invited to a follow-up survey to report the types of health information they sought. We compared the frequency of use of LLMs and search engines using ANOVA and Tukey post hoc tests. Lastly, paired-sample Wilcoxon tests compared LLMsand search engines on perceived usefulness, ease of use, trustworthiness, feelings, bias, and anthropomorphism. Results: In total, 2002 US participants recruited on Prolific participated in the screening survey about the use of LLMs and search engines. Of them, 52% (n=1045) of the participants were female, with a mean age of 39 (SD 13) years. Participants were 9.7% (n=194) Asian, 12.1% (n=242) Black, 73.3% (n=1467) White, 1.1% (n=22) Hispanic, and 3.8% (n=77) were of other races and ethnicities. Further, 1913 (95.6%) used search engines to look up health queriesversus 642 (32.6%) for LLMs. Men had higher odds (odds ratio [OR] 1.63, 95% CI 1.34-1.99; P&lt;.001) of using LLMsfor health questionsthan women. Black (OR 1.90, 95% CI 1.42-2.54; P&lt;.001) and Asian (OR 1.66, 95% CI 1.19-2.30; P&lt;.01) individuals had higher odds than White individuals. Those with excellent perceived health (OR 1.46, 95% CI 1.1-1.93; P=.01) were more likely to use LLMs than those with good health. Higher technical proficiency increased the likelihood of LLM use (OR 1.26, 95% CI 1.14-1.39; P&lt;.001). In a follow-up survey of 281 LLM users for health, most participants used search engines first (n=174, 62%) to answer health questions, but the second most common first source consulted was LLMs (n=39, 14%). LLMs were perceived as less useful (P&lt;.01) and less relevant (P=.07), but elicited fewer negative feelings (P&lt;.001), appeared more human (LLM: n=160, vs search: n=32), and were seen as less biased (P&lt;.001). Trust (P=.56) and easeof use (P=.27) showed no differences. Conclusions: Search engines are the primary source of health information; yet, positive perceptions of LLMs suggest growing use. Future work could explore whether LLM trust and usefulness are enhanced by supplementing answers with external references and limiting persuasive language to curb overreliance. Collaboration with health organizations can help improve the quality of LLMs' health output.</abstract><urls><pdf-urls><url>internal-pdf://8096/Mendel et al. - 2025 - Laypeople's Use of and Attitudes Toward Large Lang.pdf</url></pdf-urls></urls></record></records></xml>