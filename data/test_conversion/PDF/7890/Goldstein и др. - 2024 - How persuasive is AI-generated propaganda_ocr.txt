**Page 1**

PNAS NexXuUS PNAS Nexus, 2024, 3, 1-7 https://doi.org/10.1093/pnasnexus/pgae034 Research Report How persuasive is Al-generated propaganda? Josh A. Goldstein (2, Jason Shelby Alex Stamos and Michael Tomz( * Center for Security and Emerging Technology, Georgetown University, Washington, DC 20001, USA Internet Observatory, Stanford University, Stanford, CA 94305, USA Department of Political Science and Stanford Institute for Economic Policy Research, Stanford University, Stanford, CA 94305, USA Edited By: Noshir Contractor Abstract Can large language models, a form of artificial intelligence (AI), generate persuasive propaganda? We conducted a preregistered survey convincing content with limited effort. Keywords: language models, propaganda, survey experiment, artificial intelligence Significance Statement Online covert propaganda campaigns are frequent and ongoing. Recently, policymakers, technologists, and researchers have voiced Introduction Academics, journalists, online platforms, and governments have Received: August 18, 2023. Accepted: January 3, 2024 OXFORD UNIVERSITY PRESS Despite broad concern about the use of language models Competing Interest: The authors declare no competing interest. The Author(s) 2024. Published by Oxford University Press on behalf of National Academy of Sciences. This is an Open Access article SZOz euNnr SQ UO Aq pepeojuMOoG
Advance access publication 20 February 2024 *To whom correspondence should be addressed: Email: shelbygrossman@stanford.edu experiment of US respondents to investigate the persuasiveness of news articles written by foreign propagandists compared to content concern that new artificial intelligence (AI) tools could supercharge covert propaganda campaigns by allowing propagandists to mass demonstrated that online covert propaganda campaigns are fre- for propaganda and other information campaigns, only a distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits
generated by GPT-3 davinci (a large language model). We found that GPT-3 can create highly persuasive text as measured by participants produce text at low cost. Could foreign actors use Al to generate persuasive propaganda targeting audiences in the United States? To quent and ongoing (1, 2). Disclosures of Russian disinformation limited number of studies have used social science methods to unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.
agreement with propaganda theses. We further investigated whether a person fluent in English could improve propaganda investigate this, we conducted a preregistered survey experiment on 8,221 US respondents comparing the persuasiveness of campaigns on social media targeting the United States in 2016 assess the risk. Scholars have examined whether people rate
persuasiveness. Editing the prompt fed to GPT-3 and/or curating GPT-3 s output made GPT-3 even more persuasive, and, under English-language foreign covert propaganda articles sourced from real-world campaigns to text generated by a large language model, heightened awareness of these efforts (3) and caused platforms Al-generated news articles as credible (12) and recognize when
certain conditions, as persuasive as the original propaganda. Our findings suggest that propagandists could use AI to create which is a form of AI. We found that the large language model can create highly persuasive text, and that a person fluent in English to commit more resources to finding and suspending these oper- Al-generated content is false (13), and whether elected officials
could improve the persuasiveness of Al-generated propaganda with minimal effort. ations (4). Yet, covert propaganda operations continue on web- reply to Al-generated constituent letters (14). However, to
sites (5), social media platforms (6), encrypted messaging apps our knowledge, no studies examine the persuasiveness of
(7), and other channels. State-backed covert propaganda cam- Al-generated propaganda compared to an ecologically valid
paigns use short-form content and full-length articles for a range benchmark.
of goals, from self-promotion to undermining confidence in demo-
cratic institutions.

Recently, many have voiced concern that new artificial intelli- We ran an experiment with US respondents comparing the
gence (AI) tools could supercharge covert propaganda campaigns persuasiveness of foreign covert propaganda articles sourced
by allowing propagandists to mass produce text at low cost (8-10). from real-world campaigns to text created by GPT-3 davinci, a
The machine learning community has made major break- large language model developed by OpenAI. We focused on
throughs in language models that can generate original text in re- propaganda articles, rather than snippets such as tweets, since
sponse to a text input (11). These models are quickly diffusing the performance of language models typically declines as text
across society. length increases. We therefore create a relatively hard case
for the technology. Our preregistration plan is available with the
## Open Science Framework.

---
**Page 2**

2 | PNAS Nexus, 2024, Vol. 3, No. 2 Experimental design Article selection and construction We began by identifying six articles (ranging from 151 to 308 Survey deployment In December 2021, we interviewed US adults using Lucid, a survey Table 1. Researcher-written thesis statements for the six articles. Article Thesis statement civilians, rather than terrorists Syria Chemical Syria Medical Oil These sentences summarize the main point we believed the propagandist was respondent with articles on the remaining two topics and meas- Percent Agreement with Thesis No propaganda - 24.4 Original propaganda - _ ATA GPT-3 propaganda - 20 30 40 50 Scaled Agreement with Thesis No propaganda - 478 Original propaganda 228 55.0 GPT-3 propaganda - 45 50 55 60 Fig. 1. Original propaganda and GPT-3-generated propaganda were SZOz euNnr SQ UO Aq pepeojuMOoG
words long) that investigative journalists or researchers uncov- company that uses quota sampling to achieve geographic and shorthand Iran US sanctions against Iran and Russia have helped the trying to convince the target audience. In some cases, this was challenging ured agreement with the thesis statements for those topics. highly persuasive. The top panel shows the percentage of respondents
ered as part of covert, likely state-aligned propaganda campaigns demographic representativeness. Per our preregistration, re- Drones Most US drone strikes in the Middle East have targeted US control businesses and governments in Europe since articles made multiple points. Several of these statements are either false Some of the articles we presented were original propaganda; who agreed or strongly agreed with the thesis statement. The bottom
originating from either Iran or Russia (see SI Appendix 1.A for de- spondents who failed attention checks at the beginning of the sur- To justify its attack on an air base in Syria, the United or debatable. others were propaganda generated by GPT-3. (We presented one panel shows the average level of agreement on a 5-point scale, coded 0 if
tails on article selection). We then used GPT-3 to generate articles vey were not invited to continue, and respondents who completed States created fake reports saying that the Syrian article about a Syria-related topic, and one article about a strongly disagree, 25 if disagree, 50 if neither agree nor disagree, 75 if
on the same six topics. For each topic, we fed GPT-3 one or two the survey in <3 min were excluded, resulting in a final sample of government had used chemical weapons non-Syria-related topic. The articles appear in the second agree, and 100 if strongly agree. Estimates are pooled across topics and
sentences from the original propaganda article that make the ar- 8,221. Western sanctions have led to a shortage of medical SI Appendix, and details about our experimental deployment ap- outputs. SEs clustered by respondent and 95% Cls are shown.
ticle s main point, as well as three other propaganda articles on supplies in Syria pear in SI Appendix 1.C.)
unrelated topics. The three example articles informed the style The United States conducted attacks in Syria to gain
and structure of the GPT-3-generated text, while the excerpts control of an oil-rich region
from the original article informed the topic. We asked GPT-3 to Wall Saudi Arabia committed to help fund the US Mexico
generate three articles on each topic, rather than one, to avoid border wall
over-indexing on any one output since each Al-generated output
is unique. We discarded generations that were <686 characters
or >1,936 characters. These parameters were selected to keep ar-
ticles within 10% of the shortest and longest articles from the ori-
ginal or edited propaganda set. No other criteria were used to
discard GPT-3 output. (We include full information on the article
generation process in SI Appendix 1.B.)

After finding the original propaganda articles and using GPT-3 We asked each respondent how much they agreed or disagreed We then estimated how our treatments affected two measures
to create Al-generated versions, we compared the persuasiveness with the thesis statements for four of the six propaganda topics, of agreement: percent agreement, defined as the percentage of re-
of the two. To measure persuasiveness, we first summarized in selected at random, without reading an article about these topics. spondents who agreed or strongly agreed with each thesis state-
direct, plain English the main point of the original propaganda. This serves as our control data. We then presented each ment, and scaled agreement, defined as the average score on a
The thesis statements, shown in Table 1, are cleaned versions of 5-point scale from 0 ( strongly disagree ) to 100 ( strongly agree ).
the passages we fed to GPT-3 for each topic. Specifically, we regressed each measure of agreement on a com-
prehensive set of indicators for each issue and article, and used
the regression coefficients to compute quantities of interest.
## When averaging across issues, for example, we gave equal weight
to each issue, and when averaging across articles produced by
GPT-3, we gave equal weight to each article. For a complete pres-
entation of the regression models and results, see SI Appendix 2.
## Below, we focus our discussion on percent agreement, but overall
patterns and conclusions were similar when we analyzed scaled
agreement.*

## This study was approved by Stanford University s Institutional
## Review Board which focused on risks to survey respondents, and
also vetted by a cross-professional Al-specific Ethics Review
## Board that considered risks to society. All participants provided
informed consent. To mitigate nsks that respondents might
come to believe falsehoods, we informed respondents after they
completed the survey that the articles came from propaganda
sources and may have contained false information. Regarding
risks to society, propagandists are likely already well aware of
the capabilities of large language models; historically, propagand-
ists have been quick both to adopt new technologies and

---
**Page 3**

incorporate local language speakers into their work. As a result, Results Persuasiveness of GPT-3-generated propaganda To establish a benchmark against which we can evaluate GPT-3, Human-machine teaming In practice, propagandists might not use all of the output from a | 3 title or the body of the article make the claim of the thesis state- GPT-3 performance on additional metrics One potential critique of our study is that our article generation SZOz euNnr SQ UO Aq pepeojuMOoG
the societal benefit of assessing the potential risks outweighs we first assess the effect of reading the original propaganda com- model in a propaganda campaign. Instead, they could engage in ment. (For a description of this process, see SI Appendix 1.D.) process and experimental design might favor GPT-3 on the per-
the possibility that our paper would give propagandists new ideas. pared to not reading any propaganda about that topic (the con- human-machine teaming to increase the efficiency of human Two of the GPT-3 propaganda articles (out of 18 total) did not ad- suasiveness measure. As described above, we first determined
trol). We start by presenting estimates pooled across topics and propagandists while still having a measure of human oversight vance the intended claim. When we removed those articles, and what we thought the main point of each article was. For the
outputs, and later break out topics and outputs individually. As and quality control (15). focused only on outputs that make the thesis, agreement in- GPT-3 output (without curation), we fed a snippet from the origin-
shown in Fig. 1, the original propaganda was highly persuasive. creased to 45.6%, and the difference between original propaganda al propaganda article that makes the main point to GPT-3 in the
While only 24.4% of respondents who were not shown an article and curated GPT-3 propaganda ceased to be statistically signifi- prompt. For the scenario where we edited the example articles
agreed or strongly agreed with the thesis statement, the rate of cant (see Fig. 3). Thus, after discarding a small number of articles fed to GPT-3, we fed the researcher-written thesis statement to
agreement jumped to 47.4% (a 23 percentage point increase) that did not include the thesis statement, GPT-3 was as persuasive GPT-3. If we created GPT-3-generated articles based on an incor-
among respondents who read the original propaganda. Thus, as the original propaganda. rect reading of the main point of the article and used that same in-
the original propaganda nearly doubled the share of participants correct reading for our persuasiveness measure, then our process
who concurred with the thesis statement. would favor GPT-3-generated articles compared to the original
propaganda. In turn, this might overstate the power of GPT-3 in
propaganda campaigns.

GPT-3-generated propaganda was also highly persuasive, and After running the model, a human could serve as a curator by Another strategy for human involvement would be to edit the To address this concern, we compared GPT-3 with the onginal
43.5% of respondents who read a GPT-3-generated article agreed weeding out articles that do not make the point the propagandist prompt given to GPT-3. The original propaganda included typos propaganda on two additional dimensions: perceived credibility
or strongly agreed with the thesis statement, compared to 24.4% seeks to get across. To simulate this scenario, a human read and grammatical errors, perhaps indicative of an author whose and writing style. We measured credibility by asking respondents
in the control (a 19.1 percentage point increase). This suggests through each GPT-3 output carefully to see whether either the native language was not English. To simulate what would happen whether they thought the article was trustworthy, and whether
that propagandists could use GPT-3 to generate persuasive ar- if a fluent English speaker wrote the prompts for GPT-3, we made they thought the article was written to report the facts (vs. to con-
ticles with minimal human effort, by using existing articles on un- two changes: (i) we provided GPT-3 with the researcher-wnitten vince the reader of a viewpoint). For a proxy for writing style, we
related topics to guide GPT-3 about the style and length of new thesis statement, rather than an excerpt from the original article asked respondents whether they thought the article was well writ-
articles. While GPT-3-generated propaganda was highly persua- and (ii) we edited the example articles on unrelated topics, with ten and whether they thought the author s first language was
Sive, it was slightly less compelling than the original propaganda the expectation that better-written examples would lead to better English. On all these measures, GPT-3 performed as well, if not
(a 3.9% point difference). Figures S7 and S8 show that the persua- output. As Fig. 3 shows, articles generated by GPT-3 with an edited better, than the original propaganda (see Fig. S13). Our findings
sive effects of the original propaganda and GPT-3 propaganda prompt were as persuasive as the orginal propaganda: the differ- suggest that GPT-3-generated content could blend into online in-
were fairly consistent across social groups. We did not find sub- ence between 46.4 and 47.4% was small and not statistically formation environments on par with content we sourced from ex-
stantial heterogeneity in treatment effects when we split the sam- significant. isting foreign covert propaganda campaigns. While this may not
ple by demographic variables, partisanship/ideology, news be a very high bar (only 38.7% of respondents found the original
consumption, time spent on social media, and more. This sug- propaganda to be trustworthy, and only 52.4% thought the
gests that Al-generated propaganda could be compelling to a re-
markably wide range of groups in society.

In Fig. 2, we break out the results by article topic and show each Doing both editing the prompts and curating the output
of the three GPT-3-generated outputs. While baseline agreement would be even better. If a propagandist edited the input and
in the control group varied by topic, almost all GPT-3 outputs selected the best of the three outputs on each topic, the
were highly persuasive. For most issues, each GPT-3-generated GPT-3-generated propaganda would be even more persuasive
article was about as persuasive as the original propaganda. than the original propaganda (52.7% compared to 47.4%). In prac-
However, this was not always the case. For example, Syria Oil out- tice, propagandists might perform curation themselves or crowd-
put 3 and Wall outputs 2 and 3 performed significantly worse than source curation for selecting the best articles from a set of
the original propaganda on both percent agreement and scaled outputs.
## The poor performance of these articles and a few
others caused GPT-3 to perform slightly less well than the original
propaganda on average, when, in Fig. 1, we had computed an aver-
age that gave equal weight to all GPT-3-generated outputs from all
six issues. This suggests a potential role for human propagandists,
who could review the output of GPT-3 and select the high-quality
articles that make the propagandist s point.

---
**Page 4**

4 | PNAS Nexus, 2024, Vol. 3, No. 2 Percent Agreement with Thesis Drones Output 3 - lran 10 20 30 40 50 60 470 Syria Chemical No propaganda - 21.2 10 20 30 40 50 60 470 Syria Medical 10 20 30 40 50 60 470 Syria Oil 10 20 30 40 50 60 470 Wall 20.3 Output 3 - 10 20 30 40 50 60 470 Fig. 2. Most GPT-3-generated output were as persuasive as the original propaganda, but a few articles performed worse. Average agreement with the scaled Agreement with Thesis No propaganda - GPT-3 propaganda - No propaganda - GPT-3 propaganda - No propaganda - GPT-3 propaganda - No propaganda - GPT-3 propaganda - No propaganda - GPT-3 propaganda - No propaganda - GPT-3 propaganda - Drones = 1.8 52.6 49.0 20 30 40 50 60 80 lran 20 30 40 50 60 80 45.3 Syria Medical 56.7 66.7 20 30 40 50 60 80 Syria Oil 20 30 40 50 60 80 Wall 20 30 40 50 60 80 thesis statement for each treatment group for each topic. SEs clustered by respondent and 95% are shown. original propaganda was well written), language models are Conclusion Our experiment showed that language models can generate text SZOz euNnr SQ UO Aq pepeojuMOoG
No propaganda - 22.2 10 20 30 50 60 70 No propaganda - 25.9 Original propaganda - 40.5 No propaganda - No propaganda - No propaganda - Original propaganda Output 1 - Original propaganda - Output 1 - Original propaganda Output 1 - Original propaganda - Output 1 - Original propaganda - Output 1 - Original propaganda Output 1 - AV 53.8 49.6 49.8 57.8 40.5 quickly improving. If a similar study were run with more powerful that is nearly as persuasive for US audiences as content we
Original propaganda - 39.1 Original propaganda - 44.3 GPT-3 propaganda - 37.9 tas Original propaganda - __63.1 Original propaganda - 48.3 Original propaganda - 193 Output 2 - Output 2 - Output 2 - Output 2 - Output 2 - Output 2 - 69.6 _58.8 58.5 __58.6 models in the future, Al-generated propaganda would likely per-
GPT-3 propaganda - 40 GPT-3 propaganda - Output 1 - GPT-3 propaganda - 602 GPT-3 42.8 . GPT-3 propaganda - 36.5 50.7 Output 3 - Output 3 - Output 3 - Output 3 - Output 3 - Output 3 - 63.8 46.2 46.2 form even better.
## Output 1 - Output 1 - ae Output 2 - oe Output 1 - at Output 1 - Ar Output 1 - 3861 58.2
## Output 2 - Output 2 - Output 3 - Output 2 - Output 2 - op Output 2 - 50.3
38.2_ Output 3 - Output 3 - Output 3 - 30.2

---
**Page 5**

Percent Agreement with Thesis Original propaganda - 43.5 GPT-3 with edited prompt 46.4 40 45 50 55 60 Scaled Agreement with Thesis Original propaganda - 57.8 GPT-3 propaganda 55.0 GPT-3 with edited prompt Best performing outputs - 50 55 60 65 Fig. 3. Human curation made GPT-3 as persuasive as the original sourced from real-world foreign covert propaganda campaigns. | 5 test the effects of Al-generated propaganda across a wider range Notes We preregistered and implemented additional treatment arms us- b SZOz euNnr SQ UO Aq pepeojuMOoG
GPT-3 propaganda All outputs (no curation) - All outputs (no curation) - 57.0 All outputs (no curation) - propaganda. Average agreement with the thesis statement for each Moreover, human-machine teaming strategies (editing prompts of issues, to assess how the effects vary by the salience of the topic ing GPT-3 fine-tuned on articles from the Washington Post s
49.0 Outputs that make thesis - Outputs that make thesis - 59.4 Outputs that make thesis - treatment group, averaged over topics. All outputs (no curation) and curating outputs) produced articles that were as or more per- and the respondent s prior knowledge. Research could also ad- Politics section. (See preregistration plan for additional details.)
Best performing outputs - Best performing outputs - includes all GPT-3 propaganda articles. Outputs that make thesis suasive than the original propaganda. Our results go beyond earl- dress how much respondents are persuaded by Al-generated We did this to emulate a propagandist who wanted to generate
excludes GPT-3 outputs that did not make the claim of the thesis in the ier efforts by evaluating the persuasiveness of Al-generated text propaganda when they receive information from multiple sources text in the structure and style of the Washington Post.
title or body of the article. Best performing outputs is the average directly (rather than focusing on metrics like credibility) and using on a topic. Fine-tuning GPT-3 on these articles, however, adversely affected
agreement with the thesis statement for the best performing GPT-3 an ecologically valid benchmark. the substance of the Al-generated articles: only 36% of the outputs
output for each of the six topics. SEs and 95% CIs are shown. from the fine-tuned model made the thesis statement, perhaps be-
cause the fine-tuning process caused the model to prioritize the
content of the Washington Post articles. In Section 3 of the SI
Appendix, we include a detailed analysis of the effect of fine-tuning
with the Washington Post. In practice, a propagandist fine-tuning a
model to create persuasive propaganda would likely read the out-
put to see if it made the intended point. If the process failed in

All outputs (no curation) - For two reasons, our estimates may represent a lower bound on Another line of research could probe strategies to guard against this regard, they would likely adjust the fine-tuning process or
the relative persuasive potential of large language models. First, the potential misuse of language models for propaganda cam-
large language models are rapidly improving. Since our study paigns (25). If generative AI tools can scale propaganda gener-
was conducted, several companies have released larger models ation, research that improves the detection of infrastructure
(e.g. OpenAl s GPT-4) that outperform GPT-3 davinci in related needed to deliver content to a target (such as inauthentic social
tasks (16). We expect that these improved models, and others in media accounts) will become more important. These detection
the pipeline, would produce propaganda at least as persuasive methods are agnostic as to whether the content is Al-generated
as the text we administered. or human-written. Research into which systems are susceptible
to being overrun by Al-generated text (14) and how to defend
against these attacks could also mitigate the impact of
## Al-generated propaganda campaigns on democratic processes.

Outputs that make thesis - Second, our experiment estimated the effect of reading a single Future research could also focus on behavioral interventions to abandon it altogether. We believe that the failure of the one fine-
article, but propagandists could use Al to expose citizens to many reduce the likelihood that users believe misleading Al-generated tuning approach we tried (see preregistration plan for implementa-
articles. With AI, actors including ones without fluency in the content. There is work on the conditions under which people tion details) does not speak to the broader utility of fine-tuning.
target language could quickly and cheaply generate many ar- can assess whether content is Al-generated (26), and work on
ticles that convey a single narrative, while also varying in style how people understand labels that could be applied to misleading
and wording. This approach would increase the volume of propa- or Al-generated content (27). Research could build on these stud-
ganda, while also making it harder to detect, since articles that ies by exploring the effect of labeling Al-generated content on
vary in style and wording may look more like the views of real peo- both engagement with the content and whether people believe
ple or genuine news sources. Finally, Alcan save time and money, the content is Al-generated.
enable propagandists to redirect resources from creating content
to building infrastructure (e.g. fake accounts, news websites that
mask state links) that look credible and evade detection.

Best performing outputs - Our research tested the effects of propaganda about several is- Future research could explore the conditions under which fine-
sues, including drones, Iran, the US Mexico border wall, and con-
flict in Syria. Using our experimental design, future research could

tuning large language models is useful.

We made a minor error in constructing the GPT-3 prompt for

the wall topic, when we intended to select an excerpt from the
original wall article that makes the main point of the article.
SI Appendix 2.A.2 explains this error, and Figs. S9 and S10 show
results if this topic is excluded.

## In addition, Synia Medical output 3 performed significantly worse
than the original propaganda on scaled agreement, but not percent
agreement.

Research beginning with Hasher et al. has shown that people are
more likely to believe information when they are exposed to it mul-
tiple times (the illusory truth effect ) (17). Recent experiments
have shown that people were more likely to believe false headlines
sourced from Facebook around the 2016 US Presidential Election if

---
**Page 6**

6 | PNAS Nexus, 2024, Vol. 3, No. 2 exposed multiple times (18), that repeated exposure to content on a Acknowledgments We thank Siddharth Karamcheti, Percy Liang, Chris Manning, Supplementary Material Supplementary material is available at PNAS Nexus online. Funding This research was funded by Stanford University s Institute for Author Contributions J.A.G. designed research, performed research, analyzed data, Preprints An earlier version of this manuscript was posted on a preprint Data Availability Data and replication code are available on the Harvard Dataverse: References 1 Goldstein JA, Grossman S. 2021 Jan. How disinformation evolved 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Francois C, Douek E. 2021. The accidental origins, underappreci- SZOz euNnr SQ UO Aq pepeojuMOoG
social media platform leads to a higher rate of sharing that content Sara Plana, Girish Sastry, and participants at the International Human-Centered AI (grant #216301) and Stanford University s wrote the paper. J.C. designed research, performed research. server: https://osf.io/preprints/socarxiv/fp87b. https://doi.org/10.7910/DVN/LAZ7AA. in 2020. Brookings Tech Stream [accessed 2022 Dec 6]. https:// ated limits, and enduring promises of platform transparency re-
(19, 20), and that belief in false information continues to grow loga- Studies Association Annual Conference and seminars at Center for Research on Foundation Models. S.G. designed research, performed research, analyzed data, wrote www.brookings.edu/techstream/how-disinformation-evolved- porting about information operations. J Online Trust Saf. 1(1):1-30.
rithmically with additional exposures (21). Repeated exposure has Georgetown University, the Naval War College, and Stanford the paper. A.S. designed research. M.T. designed research, per- in-2020/. Stubbs J. 2020. Iranian disinformation effort went small to
also been linked to real-world harms: Bursztyn et al. found that University for feedback. We thank OpenAl for providing access formed research, analyzed data, wrote the paper. stay under big tech s radar. Reuters [accessed 2022 Dec 6].
areas in the United States with greater exposure to a media outlet to GPT-3 via their academic access program. https://www.reuters.com/article/us-usa-election-facebook-russia/
downplaying the threat of COVID-19 experienced a greater number duped-by-russia-freelancers-ensnared-in-disinformation-campaign-
of cases and deaths (22). Similarly, research in psychology has by-promise-of-easy-money-idUSKBN25T35E.
shown that people are more lhkely to believe misinformation
when it comes from multiple sources, rather than one (the mul-
tiple source effect ) (23) and more likely to adopt a position when
itis supported by a greater number of arguments (24).

2 Martin DA, Shapiro JN, Uhardt JG. 2022. Introducing the online Nimmo B, Agranovich D. 2022. Removing coordinated inauthen-
political influence efforts dataset. J Peace Res. 60(5):868-876. tic behavior from China and Russia [accessed 2022 Dec 6]. https://
about.fb.com/news/2022/09/removing-coordinated-inauthentic-
behavior-from-china-and-russia/.

3 Jamieson KH. 2020. Cyberwar: how Russian hackers and trolls helped Frenkel S. 2021. Iranian disinformation effort went small to
elect a president: what we don t, can t, and do know. New York: Oxford stay under big tech s radar. New York Times [accessed
## University Press. 2022 Dec 6].
disinformation-message-apps.html.

## Weidinger L. 2022. Taxonomy of posed by language models.
## In: Proceedings of the 2022 ACM Conference on Fairness,
## Accountability, and Transparency (FAccT 22) (Association for
## Computing Machinery, New York). p. 214-229.

Helmore E. 2023 Mar. We are a little bit scared : OpenAI CEO
warns of risks of artificial intelligence. The Guardian [accessed
2023 Aug 18]. https://www.theguardian.com/technology/2023/
mar/17/openai-sam-altman-artificial-intelligence-waming-gpt4.
## Nature. 2023 Jun. Stop talking about AI doomsday
when AI poses risks today [accessed 2023 Aug 18]. https://www.
nature.com/articles/d41586-023-02094-7,

Bommasani R, et al. 2021. On the opportunities and risks of foun-
dation models, arXiv, arXiv:2108.07258, preprint: not peer
reviewed.

Kreps S, McCain RM, Brundage M. 2020. All the news that s fit to
fabricate: Al-generated text as a tool of media misinformation. J
## Exp Polit Sci. 9:104-117.

## Spitale G, Biller-Andorno N, Germani F. 2023. Al model GPT-3
(dis)informs us better than humans. Sci Adv. 9(26):eadh185.
## Kreps S, Kriner DL. 2023. The Potential Impact of Emerging
## Technologies on Democratic Representation: Evidence from a
## Field Experiment. New Media & Society. 14614448231160526.
Buchanan B, Lohn A, Musser M, Sedova K. 2021. Truth, lies, and
automation: how language models could change disinformation.
## Center for Security and Emerging Technology [accessed 2022
## Dec 6]. https://dol.org/10.51593/2021CA003.

## Liang P, et al. 2023. Holistic evaluation of language models.
## Transact Mach Learn Res. 2835-8856.

Hasher L, Goldstein D, Toppino T. 1977. Frequency and the con-
ference of referential validity. J Verbal Learning Verbal Behav. 16(1):
107-112.

## Pennycook G, Cannon TD, Rand DG. 2018. Prior exposure in-
creases perceived accuracy of fake news. J Exp Psychol Gen.
147(12):1865.

Zhou CG, Zhao Q, Lu W. 2015. Impact of repeated exposures on in-
formation spreading in social networks. PLoS One. 10(10):
0140556.

Vellani V, Zheng S, Ercelik D, Sharot T. 2023. The illusory truth
effect leads to the spread of misinformation. Cognition. 236:
105421.

Hassan A, Barber SJ. 2021. The effects of repetition frequency on
the illusory truth effect. Cogn Res Princ Implic. 6(1):1-12.
## Bursztyn L, Rao A, Roth CP, Yanagizawa-Drott DH. 2020.
## Misinformation during a pandemic. Technical Report, National
## Bureau of Economic Research.

## Harkins SG, Petty RE. 1987. Information utility and the multiple
source effect. J Pers Soc Psychol. 52(2):260.

---
**Page 7**

Goldstein etal. | 7 24 Calder BJ, Insko CA, Yandell B. 1974. The relation of cognitive and 25 memorial processes to persuasion in a simulated jury Appl 26 Groh M, et al. 2023. Human detection of political speech deepfakes SZOz euNnr SQ UO Aq pepeojuMOoG
Soc Psychol. 4(1):62-93. across transcripts, audio, and video, arXiv, arXiv:2202.12883, pre-
print: not peer reviewed.

Goldstein JA, et al. 2023. Generative language models and auto- 27 Epstein Z, Fang CM, Arechar AA, Rand DG. 2023. What label
mated influence operations: emerging threats and potential mit- should be applied to content produced by generative AI?
igations, arXiv, arXiv:2301.04246, preprint: not peer reviewed. PsyArXiv, preprint: not peer reviewed.